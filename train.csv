label,title,text,titletext
0,The caudate nucleus undergoes dramatic and unique transcriptional changes in human prodromal Huntington's disease brain,"Background: The mechanisms underlying neurodegeneration in the striatum of Huntingon's Disease (HD) brain are currently unknown. While the striatum is massively degenerated in symptomatic individuals, which makes cellular characterization difficult, it is largely intact in asymptomatic HD gene positive (HD+) individuals. Unfortunately, as striatal tissue samples from HD+ individuals are exceedingly rare, recent focus has been on the Brodmann Area 9 (BA9), a relatively unaffected region, as a surrogate tissue. In this study, we analyze gene expression in caudate nucleus (CAU) from two HD+ individuals and compare the results with healthy and symptomatic HD brains. Methods: High-throughput mRNA sequencing (mRNA-Seq) datasets were generated from post-mortem CAU of 2 asymptomatic HD+ individuals and compared with 26 HD and 56 neurologically normal controls. Datasets were analyzed using a custom bioinformatic analysis pipeline to identify and interpret differentially expressed (DE) genes. Results were compared to publicly available brain mRNA-Seq datasets from the Genotype-Tissue Expression (GTEx) project. The analysis employed current state of the art bioinformatics tools and tailored statistical and machine learning methods. Results: The transcriptional profiles in HD+ CAU and HD BA9 samples are highly similar. Differentially expressed (DE) genes related to the heat shock response, particularly HSPA6 and HSPA1A, are common between regions. The most perturbed pathways show extensive agreement when comparing disease with control. A random forest classifier predicts that the two HD+ CAU samples strongly resemble HD BA9 and not control BA9. Nonetheless, when genes were prioritized by their specificity to HD+ CAU, pathways spanning many biological processes emerge. Comparison of HD+ BA9 with HD BA9 identified NPAS4 and REST1/2 as potential early responders to disease and reflect the active disease process. Conclusions: The caudate nucleus in HD brain is dramatically affected prior to symptom onset. Gene expression patterns observed in the HD BA9 are also present in the CAU, suggesting a common response to disease. Substantial caudate-specific differences implicate many different biological pathways including metabolism, protein folding, inflammation, and neurogenic processes. While these results are at best trends due to small sample sizes, these results nonetheless provide the most detailed insight to date into the primary HD disease process.","The caudate nucleus undergoes dramatic and unique transcriptional changes in human prodromal Huntington's disease brain. Background: The mechanisms underlying neurodegeneration in the striatum of Huntingon's Disease (HD) brain are currently unknown. While the striatum is massively degenerated in symptomatic individuals, which makes cellular characterization difficult, it is largely intact in asymptomatic HD gene positive (HD+) individuals. Unfortunately, as striatal tissue samples from HD+ individuals are exceedingly rare, recent focus has been on the Brodmann Area 9 (BA9), a relatively unaffected region, as a surrogate tissue. In this study, we analyze gene expression in caudate nucleus (CAU) from two HD+ individuals and compare the results with healthy and symptomatic HD brains. Methods: High-throughput mRNA sequencing (mRNA-Seq) datasets were generated from post-mortem CAU of 2 asymptomatic HD+ individuals and compared with 26 HD and 56 neurologically normal controls. Datasets were analyzed using a custom bioinformatic analysis pipeline to identify and interpret differentially expressed (DE) genes. Results were compared to publicly available brain mRNA-Seq datasets from the Genotype-Tissue Expression (GTEx) project. The analysis employed current state of the art bioinformatics tools and tailored statistical and machine learning methods. Results: The transcriptional profiles in HD+ CAU and HD BA9 samples are highly similar. Differentially expressed (DE) genes related to the heat shock response, particularly HSPA6 and HSPA1A, are common between regions. The most perturbed pathways show extensive agreement when comparing disease with control. A random forest classifier predicts that the two HD+ CAU samples strongly resemble HD BA9 and not control BA9. Nonetheless, when genes were prioritized by their specificity to HD+ CAU, pathways spanning many biological processes emerge. Comparison of HD+ BA9 with HD BA9 identified NPAS4 and REST1/2 as potential early responders to disease and reflect the active disease process. Conclusions: The caudate nucleus in HD brain is dramatically affected prior to symptom onset. Gene expression patterns observed in the HD BA9 are also present in the CAU, suggesting a common response to disease. Substantial caudate-specific differences implicate many different biological pathways including metabolism, protein folding, inflammation, and neurogenic processes. While these results are at best trends due to small sample sizes, these results nonetheless provide the most detailed insight to date into the primary HD disease process."
0,Calculated risk,,
0,Machine-learning-assisted selection of antibiotic prescription,,
0,REST and Neural Gene Network Dysregulation in iPSC Models of Alzheimer's Disease,"Meyer et al. derive neural progenitors, neurons, and cerebral organoids from sporadic Alzheimer's disease (SAD) and APOE4 gene-edited iPSCs. SAD and APOE4 expression alter the neural transcriptome and differentiation in part through loss of function of the transcriptional repressor REST. Thus, neural gene network dysregulation may lead to Alzheimer's disease.","REST and Neural Gene Network Dysregulation in iPSC Models of Alzheimer's Disease. Meyer et al. derive neural progenitors, neurons, and cerebral organoids from sporadic Alzheimer's disease (SAD) and APOE4 gene-edited iPSCs. SAD and APOE4 expression alter the neural transcriptome and differentiation in part through loss of function of the transcriptional repressor REST. Thus, neural gene network dysregulation may lead to Alzheimer's disease."
0,A Phage Protein Aids Bacterial Symbionts in Eukaryote Immune Evasion,"Phages are increasingly recognized as important members of host-associated microbiomes, with a vast genomic diversity. The new frontier is to understand how phages may affect higher order processes, such as in the context of host-microbe interactions. Here, we use marine sponges as a model to investigate the interplay between phages, bacterial symbionts, and eukaryotic hosts. Using viral metagenomics, we find that sponges, although massively filtering seawater, harbor species-specific and even individually unique viral signatures that are taxonomically distinct from other environments. We further discover a symbiont phage-encoded ankyrin-domain-containing protein, which is widely spread in phages of many host-associated contexts including human. We confirm in macrophage infection assays that the ankyrin protein (ANKp) modulates the eukaryotic host immune response against bacteria. We predict that the role of ANKp in nature is to facilitate coexistence in the tripartite interplay between phages, symbionts, and sponges and possibly many other host-microbe associations.","A Phage Protein Aids Bacterial Symbionts in Eukaryote Immune Evasion. Phages are increasingly recognized as important members of host-associated microbiomes, with a vast genomic diversity. The new frontier is to understand how phages may affect higher order processes, such as in the context of host-microbe interactions. Here, we use marine sponges as a model to investigate the interplay between phages, bacterial symbionts, and eukaryotic hosts. Using viral metagenomics, we find that sponges, although massively filtering seawater, harbor species-specific and even individually unique viral signatures that are taxonomically distinct from other environments. We further discover a symbiont phage-encoded ankyrin-domain-containing protein, which is widely spread in phages of many host-associated contexts including human. We confirm in macrophage infection assays that the ankyrin protein (ANKp) modulates the eukaryotic host immune response against bacteria. We predict that the role of ANKp in nature is to facilitate coexistence in the tripartite interplay between phages, symbionts, and sponges and possibly many other host-microbe associations."
0,Covalent Docking Identifies a Potent and Selective MKK7 Inhibitor,"The c-Jun NH2-terminal kinase (JNK) signaling pathway is central to the cell response to stress, inflammatory signals, and toxins. While selective inhibitors are known for JNKs and for various upstream MAP3Ks, no selective inhibitor is reported for MKK7â€“â€“one of two direct MAP2Ks that activate JNK. Here, using covalent virtual screening, we identify selective MKK7 covalent inhibitors. We optimized these compounds to low-micromolar inhibitors of JNK phosphorylation in cells. The crystal structure of a lead compound bound to MKK7 demonstrated that the binding mode was correctly predicted by docking. We asserted the selectivity of our inhibitors on a proteomic level and against a panel of 76 kinases, and validated an on-target effect using knockout cell lines. Lastly, we show that the inhibitors block activation of primary mouse B cells by lipopolysaccharide. These MKK7 tool compounds will enable better investigation of JNK signaling and may serve as starting points for therapeutics.","Covalent Docking Identifies a Potent and Selective MKK7 Inhibitor. The c-Jun NH2-terminal kinase (JNK) signaling pathway is central to the cell response to stress, inflammatory signals, and toxins. While selective inhibitors are known for JNKs and for various upstream MAP3Ks, no selective inhibitor is reported for MKK7â€“â€“one of two direct MAP2Ks that activate JNK. Here, using covalent virtual screening, we identify selective MKK7 covalent inhibitors. We optimized these compounds to low-micromolar inhibitors of JNK phosphorylation in cells. The crystal structure of a lead compound bound to MKK7 demonstrated that the binding mode was correctly predicted by docking. We asserted the selectivity of our inhibitors on a proteomic level and against a panel of 76 kinases, and validated an on-target effect using knockout cell lines. Lastly, we show that the inhibitors block activation of primary mouse B cells by lipopolysaccharide. These MKK7 tool compounds will enable better investigation of JNK signaling and may serve as starting points for therapeutics."
0,RCorp: a resource for chemical disease semantic extraction in Chinese,"BACKGROUND: To robustly identify synergistic combinations of drugs, high-throughput screenings are desirable. It will be of great help to automatically identify the relations in the published papers with machine learning based tools. To support the chemical disease semantic relation extraction especially for chronic diseases, a chronic disease specific corpus for combination therapy discovery in Chinese (RCorp) is manually annotated. METHODS: In this study, we extracted abstracts from a Chinese medical literature server and followed the annotation framework of the BioCreative CDR corpus, with the guidelines modified to make the combination therapy related relations available. An annotation tool was incorporated to the standard annotation process. RESULTS: The resulting RCorp consists of 339 Chinese biomedical articles with 2367 annotated chemicals, 2113 diseases, 237 symptoms, 164 chemical-induce-disease relations, 163 chemical-induce-symptom relations, and 805 chemical-treat-disease relations. Each annotation includes both the mention text spans and normalized concept identifiers. The corpus gets an inter-annotator agreement score of 0.883 for chemical entities, 0.791 for disease entities which are measured by F score. And the F score for chemical-treat-disease relations gets 0.788 after unifying the entity mentions. CONCLUSIONS: We extracted and manually annotated a chronic disease specific corpus for combination therapy discovery in Chinese. The result analysis of the corpus proves its quality for the combination therapy related knowledge discovery task. Our annotated corpus would be a useful resource for the modelling of entity recognition and relation extraction tools. In the future, an evaluation based on the corpus will be held.","RCorp: a resource for chemical disease semantic extraction in Chinese. BACKGROUND: To robustly identify synergistic combinations of drugs, high-throughput screenings are desirable. It will be of great help to automatically identify the relations in the published papers with machine learning based tools. To support the chemical disease semantic relation extraction especially for chronic diseases, a chronic disease specific corpus for combination therapy discovery in Chinese (RCorp) is manually annotated. METHODS: In this study, we extracted abstracts from a Chinese medical literature server and followed the annotation framework of the BioCreative CDR corpus, with the guidelines modified to make the combination therapy related relations available. An annotation tool was incorporated to the standard annotation process. RESULTS: The resulting RCorp consists of 339 Chinese biomedical articles with 2367 annotated chemicals, 2113 diseases, 237 symptoms, 164 chemical-induce-disease relations, 163 chemical-induce-symptom relations, and 805 chemical-treat-disease relations. Each annotation includes both the mention text spans and normalized concept identifiers. The corpus gets an inter-annotator agreement score of 0.883 for chemical entities, 0.791 for disease entities which are measured by F score. And the F score for chemical-treat-disease relations gets 0.788 after unifying the entity mentions. CONCLUSIONS: We extracted and manually annotated a chronic disease specific corpus for combination therapy discovery in Chinese. The result analysis of the corpus proves its quality for the combination therapy related knowledge discovery task. Our annotated corpus would be a useful resource for the modelling of entity recognition and relation extraction tools. In the future, an evaluation based on the corpus will be held."
0,Detecting Different Cell Populations Using Multispectral F-19 MRI,,
0,Using Machine Learning to Monitor Keratoconus Progression,,
0,Sequencing-based methods and resources to study antimicrobial resistance,"Antimicrobial resistance extracts high morbidity, mortality and economic costs yearly by rendering bacteria immune to antibiotics. Identifying and understanding antimicrobial resistance are imperative for clinical practice to treat resistant infections and for public health efforts to limit the spread of resistance. Technologies such as next-generation sequencing are expanding our abilities to detect and study antimicrobial resistance. This Review provides a detailed overview of antimicrobial resistance identification and characterization methods, from traditional antimicrobial susceptibility testing to recent deep-learning methods. We focus on sequencing-based resistance discovery and discuss tools and databases used in antimicrobial resistance studies.","Sequencing-based methods and resources to study antimicrobial resistance. Antimicrobial resistance extracts high morbidity, mortality and economic costs yearly by rendering bacteria immune to antibiotics. Identifying and understanding antimicrobial resistance are imperative for clinical practice to treat resistant infections and for public health efforts to limit the spread of resistance. Technologies such as next-generation sequencing are expanding our abilities to detect and study antimicrobial resistance. This Review provides a detailed overview of antimicrobial resistance identification and characterization methods, from traditional antimicrobial susceptibility testing to recent deep-learning methods. We focus on sequencing-based resistance discovery and discuss tools and databases used in antimicrobial resistance studies."
0,"gamma delta TCR ligands: the quest to solve a 500-million-year-old mystery (vol 20, pg 121, 2019)",,
0,"Single-cell, high-throughput analysis of cell docking to vessel wall","Therapeutic potential of mesenchymal stem cells (MSCs) has been reported consistently in animal models of stroke, with mechanism mainly through immunomodulation and paracrine activity. Intravenous injection has been a prevailing route for MSCs administration, but cell quantities needed when scaling-up from mouse to human are extremely high putting into question feasibility of that approach. Intra-arterial delivery directly routes the cells to the brain thus lowering the required dose. Cell engineering may additionally improve cell homing, further potentiating the value of intra-arterial route. Therefore, our goal was to create microfluidic platform for screening and fast selection of molecules that enhance the docking of stem cells to vessel wall. We hypothesized that our software will be capable of detecting distinct docking properties of naÃ¯ve and ITGA4-engineered MSCs. Indeed, the cell flow tracker analysis revealed positive effect of cell engineering on docking frequency of MSCs (42% vs. 9%, engineered vs. control cells, p < 0.001). These observations were then confirmed in an animal model of focal brain injury where cell engineering resulted in improved homing to the brain. To conclude, we developed a platform to study the docking of cells to the vessel wall which is highly relevant for intraarterial cell targeting or studies on neuroinflammation.","Single-cell, high-throughput analysis of cell docking to vessel wall. Therapeutic potential of mesenchymal stem cells (MSCs) has been reported consistently in animal models of stroke, with mechanism mainly through immunomodulation and paracrine activity. Intravenous injection has been a prevailing route for MSCs administration, but cell quantities needed when scaling-up from mouse to human are extremely high putting into question feasibility of that approach. Intra-arterial delivery directly routes the cells to the brain thus lowering the required dose. Cell engineering may additionally improve cell homing, further potentiating the value of intra-arterial route. Therefore, our goal was to create microfluidic platform for screening and fast selection of molecules that enhance the docking of stem cells to vessel wall. We hypothesized that our software will be capable of detecting distinct docking properties of naÃ¯ve and ITGA4-engineered MSCs. Indeed, the cell flow tracker analysis revealed positive effect of cell engineering on docking frequency of MSCs (42% vs. 9%, engineered vs. control cells, p < 0.001). These observations were then confirmed in an animal model of focal brain injury where cell engineering resulted in improved homing to the brain. To conclude, we developed a platform to study the docking of cells to the vessel wall which is highly relevant for intraarterial cell targeting or studies on neuroinflammation."
0,Automated Triaging of Adult Chest Radiographs with Deep Artificial Neural Networks,,
0,"E-cigarette, or vaping, product use associated lung injury (EVALI): case series and diagnostic approach",,
0,Impact of a decreasing pre-test probability on the performance of diagnostic tests for coronary artery disease,"Aims: To provide a pooled estimation of contemporary pre-test probabilities (PTPs) of significant coronary artery disease (CAD) across clinical patient categories, re-evaluate the utility of the application of diagnostic techniques according to such estimates, and propose a comprehensive diagnostic technique selection tool for suspected CAD. Methods and results: Estimates of significant CAD prevalence across sex, age, and type of chest pain categories from three large-scale studies were pooled (n = 15 815). The updated PTPs and diagnostic performance profiles of exercise electrocardiogram, invasive coronary angiography, coronary computed tomography angiography (CCTA), positron emission tomography (PET), stress cardiac magnetic resonance (CMR), and SPECT were integrated to define the PTP ranges in which ruling-out CAD is possible with a post-test probability of <10% and <5%. These ranges were then integrated in a new colour-coded tabular diagnostic technique selection tool. The Bayesian relationship between PTP and the rate of diagnostic false positives was explored to complement the characterization of their utility. Pooled CAD prevalence was 14.9% (range = 1-52), clearly lower than that used in current clinical guidelines. Ruling-out capabilities of non-invasive imaging were good overall. The greatest ruling-out capacity (i.e. post-test probability <5%) was documented by CCTA, PET, and stress CMR. With decreasing PTP, the fraction of false positive findings rapidly increased, although a lower CAD prevalence partially cancels out such effect. Conclusion: The contemporary PTP of significant CAD across symptomatic patient categories is substantially lower than currently assumed. With a low prevalence of the disease, non-invasive testing can rarely rule-in the disease and focus should shift to ruling-out obstructive CAD. The large proportion of false positive findings must be taken into account when patients with low PTP are investigated.","Impact of a decreasing pre-test probability on the performance of diagnostic tests for coronary artery disease. Aims: To provide a pooled estimation of contemporary pre-test probabilities (PTPs) of significant coronary artery disease (CAD) across clinical patient categories, re-evaluate the utility of the application of diagnostic techniques according to such estimates, and propose a comprehensive diagnostic technique selection tool for suspected CAD. Methods and results: Estimates of significant CAD prevalence across sex, age, and type of chest pain categories from three large-scale studies were pooled (n = 15 815). The updated PTPs and diagnostic performance profiles of exercise electrocardiogram, invasive coronary angiography, coronary computed tomography angiography (CCTA), positron emission tomography (PET), stress cardiac magnetic resonance (CMR), and SPECT were integrated to define the PTP ranges in which ruling-out CAD is possible with a post-test probability of <10% and <5%. These ranges were then integrated in a new colour-coded tabular diagnostic technique selection tool. The Bayesian relationship between PTP and the rate of diagnostic false positives was explored to complement the characterization of their utility. Pooled CAD prevalence was 14.9% (range = 1-52), clearly lower than that used in current clinical guidelines. Ruling-out capabilities of non-invasive imaging were good overall. The greatest ruling-out capacity (i.e. post-test probability <5%) was documented by CCTA, PET, and stress CMR. With decreasing PTP, the fraction of false positive findings rapidly increased, although a lower CAD prevalence partially cancels out such effect. Conclusion: The contemporary PTP of significant CAD across symptomatic patient categories is substantially lower than currently assumed. With a low prevalence of the disease, non-invasive testing can rarely rule-in the disease and focus should shift to ruling-out obstructive CAD. The large proportion of false positive findings must be taken into account when patients with low PTP are investigated."
0,Opening the black box of social behavior,,
0,Long data from the electrocardiogram,,
0,CHASMplus Reveals the Scope of Somatic Missense Mutations Driving Human Cancers,"Tokheim et al. introduce a computational approach to accurately separate driver from passenger mutations in cancer. Their analysis revealed that most driver mutations occur only in a few patients, presenting a challenge for precision medicine, and several cancer types will benefit from additional sequencing to identify these rare driver mutations.","CHASMplus Reveals the Scope of Somatic Missense Mutations Driving Human Cancers. Tokheim et al. introduce a computational approach to accurately separate driver from passenger mutations in cancer. Their analysis revealed that most driver mutations occur only in a few patients, presenting a challenge for precision medicine, and several cancer types will benefit from additional sequencing to identify these rare driver mutations."
0,A diverse range of factors affect the nature of neural representations underlying short-term memory,,
0,Peroxisome protein import recapitulated in Xenopus egg extracts,"Peroxisomes import their luminal proteins from the cytosol. Most substrates contain a C-terminal Ser-Lys-Leu (SKL) sequence that is recognized by the receptor Pex5. Pex5 binds to peroxisomes via a docking complex containing Pex14, and recycles back into the cytosol following its mono-ubiquitination at a conserved Cys residue. The mechanism of peroxisome protein import remains incompletely understood. Here, we developed an in vitro import system based on Xenopus egg extracts. Import is dependent on the SKL motif in the substrate and on the presence of Pex5 and Pex14, and is sustained by ATP hydrolysis. A protein lacking an SKL sequence can be coimported, providing strong evidence for import of a folded protein. The conserved cysteine in Pex5 is not essential for import or to clear import sites for subsequent rounds of translocation. This new in vitro assay will be useful for further dissecting the mechanism of peroxisome protein import.","Peroxisome protein import recapitulated in Xenopus egg extracts. Peroxisomes import their luminal proteins from the cytosol. Most substrates contain a C-terminal Ser-Lys-Leu (SKL) sequence that is recognized by the receptor Pex5. Pex5 binds to peroxisomes via a docking complex containing Pex14, and recycles back into the cytosol following its mono-ubiquitination at a conserved Cys residue. The mechanism of peroxisome protein import remains incompletely understood. Here, we developed an in vitro import system based on Xenopus egg extracts. Import is dependent on the SKL motif in the substrate and on the presence of Pex5 and Pex14, and is sustained by ATP hydrolysis. A protein lacking an SKL sequence can be coimported, providing strong evidence for import of a folded protein. The conserved cysteine in Pex5 is not essential for import or to clear import sites for subsequent rounds of translocation. This new in vitro assay will be useful for further dissecting the mechanism of peroxisome protein import."
0,The Reality of Pain Scoring in the Emergency Department: Findings From a Multiple Case Study Design,,
0,"Universal atrial coordinates applied to visualisation, registration and construction of patient specific meshes",,
0,The PI(4)P phosphatase Sac2 controls insulin granule docking and release,"Insulin granule biogenesis involves transport to, and stable docking at, the plasma membrane before priming and fusion. Defects in this pathway result in impaired insulin secretion and are a hallmark of type 2 diabetes. We now show that the phosphatidylinositol 4-phosphate phosphatase Sac2 localizes to insulin granules in a substrate-dependent manner and that loss of Sac2 results in impaired insulin secretion. Sac2 operates upstream of granule docking, since loss of Sac2 prevented granule tethering to the plasma membrane and resulted in both reduced granule density and number of exocytic events. Sac2 levels correlated positively with the number of docked granules and exocytic events in clonal Ã¢ cells and with insulin secretion in human pancreatic islets, and Sac2 expression was reduced in islets from type 2 diabetic subjects. Taken together, we identified a phosphoinositide switch on the surface on insulin granules that is required for stable granule docking at the plasma membrane and impaired in human type 2 diabetes.","The PI(4)P phosphatase Sac2 controls insulin granule docking and release. Insulin granule biogenesis involves transport to, and stable docking at, the plasma membrane before priming and fusion. Defects in this pathway result in impaired insulin secretion and are a hallmark of type 2 diabetes. We now show that the phosphatidylinositol 4-phosphate phosphatase Sac2 localizes to insulin granules in a substrate-dependent manner and that loss of Sac2 results in impaired insulin secretion. Sac2 operates upstream of granule docking, since loss of Sac2 prevented granule tethering to the plasma membrane and resulted in both reduced granule density and number of exocytic events. Sac2 levels correlated positively with the number of docked granules and exocytic events in clonal Ã¢ cells and with insulin secretion in human pancreatic islets, and Sac2 expression was reduced in islets from type 2 diabetic subjects. Taken together, we identified a phosphoinositide switch on the surface on insulin granules that is required for stable granule docking at the plasma membrane and impaired in human type 2 diabetes."
0,Enhanced Recovery After Surgery (ERAS) Pathway in Esophagectomy: Is a Reasonable Prediction of Hospital Stay Possible?,"OBJECTIVE: To assess whether perioperative variables or deviation from enhanced recovery after surgery (ERAS) items could be associated with delayed discharge after esophagectomy, and to convert them into a scoring system to predict it. SUMMARY BACKGROUND DATA: ERAS perioperative pathways have been recently applied to esophageal resections. However, low adherence to ERAS items and high rates of protocol deviations are often reported. METHODS: All patients who underwent esophagectomy between April 2012 and March 2017 were managed with a standardized perioperative pathway according to ERAS principles. The target length of stay was set at eighth postoperative day (POD). All significant variables at bivariate analysis were entered into a logistic regression to produce a predictive score. An initial validation of the score accuracy was carried out on a separate patient sample. RESULTS: Two hundred eighty-six patients were included in the study. Multivariate regression analysis showed that American Society of Anesthesiology score >/= 3, surgery duration > 255 min, ""nonhybrid"" esophagectomy, and failure to mobilize patients within 24 h from surgery were associated with delayed discharge. The logistic regression model was statistically significant (P < 0.001) and correctly classified 81.9% of cases. The sensitivity was 96.6%, and the specificity was 17.6%. The prediction score applied to 23 patients correctly identified 100% of those discharged after eighth POD. CONCLUSIONS: The results of this study seem to be clinically meaningful and in line with those from other studies. The initial validation revealed good predictive properties.","Enhanced Recovery After Surgery (ERAS) Pathway in Esophagectomy: Is a Reasonable Prediction of Hospital Stay Possible?. OBJECTIVE: To assess whether perioperative variables or deviation from enhanced recovery after surgery (ERAS) items could be associated with delayed discharge after esophagectomy, and to convert them into a scoring system to predict it. SUMMARY BACKGROUND DATA: ERAS perioperative pathways have been recently applied to esophageal resections. However, low adherence to ERAS items and high rates of protocol deviations are often reported. METHODS: All patients who underwent esophagectomy between April 2012 and March 2017 were managed with a standardized perioperative pathway according to ERAS principles. The target length of stay was set at eighth postoperative day (POD). All significant variables at bivariate analysis were entered into a logistic regression to produce a predictive score. An initial validation of the score accuracy was carried out on a separate patient sample. RESULTS: Two hundred eighty-six patients were included in the study. Multivariate regression analysis showed that American Society of Anesthesiology score >/= 3, surgery duration > 255 min, ""nonhybrid"" esophagectomy, and failure to mobilize patients within 24 h from surgery were associated with delayed discharge. The logistic regression model was statistically significant (P < 0.001) and correctly classified 81.9% of cases. The sensitivity was 96.6%, and the specificity was 17.6%. The prediction score applied to 23 patients correctly identified 100% of those discharged after eighth POD. CONCLUSIONS: The results of this study seem to be clinically meaningful and in line with those from other studies. The initial validation revealed good predictive properties."
0,Pregnancy-Adapted YEARS Algorithm for Diagnosis of Suspected Pulmonary Embolism,"BACKGROUND: Pulmonary embolism is one of the leading causes of maternal death in the Western world. Because of the low specificity and sensitivity of the d-dimer test, all pregnant women with suspected pulmonary embolism undergo computed tomographic (CT) pulmonary angiography or ventilation-perfusion scanning, both of which involve radiation exposure to the mother and fetus. Whether a pregnancy-adapted algorithm could be used to safely avoid diagnostic imaging in pregnant women with suspected pulmonary embolism is unknown. METHODS: In a prospective study involving pregnant women with suspected pulmonary embolism, we assessed three criteria from the YEARS algorithm (clinical signs of deep-vein thrombosis, hemoptysis, and pulmonary embolism as the most likely diagnosis) and measured the d-dimer level. Pulmonary embolism was ruled out if none of the three criteria were met and the d-dimer level was less than 1000 ng per milliliter or if one or more of the three criteria were met and the d-dimer level was less than 500 ng per milliliter. Adaptation of the YEARS algorithm for pregnant women involved compression ultrasonography for women with symptoms of deep-vein thrombosis; if the results were positive (i.e., a clot was present), CT pulmonary angiography was not performed. All patients in whom pulmonary embolism had not been ruled out underwent CT pulmonary angiography. The primary outcome was the incidence of venous thromboembolism at 3 months. The secondary outcome was the proportion of patients in whom CT pulmonary angiography was not indicated to safely rule out pulmonary embolism. RESULTS: A total of 510 women were screened, of whom 12 (2.4%) were excluded. Pulmonary embolism was diagnosed in 20 patients (4.0%) at baseline. During follow-up, popliteal deep-vein thrombosis was diagnosed in 1 patient (0.21%; 95% confidence interval [CI], 0.04 to 1.2); no patient had pulmonary embolism. CT pulmonary angiography was not indicated, and thus was avoided, in 195 patients (39%; 95% CI, 35 to 44). The efficiency of the algorithm was highest during the first trimester of pregnancy and lowest during the third trimester; CT pulmonary angiography was avoided in 65% of patients who began the study in the first trimester and in 32% who began the study in the third trimester. CONCLUSIONS: Pulmonary embolism was safely ruled out by the pregnancy-adapted YEARS diagnostic algorithm across all trimesters of pregnancy. CT pulmonary angiography was avoided in 32 to 65% of patients. (Funded by Leiden University Medical Center and 17 other participating hospitals; Artemis Netherlands Trial Register number, NL5726.).","Pregnancy-Adapted YEARS Algorithm for Diagnosis of Suspected Pulmonary Embolism. BACKGROUND: Pulmonary embolism is one of the leading causes of maternal death in the Western world. Because of the low specificity and sensitivity of the d-dimer test, all pregnant women with suspected pulmonary embolism undergo computed tomographic (CT) pulmonary angiography or ventilation-perfusion scanning, both of which involve radiation exposure to the mother and fetus. Whether a pregnancy-adapted algorithm could be used to safely avoid diagnostic imaging in pregnant women with suspected pulmonary embolism is unknown. METHODS: In a prospective study involving pregnant women with suspected pulmonary embolism, we assessed three criteria from the YEARS algorithm (clinical signs of deep-vein thrombosis, hemoptysis, and pulmonary embolism as the most likely diagnosis) and measured the d-dimer level. Pulmonary embolism was ruled out if none of the three criteria were met and the d-dimer level was less than 1000 ng per milliliter or if one or more of the three criteria were met and the d-dimer level was less than 500 ng per milliliter. Adaptation of the YEARS algorithm for pregnant women involved compression ultrasonography for women with symptoms of deep-vein thrombosis; if the results were positive (i.e., a clot was present), CT pulmonary angiography was not performed. All patients in whom pulmonary embolism had not been ruled out underwent CT pulmonary angiography. The primary outcome was the incidence of venous thromboembolism at 3 months. The secondary outcome was the proportion of patients in whom CT pulmonary angiography was not indicated to safely rule out pulmonary embolism. RESULTS: A total of 510 women were screened, of whom 12 (2.4%) were excluded. Pulmonary embolism was diagnosed in 20 patients (4.0%) at baseline. During follow-up, popliteal deep-vein thrombosis was diagnosed in 1 patient (0.21%; 95% confidence interval [CI], 0.04 to 1.2); no patient had pulmonary embolism. CT pulmonary angiography was not indicated, and thus was avoided, in 195 patients (39%; 95% CI, 35 to 44). The efficiency of the algorithm was highest during the first trimester of pregnancy and lowest during the third trimester; CT pulmonary angiography was avoided in 65% of patients who began the study in the first trimester and in 32% who began the study in the third trimester. CONCLUSIONS: Pulmonary embolism was safely ruled out by the pregnancy-adapted YEARS diagnostic algorithm across all trimesters of pregnancy. CT pulmonary angiography was avoided in 32 to 65% of patients. (Funded by Leiden University Medical Center and 17 other participating hospitals; Artemis Netherlands Trial Register number, NL5726.)."
0,Prodromal Parkinson disease: do we miss the signs?,,
0,"Comment on ""Utilizing Machine Learning Methods for Preoperative Prediction of Postsurgical Mortality and Intensive Care Unit Admission""",,
0,Robot assisted training for the upper limb after stroke (RATULS): a multicentre randomised controlled trial,"BACKGROUND: Loss of arm function is a common problem after stroke. Robot-assisted training might improve arm function and activities of daily living. We compared the clinical effectiveness of robot-assisted training using the MIT-Manus robotic gym with an enhanced upper limb therapy (EULT) programme based on repetitive functional task practice and with usual care. METHODS: RATULS was a pragmatic, multicentre, randomised controlled trial done at four UK centres. Stroke patients aged at least 18 years with moderate or severe upper limb functional limitation, between 1 week and 5 years after their first stroke, were randomly assigned (1:1:1) to receive robot-assisted training, EULT, or usual care. Robot-assisted training and EULT were provided for 45 min, three times per week for 12 weeks. Randomisation was internet-based using permuted block sequences. Treatment allocation was masked from outcome assessors but not from participants or therapists. The primary outcome was upper limb function success (defined using the Action Research Arm Test [ARAT]) at 3 months. Analyses were done on an intention-to-treat basis. This study is registered with the ISRCTN registry, number ISRCTN69371850. FINDINGS: Between April 14, 2014, and April 30, 2018, 770 participants were enrolled and randomly assigned to either robot-assisted training (n=257), EULT (n=259), or usual care (n=254). The primary outcome of ARAT success was achieved by 103 (44%) of 232 patients in the robot-assisted training group, 118 (50%) of 234 in the EULT group, and 85 (42%) of 203 in the usual care group. Compared with usual care, robot-assisted training (adjusted odds ratio [aOR] 1.17 [98.3% CI 0.70-1.96]) and EULT (aOR 1.51 [0.90-2.51]) did not improve upper limb function; the effects of robot-assisted training did not differ from EULT (aOR 0.78 [0.48-1.27]). More participants in the robot-assisted training group (39 [15%] of 257) and EULT group (33 [13%] of 259) had serious adverse events than in the usual care group (20 [8%] of 254), but none were attributable to the intervention. INTERPRETATION: Robot-assisted training and EULT did not improve upper limb function after stroke compared with usual care for patients with moderate or severe upper limb functional limitation. These results do not support the use of robot-assisted training as provided in this trial in routine clinical practice. FUNDING: National Institute for Health Research Health Technology Assessment Programme.","Robot assisted training for the upper limb after stroke (RATULS): a multicentre randomised controlled trial. BACKGROUND: Loss of arm function is a common problem after stroke. Robot-assisted training might improve arm function and activities of daily living. We compared the clinical effectiveness of robot-assisted training using the MIT-Manus robotic gym with an enhanced upper limb therapy (EULT) programme based on repetitive functional task practice and with usual care. METHODS: RATULS was a pragmatic, multicentre, randomised controlled trial done at four UK centres. Stroke patients aged at least 18 years with moderate or severe upper limb functional limitation, between 1 week and 5 years after their first stroke, were randomly assigned (1:1:1) to receive robot-assisted training, EULT, or usual care. Robot-assisted training and EULT were provided for 45 min, three times per week for 12 weeks. Randomisation was internet-based using permuted block sequences. Treatment allocation was masked from outcome assessors but not from participants or therapists. The primary outcome was upper limb function success (defined using the Action Research Arm Test [ARAT]) at 3 months. Analyses were done on an intention-to-treat basis. This study is registered with the ISRCTN registry, number ISRCTN69371850. FINDINGS: Between April 14, 2014, and April 30, 2018, 770 participants were enrolled and randomly assigned to either robot-assisted training (n=257), EULT (n=259), or usual care (n=254). The primary outcome of ARAT success was achieved by 103 (44%) of 232 patients in the robot-assisted training group, 118 (50%) of 234 in the EULT group, and 85 (42%) of 203 in the usual care group. Compared with usual care, robot-assisted training (adjusted odds ratio [aOR] 1.17 [98.3% CI 0.70-1.96]) and EULT (aOR 1.51 [0.90-2.51]) did not improve upper limb function; the effects of robot-assisted training did not differ from EULT (aOR 0.78 [0.48-1.27]). More participants in the robot-assisted training group (39 [15%] of 257) and EULT group (33 [13%] of 259) had serious adverse events than in the usual care group (20 [8%] of 254), but none were attributable to the intervention. INTERPRETATION: Robot-assisted training and EULT did not improve upper limb function after stroke compared with usual care for patients with moderate or severe upper limb functional limitation. These results do not support the use of robot-assisted training as provided in this trial in routine clinical practice. FUNDING: National Institute for Health Research Health Technology Assessment Programme."
0,"Cabazitaxel plus carboplatin for the treatment of men with metastatic castration-resistant prostate cancers: a randomised, open-label, phase 1-2 trial",,
0,Genomic Evaluation of Multiparametric Magnetic Resonance Imaging-visible and -nonvisible Lesions in Clinically Localised Prostate Cancer,"Background: The prostate cancer (PCa) diagnostic pathway is undergoing a radical change with the introduction of multiparametric magnetic resonance imaging (mpMRI), genomic testing, and different prostate biopsy techniques. It has been proposed that these tests should be used in a sequential manner to optimise risk stratification. Objective: To characterise the genomic, epigenomic, and transcriptomic features of mpMRI-visible and -nonvisible PCa in clinically localised disease. Design, setting, and participants: Multicore analysis of fresh prostate tissue sampled immediately after radical prostatectomy was performed for intermediate- to high-risk PCa. Intervention: Low-pass whole-genome, exome, methylation, and transcriptome profiling of patient tissue cores taken from microscopically benign and cancerous areas in the same prostate. Circulating free and germline DNA was assessed from the blood of five patients. Outcome measurement and statistical analysis: Correlations between preoperative mpMRI and genomic characteristics of tumour and benign prostate samples were assessed. Gene profiles for individual tumour cores were correlated with existing genomic classifiers currently used for prognostication. Results and limitations: A total of 43 prostate cores (22 tumour and 21 benign) were profiled from six whole prostate glands. Of the 22 tumour cores, 16 were tumours visible and six were tumours nonvisible on mpMRI. Intratumour genomic, epigenomic, and transcriptomic heterogeneity was found within mpMRI-visible lesions. This could potentially lead to misclassification of patients using signatures based on copy number or RNA expression. Moreover, three of the six cores obtained from mpMRI-nonvisible tumours harboured one or more genetic alterations commonly observed in metastatic castration-resistant PCa. No circulating free DNA alterations were found. Limitations include the small cohort size and lack of follow-up. Conclusions: Our study supports the continued use of systematic prostate sampling in addition to mpMRI, as avoidance of systematic biopsies in patients with negative mpMRI may mean that clinically significant tumours harbouring genetic alterations commonly seen in metastatic PCa are missed. Furthermore, there is inconsistency in individual genomics when genomic classifiers are applied. Patient summary: Our study shows that tumour heterogeneity within prostate tumours visible on multiparametric magnetic resonance imaging (mpMRI) can lead to misclassification of patients if only one core is used for genomic analysis. In addition, some cancers that were missed by mpMRI had genomic aberrations that are commonly seen in advanced metastatic prostate cancer. Avoiding biopsies in mpMRI-negative cases may mean that such potentially lethal cancers are missed. Our study supports the continued use of systematic prostate sampling in addition to multiparametric magnetic resonance imaging (mpMRI), as avoidance of systematic biopsies in patients with negative mpMRI can mean that clinically significant tumours harbouring genetic alterations commonly seen in metastatic prostate cancer can be missed. Furthermore, there is inconsistency in individual genomics when genomic classifiers are applied.","Genomic Evaluation of Multiparametric Magnetic Resonance Imaging-visible and -nonvisible Lesions in Clinically Localised Prostate Cancer. Background: The prostate cancer (PCa) diagnostic pathway is undergoing a radical change with the introduction of multiparametric magnetic resonance imaging (mpMRI), genomic testing, and different prostate biopsy techniques. It has been proposed that these tests should be used in a sequential manner to optimise risk stratification. Objective: To characterise the genomic, epigenomic, and transcriptomic features of mpMRI-visible and -nonvisible PCa in clinically localised disease. Design, setting, and participants: Multicore analysis of fresh prostate tissue sampled immediately after radical prostatectomy was performed for intermediate- to high-risk PCa. Intervention: Low-pass whole-genome, exome, methylation, and transcriptome profiling of patient tissue cores taken from microscopically benign and cancerous areas in the same prostate. Circulating free and germline DNA was assessed from the blood of five patients. Outcome measurement and statistical analysis: Correlations between preoperative mpMRI and genomic characteristics of tumour and benign prostate samples were assessed. Gene profiles for individual tumour cores were correlated with existing genomic classifiers currently used for prognostication. Results and limitations: A total of 43 prostate cores (22 tumour and 21 benign) were profiled from six whole prostate glands. Of the 22 tumour cores, 16 were tumours visible and six were tumours nonvisible on mpMRI. Intratumour genomic, epigenomic, and transcriptomic heterogeneity was found within mpMRI-visible lesions. This could potentially lead to misclassification of patients using signatures based on copy number or RNA expression. Moreover, three of the six cores obtained from mpMRI-nonvisible tumours harboured one or more genetic alterations commonly observed in metastatic castration-resistant PCa. No circulating free DNA alterations were found. Limitations include the small cohort size and lack of follow-up. Conclusions: Our study supports the continued use of systematic prostate sampling in addition to mpMRI, as avoidance of systematic biopsies in patients with negative mpMRI may mean that clinically significant tumours harbouring genetic alterations commonly seen in metastatic PCa are missed. Furthermore, there is inconsistency in individual genomics when genomic classifiers are applied. Patient summary: Our study shows that tumour heterogeneity within prostate tumours visible on multiparametric magnetic resonance imaging (mpMRI) can lead to misclassification of patients if only one core is used for genomic analysis. In addition, some cancers that were missed by mpMRI had genomic aberrations that are commonly seen in advanced metastatic prostate cancer. Avoiding biopsies in mpMRI-negative cases may mean that such potentially lethal cancers are missed. Our study supports the continued use of systematic prostate sampling in addition to multiparametric magnetic resonance imaging (mpMRI), as avoidance of systematic biopsies in patients with negative mpMRI can mean that clinically significant tumours harbouring genetic alterations commonly seen in metastatic prostate cancer can be missed. Furthermore, there is inconsistency in individual genomics when genomic classifiers are applied."
0,"Two-view digital breast tomosynthesis versus digital mammography in a population-based breast cancer screening programme (To-Be): a randomised, controlled trial","Background: Digital breast tomosynthesis is an advancement of mammography, and has the potential to overcome limitations of standard digital mammography. This study aimed to compare first-generation digital breast tomo-synthesis including two-dimensional (2D)synthetic mammograms versus digital mammography in a population-based screening programme. Methods: BreastScreen Norway offers all women aged 50â€“69 years two-view (craniocaudal and mediolateral oblique)mammographic screening every 2 years and does independent double reading with consensus. We asked all 32 976 women who attended the programme in Bergen in 2016â€“17, to participate in this randomised, controlled trial with a parallel group design. A study-specific software was developed to allocate women to either digital breast tomosynthesis or digital mammography using a 1:1 simple randomisation method based on participants' unique national identity numbers. The interviewing radiographer did the randomisation by entering the number into the software. Randomisation was done after consent and was therefore concealed from both the women and the radiographer at the time of consent; the algorithm was not disclosed to radiographers during the recruitment period. All data needed for analyses were complete 12 months after the recruitment period ended. The primary outcome measure was screen-detected breast cancer, stratified by screening technique (ie, digital breast tomosynthesis and digital mammography). A log-binomial regression model was used to estimate the efficacy of digital breast tomosynthesis versus digital mammography, defined as the crude risk ratios (RRs)with 95% CIs for screen-detected breast cancer for women screened during the recruitment period. A per-protocol approach was used in the analyses. This trial is registered at ClinicalTrials.gov, number NCT02835625, and is closed to accrual. Findings: Between, Jan 14, 2016, and Dec 31, 2017, 44 266 women were invited to the screening programme in Bergen, and 32 976 (74Â·5%)attended. After excluding women with breast implants and women who did not consent to participate, 29 453 (89Â·3%)were eligible for electronic randomisation. 14 734 women were allocated to digital breast tomosynthesis and 14 719 to digital mammography. After randomisation, women with a previous breast cancer were excluded (digital breast tomosynthesis group n=314, digital mammography group n=316), women with metastases from melanoma (digital breast tomosynthesis group n=1), and women who informed the radiographer about breast symptoms after providing consent (digital breast tomosynthesis group n=39, digital mammography group n=34). After exclusions, information from 28 749 women were included in the analyses (digital breast tomosynthesis group n=14 380, digital mammography group n=14 369). The proportion of screen-detected breast cancer among the screened women did not differ between the two groups (95 [0Â·66%, 0Â·53â€“0Â·79]of 14 380 vs 87 [0Â·61%, 0Â·48â€“0Â·73]of 14 369; RR 1Â·09, 95% CI 0Â·82â€“1Â·46; p=0Â·56). Interpretation: This study indicated that digital breast tomosynthesis including synthetic 2D mammograms was not significantly different from standard digital mammography as a screening tool for the detection of breast cancer in a population-based screening programme. Economic analyses and follow-up studies on interval and consecutive round screen-detected breast cancers are needed to better understand the effect of digital breast tomosynthesis in population-based breast cancer screening. Funding: Cancer Registry of Norway, Department of Radiology at Haukeland University Hospital, University of Oslo, and Research Council of Norway.","Two-view digital breast tomosynthesis versus digital mammography in a population-based breast cancer screening programme (To-Be): a randomised, controlled trial. Background: Digital breast tomosynthesis is an advancement of mammography, and has the potential to overcome limitations of standard digital mammography. This study aimed to compare first-generation digital breast tomo-synthesis including two-dimensional (2D)synthetic mammograms versus digital mammography in a population-based screening programme. Methods: BreastScreen Norway offers all women aged 50â€“69 years two-view (craniocaudal and mediolateral oblique)mammographic screening every 2 years and does independent double reading with consensus. We asked all 32 976 women who attended the programme in Bergen in 2016â€“17, to participate in this randomised, controlled trial with a parallel group design. A study-specific software was developed to allocate women to either digital breast tomosynthesis or digital mammography using a 1:1 simple randomisation method based on participants' unique national identity numbers. The interviewing radiographer did the randomisation by entering the number into the software. Randomisation was done after consent and was therefore concealed from both the women and the radiographer at the time of consent; the algorithm was not disclosed to radiographers during the recruitment period. All data needed for analyses were complete 12 months after the recruitment period ended. The primary outcome measure was screen-detected breast cancer, stratified by screening technique (ie, digital breast tomosynthesis and digital mammography). A log-binomial regression model was used to estimate the efficacy of digital breast tomosynthesis versus digital mammography, defined as the crude risk ratios (RRs)with 95% CIs for screen-detected breast cancer for women screened during the recruitment period. A per-protocol approach was used in the analyses. This trial is registered at ClinicalTrials.gov, number NCT02835625, and is closed to accrual. Findings: Between, Jan 14, 2016, and Dec 31, 2017, 44 266 women were invited to the screening programme in Bergen, and 32 976 (74Â·5%)attended. After excluding women with breast implants and women who did not consent to participate, 29 453 (89Â·3%)were eligible for electronic randomisation. 14 734 women were allocated to digital breast tomosynthesis and 14 719 to digital mammography. After randomisation, women with a previous breast cancer were excluded (digital breast tomosynthesis group n=314, digital mammography group n=316), women with metastases from melanoma (digital breast tomosynthesis group n=1), and women who informed the radiographer about breast symptoms after providing consent (digital breast tomosynthesis group n=39, digital mammography group n=34). After exclusions, information from 28 749 women were included in the analyses (digital breast tomosynthesis group n=14 380, digital mammography group n=14 369). The proportion of screen-detected breast cancer among the screened women did not differ between the two groups (95 [0Â·66%, 0Â·53â€“0Â·79]of 14 380 vs 87 [0Â·61%, 0Â·48â€“0Â·73]of 14 369; RR 1Â·09, 95% CI 0Â·82â€“1Â·46; p=0Â·56). Interpretation: This study indicated that digital breast tomosynthesis including synthetic 2D mammograms was not significantly different from standard digital mammography as a screening tool for the detection of breast cancer in a population-based screening programme. Economic analyses and follow-up studies on interval and consecutive round screen-detected breast cancers are needed to better understand the effect of digital breast tomosynthesis in population-based breast cancer screening. Funding: Cancer Registry of Norway, Department of Radiology at Haukeland University Hospital, University of Oslo, and Research Council of Norway."
0,Perirhinal circuits for memory processing,,
0,"Bleeding in cardiac patients prescribed antithrombotic drugs: electronic health record phenotyping algorithms, incidence, trends and prognosis","BACKGROUND: Clinical guidelines and public health authorities lack recommendations on scalable approaches to defining and monitoring the occurrence and severity of bleeding in populations prescribed antithrombotic therapy. METHODS: We examined linked primary care, hospital admission and death registry electronic health records (CALIBER 1998-2010, England) of patients with newly diagnosed atrial fibrillation, acute myocardial infarction, unstable angina or stable angina with the aim to develop algorithms for bleeding events. Using the developed bleeding phenotypes, Kaplan-Meier plots were used to estimate the incidence of bleeding events and we used Cox regression models to assess the prognosis for all-cause mortality, atherothrombotic events and further bleeding. RESULTS: We present electronic health record phenotyping algorithms for bleeding based on bleeding diagnosis in primary or hospital care, symptoms, transfusion, surgical procedures and haemoglobin values. In validation of the phenotype, we estimated a positive predictive value of 0.88 (95% CI 0.64, 0.99) for hospitalised bleeding. Amongst 128,815 patients, 27,259 (21.2%) had at least 1 bleeding event, with 5-year risks of bleeding of 29.1%, 21.9%, 25.3% and 23.4% following diagnoses of atrial fibrillation, acute myocardial infarction, unstable angina and stable angina, respectively. Rates of hospitalised bleeding per 1000 patients more than doubled from 1.02 (95% CI 0.83, 1.22) in January 1998 to 2.68 (95% CI 2.49, 2.88) in December 2009 coinciding with the increased rates of antiplatelet and vitamin K antagonist prescribing. Patients with hospitalised bleeding and primary care bleeding, with or without markers of severity, were at increased risk of all-cause mortality and atherothrombotic events compared to those with no bleeding. For example, the hazard ratio for all-cause mortality was 1.98 (95% CI 1.86, 2.11) for primary care bleeding with markers of severity and 1.99 (95% CI 1.92, 2.05) for hospitalised bleeding without markers of severity, compared to patients with no bleeding. CONCLUSIONS: Electronic health record bleeding phenotyping algorithms offer a scalable approach to monitoring bleeding in the population. Incidence of bleeding has doubled in incidence since 1998, affects one in four cardiovascular disease patients, and is associated with poor prognosis. Efforts are required to tackle this iatrogenic epidemic.","Bleeding in cardiac patients prescribed antithrombotic drugs: electronic health record phenotyping algorithms, incidence, trends and prognosis. BACKGROUND: Clinical guidelines and public health authorities lack recommendations on scalable approaches to defining and monitoring the occurrence and severity of bleeding in populations prescribed antithrombotic therapy. METHODS: We examined linked primary care, hospital admission and death registry electronic health records (CALIBER 1998-2010, England) of patients with newly diagnosed atrial fibrillation, acute myocardial infarction, unstable angina or stable angina with the aim to develop algorithms for bleeding events. Using the developed bleeding phenotypes, Kaplan-Meier plots were used to estimate the incidence of bleeding events and we used Cox regression models to assess the prognosis for all-cause mortality, atherothrombotic events and further bleeding. RESULTS: We present electronic health record phenotyping algorithms for bleeding based on bleeding diagnosis in primary or hospital care, symptoms, transfusion, surgical procedures and haemoglobin values. In validation of the phenotype, we estimated a positive predictive value of 0.88 (95% CI 0.64, 0.99) for hospitalised bleeding. Amongst 128,815 patients, 27,259 (21.2%) had at least 1 bleeding event, with 5-year risks of bleeding of 29.1%, 21.9%, 25.3% and 23.4% following diagnoses of atrial fibrillation, acute myocardial infarction, unstable angina and stable angina, respectively. Rates of hospitalised bleeding per 1000 patients more than doubled from 1.02 (95% CI 0.83, 1.22) in January 1998 to 2.68 (95% CI 2.49, 2.88) in December 2009 coinciding with the increased rates of antiplatelet and vitamin K antagonist prescribing. Patients with hospitalised bleeding and primary care bleeding, with or without markers of severity, were at increased risk of all-cause mortality and atherothrombotic events compared to those with no bleeding. For example, the hazard ratio for all-cause mortality was 1.98 (95% CI 1.86, 2.11) for primary care bleeding with markers of severity and 1.99 (95% CI 1.92, 2.05) for hospitalised bleeding without markers of severity, compared to patients with no bleeding. CONCLUSIONS: Electronic health record bleeding phenotyping algorithms offer a scalable approach to monitoring bleeding in the population. Incidence of bleeding has doubled in incidence since 1998, affects one in four cardiovascular disease patients, and is associated with poor prognosis. Efforts are required to tackle this iatrogenic epidemic."
0,Comparison of somatic variant detection algorithms using Ion Torrent targeted deep sequencing data,"Background: The application of next-generation sequencing in cancer has revealed the genomic landscape of many tumour types and is nowadays routinely used in research and clinical settings. Multiple algorithms have been developed to detect somatic variation from sequencing data using either paired tumour-blood or tumour-only samples. Most of these methods have been developed and evaluated for the identification of somatic variation using Illumina sequencing datasets of moderate coverage. However, a comprehensive evaluation of somatic variant detection algorithms on Ion Torrent targeted deep sequencing data has not been performed. Methods: We have applied three somatic detection algorithms, Torrent Variant Caller, MuTect2 and VarScan2, on a large cohort of ovarian cancer patients comprising of 208 paired tumour-blood samples and 253 tumour-only samples sequenced deeply on Ion Torrent Proton platform across 330 amplicons. Subsequently, the concordance and performance of the three somatic variant callers were assessed. Results: We have observed low concordance across the algorithms with only 0.5% of SNV and 0.02% of INDEL calls in common across all three methods. The intersection of all methods showed better performance when assessed using correlation with known mutational signatures, overlap with COSMIC variation and by examining the variant characteristics. The Torrent Variant Caller also performed well with the advantage of not eliminating a high number of variants that could lead to high type II error. Conclusions: Our results suggest that caution should be taken when applying state-of-the-art somatic variant algorithms to Ion Torrent targeted deep sequencing data. Better quality control procedures and strategies that combine results from multiple methods should ensure that higher accuracy is achieved. This is essential to ensure that results from bioinformatics pipelines using Ion Torrent deep sequencing can be robustly applied in cancer research and in the clinic.","Comparison of somatic variant detection algorithms using Ion Torrent targeted deep sequencing data. Background: The application of next-generation sequencing in cancer has revealed the genomic landscape of many tumour types and is nowadays routinely used in research and clinical settings. Multiple algorithms have been developed to detect somatic variation from sequencing data using either paired tumour-blood or tumour-only samples. Most of these methods have been developed and evaluated for the identification of somatic variation using Illumina sequencing datasets of moderate coverage. However, a comprehensive evaluation of somatic variant detection algorithms on Ion Torrent targeted deep sequencing data has not been performed. Methods: We have applied three somatic detection algorithms, Torrent Variant Caller, MuTect2 and VarScan2, on a large cohort of ovarian cancer patients comprising of 208 paired tumour-blood samples and 253 tumour-only samples sequenced deeply on Ion Torrent Proton platform across 330 amplicons. Subsequently, the concordance and performance of the three somatic variant callers were assessed. Results: We have observed low concordance across the algorithms with only 0.5% of SNV and 0.02% of INDEL calls in common across all three methods. The intersection of all methods showed better performance when assessed using correlation with known mutational signatures, overlap with COSMIC variation and by examining the variant characteristics. The Torrent Variant Caller also performed well with the advantage of not eliminating a high number of variants that could lead to high type II error. Conclusions: Our results suggest that caution should be taken when applying state-of-the-art somatic variant algorithms to Ion Torrent targeted deep sequencing data. Better quality control procedures and strategies that combine results from multiple methods should ensure that higher accuracy is achieved. This is essential to ensure that results from bioinformatics pipelines using Ion Torrent deep sequencing can be robustly applied in cancer research and in the clinic."
0,"Preventive cardiology in adolescents and the elderly: LDL, HDL, and inflammation",,
0,Artificial intelligence to improve the diagnosis of cardiovascular diseases,,
0,SingleCellNet: A Computational Tool to Classify Single Cell RNA-Seq Data Across Platforms and Across Species,"Single-cell RNA-seq has emerged as a powerful tool in diverse applications, from determining the cell-type composition of tissues to uncovering regulators of developmental programs. A near-universal step in the analysis of single-cell RNA-seq data is to hypothesize the identity of each cell. Often, this is achieved by searching for combinations of genes that have previously been implicated as being cell-type specific, an approach that is not quantitative and does not explicitly take advantage of other single-cell RNA-seq studies. Here, we describe our tool, SingleCellNet, which addresses these issues and enables the classification of query single-cell RNA-seq data in comparison to reference single-cell RNA-seq data. SingleCellNet compares favorably to other methods in sensitivity and specificity, and it is able to classify across platforms and species. We highlight SingleCellNet's utility by classifying previously undetermined cells, and by assessing the outcome of a cell fate engineering experiment. A major obstacle in analyzing single-cell RNA-seq data is determining the identity of each cell. Often this process is time-consuming, error prone, and lacking in quantitative rigor. We have addressed this challenge by developing SingleCellNet (SCN), which provides a quantitative classification of single-cell RNA-seq data. SCN compares favorably to other methods in sensitivity and specificity. One of the major advantages of SCN is that it is possible to use it to classify cells across platforms and across species.","SingleCellNet: A Computational Tool to Classify Single Cell RNA-Seq Data Across Platforms and Across Species. Single-cell RNA-seq has emerged as a powerful tool in diverse applications, from determining the cell-type composition of tissues to uncovering regulators of developmental programs. A near-universal step in the analysis of single-cell RNA-seq data is to hypothesize the identity of each cell. Often, this is achieved by searching for combinations of genes that have previously been implicated as being cell-type specific, an approach that is not quantitative and does not explicitly take advantage of other single-cell RNA-seq studies. Here, we describe our tool, SingleCellNet, which addresses these issues and enables the classification of query single-cell RNA-seq data in comparison to reference single-cell RNA-seq data. SingleCellNet compares favorably to other methods in sensitivity and specificity, and it is able to classify across platforms and species. We highlight SingleCellNet's utility by classifying previously undetermined cells, and by assessing the outcome of a cell fate engineering experiment. A major obstacle in analyzing single-cell RNA-seq data is determining the identity of each cell. Often this process is time-consuming, error prone, and lacking in quantitative rigor. We have addressed this challenge by developing SingleCellNet (SCN), which provides a quantitative classification of single-cell RNA-seq data. SCN compares favorably to other methods in sensitivity and specificity. One of the major advantages of SCN is that it is possible to use it to classify cells across platforms and across species."
0,"Hypolipidemic effect of novel 2,5-bis(4-hydroxybenzylidenamino)-1,3,4-thiadiazole as potential peroxisome proliferation-activated receptor-Î± agonist in acute hyperlipidemic rat model","The development of new antihyperlipidemic agents with higher potency and lower side effects is of high priority. In this study, 1,3,4 thiadiazole Schiff base derivatives were synthesized as potential peroxisome proliferation-activated receptor-Î± (PPARÎ±) agonists and characterized using elemental analysis, FTIR, 1H-NMR, 13C-NMR and mass spectroscopy and then tested for their hypolipidemic activity in Triton WR-1339-induced acute hyperlipidemic rat model in comparison with bezafibrate. The compounds showed significant hypolipidemic activity. Induced fit docking showed that the compounds are potential activators of PPARÎ± with binding scores âˆ’ 8.00Â Kcal/mol for 2,5-bis(4-hydroxybenzylidenamino)-1,3,4-thiadiazole. PCR array analysis showed an increase in the expression of several genes involved in lipid metabolism through mitochondrial fatty acid Î² oxidation and are part of PPARÎ± signaling pathway including Acsm3, Fabp4 and Hmgcs1. Gene expression of Lrp12 and Lrp1b involved in LDL uptake by liver cells and Cyp7a1 involved in cholesterol catabolism were also found to be upregulated.","Hypolipidemic effect of novel 2,5-bis(4-hydroxybenzylidenamino)-1,3,4-thiadiazole as potential peroxisome proliferation-activated receptor-Î± agonist in acute hyperlipidemic rat model. The development of new antihyperlipidemic agents with higher potency and lower side effects is of high priority. In this study, 1,3,4 thiadiazole Schiff base derivatives were synthesized as potential peroxisome proliferation-activated receptor-Î± (PPARÎ±) agonists and characterized using elemental analysis, FTIR, 1H-NMR, 13C-NMR and mass spectroscopy and then tested for their hypolipidemic activity in Triton WR-1339-induced acute hyperlipidemic rat model in comparison with bezafibrate. The compounds showed significant hypolipidemic activity. Induced fit docking showed that the compounds are potential activators of PPARÎ± with binding scores âˆ’ 8.00Â Kcal/mol for 2,5-bis(4-hydroxybenzylidenamino)-1,3,4-thiadiazole. PCR array analysis showed an increase in the expression of several genes involved in lipid metabolism through mitochondrial fatty acid Î² oxidation and are part of PPARÎ± signaling pathway including Acsm3, Fabp4 and Hmgcs1. Gene expression of Lrp12 and Lrp1b involved in LDL uptake by liver cells and Cyp7a1 involved in cholesterol catabolism were also found to be upregulated."
0,Believing in dopamine,"Midbrain dopamine signals are widely thought to report reward prediction errors that drive learning in the basal ganglia. However, dopamine has also been implicated in various probabilistic computations, such as encoding uncertainty and controlling exploration. Here, we show how these different facets of dopamine signalling can be brought together under a common reinforcement learning framework. The key idea is that multiple sources of uncertainty impinge on reinforcement learning computations: uncertainty about the state of the environment, the parameters of the value function and the optimal action policy. Each of these sources plays a distinct role in the prefrontal cortexâ€“basal ganglia circuit for reinforcement learning and is ultimately reflected in dopamine activity. The view that dopamine plays a central role in the encoding and updating of beliefs brings the classical prediction error theory into alignment with more recent theories of Bayesian reinforcement learning.","Believing in dopamine. Midbrain dopamine signals are widely thought to report reward prediction errors that drive learning in the basal ganglia. However, dopamine has also been implicated in various probabilistic computations, such as encoding uncertainty and controlling exploration. Here, we show how these different facets of dopamine signalling can be brought together under a common reinforcement learning framework. The key idea is that multiple sources of uncertainty impinge on reinforcement learning computations: uncertainty about the state of the environment, the parameters of the value function and the optimal action policy. Each of these sources plays a distinct role in the prefrontal cortexâ€“basal ganglia circuit for reinforcement learning and is ultimately reflected in dopamine activity. The view that dopamine plays a central role in the encoding and updating of beliefs brings the classical prediction error theory into alignment with more recent theories of Bayesian reinforcement learning."
0,Advancing functional connectivity research from association to causation,,
0,Genomic Risk Predicts Molecular Imaging-detected Metastatic Nodal Disease in Prostate Cancer,"Background: The Decipher genomic classifier (GC) is increasingly being used to determine metastasis risk in men with localized prostate cancer (PCa). Whether GCs predict for the presence of occult metastatic disease at presentation or subsequent metastatic progression is unknown. Objective: To determine if GC scores predict extraprostatic 68Ga prostate-specific membrane antigen (68Ga-PSMA-11) positron emission tomography (PET) positivity at presentation. Design, setting, and participants: Between December 2015 and September 2018, 91 PCa patients with both GC scores and pretreatment 68Ga-PSMA-11 PET scans were identified. Risk stratification was performed using the National Comprehensive Cancer Network (NCCN), Cancer of the Prostate Risk Assessment (CAPRA), and GC scores. Outcome measurements and statistical analysis: Logistic regression was used to identify factors correlated with PSMA-positive disease. Results and limitations: The NCCN criteria identified 23 (25.3%) and 68 patients (74.7%) as intermediate and high risk, while CAPRA scores revealed 28 (30.8%) and 63 (69.2%) as low/intermediate and high risk, respectively. By contrast, only 45 patients (49.4%) had high-risk GC scores. PSMA-avid pelvic nodal involvement was identified in 27 patients (29.7%). Higher GC score was significantly associated with pelvic nodal involvement (odds ratio [OR] 1.38 per 0.1 units; p = 0.009) and any PSMA-avid nodal involvement (pelvic or distant; OR 1.40 per 0.1 units; p = 0.007). However, higher GC score was not significantly associated with PSMA-avid osseous metastases (OR 1.11 per 0.1 units; p = 0.50). Limitations include selection bias for patients able to receive both tests and the sample size. Conclusions: Each 0.1-unit increase in GC score was associated with an approximate 40% increase in the odds of PSMA-avid lymph node involvement. These data suggest that patients with GC high risk might benefit from more nodal imaging and treatment intensification, potentially via pelvic nodal dissection, pelvic nodal irradiation, and/or the addition of chemohormonal agents. Patient summary: Patients with higher genomic classifier scores were found to have more metastatic lymph node involvement on prostate-specific membrane antigen imaging.","Genomic Risk Predicts Molecular Imaging-detected Metastatic Nodal Disease in Prostate Cancer. Background: The Decipher genomic classifier (GC) is increasingly being used to determine metastasis risk in men with localized prostate cancer (PCa). Whether GCs predict for the presence of occult metastatic disease at presentation or subsequent metastatic progression is unknown. Objective: To determine if GC scores predict extraprostatic 68Ga prostate-specific membrane antigen (68Ga-PSMA-11) positron emission tomography (PET) positivity at presentation. Design, setting, and participants: Between December 2015 and September 2018, 91 PCa patients with both GC scores and pretreatment 68Ga-PSMA-11 PET scans were identified. Risk stratification was performed using the National Comprehensive Cancer Network (NCCN), Cancer of the Prostate Risk Assessment (CAPRA), and GC scores. Outcome measurements and statistical analysis: Logistic regression was used to identify factors correlated with PSMA-positive disease. Results and limitations: The NCCN criteria identified 23 (25.3%) and 68 patients (74.7%) as intermediate and high risk, while CAPRA scores revealed 28 (30.8%) and 63 (69.2%) as low/intermediate and high risk, respectively. By contrast, only 45 patients (49.4%) had high-risk GC scores. PSMA-avid pelvic nodal involvement was identified in 27 patients (29.7%). Higher GC score was significantly associated with pelvic nodal involvement (odds ratio [OR] 1.38 per 0.1 units; p = 0.009) and any PSMA-avid nodal involvement (pelvic or distant; OR 1.40 per 0.1 units; p = 0.007). However, higher GC score was not significantly associated with PSMA-avid osseous metastases (OR 1.11 per 0.1 units; p = 0.50). Limitations include selection bias for patients able to receive both tests and the sample size. Conclusions: Each 0.1-unit increase in GC score was associated with an approximate 40% increase in the odds of PSMA-avid lymph node involvement. These data suggest that patients with GC high risk might benefit from more nodal imaging and treatment intensification, potentially via pelvic nodal dissection, pelvic nodal irradiation, and/or the addition of chemohormonal agents. Patient summary: Patients with higher genomic classifier scores were found to have more metastatic lymph node involvement on prostate-specific membrane antigen imaging."
0,Bayesian Inference of Allelic Inclusion Rates in the Human T Cell Receptor Repertoire,"High-throughput single-cell sequencing methods allow for the identification of allelic inclusion T cell receptor (TCR) sequences, though experimental errors preclude direct measurement of dual receptor T cell rates. We develop and experimentally validate a statistical inference model in order to accurately estimate the rate of Î±Î±Î² and Î±Î²Î² allelic inclusion T cells. Our results show that approximately 15% of T cells express more than one unique TCR and suggest that these allelic inclusion cells represent a functionally important component of the human TCR repertoire.","Bayesian Inference of Allelic Inclusion Rates in the Human T Cell Receptor Repertoire. High-throughput single-cell sequencing methods allow for the identification of allelic inclusion T cell receptor (TCR) sequences, though experimental errors preclude direct measurement of dual receptor T cell rates. We develop and experimentally validate a statistical inference model in order to accurately estimate the rate of Î±Î±Î² and Î±Î²Î² allelic inclusion T cells. Our results show that approximately 15% of T cells express more than one unique TCR and suggest that these allelic inclusion cells represent a functionally important component of the human TCR repertoire."
0,AGA Clinical Practice Update on Diagnosis and Monitoring of Celiac Disease-Changing Utility of Serology and Histologic Measures: Expert Review,"PURPOSE: The purpose of this clinical practice update is to define key modalities in the diagnosis and monitoring of celiac disease (CD) in adults as well as in children and adolescents. METHODS: The recommendations outlined in this expert review are based on available published evidence, including cohort and case-control studies of the diagnostic process as well as controlled and descriptive studies of disease management. Best Practice Advice 1: Serology is a crucial component of the detection and diagnosis of CD, particularly tissue transglutaminase-immunoglobulin A (TG2-IgA), IgA testing, and less frequently, endomysial IgA testing. Best Practice Advice 2: Thorough histological analysis of duodenal biopsies with Marsh classification, counting of lymphocytes per high-power field, and morphometry is important for diagnosis as well as for differential diagnosis. Best Practice Advice 2a: TG2-IgA, at high levels (> x10 upper normal limit) is a reliable and accurate test for diagnosing active CD. When such a strongly positive TG2-IgA is combined with a positive endomysial antibody in a second blood sample, the positive predictive value for CD is virtually 100%. In adults, esophagogastroduodenoscopy (EGD) and duodenal biopsies may then be performed for purposes of differential diagnosis. Best Practice Advice 3: IgA deficiency is an infrequent but important explanation for why patients with CD may be negative on IgA isotype testing despite strong suspicion. Measuring total IgA levels, IgG deamidated gliadin antibody tests, and TG2-IgG testing in that circumstance is recommended. Best Practice Advice 4: IgG isotype testing for TG2 antibody is not specific in the absence of IgA deficiency. Best Practice Advice 5: In patients found to have CD first by intestinal biopsies, celiac-specific serology should be undertaken as a confirmatory test before initiation of a gluten-free diet (GFD). Best Practice Advice 6: In patients in whom CD is strongly suspected in the face of negative biopsies, TG2-IgA should still be performed and, if positive, repeat biopsies might be considered either at that time or sometime in the future. Best Practice Advice 7: Reduction or avoidance of gluten before diagnostic testing is discouraged, as it may reduce the sensitivity of both serology and biopsy testing. Best Practice Advice 8: When patients have already started on a GFD before diagnosis, we suggest that the patient go back on a normal diet with 3 slices of wheat bread daily preferably for 1 to 3 months before repeat determination of TG2-IgA. Best Practice Advice 9: Determination of HLA-DQ2/DQ8 has a limited role in the diagnosis of CD. Its value is largely related to its negative predictive value to rule out CD in patients who are seronegative in the face of histologic changes, in patients who did not have serologic confirmation at the time of diagnosis, and in those patients with a historic diagnosis of CD; especially as very young children before the introduction of celiac-specific serology. MANAGEMENT: Best Practice Advice 10: Celiac serology has a guarded role in the detection of continued intestinal injury, in particular as to sensitivity, as negative serology in a treated patient does not guarantee that the intestinal mucosa has healed. Persistently positive serology usually indicates ongoing intestinal damage and gluten exposure. Follow-up serology should be performed 6 and 12 months after diagnosis, and yearly thereafter. Best Practice Advice 11: Patients with persistent or relapsing symptoms, without other obvious explanations for those symptoms, should undergo endoscopic biopsies to determine healing even in the presence of negative TG2-IgA.","AGA Clinical Practice Update on Diagnosis and Monitoring of Celiac Disease-Changing Utility of Serology and Histologic Measures: Expert Review. PURPOSE: The purpose of this clinical practice update is to define key modalities in the diagnosis and monitoring of celiac disease (CD) in adults as well as in children and adolescents. METHODS: The recommendations outlined in this expert review are based on available published evidence, including cohort and case-control studies of the diagnostic process as well as controlled and descriptive studies of disease management. Best Practice Advice 1: Serology is a crucial component of the detection and diagnosis of CD, particularly tissue transglutaminase-immunoglobulin A (TG2-IgA), IgA testing, and less frequently, endomysial IgA testing. Best Practice Advice 2: Thorough histological analysis of duodenal biopsies with Marsh classification, counting of lymphocytes per high-power field, and morphometry is important for diagnosis as well as for differential diagnosis. Best Practice Advice 2a: TG2-IgA, at high levels (> x10 upper normal limit) is a reliable and accurate test for diagnosing active CD. When such a strongly positive TG2-IgA is combined with a positive endomysial antibody in a second blood sample, the positive predictive value for CD is virtually 100%. In adults, esophagogastroduodenoscopy (EGD) and duodenal biopsies may then be performed for purposes of differential diagnosis. Best Practice Advice 3: IgA deficiency is an infrequent but important explanation for why patients with CD may be negative on IgA isotype testing despite strong suspicion. Measuring total IgA levels, IgG deamidated gliadin antibody tests, and TG2-IgG testing in that circumstance is recommended. Best Practice Advice 4: IgG isotype testing for TG2 antibody is not specific in the absence of IgA deficiency. Best Practice Advice 5: In patients found to have CD first by intestinal biopsies, celiac-specific serology should be undertaken as a confirmatory test before initiation of a gluten-free diet (GFD). Best Practice Advice 6: In patients in whom CD is strongly suspected in the face of negative biopsies, TG2-IgA should still be performed and, if positive, repeat biopsies might be considered either at that time or sometime in the future. Best Practice Advice 7: Reduction or avoidance of gluten before diagnostic testing is discouraged, as it may reduce the sensitivity of both serology and biopsy testing. Best Practice Advice 8: When patients have already started on a GFD before diagnosis, we suggest that the patient go back on a normal diet with 3 slices of wheat bread daily preferably for 1 to 3 months before repeat determination of TG2-IgA. Best Practice Advice 9: Determination of HLA-DQ2/DQ8 has a limited role in the diagnosis of CD. Its value is largely related to its negative predictive value to rule out CD in patients who are seronegative in the face of histologic changes, in patients who did not have serologic confirmation at the time of diagnosis, and in those patients with a historic diagnosis of CD; especially as very young children before the introduction of celiac-specific serology. MANAGEMENT: Best Practice Advice 10: Celiac serology has a guarded role in the detection of continued intestinal injury, in particular as to sensitivity, as negative serology in a treated patient does not guarantee that the intestinal mucosa has healed. Persistently positive serology usually indicates ongoing intestinal damage and gluten exposure. Follow-up serology should be performed 6 and 12 months after diagnosis, and yearly thereafter. Best Practice Advice 11: Patients with persistent or relapsing symptoms, without other obvious explanations for those symptoms, should undergo endoscopic biopsies to determine healing even in the presence of negative TG2-IgA."
0,The role of AI in diagnosing lung diseases,,
0,An integrative bioinformatics pipeline to demonstrate the alteration of the interaction between the ALDH2*2 allele with NAD+ and Disulfiram,"Alcohol use disorder (AUD) is a multifactorial psychiatric behavior disorder. Disulfiram is the first approved drug by the Food and Drug Administration for alcohol-dependent patients, which targets the ALDH2 enzyme. Several genes are known to be involved in alcohol metabolism; mutations in any of these genes are known to be associated with AUD. The E504K mutation in the ALDH2 of the precursor protein or the E487K of the mature protein (E504K/E487K; ALDH2*2 allele) is carried by approximately 8% of the world population. In this study, we aimed to test the known inactive allele ALDH2*2, to validate the use of our extensive computational pipeline (in silico tools, molecular modeling, and molecular docking) for testing the interaction between the ALDH2*2 allele, NAD+, and Disulfiram. In silico predictions showed that the E504K variant of ALDH2 to be pathogenic and destabilizing with the maximum number of prediction in silico tools. Consequently, we studied the effect of this mutation mainly on the interaction between NAD+-E504K and Disulfiram-E504K complexes using molecular docking technique, and molecular dynamics (MD) analysis. From the molecular docking analysis with NAD+, we observed that the interaction affinity of the NAD+ decreases with the impact of E504K variant. On the other hand, the drug Disulfiram showed similar interaction in both the native and mutant ALDH2 proteins. Further, the comprehensive MD analysis predicted that the E504K destabilizes the protein and influences the NAD+ and Disulfiram interactions. Our findings reveal that the interaction of NAD+ to the protein is disturbed by the E504K/E487K variant whereas the drug Disulfiram has a similar effect as both native ALDH2 and ALDH2 bearing E504K/E487K variant. This study provides a platform to understand the effect of E504K/E487K on the molecular interaction with NAD+ and Disulfiram.","An integrative bioinformatics pipeline to demonstrate the alteration of the interaction between the ALDH2*2 allele with NAD+ and Disulfiram. Alcohol use disorder (AUD) is a multifactorial psychiatric behavior disorder. Disulfiram is the first approved drug by the Food and Drug Administration for alcohol-dependent patients, which targets the ALDH2 enzyme. Several genes are known to be involved in alcohol metabolism; mutations in any of these genes are known to be associated with AUD. The E504K mutation in the ALDH2 of the precursor protein or the E487K of the mature protein (E504K/E487K; ALDH2*2 allele) is carried by approximately 8% of the world population. In this study, we aimed to test the known inactive allele ALDH2*2, to validate the use of our extensive computational pipeline (in silico tools, molecular modeling, and molecular docking) for testing the interaction between the ALDH2*2 allele, NAD+, and Disulfiram. In silico predictions showed that the E504K variant of ALDH2 to be pathogenic and destabilizing with the maximum number of prediction in silico tools. Consequently, we studied the effect of this mutation mainly on the interaction between NAD+-E504K and Disulfiram-E504K complexes using molecular docking technique, and molecular dynamics (MD) analysis. From the molecular docking analysis with NAD+, we observed that the interaction affinity of the NAD+ decreases with the impact of E504K variant. On the other hand, the drug Disulfiram showed similar interaction in both the native and mutant ALDH2 proteins. Further, the comprehensive MD analysis predicted that the E504K destabilizes the protein and influences the NAD+ and Disulfiram interactions. Our findings reveal that the interaction of NAD+ to the protein is disturbed by the E504K/E487K variant whereas the drug Disulfiram has a similar effect as both native ALDH2 and ALDH2 bearing E504K/E487K variant. This study provides a platform to understand the effect of E504K/E487K on the molecular interaction with NAD+ and Disulfiram."
0,Association between Parapapillary Choroidal Vessel Density Measured with Optical Coherence Tomography Angiography and Future Visual Field Progression in Patients with Glaucoma,"Importance: Investigating the vascular risk factors of glaucoma progression is important to individualize treatment; however, few studies have investigated these factors because the available methods have proven insufficient to evaluate the vascular features of patients with glaucoma. Recently, the advent of optical coherence tomography angiography (OCT-A) allowed both qualitative and quantitative microvascular data to be obtained, to in turn evaluate the perfusion status of different retinal layers. Objective: To determine whether baseline parapapillary choroidal vessel density (VD) as measured by OCT-A was associated with future glaucoma progression. Design, Setting, and Participants: A prospective, observational, comparative study was conducted at Seoul St Mary's Hospital of The Catholic University of Korea from March 1, 2016, to December 31, 2018, for 108 glaucomatous eyes in which the retinal nerve fiber layer thickness and mean deviation were measured by at least 5 serial OCT and visual field (VF) examinations. The participants underwent OCT-A at baseline. Vessel density was measured using the en face image of the choroidal map of OCT-A within the Î²-zone parapapillary atrophy region. Main Outcomes and Measures: Parapapillary choroidal VD, retinal nerve fiber layer thinning rate, mean deviation rate, and progression of glaucoma as measured by OCT and VF. Results: Among 108 patients (74 women and 34 men; mean [SD] age, 59.2 [13.1] years), 38 (35.2%) showed progression of glaucoma as measured by OCT and 34 (31.5%) showed progression of glaucoma as measured by VF at the last follow-up. The mean (SD) follow-up duration was 2.6 [2.3] years. The presence of disc hemorrhage (odds ratio, 5.57; 95% CI, 3.18-8.29; P =.001), baseline mean deviation (odds ratio, 0.83; 95% CI, 0.71-0.97; P =.02), and parapapillary choroidal VD (odds ratio, 1.18; 95% CI, 1.09-1.28; P =.01) were associated with progression of glaucoma as measured by VF, but not with progression of glaucoma as measured by OCT. Baseline parapapillary choroidal VD (Î², 1.08; 95% CI, 1.02-1.13; P <.001) was associated with progression of glaucoma as measured by VF using Cox proportional hazards regression analysis. Conclusions and Relevance: These data suggest that lower parapapillary choroidal VD within the Î²-zone parapapillary atrophy at baseline among individuals with glaucoma could play some role in the risk of progression of glaucoma as measured by VF. The findings suggest that patients with glaucoma with lower parapapillary choroidal VD within the Î²-zone parapapillary atrophy at baseline warrant careful monitoring for progression of glaucoma as measured by VF.","Association between Parapapillary Choroidal Vessel Density Measured with Optical Coherence Tomography Angiography and Future Visual Field Progression in Patients with Glaucoma. Importance: Investigating the vascular risk factors of glaucoma progression is important to individualize treatment; however, few studies have investigated these factors because the available methods have proven insufficient to evaluate the vascular features of patients with glaucoma. Recently, the advent of optical coherence tomography angiography (OCT-A) allowed both qualitative and quantitative microvascular data to be obtained, to in turn evaluate the perfusion status of different retinal layers. Objective: To determine whether baseline parapapillary choroidal vessel density (VD) as measured by OCT-A was associated with future glaucoma progression. Design, Setting, and Participants: A prospective, observational, comparative study was conducted at Seoul St Mary's Hospital of The Catholic University of Korea from March 1, 2016, to December 31, 2018, for 108 glaucomatous eyes in which the retinal nerve fiber layer thickness and mean deviation were measured by at least 5 serial OCT and visual field (VF) examinations. The participants underwent OCT-A at baseline. Vessel density was measured using the en face image of the choroidal map of OCT-A within the Î²-zone parapapillary atrophy region. Main Outcomes and Measures: Parapapillary choroidal VD, retinal nerve fiber layer thinning rate, mean deviation rate, and progression of glaucoma as measured by OCT and VF. Results: Among 108 patients (74 women and 34 men; mean [SD] age, 59.2 [13.1] years), 38 (35.2%) showed progression of glaucoma as measured by OCT and 34 (31.5%) showed progression of glaucoma as measured by VF at the last follow-up. The mean (SD) follow-up duration was 2.6 [2.3] years. The presence of disc hemorrhage (odds ratio, 5.57; 95% CI, 3.18-8.29; P =.001), baseline mean deviation (odds ratio, 0.83; 95% CI, 0.71-0.97; P =.02), and parapapillary choroidal VD (odds ratio, 1.18; 95% CI, 1.09-1.28; P =.01) were associated with progression of glaucoma as measured by VF, but not with progression of glaucoma as measured by OCT. Baseline parapapillary choroidal VD (Î², 1.08; 95% CI, 1.02-1.13; P <.001) was associated with progression of glaucoma as measured by VF using Cox proportional hazards regression analysis. Conclusions and Relevance: These data suggest that lower parapapillary choroidal VD within the Î²-zone parapapillary atrophy at baseline among individuals with glaucoma could play some role in the risk of progression of glaucoma as measured by VF. The findings suggest that patients with glaucoma with lower parapapillary choroidal VD within the Î²-zone parapapillary atrophy at baseline warrant careful monitoring for progression of glaucoma as measured by VF."
0,Altered NFE2 activity predisposes to leukemic transformation and myelosarcoma with AML-specific aberrations,,
0,Mortality and Health Care Utilization Among Medicare Patients Undergoing Emergency General Surgery vs Those With Acute Medical Conditions,,
0,Characterization of the thickness of the tear film lipid layer using high resolution microscopy,,
0,Robust motion correction for cardiac T1 and ECV mapping using a T1 relaxation model approach,"T1 and ECV mapping are quantitative methods for myocardial tissue characterization using cardiac MRI, and are highly relevant for the diagnosis of diffuse myocardial diseases. Since the maps are calculated pixel-by-pixel from a set of MRI images with different T1-weighting, it is critical to assure exact spatial correspondence between these images. However, in practice, different sources of motion e.g. cardiac motion, respiratory motion or patient motion, hamper accurate T1 and ECV calculation such that retrospective motion correction is required. We propose a new robust non-rigid registration framework combining a data-driven initialization with a model-based registration approach, which uses a model for T1 relaxation to avoid direct registration of images with highly varying contrast. The registration between native T1 and enhanced T1 to obtain a motion free ECV map is also calculated using information from T1 model-fitting. The method was validated on three datasets recorded with two substantially different acquisition protocols (MOLLI (dataset 1 (n=15) and dataset 2 (n=29)) and STONE (dataset 3 (n = 210))), one in breath-hold condition and one free-breathing. The average Dice coefficient increased from 72.6+/-12.1% to 82.3+/-7.4% (P<0.05) and mean boundary error decreased from 2.91+/-1.51mm to 1.62+/-0.80mm (P<0.05) for motion correction in a single T1-weighted image sequence (3 datasets) while average Dice coefficient increased from 63.4+/-22.5% to 79.2+/-8.5% (P<0.05) and mean boundary error decreased from 3.26+/-2.64mm to 1.77+/-0.86mm (P<0.05) between native and enhanced sequences (dataset 1 and 2). Overall, the native T1 SD error decreased from 67.32+/-32.57ms to 58.11+/-21.59ms (P<0.05), enhanced SD error from 30.15+/-25ms to 22.74+/-8.94ms (P<0.05) and ECV SD error from 10.08+/-9.59% to 5.42+/-3.21% (P<0.05) (dataset 1 and 2).","Robust motion correction for cardiac T1 and ECV mapping using a T1 relaxation model approach. T1 and ECV mapping are quantitative methods for myocardial tissue characterization using cardiac MRI, and are highly relevant for the diagnosis of diffuse myocardial diseases. Since the maps are calculated pixel-by-pixel from a set of MRI images with different T1-weighting, it is critical to assure exact spatial correspondence between these images. However, in practice, different sources of motion e.g. cardiac motion, respiratory motion or patient motion, hamper accurate T1 and ECV calculation such that retrospective motion correction is required. We propose a new robust non-rigid registration framework combining a data-driven initialization with a model-based registration approach, which uses a model for T1 relaxation to avoid direct registration of images with highly varying contrast. The registration between native T1 and enhanced T1 to obtain a motion free ECV map is also calculated using information from T1 model-fitting. The method was validated on three datasets recorded with two substantially different acquisition protocols (MOLLI (dataset 1 (n=15) and dataset 2 (n=29)) and STONE (dataset 3 (n = 210))), one in breath-hold condition and one free-breathing. The average Dice coefficient increased from 72.6+/-12.1% to 82.3+/-7.4% (P<0.05) and mean boundary error decreased from 2.91+/-1.51mm to 1.62+/-0.80mm (P<0.05) for motion correction in a single T1-weighted image sequence (3 datasets) while average Dice coefficient increased from 63.4+/-22.5% to 79.2+/-8.5% (P<0.05) and mean boundary error decreased from 3.26+/-2.64mm to 1.77+/-0.86mm (P<0.05) between native and enhanced sequences (dataset 1 and 2). Overall, the native T1 SD error decreased from 67.32+/-32.57ms to 58.11+/-21.59ms (P<0.05), enhanced SD error from 30.15+/-25ms to 22.74+/-8.94ms (P<0.05) and ECV SD error from 10.08+/-9.59% to 5.42+/-3.21% (P<0.05) (dataset 1 and 2)."
0,Contrasting Roles of Transcription Factors Spineless and EcR in the Highly Dynamic Chromatin Landscape of Butterfly Wing Metamorphosis,"Development requires highly coordinated changes in chromatin accessibility in order for proper gene regulation to occur. Here, we identify factors associated with major, discrete changes in chromatin accessibility during butterfly wing metamorphosis. By combining mRNA sequencing (mRNA-seq), assay for transposase-accessible chromatin using sequencing (ATAC-seq), and machine learning analysis of motifs, we show that distinct sets of transcription factors are predictive of chromatin opening at different developmental stages. Our data suggest an important role for nuclear hormone receptors early in metamorphosis, whereas PAS-domain transcription factors are strongly associated with later chromatin opening. Chromatin immunoprecipitation sequencing (ChIP-seq) validation of select candidate factors showed spineless binding to be a major predictor of opening chromatin. Surprisingly, binding of ecdysone receptor (EcR), a candidate accessibility factor in Drosophila, was not predictive of opening but instead marked persistent sites. This work characterizes the chromatin dynamics of insect wing metamorphosis, identifies candidate chromatin remodeling factors in insects, and presents a genome assembly of the model butterfly Junonia coenia.","Contrasting Roles of Transcription Factors Spineless and EcR in the Highly Dynamic Chromatin Landscape of Butterfly Wing Metamorphosis. Development requires highly coordinated changes in chromatin accessibility in order for proper gene regulation to occur. Here, we identify factors associated with major, discrete changes in chromatin accessibility during butterfly wing metamorphosis. By combining mRNA sequencing (mRNA-seq), assay for transposase-accessible chromatin using sequencing (ATAC-seq), and machine learning analysis of motifs, we show that distinct sets of transcription factors are predictive of chromatin opening at different developmental stages. Our data suggest an important role for nuclear hormone receptors early in metamorphosis, whereas PAS-domain transcription factors are strongly associated with later chromatin opening. Chromatin immunoprecipitation sequencing (ChIP-seq) validation of select candidate factors showed spineless binding to be a major predictor of opening chromatin. Surprisingly, binding of ecdysone receptor (EcR), a candidate accessibility factor in Drosophila, was not predictive of opening but instead marked persistent sites. This work characterizes the chromatin dynamics of insect wing metamorphosis, identifies candidate chromatin remodeling factors in insects, and presents a genome assembly of the model butterfly Junonia coenia."
0,Computational aided mechanistic understanding of Camellia sinensis bioactive compounds against co-chaperone p23 as potential anticancer agent,"Co-chaperon p23 has been well established as molecular chaperon for the heat shock protein 90 (Hsp90) that further leads to immorality in cancer cells by providing defense against Hsp90 inhibitors, and as stimulating agent for generating overexpressed antiapoptotic proteins, that is, Hsp70 and Hsp27. The natural compounds such as catechins from Camellia sinensis (green tea) are also well known for inhibition activity against various cancer. However, molecular interaction profile and potential lead bioactive compounds against co-chaperon p23 from green tea are not yet reported. To this context, we study the various secondary metabolites of green tea against co-chaperon p23 using structure-based virtual screening from Traditional Chinese Medicine (TCM) database. Following 26 compounds were obtained from TCM database and further studied for extra precision molecular docking that showed binding score between âˆ’10.221 and âˆ’2.276 kcal/mol with co-chaperon p23. However, relative docking score to known inhibitors, that is, ailanthone (âˆ’4.54 kcal/mol) and gedunin (3.60 kcal/mol) along with ADME profile analysis concluded epicatechin (âˆ’7.013 kcal/mol) and cis-theaspirone (âˆ’4.495 kcal/mol) as potential lead inhibitors from green tea against co-chaperone p23. Furthermore, molecular dynamics simulation and molecular mechanics generalized born surface area calculations validated that epicatechin and cis-theaspirone have significantly occupied the active region of co-chaperone p23 by hydrogen and hydrophobic interactions with various residues including most substantial amino acids, that is, Thr90, Ala94, and Lys95. Hence, these results supported the fact that green tea contained potential compounds with an ability to inhibit the cancer by disrupting the co-chaperon p23 activity.","Computational aided mechanistic understanding of Camellia sinensis bioactive compounds against co-chaperone p23 as potential anticancer agent. Co-chaperon p23 has been well established as molecular chaperon for the heat shock protein 90 (Hsp90) that further leads to immorality in cancer cells by providing defense against Hsp90 inhibitors, and as stimulating agent for generating overexpressed antiapoptotic proteins, that is, Hsp70 and Hsp27. The natural compounds such as catechins from Camellia sinensis (green tea) are also well known for inhibition activity against various cancer. However, molecular interaction profile and potential lead bioactive compounds against co-chaperon p23 from green tea are not yet reported. To this context, we study the various secondary metabolites of green tea against co-chaperon p23 using structure-based virtual screening from Traditional Chinese Medicine (TCM) database. Following 26 compounds were obtained from TCM database and further studied for extra precision molecular docking that showed binding score between âˆ’10.221 and âˆ’2.276 kcal/mol with co-chaperon p23. However, relative docking score to known inhibitors, that is, ailanthone (âˆ’4.54 kcal/mol) and gedunin (3.60 kcal/mol) along with ADME profile analysis concluded epicatechin (âˆ’7.013 kcal/mol) and cis-theaspirone (âˆ’4.495 kcal/mol) as potential lead inhibitors from green tea against co-chaperone p23. Furthermore, molecular dynamics simulation and molecular mechanics generalized born surface area calculations validated that epicatechin and cis-theaspirone have significantly occupied the active region of co-chaperone p23 by hydrogen and hydrophobic interactions with various residues including most substantial amino acids, that is, Thr90, Ala94, and Lys95. Hence, these results supported the fact that green tea contained potential compounds with an ability to inhibit the cancer by disrupting the co-chaperon p23 activity."
0,Childhood Asthma: Advances Using Machine Learning and Mechanistic Studies,"A paradigm shift brought by the recognition that childhood asthma is an aggregated diagnosis that comprises several different endotypes underpinned by different pathophysiology, coupled with advances in understanding potentially important causal mechanisms, offers a real opportunity for a step change to reduce the burden of the disease on individual children, families, and society. Data-driven methodologies facilitate the discovery of ""hidden"" structures within ""big healthcare data"" to help generate new hypotheses. These findings can be translated into clinical practice by linking discovered ""phenotypes"" to specific mechanisms and clinical presentations. Epidemiological studies have provided important clues about mechanistic avenues that should be pursued to identify interventions to prevent the development or alter the natural history of asthma-related diseases. Findings from cohort studies followed by mechanistic studies in humans and in neonatal mouse models provided evidence that environments such as traditional farming may offer protection by modulating innate immune responses and that impaired innate immunity may increase susceptibility. The key question of which component of these exposures can be translated into interventions requires confirmation. Increasing mechanistic evidence is demonstrating that shaping the microbiome in early life may modulate immune function to confer protection. Iterative dialogue and continuous interaction between experts with different but complementary skill sets, including data scientists who generate information about the hidden structures within ""big data"" assets, and medical professionals, epidemiologists, basic scientists, and geneticists who provide critical clinical and mechanistic insights about the mechanisms underpinning the architecture of the heterogeneity, are keys to delivering mechanism-based stratified treatments and prevention.","Childhood Asthma: Advances Using Machine Learning and Mechanistic Studies. A paradigm shift brought by the recognition that childhood asthma is an aggregated diagnosis that comprises several different endotypes underpinned by different pathophysiology, coupled with advances in understanding potentially important causal mechanisms, offers a real opportunity for a step change to reduce the burden of the disease on individual children, families, and society. Data-driven methodologies facilitate the discovery of ""hidden"" structures within ""big healthcare data"" to help generate new hypotheses. These findings can be translated into clinical practice by linking discovered ""phenotypes"" to specific mechanisms and clinical presentations. Epidemiological studies have provided important clues about mechanistic avenues that should be pursued to identify interventions to prevent the development or alter the natural history of asthma-related diseases. Findings from cohort studies followed by mechanistic studies in humans and in neonatal mouse models provided evidence that environments such as traditional farming may offer protection by modulating innate immune responses and that impaired innate immunity may increase susceptibility. The key question of which component of these exposures can be translated into interventions requires confirmation. Increasing mechanistic evidence is demonstrating that shaping the microbiome in early life may modulate immune function to confer protection. Iterative dialogue and continuous interaction between experts with different but complementary skill sets, including data scientists who generate information about the hidden structures within ""big data"" assets, and medical professionals, epidemiologists, basic scientists, and geneticists who provide critical clinical and mechanistic insights about the mechanisms underpinning the architecture of the heterogeneity, are keys to delivering mechanism-based stratified treatments and prevention."
0,Algorithm for Resecting Hepatocellular Carcinoma in the Caudate Lobe,"OBJECTIVE: To propose an algorithm for resecting hepatocellular carcinoma (HCC) in the caudate lobe. BACKGROUND: Owing to a deep location, resection of HCC originating in the caudate lobe is challenging, but a plausible guideline enabling safe, curable resection remains unknown. METHODS: We developed an algorithm based on sublocation or size of the tumor and liver function to guide the optimal procedure for resecting HCC in the caudate lobe, consisting of 3 portions (Spiegel, process, and caval). Partial resection was prioritized to remove Spiegel or process HCC, while total resection was aimed to remove caval HCC depending on liver function. RESULTS: According to the algorithm, we performed total (n = 43) or partial (n = 158) resections of the caudate lobe for HCC in 174 of 201 patients (compliance rate, 86.6%), with a median blood loss of 400 (10-4530) mL. Postoperative morbidity (Clavien grade >/=III b) and mortality rates were 3.0% and 0%, respectively. After a median follow-up of 2.6 years (range, 0.5-14.3), the 5-year overall and recurrence-free survival rates were 57.3% and 15.3%, respectively. Total and partial resection showed no significant difference in overall survival (71.2% vs 54.0% at 5 yr; P = 0.213), but a significant factor in survival was surgical margin (58.0% vs 45.6%, P = 0.034). The major determinant for survival was vascular invasion (hazard ratio 1.7, 95% CI 1.0-3.1, P = 0.026). CONCLUSIONS: Our algorithm-oriented strategy is appropriate for the resection of HCC originating in the caudate lobe because of the acceptable surgical safety and curability.","Algorithm for Resecting Hepatocellular Carcinoma in the Caudate Lobe. OBJECTIVE: To propose an algorithm for resecting hepatocellular carcinoma (HCC) in the caudate lobe. BACKGROUND: Owing to a deep location, resection of HCC originating in the caudate lobe is challenging, but a plausible guideline enabling safe, curable resection remains unknown. METHODS: We developed an algorithm based on sublocation or size of the tumor and liver function to guide the optimal procedure for resecting HCC in the caudate lobe, consisting of 3 portions (Spiegel, process, and caval). Partial resection was prioritized to remove Spiegel or process HCC, while total resection was aimed to remove caval HCC depending on liver function. RESULTS: According to the algorithm, we performed total (n = 43) or partial (n = 158) resections of the caudate lobe for HCC in 174 of 201 patients (compliance rate, 86.6%), with a median blood loss of 400 (10-4530) mL. Postoperative morbidity (Clavien grade >/=III b) and mortality rates were 3.0% and 0%, respectively. After a median follow-up of 2.6 years (range, 0.5-14.3), the 5-year overall and recurrence-free survival rates were 57.3% and 15.3%, respectively. Total and partial resection showed no significant difference in overall survival (71.2% vs 54.0% at 5 yr; P = 0.213), but a significant factor in survival was surgical margin (58.0% vs 45.6%, P = 0.034). The major determinant for survival was vascular invasion (hazard ratio 1.7, 95% CI 1.0-3.1, P = 0.026). CONCLUSIONS: Our algorithm-oriented strategy is appropriate for the resection of HCC originating in the caudate lobe because of the acceptable surgical safety and curability."
0,Hierarchical spherical deformation for cortical surface registration,,
0,Anchoring of actin to the plasma membrane enables tension production in the fission yeast cytokinetic ring,"The cytokinetic ring generates tensile force that drives cell division, but how tension emerges from the relatively disordered ring organization remains unclear. Long ago, a musclelike sliding filament mechanism was proposed, but evidence for sarcomeric order is lacking. Here we present quantitative evidence that in fission yeast, ring tension originates from barbed-end anchoring of actin filaments to the plasma membrane, providing resistance to myosin forces that enables filaments to develop tension. The role of anchoring was highlighted by experiments on isolated fission yeast rings, where sections of ring became unanchored from the membrane and shortened âˆ¼30-fold faster than normal. The dramatically elevated constriction rates are unexplained. Here we present a molecularly explicit simulation of constricting partially anchored rings as studied in these experiments. Simulations accurately reproduced the experimental constriction rates and showed that following anchor release, a segment becomes tensionless and shortens via a novel noncontractile reeling-in mechanism at about the velocity of load-free myosin II. The ends are reeled in by barbed endâ€“anchored actin filaments in adjacent segments. Other actin anchoring schemes failed to constrict rings. Our results quantitatively support a specific organization and anchoring scheme that generate tension in the cytokinetic ring.","Anchoring of actin to the plasma membrane enables tension production in the fission yeast cytokinetic ring. The cytokinetic ring generates tensile force that drives cell division, but how tension emerges from the relatively disordered ring organization remains unclear. Long ago, a musclelike sliding filament mechanism was proposed, but evidence for sarcomeric order is lacking. Here we present quantitative evidence that in fission yeast, ring tension originates from barbed-end anchoring of actin filaments to the plasma membrane, providing resistance to myosin forces that enables filaments to develop tension. The role of anchoring was highlighted by experiments on isolated fission yeast rings, where sections of ring became unanchored from the membrane and shortened âˆ¼30-fold faster than normal. The dramatically elevated constriction rates are unexplained. Here we present a molecularly explicit simulation of constricting partially anchored rings as studied in these experiments. Simulations accurately reproduced the experimental constriction rates and showed that following anchor release, a segment becomes tensionless and shortens via a novel noncontractile reeling-in mechanism at about the velocity of load-free myosin II. The ends are reeled in by barbed endâ€“anchored actin filaments in adjacent segments. Other actin anchoring schemes failed to constrict rings. Our results quantitatively support a specific organization and anchoring scheme that generate tension in the cytokinetic ring."
0,Identification of a 26-lncRNAs Risk Model for Predicting Overall Survival of Cervical Squamous Cell Carcinoma Based on Integrated Bioinformatics Analysis,"As a common malignancy in women, cervical squamous cell carcinoma is a major cause of cancer-related mortality globally. Recent studies have demonstrated that long non-coding RNA (lncRNA) can function as potential biomarkers in cancer prognosis; however, little is known about its role in cervical cancer. In this study, we downloaded the gene expression profiles along with the clinical data of patients with cervical squamous cell carcinoma from The Cancer Genome Atlas. By applying bioinformatics analysis including random forest selection and Least Absolute Shrinkage and Selection Operator (LASSO) cox regression model along with 10-fold cross-validation, we constructed a 26-lncRNAs risk model that can be used to predict the overall survival of cervical squamous cell carcinoma. After that, Kaplan-Meier analysis combined with log-rank p test was applied to assess the predictive accuracy of the 26-lncRNAs risk model. Further analysis showed that the prognostic value of 26-lncRNAs risk model was independent of other clinicopathological factors. At last, lncRNAs in the model were put into gene ontology biological process enrichment and Kyoto Encyclopedia of Genes and Genomes (KEGG) signaling pathways analysis, which suggested that these lncRNAs might contribute to cancer-associated processes such as cell cycle and apoptosis. This study indicated that lncRNAs signature could be a useful marker to predict the prognosis of cervical squamous cell carcinoma.","Identification of a 26-lncRNAs Risk Model for Predicting Overall Survival of Cervical Squamous Cell Carcinoma Based on Integrated Bioinformatics Analysis. As a common malignancy in women, cervical squamous cell carcinoma is a major cause of cancer-related mortality globally. Recent studies have demonstrated that long non-coding RNA (lncRNA) can function as potential biomarkers in cancer prognosis; however, little is known about its role in cervical cancer. In this study, we downloaded the gene expression profiles along with the clinical data of patients with cervical squamous cell carcinoma from The Cancer Genome Atlas. By applying bioinformatics analysis including random forest selection and Least Absolute Shrinkage and Selection Operator (LASSO) cox regression model along with 10-fold cross-validation, we constructed a 26-lncRNAs risk model that can be used to predict the overall survival of cervical squamous cell carcinoma. After that, Kaplan-Meier analysis combined with log-rank p test was applied to assess the predictive accuracy of the 26-lncRNAs risk model. Further analysis showed that the prognostic value of 26-lncRNAs risk model was independent of other clinicopathological factors. At last, lncRNAs in the model were put into gene ontology biological process enrichment and Kyoto Encyclopedia of Genes and Genomes (KEGG) signaling pathways analysis, which suggested that these lncRNAs might contribute to cancer-associated processes such as cell cycle and apoptosis. This study indicated that lncRNAs signature could be a useful marker to predict the prognosis of cervical squamous cell carcinoma."
0,Q4 Vital Signs: Can Machine Learning Protect Patients From the Machinery of Modern Medicine?,,
0,Adjuvant Endocrine Therapy for Women With Hormone Receptor-Positive Breast Cancer: ASCO Clinical Practice Guideline Focused Update,,
0,miR-148-3p and miR-152-3p synergistically regulate prostate cancer progression via repressing KLF4,"Background: miR-148-3p and miR-152-3p as the tumor suppressors have been reported in various cancer types. Our study is aimed to discuss the synergistic effect of miR-148-3p and miR-152-3p in prostate cancer (PCa). Methods: Bioinformatics algorithm and luciferase reporter assays were used to verify whether miR-148-3p and 152-3p could bind with the 3â€²-untranslated region (3â€²-UTR) of Kruppel-like factor 4 (KLF4). PCa cell growth in vivo was analyzed using the mouse xenograft tumor model. Results: miR-148-3p and miR-152-3p were reduced in PCa tumor tissues. Moreover, the protein expression of KLF4 was increased in PCa tissues. The 3â€²-UTR of KLF4 contained the conserved binding sites with miR-148-3p and miR-152-3p. The mimics or inhibitors of miR-148-3p and/or miR-152-3p could downregulated or upregulated KLF4 expression, respectively. miR-148-3p and miR-152-3p-induced PCa cell growth inhibition were observed both in vivo and in vitro. KLF4 overexpression had the ability to neutralize the antitumor effect of miR-148-3p/152-3p in vivo and in vitro. Conclusion: miR-148-3p/152-3p family could serve as tumor suppressors in PCa via repressing KLF4.","miR-148-3p and miR-152-3p synergistically regulate prostate cancer progression via repressing KLF4. Background: miR-148-3p and miR-152-3p as the tumor suppressors have been reported in various cancer types. Our study is aimed to discuss the synergistic effect of miR-148-3p and miR-152-3p in prostate cancer (PCa). Methods: Bioinformatics algorithm and luciferase reporter assays were used to verify whether miR-148-3p and 152-3p could bind with the 3â€²-untranslated region (3â€²-UTR) of Kruppel-like factor 4 (KLF4). PCa cell growth in vivo was analyzed using the mouse xenograft tumor model. Results: miR-148-3p and miR-152-3p were reduced in PCa tumor tissues. Moreover, the protein expression of KLF4 was increased in PCa tissues. The 3â€²-UTR of KLF4 contained the conserved binding sites with miR-148-3p and miR-152-3p. The mimics or inhibitors of miR-148-3p and/or miR-152-3p could downregulated or upregulated KLF4 expression, respectively. miR-148-3p and miR-152-3p-induced PCa cell growth inhibition were observed both in vivo and in vitro. KLF4 overexpression had the ability to neutralize the antitumor effect of miR-148-3p/152-3p in vivo and in vitro. Conclusion: miR-148-3p/152-3p family could serve as tumor suppressors in PCa via repressing KLF4."
0,Is 're-calibration' of standard cardiovascular disease (CVD) risk algorithms the panacea to improved CVD risk prediction and prevention?,,
0,Alisol A attenuates high-fat-diet-induced obesity and metabolic disorders via the AMPK/ACC/SREBP-1c pathway,"Obesity and its associated metabolic disorders such as diabetes, hepatic steatosis and chronic heart diseases are affecting billions of individuals. However there is no satisfactory drug to treat such diseases. In this study, we found that alisol A, a major active triterpene isolated from the Chinese traditional medicine Rhizoma Alismatis, could significantly attenuate high-fat-diet-induced obesity. Our biochemical detection demonstrated that alisol A remarkably decreased lipid levels, alleviated glucose metabolism disorders and insulin resistance in high-fat-diet-induced obese mice. We also found that alisol A reduced hepatic steatosis and improved liver function in the obese mice model.In addition, protein expression investigation revealed that alisol A had an active effect on AMPK/ACC/SREBP-1c pathway. As suggested by the molecular docking study, such bioactivity of alisol A may result from its selective binding to the catalytic region of AMPK.Therefore, we believe that Alisol A could serve as a promising agent for treatment of obesity and its related metabolic diseases.","Alisol A attenuates high-fat-diet-induced obesity and metabolic disorders via the AMPK/ACC/SREBP-1c pathway. Obesity and its associated metabolic disorders such as diabetes, hepatic steatosis and chronic heart diseases are affecting billions of individuals. However there is no satisfactory drug to treat such diseases. In this study, we found that alisol A, a major active triterpene isolated from the Chinese traditional medicine Rhizoma Alismatis, could significantly attenuate high-fat-diet-induced obesity. Our biochemical detection demonstrated that alisol A remarkably decreased lipid levels, alleviated glucose metabolism disorders and insulin resistance in high-fat-diet-induced obese mice. We also found that alisol A reduced hepatic steatosis and improved liver function in the obese mice model.In addition, protein expression investigation revealed that alisol A had an active effect on AMPK/ACC/SREBP-1c pathway. As suggested by the molecular docking study, such bioactivity of alisol A may result from its selective binding to the catalytic region of AMPK.Therefore, we believe that Alisol A could serve as a promising agent for treatment of obesity and its related metabolic diseases."
0,Learning the language of splicing,,
0,A robust fuzzy rule based integrative feature selection strategy for gene expression data in TCGA,"Background: Lots of researches have been conducted in the selection of gene signatures that could distinguish the cancer patients from the normal. However, it is still an open question on how to extract the robust gene features. Methods: In this work, a gene signature selection strategy for TCGA data was proposed by integrating the gene expression data, the methylation data and the prior knowledge about cancer biomarkers. Different from the traditional integration method, the expanded 450 K methylation data were applied instead of the original 450 K array data, and the reported biomarkers were weighted in the feature selection. Fuzzy rule based classification method and cross validation strategy were applied in the model construction for performance evaluation. Results: Our selected gene features showed prediction accuracy close to 100% in the cross validation with fuzzy rule based classification model on 6 cancers from TCGA. The cross validation performance of our proposed model is similar to other integrative models or RNA-seq only model, while the prediction performance on independent data is obviously better than other 5 models. The gene signatures extracted with our fuzzy rule based integrative feature selection strategy were more robust, and had the potential to get better prediction results. Conclusion: The results indicated that the integration of expanded methylation data would cover more genes, and had greater capacity to retrieve the signature genes compared with the original 450 K methylation data. Also, the integration of the reported biomarkers was a promising way to improve the performance. PTCHD3 gene was selected as a discriminating gene in 3 out of the 6 cancers, which suggested that it might play important role in the cancer risk and would be worthy for the intensive investigation.","A robust fuzzy rule based integrative feature selection strategy for gene expression data in TCGA. Background: Lots of researches have been conducted in the selection of gene signatures that could distinguish the cancer patients from the normal. However, it is still an open question on how to extract the robust gene features. Methods: In this work, a gene signature selection strategy for TCGA data was proposed by integrating the gene expression data, the methylation data and the prior knowledge about cancer biomarkers. Different from the traditional integration method, the expanded 450 K methylation data were applied instead of the original 450 K array data, and the reported biomarkers were weighted in the feature selection. Fuzzy rule based classification method and cross validation strategy were applied in the model construction for performance evaluation. Results: Our selected gene features showed prediction accuracy close to 100% in the cross validation with fuzzy rule based classification model on 6 cancers from TCGA. The cross validation performance of our proposed model is similar to other integrative models or RNA-seq only model, while the prediction performance on independent data is obviously better than other 5 models. The gene signatures extracted with our fuzzy rule based integrative feature selection strategy were more robust, and had the potential to get better prediction results. Conclusion: The results indicated that the integration of expanded methylation data would cover more genes, and had greater capacity to retrieve the signature genes compared with the original 450 K methylation data. Also, the integration of the reported biomarkers was a promising way to improve the performance. PTCHD3 gene was selected as a discriminating gene in 3 out of the 6 cancers, which suggested that it might play important role in the cancer risk and would be worthy for the intensive investigation."
0,Peptide selected by phage display increases survival of SH-SY5Y neurons comparable to brain-derived neurotrophic factor,"Brain-derived neurotrophic factor (BDNF) is a well-known neuroprotectant and a potent therapeutic candidate for neurodegenerative diseases. However, there are several clinical concerns about its therapeutic applications. In the current study, we designed and developed BDNF-mimicking small peptides as an alternative to circumvent these problems. A phage-displayed peptide library was screened using BDNF receptor (neurotrophic tyrosine kinase receptor type2 [NTRK2]) and evaluated by ELISA. The peptide sequences showed similarity to loop2 of BDNF, they were recognized as discontinuous epitopes though. Interestingly, in silico molecular docking showed strong interactions between the peptide three-dimensional models and the surface residues of the NTRK2 protein at the IgC2 domain. A consensus peptide sequence was then synthesized to generate a mimetic construct (named as RNYK). The affinity binding and function of this construct was confirmed by testing against the native structure of NTRK2 in SH-SY5Y cells in vitro using flow-cytometry and MTT assays, respectively. RNYK at 5 ng/mL prevented neuronal degeneration of all- trans-retinoic acid-treated SH-SY5Y with equal efficacy to or even better than BDNF at 50 ng/mL.","Peptide selected by phage display increases survival of SH-SY5Y neurons comparable to brain-derived neurotrophic factor. Brain-derived neurotrophic factor (BDNF) is a well-known neuroprotectant and a potent therapeutic candidate for neurodegenerative diseases. However, there are several clinical concerns about its therapeutic applications. In the current study, we designed and developed BDNF-mimicking small peptides as an alternative to circumvent these problems. A phage-displayed peptide library was screened using BDNF receptor (neurotrophic tyrosine kinase receptor type2 [NTRK2]) and evaluated by ELISA. The peptide sequences showed similarity to loop2 of BDNF, they were recognized as discontinuous epitopes though. Interestingly, in silico molecular docking showed strong interactions between the peptide three-dimensional models and the surface residues of the NTRK2 protein at the IgC2 domain. A consensus peptide sequence was then synthesized to generate a mimetic construct (named as RNYK). The affinity binding and function of this construct was confirmed by testing against the native structure of NTRK2 in SH-SY5Y cells in vitro using flow-cytometry and MTT assays, respectively. RNYK at 5 ng/mL prevented neuronal degeneration of all- trans-retinoic acid-treated SH-SY5Y with equal efficacy to or even better than BDNF at 50 ng/mL."
0,Fast read alignment with incorporation of known genomic variants,"BACKGROUND: Many genetic variants have been reported from sequencing projects due to decreasing experimental costs. Compared to the current typical paradigm, read mapping incorporating existing variants can improve the performance of subsequent analysis. This method is supposed to map sequencing reads efficiently to a graphical index with a reference genome and known variation to increase alignment quality and variant calling accuracy. However, storing and indexing various types of variation require costly RAM space. METHODS: Aligning reads to a graph model-based index including the whole set of variants is ultimately an NP-hard problem in theory. Here, we propose a variation-aware read alignment algorithm (VARA), which generates the alignment between read and multiple genomic sequences simultaneously utilizing the schema of the Landau-Vishkin algorithm. VARA dynamically extracts regional variants to construct a pseudo tree-based structure on-the-fly for seed extension without loading the whole genome variation into memory space. RESULTS: We developed the novel high-throughput sequencing read aligner deBGA-VARA by integrating VARA into deBGA. The deBGA-VARA is benchmarked both on simulated reads and the NA12878 sequencing dataset. The experimental results demonstrate that read alignment incorporating genetic variation knowledge can achieve high sensitivity and accuracy. CONCLUSIONS: Due to its efficiency, VARA provides a promising solution for further improvement of variant calling while maintaining small memory footprints. The deBGA-VARA is available at: https://github.com/hitbc/deBGA-VARA.","Fast read alignment with incorporation of known genomic variants. BACKGROUND: Many genetic variants have been reported from sequencing projects due to decreasing experimental costs. Compared to the current typical paradigm, read mapping incorporating existing variants can improve the performance of subsequent analysis. This method is supposed to map sequencing reads efficiently to a graphical index with a reference genome and known variation to increase alignment quality and variant calling accuracy. However, storing and indexing various types of variation require costly RAM space. METHODS: Aligning reads to a graph model-based index including the whole set of variants is ultimately an NP-hard problem in theory. Here, we propose a variation-aware read alignment algorithm (VARA), which generates the alignment between read and multiple genomic sequences simultaneously utilizing the schema of the Landau-Vishkin algorithm. VARA dynamically extracts regional variants to construct a pseudo tree-based structure on-the-fly for seed extension without loading the whole genome variation into memory space. RESULTS: We developed the novel high-throughput sequencing read aligner deBGA-VARA by integrating VARA into deBGA. The deBGA-VARA is benchmarked both on simulated reads and the NA12878 sequencing dataset. The experimental results demonstrate that read alignment incorporating genetic variation knowledge can achieve high sensitivity and accuracy. CONCLUSIONS: Due to its efficiency, VARA provides a promising solution for further improvement of variant calling while maintaining small memory footprints. The deBGA-VARA is available at: https://github.com/hitbc/deBGA-VARA."
0,Comparison of extracellular and hepatobiliary MR contrast agents for the diagnosis of small HCCs,,
0,End-to-End Differentiable Learning of Protein Structure,"Predicting protein structure from sequence is a central challenge of biochemistry. Co-evolution methods show promise, but an explicit sequence-to-structure map remains elusive. Advances in deep learning that replace complex, human-designed pipelines with differentiable models optimized end to end suggest the potential benefits of similarly reformulating structure prediction. Here, we introduce an end-to-end differentiable model for protein structure learning. The model couples local and global protein structure via geometric units that optimize global geometry without violating local covalent chemistry. We test our model using two challenging tasks: predicting novel folds without co-evolutionary data and predicting known folds without structural templates. In the first task, the model achieves state-of-the-art accuracy, and in the second, it comes within 1â€“2 Ã…; competing methods using co-evolution and experimental templates have been refined over many years, and it is likely that the differentiable approach has substantial room for further improvement, with applications ranging from drug discovery to protein design.","End-to-End Differentiable Learning of Protein Structure. Predicting protein structure from sequence is a central challenge of biochemistry. Co-evolution methods show promise, but an explicit sequence-to-structure map remains elusive. Advances in deep learning that replace complex, human-designed pipelines with differentiable models optimized end to end suggest the potential benefits of similarly reformulating structure prediction. Here, we introduce an end-to-end differentiable model for protein structure learning. The model couples local and global protein structure via geometric units that optimize global geometry without violating local covalent chemistry. We test our model using two challenging tasks: predicting novel folds without co-evolutionary data and predicting known folds without structural templates. In the first task, the model achieves state-of-the-art accuracy, and in the second, it comes within 1â€“2 Ã…; competing methods using co-evolution and experimental templates have been refined over many years, and it is likely that the differentiable approach has substantial room for further improvement, with applications ranging from drug discovery to protein design."
0,Metabolic Profile of Supragingival Plaque Exposed to Arginine and Fluoride,"Caries lesions develop when acid production from bacterial metabolism of dietary carbohydrates outweighs the various mechanisms that promote pH homeostasis, including bacterial alkali production. Therapies that provide arginine as a substrate for alkali production in supragingival oral biofilms have strong anticaries potential. The objective of this study was to investigate the metabolic profile of site-specific supragingival plaque in response to the use of arginine (Arg: 1.5% arginine, fluoride-free) or fluoride (F: 1,100 ppm F/NaF) toothpastes. Eighty-three adults of different caries status were recruited and assigned to treatment with Arg or F for 12 wk. Caries lesions were diagnosed using International Caries Detection and Assessment System II, and plaque samples were collected from caries-free and carious tooth surfaces. Taxonomic profiles were obtained by HOMINGS (Human Oral Microbe Identification using Next Generation Sequencing), and plaque metabolism was assessed by the levels of arginine catabolism via the arginine deiminase pathway (ADS), acidogenicity, and global metabolomics. Principal component analysis (PCA), partial least squares-discriminant analysis, analysis of variance, and random forest tests were used to distinguish metabolic profiles. Of the 509 active lesions diagnosed at baseline, 70 (14%) were inactive after 12 wk. Generalized linear model showed that enamel lesions were significantly more likely to become inactive compared to dentin lesions (P < 0.0001), but no difference was found when treatment with Arg was compared to F (P = 0.46). Arg significantly increased plaque ADS activity (P = 0.031) and plaque pH values after incubation with glucose (P = 0.001). F reduced plaque lactate production from endogenous sources (P = 0.02). PCA revealed differences between the metabolic profiles of plaque treated with Arg or F. Arg significantly affected the concentrations of 16 metabolites, including phenethylamine, agmatine, and glucosamine-6-phosphate (P < 0.05), while F affected the concentrations of 9 metabolites, including phenethylamine, N-methyl-glutamate, and agmatine (P < 0.05). The anticaries mechanisms of action of arginine and fluoride are distinct. Arginine metabolism promotes biofilm pH homeostasis, whereas fluoride is thought to enhance resistance of tooth minerals to low pH and reduce acid production by supragingival oral biofilms.","Metabolic Profile of Supragingival Plaque Exposed to Arginine and Fluoride. Caries lesions develop when acid production from bacterial metabolism of dietary carbohydrates outweighs the various mechanisms that promote pH homeostasis, including bacterial alkali production. Therapies that provide arginine as a substrate for alkali production in supragingival oral biofilms have strong anticaries potential. The objective of this study was to investigate the metabolic profile of site-specific supragingival plaque in response to the use of arginine (Arg: 1.5% arginine, fluoride-free) or fluoride (F: 1,100 ppm F/NaF) toothpastes. Eighty-three adults of different caries status were recruited and assigned to treatment with Arg or F for 12 wk. Caries lesions were diagnosed using International Caries Detection and Assessment System II, and plaque samples were collected from caries-free and carious tooth surfaces. Taxonomic profiles were obtained by HOMINGS (Human Oral Microbe Identification using Next Generation Sequencing), and plaque metabolism was assessed by the levels of arginine catabolism via the arginine deiminase pathway (ADS), acidogenicity, and global metabolomics. Principal component analysis (PCA), partial least squares-discriminant analysis, analysis of variance, and random forest tests were used to distinguish metabolic profiles. Of the 509 active lesions diagnosed at baseline, 70 (14%) were inactive after 12 wk. Generalized linear model showed that enamel lesions were significantly more likely to become inactive compared to dentin lesions (P < 0.0001), but no difference was found when treatment with Arg was compared to F (P = 0.46). Arg significantly increased plaque ADS activity (P = 0.031) and plaque pH values after incubation with glucose (P = 0.001). F reduced plaque lactate production from endogenous sources (P = 0.02). PCA revealed differences between the metabolic profiles of plaque treated with Arg or F. Arg significantly affected the concentrations of 16 metabolites, including phenethylamine, agmatine, and glucosamine-6-phosphate (P < 0.05), while F affected the concentrations of 9 metabolites, including phenethylamine, N-methyl-glutamate, and agmatine (P < 0.05). The anticaries mechanisms of action of arginine and fluoride are distinct. Arginine metabolism promotes biofilm pH homeostasis, whereas fluoride is thought to enhance resistance of tooth minerals to low pH and reduce acid production by supragingival oral biofilms."
0,Quantifying homologous proteins and proteoforms,"Many proteoforms-arising from alternative splicing, post-translational modifications (PTM), or paralogous genes-have distinct biological functions, such as histone PTM proteoforms. However, their quantification by existing bottom-up mass-spectrometry (MS) methods is undermined by peptide-specific biases. To avoid these biases, we developed and implemented a first-principles model (HIquant) for quantifying proteoform stoichiometries. We characterized when MS data allow inferring proteoform stoichiometries by HIquant and derived an algorithm for optimal inference. We applied this algorithm to infer proteoform stoichiometries in two experimental systems that supported rigorous bench-marking: alkylated proteoforms spiked-in at known ratios and endogenous histone 3 PTM proteoforms quantified relative to internal heavy standards. When compared with the benchmarks, the proteoform stoichiometries interfered by HIquant without using external standards had relative error of 5-15% for simple proteoforms and 20-30% for complex proteoforms.","Quantifying homologous proteins and proteoforms. Many proteoforms-arising from alternative splicing, post-translational modifications (PTM), or paralogous genes-have distinct biological functions, such as histone PTM proteoforms. However, their quantification by existing bottom-up mass-spectrometry (MS) methods is undermined by peptide-specific biases. To avoid these biases, we developed and implemented a first-principles model (HIquant) for quantifying proteoform stoichiometries. We characterized when MS data allow inferring proteoform stoichiometries by HIquant and derived an algorithm for optimal inference. We applied this algorithm to infer proteoform stoichiometries in two experimental systems that supported rigorous bench-marking: alkylated proteoforms spiked-in at known ratios and endogenous histone 3 PTM proteoforms quantified relative to internal heavy standards. When compared with the benchmarks, the proteoform stoichiometries interfered by HIquant without using external standards had relative error of 5-15% for simple proteoforms and 20-30% for complex proteoforms."
0,Identification of Chemotype Agonists for Human Resolvin D1 Receptor DRV1 with Pro-Resolving Functions,"Resolution of acute inflammation is governed, in part, by specialized pro-resolving mediators, including lipoxins, resolvins, protectins, and maresins. Among them, resolvin D1 (RvD1) exhibits potent pro-resolving functions via activating human resolvin D1 receptor (DRV1/GPR32). RvD1 is a complex molecule that requires challenging organic synthesis, diminishing its potential as a therapeutic. Therefore, we implemented a high-throughput screening of small-molecule libraries and identified several chemotypes that activated recombinant DRV1, represented by NCGC00120943 (C1A), NCGC00135472 (C2A), pMPPF, and pMPPI. These chemotypes also elicited rapid impedance changes in cells overexpressing recombinant DRV1. With human macrophages, they each stimulated phagocytosis of serum-treated zymosan at concentrations comparable with that of RvD1, the endogenous DRV1 ligand. In addition, macrophage phagocytosis of live E. coli was significantly increased by these chemotypes in DRV1-transfected macrophages, compared with mock-transfected cells. Taken together, these chemotypes identified by unbiased screens act as RvD1 mimetics, exhibiting pro-resolving functions via interacting with human DRV1. Chiang et al. identified synthetic molecules that activate human DRV1/GPR32 receptor using high-throughput screening (>48,000 compounds). DRV1/GPR32 and its ligand resolvin D1 display pro-resolving actions in inflammation. These molecules act as resolvin D1 mimetics, offering templates to facilitate therapeutic development targeting DRV1 to promote resolution of inflammation.","Identification of Chemotype Agonists for Human Resolvin D1 Receptor DRV1 with Pro-Resolving Functions. Resolution of acute inflammation is governed, in part, by specialized pro-resolving mediators, including lipoxins, resolvins, protectins, and maresins. Among them, resolvin D1 (RvD1) exhibits potent pro-resolving functions via activating human resolvin D1 receptor (DRV1/GPR32). RvD1 is a complex molecule that requires challenging organic synthesis, diminishing its potential as a therapeutic. Therefore, we implemented a high-throughput screening of small-molecule libraries and identified several chemotypes that activated recombinant DRV1, represented by NCGC00120943 (C1A), NCGC00135472 (C2A), pMPPF, and pMPPI. These chemotypes also elicited rapid impedance changes in cells overexpressing recombinant DRV1. With human macrophages, they each stimulated phagocytosis of serum-treated zymosan at concentrations comparable with that of RvD1, the endogenous DRV1 ligand. In addition, macrophage phagocytosis of live E. coli was significantly increased by these chemotypes in DRV1-transfected macrophages, compared with mock-transfected cells. Taken together, these chemotypes identified by unbiased screens act as RvD1 mimetics, exhibiting pro-resolving functions via interacting with human DRV1. Chiang et al. identified synthetic molecules that activate human DRV1/GPR32 receptor using high-throughput screening (>48,000 compounds). DRV1/GPR32 and its ligand resolvin D1 display pro-resolving actions in inflammation. These molecules act as resolvin D1 mimetics, offering templates to facilitate therapeutic development targeting DRV1 to promote resolution of inflammation."
0,Synthesis and DNase I inhibitory properties of some 4-thiazolidinone derivatives,"Twelve new thiazolidinones were synthesized and, together with 41 previously synthesized thiazolidinones,Â evaluated for inhibitory activity against deoxyribonuclease I (DNase I) in vitro. Ten compounds inhibited commercial bovine pancreatic DNase I with an IC50 below 200 Î¼M and showed to be more potent DNase I inhibitors than crystal violet (IC50 = 365.90 Â± 47.33 Î¼M), used as a positive control. Moreover, three compounds were active against DNase I in rat liver homogenate, having an IC50 below 200 Î¼M. (3-Methyl-1,4-dioxothiazolidin-2-ylidene)-N-(2-phenylethyl)ethanamide (41) exhibited the most potent DNase I inhibition against both commercial and rat liver DNase I with IC50 values of 115.96 Â± 11.70 and 151.36 Â± 15.85 Î¼M, respectively. Site Finder and molecular docking defined the thiazolidinones interactions with the most important catalytic residues of DNase I, including the H-acceptor interaction with residues His 134 and His 252 and/or H-donor interaction with residues Glu 39 and Asp 168. The three most active compounds against both commercial and rat liver DNase I (31, 38, and 41) exhibited favorable physico-chemical, pharmacokinetic, and toxicological properties. These observations could be utilized to guide the rational design and optimization of novel thiazolidinone inhibitors. Thiazolidinones as novel DNase I inhibitors could have potential therapeutic applications due to the significant involvement of DNase I in the pathophysiology of many disease conditions.","Synthesis and DNase I inhibitory properties of some 4-thiazolidinone derivatives. Twelve new thiazolidinones were synthesized and, together with 41 previously synthesized thiazolidinones,Â evaluated for inhibitory activity against deoxyribonuclease I (DNase I) in vitro. Ten compounds inhibited commercial bovine pancreatic DNase I with an IC50 below 200 Î¼M and showed to be more potent DNase I inhibitors than crystal violet (IC50 = 365.90 Â± 47.33 Î¼M), used as a positive control. Moreover, three compounds were active against DNase I in rat liver homogenate, having an IC50 below 200 Î¼M. (3-Methyl-1,4-dioxothiazolidin-2-ylidene)-N-(2-phenylethyl)ethanamide (41) exhibited the most potent DNase I inhibition against both commercial and rat liver DNase I with IC50 values of 115.96 Â± 11.70 and 151.36 Â± 15.85 Î¼M, respectively. Site Finder and molecular docking defined the thiazolidinones interactions with the most important catalytic residues of DNase I, including the H-acceptor interaction with residues His 134 and His 252 and/or H-donor interaction with residues Glu 39 and Asp 168. The three most active compounds against both commercial and rat liver DNase I (31, 38, and 41) exhibited favorable physico-chemical, pharmacokinetic, and toxicological properties. These observations could be utilized to guide the rational design and optimization of novel thiazolidinone inhibitors. Thiazolidinones as novel DNase I inhibitors could have potential therapeutic applications due to the significant involvement of DNase I in the pathophysiology of many disease conditions."
0,Ultra-Low-Dose F-18-Florbetaben Amyloid PET Imaging Using Deep Learning with Multi-Contrast MRI Inputs,,
0,"Early inhibition of endothelial retinoid uptake upon myocardial infarction restores cardiac function and prevents cell, tissue, and animal death","Physiologically, following myocardial infarction (MI), retinoid levels elevate locally in the infarcted area. Whereas therapeutic systemic application of retinoids was shown to reduce the progression of ventricular dilatation and the onset of heart failure, the role of acute physiologically increased retinoids in the infarction zone is unknown to date. To reveal the role of local retinoids in the MI zone is the central aim of this study. Using human cell culture and co-culture models for hypoxia as well as various assays systems, lentivirus-based transgene expression, in silico molecular docking studies, and an MI model in rats, we analysed the impact of the retinoid all-trans retinoic acid (ATRA) on cell signalling, cell viability, tissue survival, heart function, and MI-induced death in rats. Based on our results, ATRA-mediated signalling does aggravate the MI phenotype (e.g. 2.5-fold increased mortality compared to control), whereas 5â€²-methoxyleoligin (5ML), a new agent which interferes with ATRA-signalling rescues the ATRA-dependent phenotype. On the molecular level, ATRA signalling causes induction of TXNIP, a potent inhibitor of the physiological antioxidant thioredoxin (TRX1) and sensitizes cells to necrotic cell death upon hypoxia. 5ML-mediated prevention of ATRA effects were shown to be based on the inhibition of cellular ATRA uptake by interference with the cholesterol (and retinol) binding motif of the transmembrane protein STRA6. 5ML-mediated inhibition of ATRA uptake led to a strong reduction of ATRA-dependent gene expression, reduced ROS formation, and protection from necrotic cell death. As 5ML exerted a cardioprotective effect, also independent of its inhibition of cellular ATRA uptake, the agent likely has another cardioprotective property, which may rely on the induction of TRX1 activity. In summary, this is the first study to show i) that local retinoids in the early MI zone may worsen disease outcome, ii) that inhibition of endothelial retinoid uptake using 5ML may constitute a novel treatment strategy, and iii) that targeting endothelial and myocardial retinoid uptake (e.g. via STRA6 inhibition) may constitute a novel treatment target in acute MI.","Early inhibition of endothelial retinoid uptake upon myocardial infarction restores cardiac function and prevents cell, tissue, and animal death. Physiologically, following myocardial infarction (MI), retinoid levels elevate locally in the infarcted area. Whereas therapeutic systemic application of retinoids was shown to reduce the progression of ventricular dilatation and the onset of heart failure, the role of acute physiologically increased retinoids in the infarction zone is unknown to date. To reveal the role of local retinoids in the MI zone is the central aim of this study. Using human cell culture and co-culture models for hypoxia as well as various assays systems, lentivirus-based transgene expression, in silico molecular docking studies, and an MI model in rats, we analysed the impact of the retinoid all-trans retinoic acid (ATRA) on cell signalling, cell viability, tissue survival, heart function, and MI-induced death in rats. Based on our results, ATRA-mediated signalling does aggravate the MI phenotype (e.g. 2.5-fold increased mortality compared to control), whereas 5â€²-methoxyleoligin (5ML), a new agent which interferes with ATRA-signalling rescues the ATRA-dependent phenotype. On the molecular level, ATRA signalling causes induction of TXNIP, a potent inhibitor of the physiological antioxidant thioredoxin (TRX1) and sensitizes cells to necrotic cell death upon hypoxia. 5ML-mediated prevention of ATRA effects were shown to be based on the inhibition of cellular ATRA uptake by interference with the cholesterol (and retinol) binding motif of the transmembrane protein STRA6. 5ML-mediated inhibition of ATRA uptake led to a strong reduction of ATRA-dependent gene expression, reduced ROS formation, and protection from necrotic cell death. As 5ML exerted a cardioprotective effect, also independent of its inhibition of cellular ATRA uptake, the agent likely has another cardioprotective property, which may rely on the induction of TRX1 activity. In summary, this is the first study to show i) that local retinoids in the early MI zone may worsen disease outcome, ii) that inhibition of endothelial retinoid uptake using 5ML may constitute a novel treatment strategy, and iii) that targeting endothelial and myocardial retinoid uptake (e.g. via STRA6 inhibition) may constitute a novel treatment target in acute MI."
0,Computational anatomy for multi-organ analysis in medical imaging: A review,,
0,Automated detection of nonmelanoma skin cancer using digital images: a systematic review,"BACKGROUND: Computer-aided diagnosis of skin lesions is a growing area of research, but its application to nonmelanoma skin cancer (NMSC) is relatively under-studied. The purpose of this review is to synthesize the research that has been conducted on automated detection of NMSC using digital images and to assess the quality of evidence for the diagnostic accuracy of these technologies. METHODS: Eight databases (PubMed, Google Scholar, Embase, IEEE Xplore, Web of Science, SpringerLink, ScienceDirect, and the ACM Digital Library) were searched to identify diagnostic studies of NMSC using image-based machine learning models. Two reviewers independently screened eligible articles. The level of evidence of each study was evaluated using a five tier rating system, and the applicability and risk of bias of each study was assessed using the Quality Assessment of Diagnostic Accuracy Studies tool. RESULTS: Thirty-nine studies were reviewed. Twenty-four models were designed to detect basal cell carcinoma, two were designed to detect squamous cell carcinoma, and thirteen were designed to detect both. All studies were conducted in silico. The overall diagnostic accuracy of the classifiers, defined as concordance with histopathologic diagnosis, was high, with reported accuracies ranging from 72 to 100% and areas under the receiver operating characteristic curve ranging from 0.832 to 1. Most studies had substantial methodological limitations, but several were robustly designed and presented a high level of evidence. CONCLUSION: Most studies of image-based NMSC classifiers report performance greater than or equal to the reported diagnostic accuracy of the average dermatologist, but relatively few studies have presented a high level of evidence. Clinical studies are needed to assess whether these technologies can feasibly be implemented as a real-time aid for clinical diagnosis of NMSC.","Automated detection of nonmelanoma skin cancer using digital images: a systematic review. BACKGROUND: Computer-aided diagnosis of skin lesions is a growing area of research, but its application to nonmelanoma skin cancer (NMSC) is relatively under-studied. The purpose of this review is to synthesize the research that has been conducted on automated detection of NMSC using digital images and to assess the quality of evidence for the diagnostic accuracy of these technologies. METHODS: Eight databases (PubMed, Google Scholar, Embase, IEEE Xplore, Web of Science, SpringerLink, ScienceDirect, and the ACM Digital Library) were searched to identify diagnostic studies of NMSC using image-based machine learning models. Two reviewers independently screened eligible articles. The level of evidence of each study was evaluated using a five tier rating system, and the applicability and risk of bias of each study was assessed using the Quality Assessment of Diagnostic Accuracy Studies tool. RESULTS: Thirty-nine studies were reviewed. Twenty-four models were designed to detect basal cell carcinoma, two were designed to detect squamous cell carcinoma, and thirteen were designed to detect both. All studies were conducted in silico. The overall diagnostic accuracy of the classifiers, defined as concordance with histopathologic diagnosis, was high, with reported accuracies ranging from 72 to 100% and areas under the receiver operating characteristic curve ranging from 0.832 to 1. Most studies had substantial methodological limitations, but several were robustly designed and presented a high level of evidence. CONCLUSION: Most studies of image-based NMSC classifiers report performance greater than or equal to the reported diagnostic accuracy of the average dermatologist, but relatively few studies have presented a high level of evidence. Clinical studies are needed to assess whether these technologies can feasibly be implemented as a real-time aid for clinical diagnosis of NMSC."
0,AI-derived adipose tissue biomarker for risk prediction using CCTA,,
0,Automated Triaging of Adult Chest Radiographs,,
0,Assessing Nasal Soft-Tissue Envelope Thickness for Rhinoplasty: Normative Data and a Predictive Algorithm,"Importance: Preoperative assessment of nasal soft-tissue envelope (STE) thickness is an important component of rhinoplasty that presently lacks validated tools. Objective: To measure and assess the distribution of nasal STE thickness in a large patient population and to determine if facial plastic surgery clinicians can predict nasal STE thickness based on visual examination of the nose. Design, Setting, and Participants: This retrospective review and prospective assessment of 190 adult patients by 4 expert raters was conducted at an academic tertiary referral center. The patients had high-resolution maxillofacial computed tomography (CT) scans and standardized facial photographs on file and did not have a history of nasal fracture, septal perforation, rhinoplasty, or other surgery or medical conditions altering nasal form. Data were analyzed in March 2019. Main Outcomes and Measures: Measure nasal STE thickness at defined anatomic subsites using high-resolution CT scans. Measure expert-predicted nasal STE thickness based on visual examination of the nose using a scale from 0 (thinnest) to 100 (thickest). Results: Of the 190 patients, 78 were women and the mean (SD) age was 45 (17) years. The nasal STE was thickest at the sellion (mean [SD]) (6.7 [1.7] mm), thinnest at the rhinion (2.1 [0.7] mm), thickened over the supratip (4.8 [1.0] mm) and nasal tip (3.1 [0.6] mm), and thinned over the columella (2.6 [0.4] mm). In the study population, nasal STE thickness followed a nearly normal distribution for each measured subsite, with the majority of patients in a medium thickness range. Comparison of predicted and actual nasal STE thickness showed that experts could accurately predict nasal STE thickness, with the highest accuracy at the nasal tip (r, 0.73; prediction accuracy, 91%). A strong positive correlation was noted among the experts' STE estimates (r, 0.83-0.89), suggesting a high level of agreement between individual raters. Conclusions and Relevance: There is variable thickness of the nasal STE, which influences the external nasal contour and rhinoplasty outcomes. With visual analysis of the nose, experts can agree on and predict nasal STE thickness, with the highest accuracy at the nasal tip. These data can aid in preoperative planning for rhinoplasty, allowing implementation of preoperative, intraoperative, and postoperative strategies to optimize the nasal STE, which may ultimately improve patient outcomes and satisfaction.NA.","Assessing Nasal Soft-Tissue Envelope Thickness for Rhinoplasty: Normative Data and a Predictive Algorithm. Importance: Preoperative assessment of nasal soft-tissue envelope (STE) thickness is an important component of rhinoplasty that presently lacks validated tools. Objective: To measure and assess the distribution of nasal STE thickness in a large patient population and to determine if facial plastic surgery clinicians can predict nasal STE thickness based on visual examination of the nose. Design, Setting, and Participants: This retrospective review and prospective assessment of 190 adult patients by 4 expert raters was conducted at an academic tertiary referral center. The patients had high-resolution maxillofacial computed tomography (CT) scans and standardized facial photographs on file and did not have a history of nasal fracture, septal perforation, rhinoplasty, or other surgery or medical conditions altering nasal form. Data were analyzed in March 2019. Main Outcomes and Measures: Measure nasal STE thickness at defined anatomic subsites using high-resolution CT scans. Measure expert-predicted nasal STE thickness based on visual examination of the nose using a scale from 0 (thinnest) to 100 (thickest). Results: Of the 190 patients, 78 were women and the mean (SD) age was 45 (17) years. The nasal STE was thickest at the sellion (mean [SD]) (6.7 [1.7] mm), thinnest at the rhinion (2.1 [0.7] mm), thickened over the supratip (4.8 [1.0] mm) and nasal tip (3.1 [0.6] mm), and thinned over the columella (2.6 [0.4] mm). In the study population, nasal STE thickness followed a nearly normal distribution for each measured subsite, with the majority of patients in a medium thickness range. Comparison of predicted and actual nasal STE thickness showed that experts could accurately predict nasal STE thickness, with the highest accuracy at the nasal tip (r, 0.73; prediction accuracy, 91%). A strong positive correlation was noted among the experts' STE estimates (r, 0.83-0.89), suggesting a high level of agreement between individual raters. Conclusions and Relevance: There is variable thickness of the nasal STE, which influences the external nasal contour and rhinoplasty outcomes. With visual analysis of the nose, experts can agree on and predict nasal STE thickness, with the highest accuracy at the nasal tip. These data can aid in preoperative planning for rhinoplasty, allowing implementation of preoperative, intraoperative, and postoperative strategies to optimize the nasal STE, which may ultimately improve patient outcomes and satisfaction.NA."
0,Precision de novo peptide sequencing using mirror proteases of ac-lysarginase and trypsin for large-scale proteomics,"De novo peptide sequencing for large-scale proteomics remains challenging because of the lack of full coverage of ion series in tandem mass spectra. We developed a mirror protease of trypsin, acetylated LysargiNase (Ac-LysargiNase), with superior activity and stability. The mirror spectrum pairs derived from the Ac-LysargiNase and trypsin treated samples can generate full b and y ion series, which provide mutual complementarity of each other, and allow us to develop a novel algorithm, pNovoM, for de novo sequencing. Using pNovoM to sequence peptides of purified proteins, the accuracy of the sequence was close to 100%. More importantly, from a large-scale yeast proteome sample digested with trypsin and Ac-LysargiNase individually, 48% of all tandem mass spectra formed mirror spectrum pairs, 97% of which contained full coverage of ion series, resulting in precision de novo sequencing of full-length peptides by pNovoM. This enabled pNovoM to successfully sequence 21,249 peptides from 3,753 proteins and interpreted 44-152% more spectra than pNovo and PEAKS at a 5% FDR at the spectrum level. Moreover, the mirror protease strategy had an obvious advantage in sequencing long peptides. We believe that the combination of mirror protease strategy and pNovoM will be an effective approach for precision de novo sequencing on both single proteins and proteome samples.","Precision de novo peptide sequencing using mirror proteases of ac-lysarginase and trypsin for large-scale proteomics. De novo peptide sequencing for large-scale proteomics remains challenging because of the lack of full coverage of ion series in tandem mass spectra. We developed a mirror protease of trypsin, acetylated LysargiNase (Ac-LysargiNase), with superior activity and stability. The mirror spectrum pairs derived from the Ac-LysargiNase and trypsin treated samples can generate full b and y ion series, which provide mutual complementarity of each other, and allow us to develop a novel algorithm, pNovoM, for de novo sequencing. Using pNovoM to sequence peptides of purified proteins, the accuracy of the sequence was close to 100%. More importantly, from a large-scale yeast proteome sample digested with trypsin and Ac-LysargiNase individually, 48% of all tandem mass spectra formed mirror spectrum pairs, 97% of which contained full coverage of ion series, resulting in precision de novo sequencing of full-length peptides by pNovoM. This enabled pNovoM to successfully sequence 21,249 peptides from 3,753 proteins and interpreted 44-152% more spectra than pNovo and PEAKS at a 5% FDR at the spectrum level. Moreover, the mirror protease strategy had an obvious advantage in sequencing long peptides. We believe that the combination of mirror protease strategy and pNovoM will be an effective approach for precision de novo sequencing on both single proteins and proteome samples."
0,A cellular complex of BACE1 and Î³-secretase sequentially generates AÎ² from its full-length precursor,"Intramembrane proteolysis of transmembrane substrates by the presenilin-Î³-secretase complex is preceded and regulated by shedding of the substrate's ectodomain by a-or Î²-secretase. We asked whether Î²-and Î³-secretases interact to mediate efcient sequential processing of APP, generating the amyloid Î² (AÎ²) peptides that initiate Alzheimer's disease. We describe a hitherto unrecognized multiprotease complex containing active Î²-and Î³-secretases. BACE1 coimmunoprecipitated and cofractionated with Î³-secretase in cultured cells and in mouse and human brain. An endogenous high molecular weight (HMW) complex (âˆ¼5 MD) containing Î²-and Î³-secretases and holo-APP was catalytically active in vitro and generated a full array of AÎ² peptides, with physiological AÎ²42/40 ratios. Te isolated complex responded properly to Î³-secretase modulators. Alzheimer's-causing mutations in presenilin altered the AÎ²42/40 peptide ratio generated by the HMW Î²/Î³-secretase complex indistinguishably from that observed in whole cells. Tus, AÎ² is generated from holo-APP by a BACE1-Î³-secretase complex that provides sequential, efcient RIP processing of full-length substrates to fnal products.","A cellular complex of BACE1 and Î³-secretase sequentially generates AÎ² from its full-length precursor. Intramembrane proteolysis of transmembrane substrates by the presenilin-Î³-secretase complex is preceded and regulated by shedding of the substrate's ectodomain by a-or Î²-secretase. We asked whether Î²-and Î³-secretases interact to mediate efcient sequential processing of APP, generating the amyloid Î² (AÎ²) peptides that initiate Alzheimer's disease. We describe a hitherto unrecognized multiprotease complex containing active Î²-and Î³-secretases. BACE1 coimmunoprecipitated and cofractionated with Î³-secretase in cultured cells and in mouse and human brain. An endogenous high molecular weight (HMW) complex (âˆ¼5 MD) containing Î²-and Î³-secretases and holo-APP was catalytically active in vitro and generated a full array of AÎ² peptides, with physiological AÎ²42/40 ratios. Te isolated complex responded properly to Î³-secretase modulators. Alzheimer's-causing mutations in presenilin altered the AÎ²42/40 peptide ratio generated by the HMW Î²/Î³-secretase complex indistinguishably from that observed in whole cells. Tus, AÎ² is generated from holo-APP by a BACE1-Î³-secretase complex that provides sequential, efcient RIP processing of full-length substrates to fnal products."
0,Clinical Practice Settings vs Clinical Trials: Is Artificial Intelligence the Answer?,,
0,Reproducibility and intercorrelation of graph theoretical measures in structural brain connectivity networks,"Diffusion-weighted magnetic resonance imaging can be used to non-invasively probe the brain microstructure. In addition, recent advances have enabled the identification of complex fiber configurations present in most of the white matter. This has improved the investigation of structural connectivity with tractography methods. Whole-brain structural connectivity networks, or connectomes, are reconstructed by parcellating the gray matter and performing tractography to determine connectivity between these regions. These complex networks can be analyzed with graph theoretical methods, which measure their global and local properties. However, as these tools have only recently been applied to structural brain networks, there is little information about the reproducibility and intercorrelation of network properties, connectivity weights and fiber tractography reconstruction parameters in the brain. We studied the reproducibility and correlation in structural brain connectivity networks reconstructed with constrained spherical deconvolution based probabilistic streamlines tractography. Diffusion-weighted data from 19 subjects were acquired with b = 2800s/mm(2) and 75 gradient orientations. Intrasubject variability was computed with residual bootstrapping. Our findings indicate that the reproducibility of graph theoretical metrics is generally excellent with the exception of betweenness centrality. A reconstruction density of approximately one million streamlines is necessary for excellent reproducibility, but the reproducibility increases further with higher densities. The reproducibility decreases, but only slightly, when switching to a higher order in constrained spherical deconvolution. Moreover, in binary networks, using sufficiently high threshold values improves the reproducibility. We show that multiple network properties and connectivity weights are highly intercorrelated. The experiments were replicated by using a test-retest dataset of 44 healthy subjects provided by the Human Connectome Project. In conclusion, our results provide guidelines for reproducible investigation of structural brain networks.","Reproducibility and intercorrelation of graph theoretical measures in structural brain connectivity networks. Diffusion-weighted magnetic resonance imaging can be used to non-invasively probe the brain microstructure. In addition, recent advances have enabled the identification of complex fiber configurations present in most of the white matter. This has improved the investigation of structural connectivity with tractography methods. Whole-brain structural connectivity networks, or connectomes, are reconstructed by parcellating the gray matter and performing tractography to determine connectivity between these regions. These complex networks can be analyzed with graph theoretical methods, which measure their global and local properties. However, as these tools have only recently been applied to structural brain networks, there is little information about the reproducibility and intercorrelation of network properties, connectivity weights and fiber tractography reconstruction parameters in the brain. We studied the reproducibility and correlation in structural brain connectivity networks reconstructed with constrained spherical deconvolution based probabilistic streamlines tractography. Diffusion-weighted data from 19 subjects were acquired with b = 2800s/mm(2) and 75 gradient orientations. Intrasubject variability was computed with residual bootstrapping. Our findings indicate that the reproducibility of graph theoretical metrics is generally excellent with the exception of betweenness centrality. A reconstruction density of approximately one million streamlines is necessary for excellent reproducibility, but the reproducibility increases further with higher densities. The reproducibility decreases, but only slightly, when switching to a higher order in constrained spherical deconvolution. Moreover, in binary networks, using sufficiently high threshold values improves the reproducibility. We show that multiple network properties and connectivity weights are highly intercorrelated. The experiments were replicated by using a test-retest dataset of 44 healthy subjects provided by the Human Connectome Project. In conclusion, our results provide guidelines for reproducible investigation of structural brain networks."
0,"Fungal secondary metabolism: regulation, function and drug discovery",,
0,"North American clinical management guidelines for hidradenitis suppurativa: A publication from the United States and Canadian Hidradenitis Suppurativa Foundations Part II: Topical, intralesional, and systemic medical management",,
0,Deep Learning Reveals Cancer Metastasis and Therapeutic Antibody Targeting in the Entire Body,,
0,High-order coordination of cortical spiking activity modulates perceptual accuracy,"The accurate relay of electrical signals within cortical networks is key to perception and cognitive function. Theoretically, it has long been proposed that temporal coordination of neuronal spiking activity controls signal transmission and behavior. However, whether and how temporally precise neuronal coordination in population activity influences perception are unknown. Here, we recorded populations of neurons in early and mid-level visual cortex (areas V1 and V4) simultaneously to discover that the precise temporal coordination between the spiking activity of three or more cells carries information about visual perception in the absence of firing rate modulation. The accuracy of perceptual responses correlated with high-order spiking coordination within V4, but not V1, and with feedforward coordination between V1 and V4. These results indicate that while visual stimuli are encoded in the discharge rates of neurons, perceptual accuracy is related to temporally precise spiking coordination within and between cortical networks.","High-order coordination of cortical spiking activity modulates perceptual accuracy. The accurate relay of electrical signals within cortical networks is key to perception and cognitive function. Theoretically, it has long been proposed that temporal coordination of neuronal spiking activity controls signal transmission and behavior. However, whether and how temporally precise neuronal coordination in population activity influences perception are unknown. Here, we recorded populations of neurons in early and mid-level visual cortex (areas V1 and V4) simultaneously to discover that the precise temporal coordination between the spiking activity of three or more cells carries information about visual perception in the absence of firing rate modulation. The accuracy of perceptual responses correlated with high-order spiking coordination within V4, but not V1, and with feedforward coordination between V1 and V4. These results indicate that while visual stimuli are encoded in the discharge rates of neurons, perceptual accuracy is related to temporally precise spiking coordination within and between cortical networks."
0,Transcriptomic analysis of fetal membranes reveals pathways involved in preterm birth,"Background: Preterm birth (PTB), defined as infant delivery before 37 weeks of completed gestation, results from the interaction of both genetic and environmental components and constitutes a complex multifactorial syndrome. Transcriptome analysis of PTB has proven challenging because of the multiple causes of PTB and the numerous maternal and fetal gestational tissues that must interact to facilitate parturition. The transcriptome of the chorioamnion membranes at the site of rupture in PTB and term fetuses may reflect the molecular pathways of preterm labor. Methods: In this work, chorioamnion membranes from severe preterm and term fetuses were analyzed using RNA sequencing. Functional annotations and pathway analysis of differentially expressed genes were performed with the GAGE and GOSeq packages. A subset of differentially expressed genes in PTB was validated in a larger cohort using qRT-PCR and by comparing our results with genes and pathways previously reported in the literature. Results: A total of 270 genes were differentially expressed (DE): 252 were upregulated and 18 were down-regulated in severe preterm births relative to term births. Inflammatory and immunological pathways were upregulated in PTB. Both types of pathways were previously suggested to lead to PTB. Pathways that were not previously reported in PTB, such as the hemopoietic pathway, appeared upregulated in preterm membranes. A group of 18 downregulated genes discriminated between term and severe preterm cases. These genes potentially characterize a severe preterm transcriptome pattern and therefore are candidate genes for understanding the syndrome. Some of the downregulated genes are involved in the nervous system, morphogenesis (WNT1, DLX5, PAPPA2) and ion channel complexes (KCNJ16, KCNB1), making them good candidates as biomarkers of PTB. Conclusions: The identification of this DE gene pattern will help with the development of a multi-gene disease classifier. These markers were generated in an admixed South American population in which PTB has a high incidence. Since the genetic background may differentially impact different populations, it is necessary to include populations such as those from South America and Africa, which are usually excluded from high-throughput approaches. These classifiers should be compared to those in other populations to obtain a global landscape of PTB.","Transcriptomic analysis of fetal membranes reveals pathways involved in preterm birth. Background: Preterm birth (PTB), defined as infant delivery before 37 weeks of completed gestation, results from the interaction of both genetic and environmental components and constitutes a complex multifactorial syndrome. Transcriptome analysis of PTB has proven challenging because of the multiple causes of PTB and the numerous maternal and fetal gestational tissues that must interact to facilitate parturition. The transcriptome of the chorioamnion membranes at the site of rupture in PTB and term fetuses may reflect the molecular pathways of preterm labor. Methods: In this work, chorioamnion membranes from severe preterm and term fetuses were analyzed using RNA sequencing. Functional annotations and pathway analysis of differentially expressed genes were performed with the GAGE and GOSeq packages. A subset of differentially expressed genes in PTB was validated in a larger cohort using qRT-PCR and by comparing our results with genes and pathways previously reported in the literature. Results: A total of 270 genes were differentially expressed (DE): 252 were upregulated and 18 were down-regulated in severe preterm births relative to term births. Inflammatory and immunological pathways were upregulated in PTB. Both types of pathways were previously suggested to lead to PTB. Pathways that were not previously reported in PTB, such as the hemopoietic pathway, appeared upregulated in preterm membranes. A group of 18 downregulated genes discriminated between term and severe preterm cases. These genes potentially characterize a severe preterm transcriptome pattern and therefore are candidate genes for understanding the syndrome. Some of the downregulated genes are involved in the nervous system, morphogenesis (WNT1, DLX5, PAPPA2) and ion channel complexes (KCNJ16, KCNB1), making them good candidates as biomarkers of PTB. Conclusions: The identification of this DE gene pattern will help with the development of a multi-gene disease classifier. These markers were generated in an admixed South American population in which PTB has a high incidence. Since the genetic background may differentially impact different populations, it is necessary to include populations such as those from South America and Africa, which are usually excluded from high-throughput approaches. These classifiers should be compared to those in other populations to obtain a global landscape of PTB."
0,Gastroprotective effect of araloside A on ethanol- and aspirin-induced gastric ulcer in mice: involvement of H+/K+-ATPase and mitochondrial-mediated signaling pathway,"The aim of this study was to elucidate the gastroprotective activity and possible mechanism of involvement of araloside A (ARA) against ethanol- and aspirin-induced gastric ulcer in mice. The experimental mice were randomly divided into control, model, omeprazole (20Â mg/kg, orally) and ARA (10, 20 and 40Â mg/kg, orally). Gastric ulcer in mice was induced by intragastric administration of 80% ethanol (10Â mL/kg) containing 15Â mg/mL aspirin 4Â h after drug administration on day 7. The results indicated that ARA could significantly raise gastric juice volume and acidity; ameliorate gastric mucosal blood flow, gastric binding mucus volume, ulcer index and ulcer inhibition rate; suppress H+/K+-ATPase activity, which was confirmed by computer-aided docking simulations; inhibit the release of mitochondrial cytochrome c into the cytoplasm; inhibit caspase-9 and caspase-3 activities and down-regulate mRNA expression levels; down-regulate the mRNA and protein expressions of apoptosis protease-activating factor-1 and protein expression of cleaved poly(ADP ribose) polymerase-1; and up-regulate Bcl-2 mRNA and protein expressions and down-regulate Bax mRNA and protein expressions, thus elevating the Bcl-2/Bax ratio in a dose-dependent manner. Histopathological observations further provided supportive evidence for the aforementioned results. The results demonstrated that ARA exerted beneficial gastroprotective effects on alcohol- and aspirin-induced gastric ulcer in mice, which was related to suppressing H+/K+-ATPase activity as well as pro-apoptotic protein expression, and promoting anti-apoptotic protein expression, thus alleviating gastric mucosal injury and cell death.","Gastroprotective effect of araloside A on ethanol- and aspirin-induced gastric ulcer in mice: involvement of H+/K+-ATPase and mitochondrial-mediated signaling pathway. The aim of this study was to elucidate the gastroprotective activity and possible mechanism of involvement of araloside A (ARA) against ethanol- and aspirin-induced gastric ulcer in mice. The experimental mice were randomly divided into control, model, omeprazole (20Â mg/kg, orally) and ARA (10, 20 and 40Â mg/kg, orally). Gastric ulcer in mice was induced by intragastric administration of 80% ethanol (10Â mL/kg) containing 15Â mg/mL aspirin 4Â h after drug administration on day 7. The results indicated that ARA could significantly raise gastric juice volume and acidity; ameliorate gastric mucosal blood flow, gastric binding mucus volume, ulcer index and ulcer inhibition rate; suppress H+/K+-ATPase activity, which was confirmed by computer-aided docking simulations; inhibit the release of mitochondrial cytochrome c into the cytoplasm; inhibit caspase-9 and caspase-3 activities and down-regulate mRNA expression levels; down-regulate the mRNA and protein expressions of apoptosis protease-activating factor-1 and protein expression of cleaved poly(ADP ribose) polymerase-1; and up-regulate Bcl-2 mRNA and protein expressions and down-regulate Bax mRNA and protein expressions, thus elevating the Bcl-2/Bax ratio in a dose-dependent manner. Histopathological observations further provided supportive evidence for the aforementioned results. The results demonstrated that ARA exerted beneficial gastroprotective effects on alcohol- and aspirin-induced gastric ulcer in mice, which was related to suppressing H+/K+-ATPase activity as well as pro-apoptotic protein expression, and promoting anti-apoptotic protein expression, thus alleviating gastric mucosal injury and cell death."
0,The risk of incident extrahepatic cancers is higher in non-alcoholic fatty liver disease than obesity - A longitudinal cohort study,,
0,"Safety and efficacy of ozanimod versus interferon beta-1a in relapsing multiple sclerosis (SUNBEAM): a multicentre, randomised, minimum 12-month, phase 3 trial",,
0,Illuminating the dark side of machine learning,,
0,Mortality in adult patients with culture-positive and culture-negative meningitis in the Botswana national meningitis survey: a prevalent cohort study,,
0,Comparison of conventional and Si-photomultiplier-based PET systems for image quality and diagnostic performance,"BACKGROUND: A new generation of positron emission tomography with computed tomography (PET-CT) was recently introduced using silicon (Si) photomultiplier (PM)-based technology. Our aim was to compare the image quality and diagnostic performance of a SiPM-based PET-CT (Discovery MI; GE Healthcare, Milwaukee, WI, USA) with a time-of-flight PET-CT scanner with a conventional PM detector (Gemini TF; Philips Healthcare, Cleveland, OH, USA), including reconstruction algorithms per vendor's recommendations. METHODS: Imaging of the National Electrical Manufacturers Association IEC body phantom and 16 patients was carried out using 1.5â€‰min/bed for the Discovery MI PET-CT and 2â€‰min/bed for the Gemini TF PET-CT. Images were analysed for recovery coefficients for the phantom, signal-to-noise ratio in the liver, standardized uptake values (SUV) in lesions, number of lesions and metabolic TNM classifications in patients. RESULTS: In phantom, the correct (>â€‰90%) activity level was measured for spheres â‰¥17â€‰mm for Discovery MI, whereas the Gemini TF reached a correct measured activity level for the 37-mm sphere. In patient studies, metabolic TNM classification was worse using images obtained from the Discovery MI compared those obtained from the Gemini TF in 4 of 15 patients. A trend toward more malignant, inflammatory and unclear lesions was found using images acquired with the Discovery MI compared with the Gemini TF, but this was not statistically significant. Lesion-to-blood-pool SUV ratios were significantly higher in images from the Discovery MI compared with the Gemini TF for lesions smaller than 1â€‰cm (pâ€‰<â€‰0.001), but this was not the case for larger lesions (pâ€‰=â€‰0.053). The signal-to-noise ratio in the liver was similar between platforms (pâ€‰=â€‰0.52). Also, shorter acquisition times were possible using the Discovery MI, with preserved signal-to-noise ratio in the liver. CONCLUSIONS: Image quality was better with Discovery MI compared to conventional Gemini TF. Although no gold standard was available, the results indicate that the new PET-CT generation will provide potentially better diagnostic performance.","Comparison of conventional and Si-photomultiplier-based PET systems for image quality and diagnostic performance. BACKGROUND: A new generation of positron emission tomography with computed tomography (PET-CT) was recently introduced using silicon (Si) photomultiplier (PM)-based technology. Our aim was to compare the image quality and diagnostic performance of a SiPM-based PET-CT (Discovery MI; GE Healthcare, Milwaukee, WI, USA) with a time-of-flight PET-CT scanner with a conventional PM detector (Gemini TF; Philips Healthcare, Cleveland, OH, USA), including reconstruction algorithms per vendor's recommendations. METHODS: Imaging of the National Electrical Manufacturers Association IEC body phantom and 16 patients was carried out using 1.5â€‰min/bed for the Discovery MI PET-CT and 2â€‰min/bed for the Gemini TF PET-CT. Images were analysed for recovery coefficients for the phantom, signal-to-noise ratio in the liver, standardized uptake values (SUV) in lesions, number of lesions and metabolic TNM classifications in patients. RESULTS: In phantom, the correct (>â€‰90%) activity level was measured for spheres â‰¥17â€‰mm for Discovery MI, whereas the Gemini TF reached a correct measured activity level for the 37-mm sphere. In patient studies, metabolic TNM classification was worse using images obtained from the Discovery MI compared those obtained from the Gemini TF in 4 of 15 patients. A trend toward more malignant, inflammatory and unclear lesions was found using images acquired with the Discovery MI compared with the Gemini TF, but this was not statistically significant. Lesion-to-blood-pool SUV ratios were significantly higher in images from the Discovery MI compared with the Gemini TF for lesions smaller than 1â€‰cm (pâ€‰<â€‰0.001), but this was not the case for larger lesions (pâ€‰=â€‰0.053). The signal-to-noise ratio in the liver was similar between platforms (pâ€‰=â€‰0.52). Also, shorter acquisition times were possible using the Discovery MI, with preserved signal-to-noise ratio in the liver. CONCLUSIONS: Image quality was better with Discovery MI compared to conventional Gemini TF. Although no gold standard was available, the results indicate that the new PET-CT generation will provide potentially better diagnostic performance."
0,A new era: artificial intelligence and machine learning in prostate cancer,"Artificial intelligence (AI) - the ability of a machine to perform cognitive tasks to achieve a particular goal based on provided data - is revolutionizing and reshaping our health-care systems. The current availability of ever-increasing computational power, highly developed pattern recognition algorithms and advanced image processing software working at very high speeds has led to the emergence of computer-based systems that are trained to perform complex tasks in bioinformatics, medical imaging and medical robotics. Accessibility to 'big data' enables the 'cognitive' computer to scan billions of bits of unstructured information, extract the relevant information and recognize complex patterns with increasing confidence. Computer-based decision-support systems based on machine learning (ML) have the potential to revolutionize medicine by performing complex tasks that are currently assigned to specialists to improve diagnostic accuracy, increase efficiency of throughputs, improve clinical workflow, decrease human resource costs and improve treatment choices. These characteristics could be especially helpful in the management of prostate cancer, with growing applications in diagnostic imaging, surgical interventions, skills training and assessment, digital pathology and genomics. Medicine must adapt to this changing world, and urologists, oncologists, radiologists and pathologists, as high-volume users of imaging and pathology, need to understand this burgeoning science and acknowledge that the development of highly accurate AI-based decision-support applications of ML will require collaboration between data scientists, computer researchers and engineers.","A new era: artificial intelligence and machine learning in prostate cancer. Artificial intelligence (AI) - the ability of a machine to perform cognitive tasks to achieve a particular goal based on provided data - is revolutionizing and reshaping our health-care systems. The current availability of ever-increasing computational power, highly developed pattern recognition algorithms and advanced image processing software working at very high speeds has led to the emergence of computer-based systems that are trained to perform complex tasks in bioinformatics, medical imaging and medical robotics. Accessibility to 'big data' enables the 'cognitive' computer to scan billions of bits of unstructured information, extract the relevant information and recognize complex patterns with increasing confidence. Computer-based decision-support systems based on machine learning (ML) have the potential to revolutionize medicine by performing complex tasks that are currently assigned to specialists to improve diagnostic accuracy, increase efficiency of throughputs, improve clinical workflow, decrease human resource costs and improve treatment choices. These characteristics could be especially helpful in the management of prostate cancer, with growing applications in diagnostic imaging, surgical interventions, skills training and assessment, digital pathology and genomics. Medicine must adapt to this changing world, and urologists, oncologists, radiologists and pathologists, as high-volume users of imaging and pathology, need to understand this burgeoning science and acknowledge that the development of highly accurate AI-based decision-support applications of ML will require collaboration between data scientists, computer researchers and engineers."
0,Quantitative pixel intensity- and color-based image analysis on minimally compressed files: implications for whole-slide imaging,"Current best practice in the quantitative analysis of microscopy images dictates that image files should be saved in a lossless format such as TIFF. Use of lossy files, including those processed with the JPEG algorithm, is highly discouraged due to effects of compression on pixel characteristics. However, with the growing popularity of whole-slide imaging (WSI) and its attendant large file sizes, compressed image files are becoming more prevelent. This prompted us to perform a color-based quantitative pixel analysis of minimally compressed WSI images. Sections from three tissues stained with one of three reagents representing the colors blue (hematoxylin), red (Oil-Red-O), and brown (immunoperoxidase) were scanned with a whole slide imager in triplicate at 20x, 40x, and 63xÂ magnifications. The resulting files were in the form of a BigTIFF with a JPEG compression automatically applied during acquisition. Images were imported into analysis software, six regions of interest were applied to various morphological locations, and the areas assessed for the color of interest. Whereas the number of designated weakly or strongly positive pixels was variable across the triplicate scans for the individual regions of interest, the total number of positive pixels was consistent. These results suggest that total positivity for a specific color representing a histochemical or immunohistochemical stain can be adequately quantitated on compressed images, but degrees of positivity (e.g., weak vs. strong) may not be as reliable. However, it is important to assess individual whole-slide imagers, file compression level and algorithm, and analysis software for reproducibility.","Quantitative pixel intensity- and color-based image analysis on minimally compressed files: implications for whole-slide imaging. Current best practice in the quantitative analysis of microscopy images dictates that image files should be saved in a lossless format such as TIFF. Use of lossy files, including those processed with the JPEG algorithm, is highly discouraged due to effects of compression on pixel characteristics. However, with the growing popularity of whole-slide imaging (WSI) and its attendant large file sizes, compressed image files are becoming more prevelent. This prompted us to perform a color-based quantitative pixel analysis of minimally compressed WSI images. Sections from three tissues stained with one of three reagents representing the colors blue (hematoxylin), red (Oil-Red-O), and brown (immunoperoxidase) were scanned with a whole slide imager in triplicate at 20x, 40x, and 63xÂ magnifications. The resulting files were in the form of a BigTIFF with a JPEG compression automatically applied during acquisition. Images were imported into analysis software, six regions of interest were applied to various morphological locations, and the areas assessed for the color of interest. Whereas the number of designated weakly or strongly positive pixels was variable across the triplicate scans for the individual regions of interest, the total number of positive pixels was consistent. These results suggest that total positivity for a specific color representing a histochemical or immunohistochemical stain can be adequately quantitated on compressed images, but degrees of positivity (e.g., weak vs. strong) may not be as reliable. However, it is important to assess individual whole-slide imagers, file compression level and algorithm, and analysis software for reproducibility."
0,Integrated identification and quantification error probabilities for shotgun proteomics,"Protein quantification by label-free shotgun proteomics experiments is plagued by a multitude of error sources. Typical pipelines for identifying differential proteins use intermediate filters to control the error rate. However, they often ignore certain error sources and, moreover, regard filtered lists as completely correct in subsequent steps. These two indiscretions can easily lead to a loss of control of the false discovery rate (FDR). We propose a probabilistic graphical model, Triqler, that propagates error information through all steps, employing distributions in favor of point estimates, most notably for missing value imputation. The model outputs posterior probabilities for fold changes between treatment groups, highlighting uncertainty rather than hiding it. We analyzed 3 engineered data sets and achieved FDR control and high sensitivity, even for truly absent proteins. In a bladder cancer clinical data set we discovered 35 proteins at 5% FDR, whereas the original study discovered 1 and MaxQuant/Perseus 4 proteins at this threshold. Compellingly, these 35 proteins showed enrichment for functional annotation terms, whereas the top ranked proteins reported by MaxQuant/ Perseus showed no enrichment. The model executes in minutes and is freely available at https://pypi.org/ project/triqler/.","Integrated identification and quantification error probabilities for shotgun proteomics. Protein quantification by label-free shotgun proteomics experiments is plagued by a multitude of error sources. Typical pipelines for identifying differential proteins use intermediate filters to control the error rate. However, they often ignore certain error sources and, moreover, regard filtered lists as completely correct in subsequent steps. These two indiscretions can easily lead to a loss of control of the false discovery rate (FDR). We propose a probabilistic graphical model, Triqler, that propagates error information through all steps, employing distributions in favor of point estimates, most notably for missing value imputation. The model outputs posterior probabilities for fold changes between treatment groups, highlighting uncertainty rather than hiding it. We analyzed 3 engineered data sets and achieved FDR control and high sensitivity, even for truly absent proteins. In a bladder cancer clinical data set we discovered 35 proteins at 5% FDR, whereas the original study discovered 1 and MaxQuant/Perseus 4 proteins at this threshold. Compellingly, these 35 proteins showed enrichment for functional annotation terms, whereas the top ranked proteins reported by MaxQuant/ Perseus showed no enrichment. The model executes in minutes and is freely available at https://pypi.org/ project/triqler/."
0,Suite of meshless algorithms for accurate computation of soft tissue deformation for surgical simulation,"The ability to predict patient-specific soft tissue deformations is key for computer-integrated surgery systems and the core enabling technology for a new era of personalized medicine. Element-Free Galerkin (EFG) methods are better suited for solving soft tissue deformation problems than the finite element method (FEM) due to their capability of handling large deformation while also eliminating the necessity of creating a complex predefined mesh. Nevertheless, meshless methods based on EFG formulation, exhibit three major limitations: (i) meshless shape functions using higher order basis cannot always be computed for arbitrarily distributed nodes (irregular node placement is crucial for facilitating automated discretization of complex geometries); (ii) imposition of the Essential Boundary Conditions (EBC) is not straightforward; and, (iii) numerical (Gauss) integration in space is not exact as meshless shape functions are not polynomial. This paper presents a suite of Meshless Total Lagrangian Explicit Dynamics (MTLED) algorithms incorporating a Modified Moving Least Squares (MMLS) method for interpolating scattered data both for visualization and for numerical computations of soft tissue deformation, a novel way of imposing EBC for explicit time integration, and an adaptive numerical integration procedure within the Meshless Total Lagrangian Explicit Dynamics algorithm. The appropriateness and effectiveness of the proposed methods is demonstrated using comparisons with the established non-linear procedures from commercial finite element software ABAQUS and experiments with very large deformations. To demonstrate the translational benefits of MTLED we also present a realistic brain-shift computation.","Suite of meshless algorithms for accurate computation of soft tissue deformation for surgical simulation. The ability to predict patient-specific soft tissue deformations is key for computer-integrated surgery systems and the core enabling technology for a new era of personalized medicine. Element-Free Galerkin (EFG) methods are better suited for solving soft tissue deformation problems than the finite element method (FEM) due to their capability of handling large deformation while also eliminating the necessity of creating a complex predefined mesh. Nevertheless, meshless methods based on EFG formulation, exhibit three major limitations: (i) meshless shape functions using higher order basis cannot always be computed for arbitrarily distributed nodes (irregular node placement is crucial for facilitating automated discretization of complex geometries); (ii) imposition of the Essential Boundary Conditions (EBC) is not straightforward; and, (iii) numerical (Gauss) integration in space is not exact as meshless shape functions are not polynomial. This paper presents a suite of Meshless Total Lagrangian Explicit Dynamics (MTLED) algorithms incorporating a Modified Moving Least Squares (MMLS) method for interpolating scattered data both for visualization and for numerical computations of soft tissue deformation, a novel way of imposing EBC for explicit time integration, and an adaptive numerical integration procedure within the Meshless Total Lagrangian Explicit Dynamics algorithm. The appropriateness and effectiveness of the proposed methods is demonstrated using comparisons with the established non-linear procedures from commercial finite element software ABAQUS and experiments with very large deformations. To demonstrate the translational benefits of MTLED we also present a realistic brain-shift computation."
0,"Placental growth factor testing to assess women with suspected pre-eclampsia: a multicentre, pragmatic, stepped-wedge cluster-randomised controlled trial","BACKGROUND: Previous prospective cohort studies have shown that angiogenic factors have a high diagnostic accuracy in women with suspected pre-eclampsia, but we remain uncertain of the effectiveness of these tests in a real-world setting. We therefore aimed to determine whether knowledge of the circulating concentration of placental growth factor (PlGF), an angiogenic factor, integrated with a clinical management algorithm, decreased the time for clinicians to make a diagnosis in women with suspected pre-eclampsia, and whether this approach reduced subsequent maternal or perinatal adverse outcomes. METHODS: We did a multicentre, pragmatic, stepped-wedge cluster-randomised controlled trial in 11 maternity units in the UK, which were each responsible for 3000-9000 deliveries per year. Women aged 18 years and older who presented with suspected pre-eclampsia between 20 weeks and 0 days of gestation and 36 weeks and 6 days of gestation, with a live, singleton fetus were invited to participate by the clinical research team. Suspected pre-eclampsia was defined as new-onset or worsening of existing hypertension, dipstick proteinuria, epigastric or right upper-quadrant pain, headache with visual disturbances, fetal growth restriction, or abnormal maternal blood tests that were suggestive of disease (such as thrombocytopenia or hepatic or renal dysfunction). Women were approached individually, they consented for study inclusion, and they were asked to give blood samples. We randomly allocated the maternity units, representing the clusters, to blocks. Blocks represented an intervention initiation time, which occurred at equally spaced 6-week intervals throughout the trial. At the start of the trial, all units had usual care (in which PlGF measurements were also taken but were concealed from clinicians and women). At the initiation time of each successive block, a site began to use the intervention (in which the circulating PlGF measurement was revealed and a clinical management algorithm was used). Enrolment of women continued for the duration of the blocks either to concealed PlGF testing, or after implementation, to revealed PlGF testing. The primary outcome was the time from presentation with suspected pre-eclampsia to documented pre-eclampsia in women enrolled in the trial who received a diagnosis of pre-eclampsia by their treating clinicians. This trial is registered with ISRCTN, number 16842031. FINDINGS: Between June 13, 2016, and Oct 27, 2017, we enrolled and assessed 1035 women with suspected pre-eclampsia. 12 (1%) women were found to be ineligible. Of the 1023 eligible women, 576 (56%) women were assigned to the intervention (revealed testing) group, and 447 (44%) women were assigned to receive usual care with additional concealed testing (concealed testing group). Three (1%) women in the revealed testing group were lost to follow-up, so 573 (99%) women in this group were included in the analyses. One (<1%) woman in the concealed testing group withdrew consent to follow-up data collection, so 446 (>99%) women in this group were included in the analyses. The median time to pre-eclampsia diagnosis was 4.1 days with concealed testing versus 1.9 days with revealed testing (time ratio 0.36, 95% CI 0.15-0.87; p=0.027). Maternal severe adverse outcomes were reported in 24 (5%) of 447 women in the concealed testing group versus 22 (4%) of 573 women in the revealed testing group (adjusted odds ratio 0.32, 95% CI 0.11-0.96; p=0.043), but there was no evidence of a difference in perinatal adverse outcomes (15% vs 14%, 1.45, 0.73-2.90) or gestation at delivery (36.6 weeks vs 36.8 weeks; mean difference -0.52, 95% CI -0.63 to 0.73). INTERPRETATION: We found that the availability of PlGF test results substantially reduced the time to clinical confirmation of pre-eclampsia. Where PlGF was implemented, we found a lower incidence of maternal adverse outcomes, consistent with adoption of targeted, enhanced surveillance, as recommended in the clinical management algorithm for clinicians. Adoption of PlGF testing in women with suspected pre-eclampsia is supported by the results of this study. FUNDING: National Institute for Health Research.","Placental growth factor testing to assess women with suspected pre-eclampsia: a multicentre, pragmatic, stepped-wedge cluster-randomised controlled trial. BACKGROUND: Previous prospective cohort studies have shown that angiogenic factors have a high diagnostic accuracy in women with suspected pre-eclampsia, but we remain uncertain of the effectiveness of these tests in a real-world setting. We therefore aimed to determine whether knowledge of the circulating concentration of placental growth factor (PlGF), an angiogenic factor, integrated with a clinical management algorithm, decreased the time for clinicians to make a diagnosis in women with suspected pre-eclampsia, and whether this approach reduced subsequent maternal or perinatal adverse outcomes. METHODS: We did a multicentre, pragmatic, stepped-wedge cluster-randomised controlled trial in 11 maternity units in the UK, which were each responsible for 3000-9000 deliveries per year. Women aged 18 years and older who presented with suspected pre-eclampsia between 20 weeks and 0 days of gestation and 36 weeks and 6 days of gestation, with a live, singleton fetus were invited to participate by the clinical research team. Suspected pre-eclampsia was defined as new-onset or worsening of existing hypertension, dipstick proteinuria, epigastric or right upper-quadrant pain, headache with visual disturbances, fetal growth restriction, or abnormal maternal blood tests that were suggestive of disease (such as thrombocytopenia or hepatic or renal dysfunction). Women were approached individually, they consented for study inclusion, and they were asked to give blood samples. We randomly allocated the maternity units, representing the clusters, to blocks. Blocks represented an intervention initiation time, which occurred at equally spaced 6-week intervals throughout the trial. At the start of the trial, all units had usual care (in which PlGF measurements were also taken but were concealed from clinicians and women). At the initiation time of each successive block, a site began to use the intervention (in which the circulating PlGF measurement was revealed and a clinical management algorithm was used). Enrolment of women continued for the duration of the blocks either to concealed PlGF testing, or after implementation, to revealed PlGF testing. The primary outcome was the time from presentation with suspected pre-eclampsia to documented pre-eclampsia in women enrolled in the trial who received a diagnosis of pre-eclampsia by their treating clinicians. This trial is registered with ISRCTN, number 16842031. FINDINGS: Between June 13, 2016, and Oct 27, 2017, we enrolled and assessed 1035 women with suspected pre-eclampsia. 12 (1%) women were found to be ineligible. Of the 1023 eligible women, 576 (56%) women were assigned to the intervention (revealed testing) group, and 447 (44%) women were assigned to receive usual care with additional concealed testing (concealed testing group). Three (1%) women in the revealed testing group were lost to follow-up, so 573 (99%) women in this group were included in the analyses. One (<1%) woman in the concealed testing group withdrew consent to follow-up data collection, so 446 (>99%) women in this group were included in the analyses. The median time to pre-eclampsia diagnosis was 4.1 days with concealed testing versus 1.9 days with revealed testing (time ratio 0.36, 95% CI 0.15-0.87; p=0.027). Maternal severe adverse outcomes were reported in 24 (5%) of 447 women in the concealed testing group versus 22 (4%) of 573 women in the revealed testing group (adjusted odds ratio 0.32, 95% CI 0.11-0.96; p=0.043), but there was no evidence of a difference in perinatal adverse outcomes (15% vs 14%, 1.45, 0.73-2.90) or gestation at delivery (36.6 weeks vs 36.8 weeks; mean difference -0.52, 95% CI -0.63 to 0.73). INTERPRETATION: We found that the availability of PlGF test results substantially reduced the time to clinical confirmation of pre-eclampsia. Where PlGF was implemented, we found a lower incidence of maternal adverse outcomes, consistent with adoption of targeted, enhanced surveillance, as recommended in the clinical management algorithm for clinicians. Adoption of PlGF testing in women with suspected pre-eclampsia is supported by the results of this study. FUNDING: National Institute for Health Research."
0,Autonomous Self-Driving Vehicles,,
0,Using parallelized incremental meta-docking can solve the conformational sampling issue when docking large ligands to proteins,"Background: Docking large ligands, and especially peptides, to protein receptors is still considered a challenge in computational structural biology. Besides the issue of accurately scoring the binding modes of a protein-ligand complex produced by a molecular docking tool, the conformational sampling of a large ligand is also often considered a challenge because of its underlying combinatorial complexity. In this study, we evaluate the impact of using parallelized and incremental paradigms on the accuracy and performance of conformational sampling when docking large ligands. We use five datasets of protein-ligand complexes involving ligands that could not be accurately docked by classical protein-ligand docking tools in previous similar studies. Results: Our computational evaluation shows that simply increasing the amount of conformational sampling performed by a protein-ligand docking tool, such as Vina, by running it for longer is rarely beneficial. Instead, it is more efficient and advantageous to run several short instances of this docking tool in parallel and group their results together, in a straightforward parallelized docking protocol. Even greater accuracy and efficiency are achieved by our parallelized incremental meta-docking tool, DINC, showing the additional benefits of its incremental paradigm. Using DINC, we could accurately reproduce the vast majority of the protein-ligand complexes we considered. Conclusions: Our study suggests that, even when trying to dock large ligands to proteins, the conformational sampling of the ligand should no longer be considered an issue, as simple docking protocols using existing tools can solve it. Therefore, scoring should currently be regarded as the biggest unmet challenge in molecular docking.","Using parallelized incremental meta-docking can solve the conformational sampling issue when docking large ligands to proteins. Background: Docking large ligands, and especially peptides, to protein receptors is still considered a challenge in computational structural biology. Besides the issue of accurately scoring the binding modes of a protein-ligand complex produced by a molecular docking tool, the conformational sampling of a large ligand is also often considered a challenge because of its underlying combinatorial complexity. In this study, we evaluate the impact of using parallelized and incremental paradigms on the accuracy and performance of conformational sampling when docking large ligands. We use five datasets of protein-ligand complexes involving ligands that could not be accurately docked by classical protein-ligand docking tools in previous similar studies. Results: Our computational evaluation shows that simply increasing the amount of conformational sampling performed by a protein-ligand docking tool, such as Vina, by running it for longer is rarely beneficial. Instead, it is more efficient and advantageous to run several short instances of this docking tool in parallel and group their results together, in a straightforward parallelized docking protocol. Even greater accuracy and efficiency are achieved by our parallelized incremental meta-docking tool, DINC, showing the additional benefits of its incremental paradigm. Using DINC, we could accurately reproduce the vast majority of the protein-ligand complexes we considered. Conclusions: Our study suggests that, even when trying to dock large ligands to proteins, the conformational sampling of the ligand should no longer be considered an issue, as simple docking protocols using existing tools can solve it. Therefore, scoring should currently be regarded as the biggest unmet challenge in molecular docking."
0,Activation of Caspase-6 Is Promoted by a Mutant Huntingtin Fragment and Blocked by an Allosteric Inhibitor Compound,"Aberrant activation of caspase-6 (C6) in the absence of other hallmarks of apoptosis has been demonstrated in cells and tissues from patients with Huntington disease (HD) and animal models. C6 activity correlates with disease progression in patients with HD and the cleavage of mutant huntingtin (mHTT) protein is thought to strongly contribute to disease pathogenesis. Here we show that the mHTT1-586 fragment generated by C6 cleavage interacts with the zymogen form of the enzyme, stabilizing a conformation that contains an active site and is prone to full activation. This shift toward enhanced activity can be prevented by a small-molecule inhibitor that blocks the interaction between C6 and mHTT1-586. Molecular docking studies suggest that the inhibitor binds an allosteric site in the C6 zymogen. The interaction of mHTT1-586 with C6 may therefore promote a self-reinforcing, feedforward cycle of C6 zymogen activation and mHTT cleavage driving HD pathogenesis.","Activation of Caspase-6 Is Promoted by a Mutant Huntingtin Fragment and Blocked by an Allosteric Inhibitor Compound. Aberrant activation of caspase-6 (C6) in the absence of other hallmarks of apoptosis has been demonstrated in cells and tissues from patients with Huntington disease (HD) and animal models. C6 activity correlates with disease progression in patients with HD and the cleavage of mutant huntingtin (mHTT) protein is thought to strongly contribute to disease pathogenesis. Here we show that the mHTT1-586 fragment generated by C6 cleavage interacts with the zymogen form of the enzyme, stabilizing a conformation that contains an active site and is prone to full activation. This shift toward enhanced activity can be prevented by a small-molecule inhibitor that blocks the interaction between C6 and mHTT1-586. Molecular docking studies suggest that the inhibitor binds an allosteric site in the C6 zymogen. The interaction of mHTT1-586 with C6 may therefore promote a self-reinforcing, feedforward cycle of C6 zymogen activation and mHTT cleavage driving HD pathogenesis."
0,Guideline-Based Clinical Assessment Versus Procalcitonin-Guided Antibiotic Use in Pneumonia: A Pragmatic Randomized Trial,"STUDY OBJECTIVE: Efforts to reduce unnecessary and unnecessarily long antibiotic treatment for community-acquired pneumonia have been attempted through use of procalcitonin and through guidelines based on serial clinical assessment. Our aim is to compare guideline-based clinical assessment- and procalcitonin algorithm-guided antibiotic use among patients with community-acquired pneumonia. METHODS: We performed a pragmatic, randomized, multicenter trial from November 2012 to April 2015 at 12 French hospitals. We included emergency department (ED) patients older than 18 years with community-acquired pneumonia. Patients were randomly assigned to either the procalcitonin-guided or clinical assessment group. In accordance with past studies, we hypothesized that serial clinical assessment would be superior to procalcitonin-guided care. The primary outcome was antibiotic duration, and secondary outcomes included rates of antibiotic duration less than or equal to 5 days, and clinical success and combined serious adverse outcomes at 30 days in the intention-to-treat population. RESULTS: Of 370 eligible patients, 285 (77%) were randomly assigned to either clinical assessment- (n=143) or procalcitonin-guided care (n=142). Median age was 67 years (range 18 to 93 years) and 40% of patients were deemed to have Pneumonia Severity Index class IV or V. Procalcitonin algorithm adherence was 76%. Antibiotic duration was not significantly different between clinical assessment- and procalcitonin-guided groups (median 9 versus 10 days, respectively). Clinical success rate was 92% in each group and serious adverse outcome rates were similar (15% versus 20%, respectively). CONCLUSION: Guideline-based serial clinical assessment did not reduce antibiotic exposure compared with procalcitonin-guided care among ED patients with community-acquired pneumonia. The strategies were similar in terms of duration of antibiotic use and clinical outcomes.","Guideline-Based Clinical Assessment Versus Procalcitonin-Guided Antibiotic Use in Pneumonia: A Pragmatic Randomized Trial. STUDY OBJECTIVE: Efforts to reduce unnecessary and unnecessarily long antibiotic treatment for community-acquired pneumonia have been attempted through use of procalcitonin and through guidelines based on serial clinical assessment. Our aim is to compare guideline-based clinical assessment- and procalcitonin algorithm-guided antibiotic use among patients with community-acquired pneumonia. METHODS: We performed a pragmatic, randomized, multicenter trial from November 2012 to April 2015 at 12 French hospitals. We included emergency department (ED) patients older than 18 years with community-acquired pneumonia. Patients were randomly assigned to either the procalcitonin-guided or clinical assessment group. In accordance with past studies, we hypothesized that serial clinical assessment would be superior to procalcitonin-guided care. The primary outcome was antibiotic duration, and secondary outcomes included rates of antibiotic duration less than or equal to 5 days, and clinical success and combined serious adverse outcomes at 30 days in the intention-to-treat population. RESULTS: Of 370 eligible patients, 285 (77%) were randomly assigned to either clinical assessment- (n=143) or procalcitonin-guided care (n=142). Median age was 67 years (range 18 to 93 years) and 40% of patients were deemed to have Pneumonia Severity Index class IV or V. Procalcitonin algorithm adherence was 76%. Antibiotic duration was not significantly different between clinical assessment- and procalcitonin-guided groups (median 9 versus 10 days, respectively). Clinical success rate was 92% in each group and serious adverse outcome rates were similar (15% versus 20%, respectively). CONCLUSION: Guideline-based serial clinical assessment did not reduce antibiotic exposure compared with procalcitonin-guided care among ED patients with community-acquired pneumonia. The strategies were similar in terms of duration of antibiotic use and clinical outcomes."
0,Tensor decomposition of hyperspectral images to study autofluorescence in age-related macular degeneration,,
0,Factors Associated with Progression of Japanese Open-Angle Glaucoma with Lower Normal Intraocular Pressure,,
0,Cost effectiveness of chimeric antigen receptor T-cell therapy in multiply relapsed or refractory adult large B-cell lymphoma,"PURPOSE Two anti-CD19 chimeric antigen receptor T-cell (CAR-T) therapies are approved for diffuse large B-cell lymphoma, axicabtagene ciloleucel (axi-cel) and tisagenlecleucel; each costs $373,000. We evaluated their cost effectiveness. METHODS We used a decision analytic Markov model informed by recent multicenter, single-arm trials to evaluate axi-cel and tisagenlecleucel in multiply relapsed/refractory, adult, diffuse large B-cell lymphoma from a US health payer perspective over a lifetime horizon. Under a range of plausible long-term effectiveness assumptions, each therapy was compared with salvage chemoimmunotherapy regimens and stem-cell transplantation. Main outcomes were undiscounted life years, discounted lifetime costs, discounted quality-adjusted life years (QALYs), and incremental cost-effectiveness ratio (3% annual discount rate). Sensitivity analyses explored uncertainty. RESULTS In an optimistic scenario, assuming a 40% 5-year progression-free survival (PFS), axi-cel increased life expectancy by 8.2 years at $129,000/QALY gained (95% uncertainty interval, $90,000 to $219,000). At a 30% 5-year PFS, improvements in life expectancy were more modest (6.4 years) and expensive ($159,000/QALY gained [95% uncertainty interval, $105,000 to $284,000]). In an optimistic scenario, assuming a 35% 5-year PFS, tisagenlecleucel increased life expectancy by 4.6 years at $168,000/QALY gained (95% uncertainty interval, $105,000 to $414,000/QALY). At a 25% 5-year PFS, improvements in life expectancy were smaller (3.4 years) and more expensive ($223,000/QALY gained [95% uncertainty interval, $123,000 to $1,170,000/QALY]). Administering CAR-T to all indicated patients would increase US health care costs by approximately $10 billion over 5 years. Price reductions to $250,000 and $200,000, respectively, or payment only for initial complete response (at current prices) would allow axi-cel and tisagenlecleucel to cost less than $150,000/QALY, even at 25% PFS. CONCLUSION At 2018 prices, it is possible that both CAR-T therapies meet a less than $150,000/QALY threshold. This depends on long-term outcomes compared with chemoimmunotherapy and stem-cell transplantation, which are uncertain. Widespread adoption would substantially increase non-Hodgkin lymphoma health care costs. Price reductions or payment for initial response would improve cost effectiveness, even with modest long-term outcomes.","Cost effectiveness of chimeric antigen receptor T-cell therapy in multiply relapsed or refractory adult large B-cell lymphoma. PURPOSE Two anti-CD19 chimeric antigen receptor T-cell (CAR-T) therapies are approved for diffuse large B-cell lymphoma, axicabtagene ciloleucel (axi-cel) and tisagenlecleucel; each costs $373,000. We evaluated their cost effectiveness. METHODS We used a decision analytic Markov model informed by recent multicenter, single-arm trials to evaluate axi-cel and tisagenlecleucel in multiply relapsed/refractory, adult, diffuse large B-cell lymphoma from a US health payer perspective over a lifetime horizon. Under a range of plausible long-term effectiveness assumptions, each therapy was compared with salvage chemoimmunotherapy regimens and stem-cell transplantation. Main outcomes were undiscounted life years, discounted lifetime costs, discounted quality-adjusted life years (QALYs), and incremental cost-effectiveness ratio (3% annual discount rate). Sensitivity analyses explored uncertainty. RESULTS In an optimistic scenario, assuming a 40% 5-year progression-free survival (PFS), axi-cel increased life expectancy by 8.2 years at $129,000/QALY gained (95% uncertainty interval, $90,000 to $219,000). At a 30% 5-year PFS, improvements in life expectancy were more modest (6.4 years) and expensive ($159,000/QALY gained [95% uncertainty interval, $105,000 to $284,000]). In an optimistic scenario, assuming a 35% 5-year PFS, tisagenlecleucel increased life expectancy by 4.6 years at $168,000/QALY gained (95% uncertainty interval, $105,000 to $414,000/QALY). At a 25% 5-year PFS, improvements in life expectancy were smaller (3.4 years) and more expensive ($223,000/QALY gained [95% uncertainty interval, $123,000 to $1,170,000/QALY]). Administering CAR-T to all indicated patients would increase US health care costs by approximately $10 billion over 5 years. Price reductions to $250,000 and $200,000, respectively, or payment only for initial complete response (at current prices) would allow axi-cel and tisagenlecleucel to cost less than $150,000/QALY, even at 25% PFS. CONCLUSION At 2018 prices, it is possible that both CAR-T therapies meet a less than $150,000/QALY threshold. This depends on long-term outcomes compared with chemoimmunotherapy and stem-cell transplantation, which are uncertain. Widespread adoption would substantially increase non-Hodgkin lymphoma health care costs. Price reductions or payment for initial response would improve cost effectiveness, even with modest long-term outcomes."
0,Automated profiling of growth cone heterogeneity defines relations between morphology and motility,"Growth cones are complex, motile structures at the tip of an outgrowing neurite. They often exhibit a high density of filopodia (thin actin bundles), which complicates the unbiased quantification of their morphologies by software. Contemporary image processing methods require extensive tuning of segmentation parameters, require significant manual curation, and are often not sufficiently adaptable to capture morphology changes associated with switches in regulatory signals. To overcome these limitations, we developed Growth Cone Analyzer (GCA). GCA is designed to quantify growth cone morphodynamics from time-lapse sequences imaged both in vitro and in vivo, but is sufficiently generic that it may be applied to nonneuronal cellular structures. We demonstrate the adaptability of GCA through the analysis of growth cone morphological variation and its relation to motility in both an unperturbed system and in the context of modified Rho GTPase signaling. We find that perturbations inducing similar changes in neurite length exhibit underappreciated phenotypic nuance at the scale of the growth cone.","Automated profiling of growth cone heterogeneity defines relations between morphology and motility. Growth cones are complex, motile structures at the tip of an outgrowing neurite. They often exhibit a high density of filopodia (thin actin bundles), which complicates the unbiased quantification of their morphologies by software. Contemporary image processing methods require extensive tuning of segmentation parameters, require significant manual curation, and are often not sufficiently adaptable to capture morphology changes associated with switches in regulatory signals. To overcome these limitations, we developed Growth Cone Analyzer (GCA). GCA is designed to quantify growth cone morphodynamics from time-lapse sequences imaged both in vitro and in vivo, but is sufficiently generic that it may be applied to nonneuronal cellular structures. We demonstrate the adaptability of GCA through the analysis of growth cone morphological variation and its relation to motility in both an unperturbed system and in the context of modified Rho GTPase signaling. We find that perturbations inducing similar changes in neurite length exhibit underappreciated phenotypic nuance at the scale of the growth cone."
0,Back to baseline: sleep recalibrates synapses,,
0,Catestatin regulates vesicular quanta through modulation of cholinergic and peptidergic (PACAPergic) stimulation in PC12 cells,"We have previously shown that the chromogranin A (CgA)-derived peptide catestatin (CST: hCgA352â€“372) inhibits nicotine-induced secretion of catecholamines from the adrenal medulla and chromaffin cells. In the present study, we seek to determine whether CST regulates dense core (DC) vesicle (DCV) quanta (catecholamine and chromogranin/secretogranin proteins) during acute (0.5-h treatment) or chronic (24-h treatment) cholinergic (nicotine) or peptidergic (PACAP, pituitary adenylyl cyclase activating polypeptide) stimulation of PC12 cells. In acute experiments, we found that both nicotine (60 Î¼M) and PACAP (0.1Â Î¼M) decreased intracellular norepinephrine (NE) content and increased 3Hâ€NE secretion, with both effects markedly inhibited by co-treatment with CST (2Â Î¼M). In chronic experiments, we found that nicotine and PACAP both reduced DCV and DC diameters and that this effect was likewise prevented by CST. Nicotine or CST alone increased expression of CgA protein and together elicited an additional increase in CgA protein, implying that nicotine and CST utilize separate signaling pathways to activate CgA expression. In contrast, PACAP increased expression of CgB and SgII proteins, with a further potentiation by CST. CST augmented the expression of tyrosine hydroxylase (TH) but did not increase intracellular NE levels, presumably due to its inability to cause post-translational activation of TH through serine phosphorylation. Co-treatment of CST with nicotine or PACAP increased quantal size, plausibly due to increased synthesis of CgA, CgB and SgII by CST. We conclude that CST regulates DCV quanta by acutely inhibiting catecholamine secretion and chronicallyÂ increasing expression of CgA after nicotinic stimulation and CgB and SgII after PACAPergic stimulation.","Catestatin regulates vesicular quanta through modulation of cholinergic and peptidergic (PACAPergic) stimulation in PC12 cells. We have previously shown that the chromogranin A (CgA)-derived peptide catestatin (CST: hCgA352â€“372) inhibits nicotine-induced secretion of catecholamines from the adrenal medulla and chromaffin cells. In the present study, we seek to determine whether CST regulates dense core (DC) vesicle (DCV) quanta (catecholamine and chromogranin/secretogranin proteins) during acute (0.5-h treatment) or chronic (24-h treatment) cholinergic (nicotine) or peptidergic (PACAP, pituitary adenylyl cyclase activating polypeptide) stimulation of PC12 cells. In acute experiments, we found that both nicotine (60 Î¼M) and PACAP (0.1Â Î¼M) decreased intracellular norepinephrine (NE) content and increased 3Hâ€NE secretion, with both effects markedly inhibited by co-treatment with CST (2Â Î¼M). In chronic experiments, we found that nicotine and PACAP both reduced DCV and DC diameters and that this effect was likewise prevented by CST. Nicotine or CST alone increased expression of CgA protein and together elicited an additional increase in CgA protein, implying that nicotine and CST utilize separate signaling pathways to activate CgA expression. In contrast, PACAP increased expression of CgB and SgII proteins, with a further potentiation by CST. CST augmented the expression of tyrosine hydroxylase (TH) but did not increase intracellular NE levels, presumably due to its inability to cause post-translational activation of TH through serine phosphorylation. Co-treatment of CST with nicotine or PACAP increased quantal size, plausibly due to increased synthesis of CgA, CgB and SgII by CST. We conclude that CST regulates DCV quanta by acutely inhibiting catecholamine secretion and chronicallyÂ increasing expression of CgA after nicotinic stimulation and CgB and SgII after PACAPergic stimulation."
0,Regional times to equilibria and their impact on semi-quantification of [18F]AV-1451 uptake,"The semi-quantitative estimate standardised uptake value ratios (SUVR) correlate well with specific binding of the tracer expressed as distribution volume ratios (DVR) for the tau positron emission tomography tracer [18F]AV-1451 uptake and are therefore widely used as proxy for tracer binding. With regard to tracer kinetic modelling, there exists a time point when SUVR deviates minimally from DVR, occurring when the specific binding reaches a transient equilibrium. Here, we have investigated whether the time to equilibrium affects the agreement between SUVR and DVR across different brain regions. We show that the time required to reach equilibrium differs across brain regions, resulting in region-specific biases. However, even though the 80â€“100 min post-injection time window did not show the smallest bias numerically, the disagreement between SUVR and DVR varied least between regions during this time. In conclusion, our findings suggest a regional component to the bias of SUVR related to the time to transient equilibrium of the specific binding. [18F]AV-1451 uptake should consequently be interpreted with some caution when compared across brain regions using this method of quantification. The commonly used time window 80â€“100 min post-injection shows the most consistent bias across regions and is recommended for semi-quantification of [18F]AV-1451.","Regional times to equilibria and their impact on semi-quantification of [18F]AV-1451 uptake. The semi-quantitative estimate standardised uptake value ratios (SUVR) correlate well with specific binding of the tracer expressed as distribution volume ratios (DVR) for the tau positron emission tomography tracer [18F]AV-1451 uptake and are therefore widely used as proxy for tracer binding. With regard to tracer kinetic modelling, there exists a time point when SUVR deviates minimally from DVR, occurring when the specific binding reaches a transient equilibrium. Here, we have investigated whether the time to equilibrium affects the agreement between SUVR and DVR across different brain regions. We show that the time required to reach equilibrium differs across brain regions, resulting in region-specific biases. However, even though the 80â€“100 min post-injection time window did not show the smallest bias numerically, the disagreement between SUVR and DVR varied least between regions during this time. In conclusion, our findings suggest a regional component to the bias of SUVR related to the time to transient equilibrium of the specific binding. [18F]AV-1451 uptake should consequently be interpreted with some caution when compared across brain regions using this method of quantification. The commonly used time window 80â€“100 min post-injection shows the most consistent bias across regions and is recommended for semi-quantification of [18F]AV-1451."
0,ADAPT: An Algorithm Incorporating PRO-C3 Accurately Identifies Patients With NAFLD and Advanced Fibrosis,,
0,Integration of transthoracic focused cardiac ultrasound in the diagnostic algorithm for suspected acute aortic syndromes,"AIMS: The diagnosis of acute aortic syndromes (AASs) is challenging and requires integrated strategies. Transthoracic focused cardiac ultrasound (FoCUS) is endorsed by guidelines as a first-line/triage tool allowing rapid bedside assessment of the aorta. However, the performance of FoCUS in the European Society of Cardiology-recommended workup of AASs awaits validation. METHODS AND RESULTS: This was a prespecified subanalysis of the ADvISED multicentre prospective study. Patients with suspected AAS underwent FoCUS for detection of direct/indirect signs of AAS. Clinical probability assessment was performed with the aortic dissection detection risk score (ADD-RS). Case adjudication was based on advanced imaging, surgery, autopsy, or 14-day follow-up. An AAS was diagnosed in 146 (17.4%) of 839 patients. Presence of direct FoCUS signs had a sensitivity and specificity of 45.2% [95% confidence interval (CI) 37-53.6%] and 97.4% (95% CI 95.9-98.4%), while presence of any FoCUS sign had a sensitivity and specificity of 89% (95% CI 82.8-93.6%) and 74.5% (95% CI 71-77.7%) for AAS. The additive value of FoCUS was most evident within low clinical probability (ADD-RS </=1). Herein, direct FoCUS signs were identified in 40 (4.8%) patients (P < 0.001), including 29 with AAS. ADD-RS </=1 plus negative FoCUS for AAS rule-out had a sensitivity of 93.8% (95% CI 88.6-97.1%) and a failure rate of 1.9% (95% CI 0.9-3.6%). Addition of negative D-dimer led to a failure rate of 0% (95% CI 0-1.2%). CONCLUSION: FoCUS has additive value in the workup of AASs. Direct FoCUS signs can rapidly identify patients requiring advanced imaging despite low clinical probability. In integrated bundles, negative FoCUS is useful for rule-out of AASs.","Integration of transthoracic focused cardiac ultrasound in the diagnostic algorithm for suspected acute aortic syndromes. AIMS: The diagnosis of acute aortic syndromes (AASs) is challenging and requires integrated strategies. Transthoracic focused cardiac ultrasound (FoCUS) is endorsed by guidelines as a first-line/triage tool allowing rapid bedside assessment of the aorta. However, the performance of FoCUS in the European Society of Cardiology-recommended workup of AASs awaits validation. METHODS AND RESULTS: This was a prespecified subanalysis of the ADvISED multicentre prospective study. Patients with suspected AAS underwent FoCUS for detection of direct/indirect signs of AAS. Clinical probability assessment was performed with the aortic dissection detection risk score (ADD-RS). Case adjudication was based on advanced imaging, surgery, autopsy, or 14-day follow-up. An AAS was diagnosed in 146 (17.4%) of 839 patients. Presence of direct FoCUS signs had a sensitivity and specificity of 45.2% [95% confidence interval (CI) 37-53.6%] and 97.4% (95% CI 95.9-98.4%), while presence of any FoCUS sign had a sensitivity and specificity of 89% (95% CI 82.8-93.6%) and 74.5% (95% CI 71-77.7%) for AAS. The additive value of FoCUS was most evident within low clinical probability (ADD-RS </=1). Herein, direct FoCUS signs were identified in 40 (4.8%) patients (P < 0.001), including 29 with AAS. ADD-RS </=1 plus negative FoCUS for AAS rule-out had a sensitivity of 93.8% (95% CI 88.6-97.1%) and a failure rate of 1.9% (95% CI 0.9-3.6%). Addition of negative D-dimer led to a failure rate of 0% (95% CI 0-1.2%). CONCLUSION: FoCUS has additive value in the workup of AASs. Direct FoCUS signs can rapidly identify patients requiring advanced imaging despite low clinical probability. In integrated bundles, negative FoCUS is useful for rule-out of AASs."
0,Risk Prediction Tools to Improve Patient Selection for Carotid Endarterectomy Among Patients With Asymptomatic Carotid Stenosis,,
0,Quercetin relieved diabetic neuropathic pain by inhibiting upregulated P2X4 receptor in dorsal root ganglia,"The upregulation of nociceptive ion channels expressed in dorsal root ganglia (DRG) contributes to the development and retaining of diabetic pain symptoms. The flavonoid quercetin (3,3â€²,4â€²,5,7-pentahydroxyflavone) is a component extracted from various fruits and vegetables and exerts anti-inflammatory, analgesic, anticarcinogenic, antiulcer, and antihypertensive effects. However, the exact mechanism underlying quercetin's analgesic action remains poorly understood. The aim of this study was to investigate the effects of quercetin on diabetic neuropathic pain related to the P2X4 receptor in the DRG of type 2 diabetic rat model. Our data showed that both mechanical withdrawal threshold and thermal withdrawal latency in diabetic rats treated with quercetin were higher compared with those in untreated diabetic rats. The expression levels of P2X4 messenger RNA and protein in the DRG of diabetic rats were increased compared with the control rats, while quercetin treatment significantly inhibited such enhanced P2X4 expression in diabetic rats. The satellite glial cells (SGCs) enwrap the neuronal soma in the DRG. Quercetin treatment also lowered the elevated coexpression of P2X4 and glial fibrillary acidic protein (a marker of SGCs) and decreased the upregulation of phosphorylated p38 mitogen-activated protein kinase (p38MAPK) in the DRG of diabetic rats. Quercetin significantly reduced the P2X4 agonist adenosine triphosphate-activated currents in HEK293 cells transfected with P2X4 receptors. Thus, our data demonstrate that quercetin may decrease the upregulation of the P2X4 receptor in DRG SGCs, and consequently inhibit P2X4 receptor-mediated p38MAPK activation to relieve the mechanical and thermal hyperalgesia in diabetic rats.","Quercetin relieved diabetic neuropathic pain by inhibiting upregulated P2X4 receptor in dorsal root ganglia. The upregulation of nociceptive ion channels expressed in dorsal root ganglia (DRG) contributes to the development and retaining of diabetic pain symptoms. The flavonoid quercetin (3,3â€²,4â€²,5,7-pentahydroxyflavone) is a component extracted from various fruits and vegetables and exerts anti-inflammatory, analgesic, anticarcinogenic, antiulcer, and antihypertensive effects. However, the exact mechanism underlying quercetin's analgesic action remains poorly understood. The aim of this study was to investigate the effects of quercetin on diabetic neuropathic pain related to the P2X4 receptor in the DRG of type 2 diabetic rat model. Our data showed that both mechanical withdrawal threshold and thermal withdrawal latency in diabetic rats treated with quercetin were higher compared with those in untreated diabetic rats. The expression levels of P2X4 messenger RNA and protein in the DRG of diabetic rats were increased compared with the control rats, while quercetin treatment significantly inhibited such enhanced P2X4 expression in diabetic rats. The satellite glial cells (SGCs) enwrap the neuronal soma in the DRG. Quercetin treatment also lowered the elevated coexpression of P2X4 and glial fibrillary acidic protein (a marker of SGCs) and decreased the upregulation of phosphorylated p38 mitogen-activated protein kinase (p38MAPK) in the DRG of diabetic rats. Quercetin significantly reduced the P2X4 agonist adenosine triphosphate-activated currents in HEK293 cells transfected with P2X4 receptors. Thus, our data demonstrate that quercetin may decrease the upregulation of the P2X4 receptor in DRG SGCs, and consequently inhibit P2X4 receptor-mediated p38MAPK activation to relieve the mechanical and thermal hyperalgesia in diabetic rats."
0,Dynamic Organellar Maps for Spatial Proteomics,"Eukaryotic cells are highly compartmentalized and protein subcellular localization critically influences protein function. Identification of the subcellular localizations of proteins and their translocation events upon perturbation has mostly been confined to targeted studies or laborious microscopy-based methods. Here we describe a systematic mass spectrometry-based method for spatial proteomics. The approach uses simple fractionation profiling and has two applications: Firstly it can be used to infer subcellular protein localization on a proteome-wide scale, resulting in a protein map of the cell. Secondly, the method permits identification of changes in protein localization, by comparing maps made under different conditions, as a tool for unbiased systems cell biology. Â© 2018 by John Wiley & Sons, Inc.","Dynamic Organellar Maps for Spatial Proteomics. Eukaryotic cells are highly compartmentalized and protein subcellular localization critically influences protein function. Identification of the subcellular localizations of proteins and their translocation events upon perturbation has mostly been confined to targeted studies or laborious microscopy-based methods. Here we describe a systematic mass spectrometry-based method for spatial proteomics. The approach uses simple fractionation profiling and has two applications: Firstly it can be used to infer subcellular protein localization on a proteome-wide scale, resulting in a protein map of the cell. Secondly, the method permits identification of changes in protein localization, by comparing maps made under different conditions, as a tool for unbiased systems cell biology. Â© 2018 by John Wiley & Sons, Inc."
0,"A novel resveratrol derivative induces mitotic arrest, centrosome fragmentation and cancer cell death by inhibiting Î³-tubulin","Background: Resveratrol and its natural stilbene-containing derivatives have been extensively investigated as potential chemotherapeutic agents. The synthetic manipulation of the stilbene scaffold has led to the generation of new analogues with improved anticancer activity and better bioavailability. In the present study we investigated the anticancer activity of a novel trimethoxystilbene derivative (3,4,4â€²-trimethoxylstilbene), where two methoxyl groups are adjacent on the benzene ring (ortho configuration), and compared its activity to 3,5,4â€²-trimethoxylstilbene, whose methoxyl groups are in meta configuration. Results: We provide evidence that the presence of the two methoxyl groups in ortho configuration renders 3,4,4â€²-trimethoxystilbene more efficient than the meta isomer in inhibiting cell proliferation and producing apoptotic death in colorectal cancer cells. Confocal microscopy of Î±- and Î³-tubulin staining shows that the novel compound strongly depolymerizes the mitotic spindle and produces fragmentation of the pericentrosomal material. Computer assisted docking studies indicate that both molecules potentially interact with Î³-tubulin, and that 3,4,4â€²-trimethoxystilbene is likely to establish stronger interactions with the protein. Conclusions: These findings demonstrate the ortho configuration confers higher specificity for Î³-tubulin with respect to Î±-tubulin on 3,4,4â€² trimethoxystilbene, allowing it to be defined as a new Î³-tubulin inhibitor. A strong interaction with Î³-tubulin might be a defining feature of molecules with high anticancer activity, as shown for the 3,4,4â€² isomer.","A novel resveratrol derivative induces mitotic arrest, centrosome fragmentation and cancer cell death by inhibiting Î³-tubulin. Background: Resveratrol and its natural stilbene-containing derivatives have been extensively investigated as potential chemotherapeutic agents. The synthetic manipulation of the stilbene scaffold has led to the generation of new analogues with improved anticancer activity and better bioavailability. In the present study we investigated the anticancer activity of a novel trimethoxystilbene derivative (3,4,4â€²-trimethoxylstilbene), where two methoxyl groups are adjacent on the benzene ring (ortho configuration), and compared its activity to 3,5,4â€²-trimethoxylstilbene, whose methoxyl groups are in meta configuration. Results: We provide evidence that the presence of the two methoxyl groups in ortho configuration renders 3,4,4â€²-trimethoxystilbene more efficient than the meta isomer in inhibiting cell proliferation and producing apoptotic death in colorectal cancer cells. Confocal microscopy of Î±- and Î³-tubulin staining shows that the novel compound strongly depolymerizes the mitotic spindle and produces fragmentation of the pericentrosomal material. Computer assisted docking studies indicate that both molecules potentially interact with Î³-tubulin, and that 3,4,4â€²-trimethoxystilbene is likely to establish stronger interactions with the protein. Conclusions: These findings demonstrate the ortho configuration confers higher specificity for Î³-tubulin with respect to Î±-tubulin on 3,4,4â€² trimethoxystilbene, allowing it to be defined as a new Î³-tubulin inhibitor. A strong interaction with Î³-tubulin might be a defining feature of molecules with high anticancer activity, as shown for the 3,4,4â€² isomer."
0,Targeting Glioblastoma Stem Cells through Disruption of the Circadian Clock,,
0,"TNeoadjuvant letrozole plus taselisib versus letrozole plus placebo in postmenopausal women with oestrogen receptor-positive, HER2-negative, early-stage breast cancer (LORELEI): a multicentre, randomised, double-blind, placebo-controlled, phase 2 trial",,
0,"Postmortem Cortex Samples Identify Distinct Molecular Subtypes of ALS: Retrotransposon Activation, Oxidative Stress, and Activated Glia","Amyotrophic lateral sclerosis (ALS) is a fatal neurodegenerative disease characterized by the progressive loss of motor neurons. While several pathogenic mutations have been identified, the vast majority of ALS cases have no family history of disease. Thus, for most ALS cases, the disease may be a product of multiple pathways contributing to varying degrees in each patient. Using machine learning algorithms, we stratify the transcriptomes of 148 ALS postmortem cortex samples into three distinct molecular subtypes. The largest cluster, identified in 61% of patient samples, displays hallmarks of oxidative and proteotoxic stress. Another 19% of the samples shows predominant signatures of glial activation. Finally, a third group (20%) exhibits high levels of retrotransposon expression and signatures of TARDBP/TDP-43 dysfunction. We further demonstrate that TDP-43 (1) directly binds a subset of retrotransposon transcripts and contributes to their silencing in vitro, and (2) pathological TDP-43 aggregation correlates with retrotransposon de-silencing in vivo.","Postmortem Cortex Samples Identify Distinct Molecular Subtypes of ALS: Retrotransposon Activation, Oxidative Stress, and Activated Glia. Amyotrophic lateral sclerosis (ALS) is a fatal neurodegenerative disease characterized by the progressive loss of motor neurons. While several pathogenic mutations have been identified, the vast majority of ALS cases have no family history of disease. Thus, for most ALS cases, the disease may be a product of multiple pathways contributing to varying degrees in each patient. Using machine learning algorithms, we stratify the transcriptomes of 148 ALS postmortem cortex samples into three distinct molecular subtypes. The largest cluster, identified in 61% of patient samples, displays hallmarks of oxidative and proteotoxic stress. Another 19% of the samples shows predominant signatures of glial activation. Finally, a third group (20%) exhibits high levels of retrotransposon expression and signatures of TARDBP/TDP-43 dysfunction. We further demonstrate that TDP-43 (1) directly binds a subset of retrotransposon transcripts and contributes to their silencing in vitro, and (2) pathological TDP-43 aggregation correlates with retrotransposon de-silencing in vivo."
0,Genetically elevated circulating homocysteine concentrations increase the risk of diabetic kidney disease in Chinese diabetic patients,"Diabetic kidney disease (DKD) is a devastating and frequent complication of diabetes mellitus. Here, we first adopted methylenetetrahytrofolate reductase (MTHFR) gene C677T polymorphism as an instrument to infer the possible causal relevance between circulating homocysteine and DKD risk in a Chinese population and next attempted to build a risk prediction model for DKD. This is a hospital-based case-control association study. Total 1107 study participants were diagnosed with type 2 diabetes mellitus, including 547 patients with newly diagnosed and histologically confirmed DKD. MTHFR gene C677T polymorphism was determined using the TaqMan method. Carriers of 677TT genotype (14.55Â Î¼mol/L) had significantly higher homocysteine concentrations than carriers of 677CT genotype (12.88Â Î¼mol/L) (PÂ <Â 0.001). Carriers of 677TT genotype had a 1.57-fold increased risk of DKD (odds ratio: 1.57, 95% CI: 1.21-2.05, PÂ =Â 0.001) relative to carriers of 677CT genotype after adjusting for confounders. Mendelian randomization analysis revealed that the odds ratio for DKD relative to diabetes mellitus per 5Â Î¼mol/L increment of circulating homocysteine concentrations was 3.86 (95% confidence interval: 1.21-2.05, PÂ <Â 0.001). In the Logistic regression analysis, hypertension, homocysteine and triglyceride were significantly associated with an increased risk of DKD and they constituted a risk prediction model with good test performance and discriminatory capacity. Taken together, our findings provide evidence that elevated circulating homocysteine concentrations were causally associated with an increased risk of DKD in Chinese diabetic patients.","Genetically elevated circulating homocysteine concentrations increase the risk of diabetic kidney disease in Chinese diabetic patients. Diabetic kidney disease (DKD) is a devastating and frequent complication of diabetes mellitus. Here, we first adopted methylenetetrahytrofolate reductase (MTHFR) gene C677T polymorphism as an instrument to infer the possible causal relevance between circulating homocysteine and DKD risk in a Chinese population and next attempted to build a risk prediction model for DKD. This is a hospital-based case-control association study. Total 1107 study participants were diagnosed with type 2 diabetes mellitus, including 547 patients with newly diagnosed and histologically confirmed DKD. MTHFR gene C677T polymorphism was determined using the TaqMan method. Carriers of 677TT genotype (14.55Â Î¼mol/L) had significantly higher homocysteine concentrations than carriers of 677CT genotype (12.88Â Î¼mol/L) (PÂ <Â 0.001). Carriers of 677TT genotype had a 1.57-fold increased risk of DKD (odds ratio: 1.57, 95% CI: 1.21-2.05, PÂ =Â 0.001) relative to carriers of 677CT genotype after adjusting for confounders. Mendelian randomization analysis revealed that the odds ratio for DKD relative to diabetes mellitus per 5Â Î¼mol/L increment of circulating homocysteine concentrations was 3.86 (95% confidence interval: 1.21-2.05, PÂ <Â 0.001). In the Logistic regression analysis, hypertension, homocysteine and triglyceride were significantly associated with an increased risk of DKD and they constituted a risk prediction model with good test performance and discriminatory capacity. Taken together, our findings provide evidence that elevated circulating homocysteine concentrations were causally associated with an increased risk of DKD in Chinese diabetic patients."
0,TMB Faces Validation Hurdles,,
0,Radiomic Machine Learning: Is It Really a Useful Method for the Characterization of Prostate Cancer?,,
0,"Exploiting structural redundancy in q-space for improved EAP reconstruction from highly undersampled (k, q)-space in DMRI",,
0,Quercetin modifies 5â€²CpG promoter methylation and reactivates various tumor suppressor genes by modulating epigenetic marks in human cervical cancer cells,"The central role of epigenomic alterations in carcinogenesis has been widely acknowledged, particularly the impact of DNA methylation on gene expression across all stages of carcinogenesis is considered vital for both diagnostic and therapeutic strategies. Dietary phytochemicals hold great promise as safe anticancer agents and effective epigenetic modulators. This study was designed to investigate the potential of a phytochemical, quercetin as a modulator of the epigenetic pathways for anticancer strategies. Biochemical activity of DNA methyltransferases (DNMTs), histone deacetylases (HDACs), histone methyltransferases (HMTs), and global genomic DNA methylation was quantitated by an enzyme-linked immunosorbent assay based assay in quercetin-treated HeLa cells. Molecular docking studies were performed to predict the interaction of quercetin with DNMTs and HDACs. Quantitative methylation array was used to assess quercetin-mediated alterations in the promoter methylation of selected tumor suppressor genes (TSGs). Quercetin induced modulation of chromatin modifiers including DNMTs, HDACs, histone acetyltransferases (HAT) and HMTs, and TSGs were assessed by quantitative reverse transcription PCR (qRT-PCR). It was found that quercetin modulates the expression of various chromatin modifiers and decreases the activity of DNMTs, HDACs, and HMTs in a dose-dependent manner. Molecular docking results suggest that quercetin could function as a competitive inhibitor by interacting with residues in the catalytic cavity of several DNMTs and HDACs. Quercetin downregulated global DNA methylation levels in a dose- and time-dependent manner. The tested TSGs showed steep dose-dependent decline in promoter methylation with the restoration of their expression. Our study provides an understanding of the quercetin's mechanism of action and will aid in its development as a candidate for epigenetic-based anticancer therapy.","Quercetin modifies 5â€²CpG promoter methylation and reactivates various tumor suppressor genes by modulating epigenetic marks in human cervical cancer cells. The central role of epigenomic alterations in carcinogenesis has been widely acknowledged, particularly the impact of DNA methylation on gene expression across all stages of carcinogenesis is considered vital for both diagnostic and therapeutic strategies. Dietary phytochemicals hold great promise as safe anticancer agents and effective epigenetic modulators. This study was designed to investigate the potential of a phytochemical, quercetin as a modulator of the epigenetic pathways for anticancer strategies. Biochemical activity of DNA methyltransferases (DNMTs), histone deacetylases (HDACs), histone methyltransferases (HMTs), and global genomic DNA methylation was quantitated by an enzyme-linked immunosorbent assay based assay in quercetin-treated HeLa cells. Molecular docking studies were performed to predict the interaction of quercetin with DNMTs and HDACs. Quantitative methylation array was used to assess quercetin-mediated alterations in the promoter methylation of selected tumor suppressor genes (TSGs). Quercetin induced modulation of chromatin modifiers including DNMTs, HDACs, histone acetyltransferases (HAT) and HMTs, and TSGs were assessed by quantitative reverse transcription PCR (qRT-PCR). It was found that quercetin modulates the expression of various chromatin modifiers and decreases the activity of DNMTs, HDACs, and HMTs in a dose-dependent manner. Molecular docking results suggest that quercetin could function as a competitive inhibitor by interacting with residues in the catalytic cavity of several DNMTs and HDACs. Quercetin downregulated global DNA methylation levels in a dose- and time-dependent manner. The tested TSGs showed steep dose-dependent decline in promoter methylation with the restoration of their expression. Our study provides an understanding of the quercetin's mechanism of action and will aid in its development as a candidate for epigenetic-based anticancer therapy."
0,Single-Cell Transcriptomic Analyses of Cell Fate Transitions during Human Cardiac Reprogramming,"Direct cellular reprogramming provides a powerful platform to study cell plasticity and dissect mechanisms underlying cell fate determination. Here, we report a single-cell transcriptomic study of human cardiac (hiCM) reprogramming that utilizes an analysis pipeline incorporating current data normalization methods, multiple trajectory prediction algorithms, and a cell fate index calculation we developed to measure reprogramming progression. These analyses revealed hiCM reprogramming-specific features and a decision point at which cells either embark on reprogramming or regress toward their original fibroblast state. In combination with functional screening, we found that immune-response-associated DNA methylation is required for hiCM induction and validated several downstream targets of reprogramming factors as necessary for productive hiCM reprograming. Collectively, this single-cell transcriptomics study provides detailed datasets that reveal molecular features underlying hiCM determination and rigorous analytical pipelines for predicting cell fate conversion. Zhou et al. performed single-cell RNA sequencing to unravel molecular features of human cardiac reprogramming. They identified a â€œdecisionâ€ point where cells either reprogram or regress to initial fate. Further, progression of reprogramming was quantitatively assessed by their developed â€œcell fate index,â€ which could be used for studying other biological processes.","Single-Cell Transcriptomic Analyses of Cell Fate Transitions during Human Cardiac Reprogramming. Direct cellular reprogramming provides a powerful platform to study cell plasticity and dissect mechanisms underlying cell fate determination. Here, we report a single-cell transcriptomic study of human cardiac (hiCM) reprogramming that utilizes an analysis pipeline incorporating current data normalization methods, multiple trajectory prediction algorithms, and a cell fate index calculation we developed to measure reprogramming progression. These analyses revealed hiCM reprogramming-specific features and a decision point at which cells either embark on reprogramming or regress toward their original fibroblast state. In combination with functional screening, we found that immune-response-associated DNA methylation is required for hiCM induction and validated several downstream targets of reprogramming factors as necessary for productive hiCM reprograming. Collectively, this single-cell transcriptomics study provides detailed datasets that reveal molecular features underlying hiCM determination and rigorous analytical pipelines for predicting cell fate conversion. Zhou et al. performed single-cell RNA sequencing to unravel molecular features of human cardiac reprogramming. They identified a â€œdecisionâ€ point where cells either reprogram or regress to initial fate. Further, progression of reprogramming was quantitatively assessed by their developed â€œcell fate index,â€ which could be used for studying other biological processes."
0,Associative conditioning remaps odor representations and modifies inhibition in a higher olfactory brain area,,
0,Impact of All-Oral Direct-Acting Antivirals on Clinical and Economic Outcomes in Patients With Chronic Hepatitis C in the United States,,
0,Deep convolutional neural network models for the diagnosis of thyroid cancer,,
0,Assessment of risk based on variant pathways and establishment of an artificial neural network model of thyroid cancer,"Background: This study aimed to establish an artificial neural network (ANN) model based on variant pathways to predict the risk of thyroid cancer. Methods: The RNASeq data of 482 thyroid cancer samples were downloaded from the TCGA database. The samples were divided into low-risk and high-risk groups, followed by identification of differentially expressed genes (DEGs). Co-expression analysis and pathway enrichment analysis were then performed. The variant pathways were screened according to the functional deviation score of each pathway, and an ANN model was established. Finally, the efficiency of the ANN model for risk assessment was validated by survival analysis and analysis of an independent microarray dataset (GSE34289) for thyroid cancer. Results: In total, 190 DEGs (85 up-regulated and 105 down-regulated) were identified between the low-risk and high-risk groups. Ten risk-related variant pathways were identified between the low-risk and high-risk groups, which were related to inflammatory and immune responses. Based on these variant pathways, an ANN model was built, consisting of an input layer, two hidden layers, and an output layer, corresponding to 15, 8, 5, and 1 neuron, respectively. Survival analysis showed that this model could effectively distinguish the samples with different risks. Analysis of microarray dataset GSE34289 showed that the accuracy of this model for predicating low-risk and high-risk samples was 77.5 and 86.0%, respectively. Conclusions: This study suggests that the ANN model based on variant pathways can be used for effectively evaluating the risk of thyroid cancer.","Assessment of risk based on variant pathways and establishment of an artificial neural network model of thyroid cancer. Background: This study aimed to establish an artificial neural network (ANN) model based on variant pathways to predict the risk of thyroid cancer. Methods: The RNASeq data of 482 thyroid cancer samples were downloaded from the TCGA database. The samples were divided into low-risk and high-risk groups, followed by identification of differentially expressed genes (DEGs). Co-expression analysis and pathway enrichment analysis were then performed. The variant pathways were screened according to the functional deviation score of each pathway, and an ANN model was established. Finally, the efficiency of the ANN model for risk assessment was validated by survival analysis and analysis of an independent microarray dataset (GSE34289) for thyroid cancer. Results: In total, 190 DEGs (85 up-regulated and 105 down-regulated) were identified between the low-risk and high-risk groups. Ten risk-related variant pathways were identified between the low-risk and high-risk groups, which were related to inflammatory and immune responses. Based on these variant pathways, an ANN model was built, consisting of an input layer, two hidden layers, and an output layer, corresponding to 15, 8, 5, and 1 neuron, respectively. Survival analysis showed that this model could effectively distinguish the samples with different risks. Analysis of microarray dataset GSE34289 showed that the accuracy of this model for predicating low-risk and high-risk samples was 77.5 and 86.0%, respectively. Conclusions: This study suggests that the ANN model based on variant pathways can be used for effectively evaluating the risk of thyroid cancer."
0,In silico analysis and molecular dynamics simulation of human superoxide dismutase 3 (SOD3) genetic variants,"Oxidative stress is a major factor in aging processes. Superoxide dismutase 3 (SOD3) plays a key role in the protection of extracellular oxidative stress. Missense mutations in SOD3 have been described to be associated with the occurrence of pulmonary, cardiovascular, and neoplastic diseases. This study aims to analyze the effects of missense mutations on the SOD3 structure and function by modeling a complete SOD3 structure as well as analyzing the differences between the wild-types and mutants using computational simulations. Here, ten algorithms were used to predict the structural and functional effects of missense mutations. A complete model of SOD3 protein was made by ab initio and comparative modeling using the Rosetta algorithm and validated by PROCHECK, Verify 3D, QMEAN, and ProSa. Molecular dynamics (MD) simulations were performed and analyzed using the GROMACS package. The deleterious potential of the A58T and R231G mutants was not predicted by the majority of the used algorithms. The analyzed mutations were predicted as destabilizing by at least one algorithm. The MD analyses indicated that protein flexibility may be increased by all of the analyzed mutations, while the protein-ligand stability may be decreased. They also suggested that the variants A91T and R231G increase the overall dimensions of SOD3 and decrease its accessible surface area. Our findings, therefore, indicated that the analyzed mutations could affect the protein structure and its ability to interact with other molecules, which may be related to the functional impairment of SOD3 upon A58T and R231G mutations, as well as their involvement in pathologies.","In silico analysis and molecular dynamics simulation of human superoxide dismutase 3 (SOD3) genetic variants. Oxidative stress is a major factor in aging processes. Superoxide dismutase 3 (SOD3) plays a key role in the protection of extracellular oxidative stress. Missense mutations in SOD3 have been described to be associated with the occurrence of pulmonary, cardiovascular, and neoplastic diseases. This study aims to analyze the effects of missense mutations on the SOD3 structure and function by modeling a complete SOD3 structure as well as analyzing the differences between the wild-types and mutants using computational simulations. Here, ten algorithms were used to predict the structural and functional effects of missense mutations. A complete model of SOD3 protein was made by ab initio and comparative modeling using the Rosetta algorithm and validated by PROCHECK, Verify 3D, QMEAN, and ProSa. Molecular dynamics (MD) simulations were performed and analyzed using the GROMACS package. The deleterious potential of the A58T and R231G mutants was not predicted by the majority of the used algorithms. The analyzed mutations were predicted as destabilizing by at least one algorithm. The MD analyses indicated that protein flexibility may be increased by all of the analyzed mutations, while the protein-ligand stability may be decreased. They also suggested that the variants A91T and R231G increase the overall dimensions of SOD3 and decrease its accessible surface area. Our findings, therefore, indicated that the analyzed mutations could affect the protein structure and its ability to interact with other molecules, which may be related to the functional impairment of SOD3 upon A58T and R231G mutations, as well as their involvement in pathologies."
0,Estimating the loss of lifetime function using flexible parametric relative survival models,"BACKGROUND: Within cancer care, dynamic evaluations of the loss in expectation of life provides useful information to patients as well as physicians. The loss of lifetime function yields the conditional loss in expectation of life given survival up to a specific time point. Due to the inevitable censoring in time-to-event data, loss of lifetime estimation requires extrapolation of both the patient and general population survival function. In this context, the accuracy of different extrapolation approaches has not previously been evaluated. METHODS: The loss of lifetime function was computed by decomposing the all-cause survival function using the relative and general population survival function. To allow extrapolation, the relative survival function was fitted using existing parametric relative survival models. In addition, we introduced a novel mixture cure model suitable for extrapolation. The accuracy of the estimated loss of lifetime function using various extrapolation approaches was assessed in a simulation study and by data from the Danish Cancer Registry where complete follow-up was available. In addition, we illustrated the proposed methodology by analyzing recent data from the Danish Lymphoma Registry. RESULTS: No uniformly superior extrapolation method was found, but flexible parametric mixture cure models and flexible parametric relative survival models seemed to be suitable in various scenarios. CONCLUSION: Using extrapolation to estimate the loss of lifetime function requires careful consideration of the relative survival function outside the available follow-up period. We propose extensive sensitivity analyses when estimating the loss of lifetime function.","Estimating the loss of lifetime function using flexible parametric relative survival models. BACKGROUND: Within cancer care, dynamic evaluations of the loss in expectation of life provides useful information to patients as well as physicians. The loss of lifetime function yields the conditional loss in expectation of life given survival up to a specific time point. Due to the inevitable censoring in time-to-event data, loss of lifetime estimation requires extrapolation of both the patient and general population survival function. In this context, the accuracy of different extrapolation approaches has not previously been evaluated. METHODS: The loss of lifetime function was computed by decomposing the all-cause survival function using the relative and general population survival function. To allow extrapolation, the relative survival function was fitted using existing parametric relative survival models. In addition, we introduced a novel mixture cure model suitable for extrapolation. The accuracy of the estimated loss of lifetime function using various extrapolation approaches was assessed in a simulation study and by data from the Danish Cancer Registry where complete follow-up was available. In addition, we illustrated the proposed methodology by analyzing recent data from the Danish Lymphoma Registry. RESULTS: No uniformly superior extrapolation method was found, but flexible parametric mixture cure models and flexible parametric relative survival models seemed to be suitable in various scenarios. CONCLUSION: Using extrapolation to estimate the loss of lifetime function requires careful consideration of the relative survival function outside the available follow-up period. We propose extensive sensitivity analyses when estimating the loss of lifetime function."
0,Dealing with internet-based information obtained by families of critically ill patients,,
0,Long-Read RNA Sequencing Identifies Alternative Splice Variants in Hepatocellular Carcinoma and Tumor-Specific Isoforms,,
0,WHO and ITU establish benchmarking process for artificial intelligence in health,,
0,"Platelet and red blood cell counts, as well as the concentrations of uric acid, but not homocysteinaemia or oxidative stress, contribute mostly to platelet reactivity in older adults","Purpose. The goal of this study was to estimate the hierarchical contribution of the most commonly recognized cardiovascular risk factors associated with atherogenesis to activation and reactivity of blood platelets in a group of men and women at ages 60-65. Methods. Socioeconomic and anthropometric data were taken from questionnaires. Blood morphology and biochemistry were measured with standard diagnostic methods. Plasma serum homocysteine was measured by immunochemical method. Plasma concentrations of VCAM, ICAM, total antioxidant status, and total oxidant status were estimated with commercial ELISA kits. Markers of oxidative stress of plasma and platelet proteins (concentrations of protein free thiol and amino groups) and lipids (concentrations of lipid peroxides) and generation of superoxide anion by platelets were measured with colorimetric methods. Platelet reactivity was estimated by impedance aggregometry with arachidonate, collagen, and ADP as agonists. Expression of selectin-P and GPIIb/IIIa on blood platelets was tested by flow cytometry. Results. Platelet aggregation associated significantly negatively with HGB and age and significantly positively with PLT, MPV, PCT, PDW, and P-LCR. When platelet reactivity (â€œcumulative platelet reactivity_aggregationâ€) was analyzed in a cumulated manner, the negative association with serum concentration of uric acid (Rs = âˆ’0 169, p = 0 003) was confirmed. Multivariate analysis revealed that amongst blood morphological parameters, platelet count, plateletcrit, and number of large platelets and uric acid are the most predictive variables for platelet reactivity. Conclusions. The most significant contributors to platelet reactivity in older subjects are platelet morphology, plasma uricaemia, and erythrocyte morphology.","Platelet and red blood cell counts, as well as the concentrations of uric acid, but not homocysteinaemia or oxidative stress, contribute mostly to platelet reactivity in older adults. Purpose. The goal of this study was to estimate the hierarchical contribution of the most commonly recognized cardiovascular risk factors associated with atherogenesis to activation and reactivity of blood platelets in a group of men and women at ages 60-65. Methods. Socioeconomic and anthropometric data were taken from questionnaires. Blood morphology and biochemistry were measured with standard diagnostic methods. Plasma serum homocysteine was measured by immunochemical method. Plasma concentrations of VCAM, ICAM, total antioxidant status, and total oxidant status were estimated with commercial ELISA kits. Markers of oxidative stress of plasma and platelet proteins (concentrations of protein free thiol and amino groups) and lipids (concentrations of lipid peroxides) and generation of superoxide anion by platelets were measured with colorimetric methods. Platelet reactivity was estimated by impedance aggregometry with arachidonate, collagen, and ADP as agonists. Expression of selectin-P and GPIIb/IIIa on blood platelets was tested by flow cytometry. Results. Platelet aggregation associated significantly negatively with HGB and age and significantly positively with PLT, MPV, PCT, PDW, and P-LCR. When platelet reactivity (â€œcumulative platelet reactivity_aggregationâ€) was analyzed in a cumulated manner, the negative association with serum concentration of uric acid (Rs = âˆ’0 169, p = 0 003) was confirmed. Multivariate analysis revealed that amongst blood morphological parameters, platelet count, plateletcrit, and number of large platelets and uric acid are the most predictive variables for platelet reactivity. Conclusions. The most significant contributors to platelet reactivity in older subjects are platelet morphology, plasma uricaemia, and erythrocyte morphology."
0,Detecting virus integration sites based on multiple related sequencing data by VirTect,"Background: Since tumor often has a high level of intra-tumor heterogeneity, multiple tumor samples from the same patient at different locations or different time points are often sequenced to study tumor intra-heterogeneity or tumor evolution. In virus-related tumors such as human papillomavirus- and Hepatitis B Virus-related tumors, virus genome integrations can be critical driving events. It is thus important to investigate the integration sites of the virus genomes. Currently, a few algorithms for detecting virus integration sites based on high-throughput sequencing have been developed, but their insufficient performance in their sensitivity, specificity and computational complexity hinders their applications in multiple related tumor sequencing. Results: We develop VirTect for detecting virus integration sites simultaneously from multiple related-sample data. This algorithm is mainly based on the joint analysis of short reads spanning breakpoints of integration sites from multiple samples. To achieve high specificity and breakpoint accuracy, a local precise sandwich alignment algorithm is used. Simulation and real data analyses show that, compared with other algorithms, VirTect is significantly more sensitive and has a similar or lower false discovery rate. Conclusions: VirTect can provide more accurate breakpoint position and is computationally much more efficient in terms both memory requirement and computational time.","Detecting virus integration sites based on multiple related sequencing data by VirTect. Background: Since tumor often has a high level of intra-tumor heterogeneity, multiple tumor samples from the same patient at different locations or different time points are often sequenced to study tumor intra-heterogeneity or tumor evolution. In virus-related tumors such as human papillomavirus- and Hepatitis B Virus-related tumors, virus genome integrations can be critical driving events. It is thus important to investigate the integration sites of the virus genomes. Currently, a few algorithms for detecting virus integration sites based on high-throughput sequencing have been developed, but their insufficient performance in their sensitivity, specificity and computational complexity hinders their applications in multiple related tumor sequencing. Results: We develop VirTect for detecting virus integration sites simultaneously from multiple related-sample data. This algorithm is mainly based on the joint analysis of short reads spanning breakpoints of integration sites from multiple samples. To achieve high specificity and breakpoint accuracy, a local precise sandwich alignment algorithm is used. Simulation and real data analyses show that, compared with other algorithms, VirTect is significantly more sensitive and has a similar or lower false discovery rate. Conclusions: VirTect can provide more accurate breakpoint position and is computationally much more efficient in terms both memory requirement and computational time."
0,Artificial Intelligence Classification of Central Visual Field Patterns in Glaucoma,"PURPOSE: To quantify the central visual field (VF) loss patterns in glaucoma using artificial intelligence. DESIGN: Retrospective study. PARTICIPANTS: VFs of 8712 patients with 13 951 Humphrey 10-2 test results from 13 951 eyes for cross-sectional analyses, and 824 patients with at least 5 reliable 10-2 test results at 6-month intervals or more from 1191 eyes for longitudinal analyses. METHODS: Total deviation values were used to determine the central VF patterns using the most recent 10-2 test results. A 24-2 VF within a 3-month window of the 10-2 tests was used to stage eyes into mild, moderate, or severe functional loss using the Hodapp-Anderson-Parrish scale at baseline. Archetypal analysis was applied to determine the central VF patterns. Cross-validation was performed to determine the optimal number of patterns. Stepwise regression was applied to select the optimal feature combination of global indices, average baseline decomposition coefficients from central VFs archetypes, and other factors to predict central VF mean deviation (MD) slope based on the Bayesian information criterion (BIC). MAIN OUTCOME MEASURES: The central VF patterns stratified by severity stage based on 24-2 test results and a model to predict the central VF MD change over time using baseline test results. RESULTS: From cross-sectional analysis, 17 distinct central VF patterns were determined for the 13 951 eyes across the spectrum of disease severity. These central VF patterns could be divided into isolated superior loss, isolated inferior loss, diffuse loss, and other loss patterns. Notably, 4 of the 5 patterns of diffuse VF loss preserved the less vulnerable inferotemporal zone, whereas they lost most of the remaining more vulnerable zone described by the Hood model. Inclusion of coefficients from central VF archetypical patterns strongly improved the prediction of central VF MD slope (BIC decrease, 35; BIC decrease of >6 indicating strong prediction improvement) than using only the global indices of 2 baseline VF results. Eyes with baseline VF results with more superonasal and inferonasal loss were more likely to show worsening MD over time. CONCLUSIONS: We quantified central VF patterns in glaucoma, which were used to improve the prediction of central VF worsening compared with using only global indices.","Artificial Intelligence Classification of Central Visual Field Patterns in Glaucoma. PURPOSE: To quantify the central visual field (VF) loss patterns in glaucoma using artificial intelligence. DESIGN: Retrospective study. PARTICIPANTS: VFs of 8712 patients with 13 951 Humphrey 10-2 test results from 13 951 eyes for cross-sectional analyses, and 824 patients with at least 5 reliable 10-2 test results at 6-month intervals or more from 1191 eyes for longitudinal analyses. METHODS: Total deviation values were used to determine the central VF patterns using the most recent 10-2 test results. A 24-2 VF within a 3-month window of the 10-2 tests was used to stage eyes into mild, moderate, or severe functional loss using the Hodapp-Anderson-Parrish scale at baseline. Archetypal analysis was applied to determine the central VF patterns. Cross-validation was performed to determine the optimal number of patterns. Stepwise regression was applied to select the optimal feature combination of global indices, average baseline decomposition coefficients from central VFs archetypes, and other factors to predict central VF mean deviation (MD) slope based on the Bayesian information criterion (BIC). MAIN OUTCOME MEASURES: The central VF patterns stratified by severity stage based on 24-2 test results and a model to predict the central VF MD change over time using baseline test results. RESULTS: From cross-sectional analysis, 17 distinct central VF patterns were determined for the 13 951 eyes across the spectrum of disease severity. These central VF patterns could be divided into isolated superior loss, isolated inferior loss, diffuse loss, and other loss patterns. Notably, 4 of the 5 patterns of diffuse VF loss preserved the less vulnerable inferotemporal zone, whereas they lost most of the remaining more vulnerable zone described by the Hood model. Inclusion of coefficients from central VF archetypical patterns strongly improved the prediction of central VF MD slope (BIC decrease, 35; BIC decrease of >6 indicating strong prediction improvement) than using only the global indices of 2 baseline VF results. Eyes with baseline VF results with more superonasal and inferonasal loss were more likely to show worsening MD over time. CONCLUSIONS: We quantified central VF patterns in glaucoma, which were used to improve the prediction of central VF worsening compared with using only global indices."
0,Fine structure and dynamics of EB3 binding zones on microtubules in fibroblast cells,"End-binding (EB) proteins associate with the growing tips of microtubules (MTs) and modulate their dynamics directly and indirectly, by recruiting essential factors to fine-tune MTs for their many essential roles in cells. Previously EB proteins have been shown to recognize a stabilizing GTP/GDP-Pi cap at the tip of growing MTs, but information about additional EB-binding zones on MTs has been limited. In this work, we studied fluorescence intensity profiles of one of the three mammalian EB-proteins, EB3, fused with red fluorescent protein (RFP). The distribution of EB3 on MTs in mouse fibroblasts frequently deviated from single exponential decay and exhibited secondary peaks. Those secondary peaks, which we refer to as EB3-islands, were detected on 56% comets of growing MTs and were encountered once per 44 s of EB3-RFP comet growth time with about 5 s half-lifetime. The majority of EB3-islands in the vicinity of MT tips was stationary and originated from EB3 comets moving with the growing MT tips. Computational modeling of the decoration of dynamic MT tips by EB3 suggested that the EB3-islands could not be explained simply by a stochastic first-order GTP hydrolysis/phosphate release. We speculate that additional protein factors contribute to EB3 residence time on MTs in cells, likely affecting MT dynamics.","Fine structure and dynamics of EB3 binding zones on microtubules in fibroblast cells. End-binding (EB) proteins associate with the growing tips of microtubules (MTs) and modulate their dynamics directly and indirectly, by recruiting essential factors to fine-tune MTs for their many essential roles in cells. Previously EB proteins have been shown to recognize a stabilizing GTP/GDP-Pi cap at the tip of growing MTs, but information about additional EB-binding zones on MTs has been limited. In this work, we studied fluorescence intensity profiles of one of the three mammalian EB-proteins, EB3, fused with red fluorescent protein (RFP). The distribution of EB3 on MTs in mouse fibroblasts frequently deviated from single exponential decay and exhibited secondary peaks. Those secondary peaks, which we refer to as EB3-islands, were detected on 56% comets of growing MTs and were encountered once per 44 s of EB3-RFP comet growth time with about 5 s half-lifetime. The majority of EB3-islands in the vicinity of MT tips was stationary and originated from EB3 comets moving with the growing MT tips. Computational modeling of the decoration of dynamic MT tips by EB3 suggested that the EB3-islands could not be explained simply by a stochastic first-order GTP hydrolysis/phosphate release. We speculate that additional protein factors contribute to EB3 residence time on MTs in cells, likely affecting MT dynamics."
0,The QT Interval An Emerging Vital Sign for the Precision Medicine Era?,,
0,Discovery of Human Signaling Systems: Pairing Peptides to G Protein-Coupled Receptors,,
0,In Defense of High-Speed Video Microscopy in Evaluating Patients with Suspected Primary Ciliary Dyskinesia,,
0,Analysis of Transcriptionally Active Bacteria Throughout the Gastrointestinal Tract of Healthy Individuals,,
0,End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography,"With an estimated 160,000 deaths in 2018, lung cancer is the most common cause of cancer death in the United States(1). Lung cancer screening using low-dose computed tomography has been shown to reduce mortality by 20-43% and is now included in US screening guidelines(1-6). Existing challenges include inter-grader variability and high false-positive and false-negative rates(7-10). We propose a deep learning algorithm that uses a patient's current and prior computed tomography volumes to predict the risk of lung cancer. Our model achieves a state-of-the-art performance (94.4% area under the curve) on 6,716 National Lung Cancer Screening Trial cases, and performs similarly on an independent clinical validation set of 1,139 cases. We conducted two reader studies. When prior computed tomography imaging was not available, our model outperformed all six radiologists with absolute reductions of 11% in false positives and 5% in false negatives. Where prior computed tomography imaging was available, the model performance was on-par with the same radiologists. This creates an opportunity to optimize the screening process via computer assistance and automation. While the vast majority of patients remain unscreened, we show the potential for deep learning models to increase the accuracy, consistency and adoption of lung cancer screening worldwide.","End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. With an estimated 160,000 deaths in 2018, lung cancer is the most common cause of cancer death in the United States(1). Lung cancer screening using low-dose computed tomography has been shown to reduce mortality by 20-43% and is now included in US screening guidelines(1-6). Existing challenges include inter-grader variability and high false-positive and false-negative rates(7-10). We propose a deep learning algorithm that uses a patient's current and prior computed tomography volumes to predict the risk of lung cancer. Our model achieves a state-of-the-art performance (94.4% area under the curve) on 6,716 National Lung Cancer Screening Trial cases, and performs similarly on an independent clinical validation set of 1,139 cases. We conducted two reader studies. When prior computed tomography imaging was not available, our model outperformed all six radiologists with absolute reductions of 11% in false positives and 5% in false negatives. Where prior computed tomography imaging was available, the model performance was on-par with the same radiologists. This creates an opportunity to optimize the screening process via computer assistance and automation. While the vast majority of patients remain unscreened, we show the potential for deep learning models to increase the accuracy, consistency and adoption of lung cancer screening worldwide."
0,Airway obstruction and bronchial reactivity from age 1 month until 13 years in children with asthma: A prospective birth cohort study,,
0,Mitochondrial gene cytochrome c oxidase I (CO1) used for molecular identification of Bactrocera zonata in Pakistan,"Bactrocera zonata is fruit pest mostly attacked on peach and cause heavy destruction in production of peach fruits by sucking their juice. For their management, we start to detect them on basis of their molecular characterization. As mitochondrial genome encodes a gene COI used as biomarker for identification of eukaryotes including insects. In present study, we amplified COI gene and cloned into pTZ57R/T vector (Fermentas). Cloned gene was confirmed through restriction analysis and sequenced through its entirety on both strands from Macrogen (South Korea) by Sanger sequencing method. Different computational tools were utilized for comparative analysis of sequence with other related sequences retrieved from databases. Related species were identified through phylogenetic analysis using Mega 7 tool. Pairwise sequence alignment showed the sequence identity about 96% with Bactrocera zonata. By identifying the pests with more authentic molecular biomarker may help the research to control them more effectively in future.","Mitochondrial gene cytochrome c oxidase I (CO1) used for molecular identification of Bactrocera zonata in Pakistan. Bactrocera zonata is fruit pest mostly attacked on peach and cause heavy destruction in production of peach fruits by sucking their juice. For their management, we start to detect them on basis of their molecular characterization. As mitochondrial genome encodes a gene COI used as biomarker for identification of eukaryotes including insects. In present study, we amplified COI gene and cloned into pTZ57R/T vector (Fermentas). Cloned gene was confirmed through restriction analysis and sequenced through its entirety on both strands from Macrogen (South Korea) by Sanger sequencing method. Different computational tools were utilized for comparative analysis of sequence with other related sequences retrieved from databases. Related species were identified through phylogenetic analysis using Mega 7 tool. Pairwise sequence alignment showed the sequence identity about 96% with Bactrocera zonata. By identifying the pests with more authentic molecular biomarker may help the research to control them more effectively in future."
0,Artificial intelligence in cancer imaging: Clinical challenges and applications,"Judgement, as one of the core tenets of medicine, relies upon the integration of multilayered data with nuanced decision making. Cancer offers a unique context for medical decisions given not only its variegated forms with evolution of disease but also the need to take into account the individual condition of patients, their ability to receive treatment, and their responses to treatment. Challenges remain in the accurate detection, characterization, and monitoring of cancers despite improved technologies. Radiographic assessment of disease most commonly relies upon visual evaluations, the interpretations of which may be augmented by advanced computational analyses. In particular, artificial intelligence (AI) promises to make great strides in the qualitative interpretation of cancer imaging by expert clinicians, including volumetric delineation of tumors over time, extrapolation of the tumor genotype and biological course from its radiographic phenotype, prediction of clinical outcome, and assessment of the impact of disease and treatment on adjacent organs. AI may automate processes in the initial interpretation of images and shift the clinical workflow of radiographic detection, management decisions on whether or not to administer an intervention, and subsequent observation to a yet to be envisioned paradigm. Here, the authors review the current state of AI as applied to medical imaging of cancer and describe advances in 4 tumor types (lung, brain, breast, and prostate) to illustrate how common clinical problems are being addressed. Although most studies evaluating AI applications in oncology to date have not been vigorously validated for reproducibility and generalizability, the results do highlight increasingly concerted efforts in pushing AI technology to clinical use and to impact future directions in cancer care.","Artificial intelligence in cancer imaging: Clinical challenges and applications. Judgement, as one of the core tenets of medicine, relies upon the integration of multilayered data with nuanced decision making. Cancer offers a unique context for medical decisions given not only its variegated forms with evolution of disease but also the need to take into account the individual condition of patients, their ability to receive treatment, and their responses to treatment. Challenges remain in the accurate detection, characterization, and monitoring of cancers despite improved technologies. Radiographic assessment of disease most commonly relies upon visual evaluations, the interpretations of which may be augmented by advanced computational analyses. In particular, artificial intelligence (AI) promises to make great strides in the qualitative interpretation of cancer imaging by expert clinicians, including volumetric delineation of tumors over time, extrapolation of the tumor genotype and biological course from its radiographic phenotype, prediction of clinical outcome, and assessment of the impact of disease and treatment on adjacent organs. AI may automate processes in the initial interpretation of images and shift the clinical workflow of radiographic detection, management decisions on whether or not to administer an intervention, and subsequent observation to a yet to be envisioned paradigm. Here, the authors review the current state of AI as applied to medical imaging of cancer and describe advances in 4 tumor types (lung, brain, breast, and prostate) to illustrate how common clinical problems are being addressed. Although most studies evaluating AI applications in oncology to date have not been vigorously validated for reproducibility and generalizability, the results do highlight increasingly concerted efforts in pushing AI technology to clinical use and to impact future directions in cancer care."
0,The promise of technology in the future of dementia care,"Dementia is a leading cause of disability, and the prevalence of dementia is steadily increasing. Although people with dementia are living longer lives in the community, without adequate support for their declining physical and psychological needs, the majority of these individuals end up in nursing homes. With no cure in sight, and in the context of population ageing, we must consider how to care for these individuals in the future. Technologies that augment existing care can maintain a person comfortably in their community, maximize individual autonomy and promote social participation. However, to date, such technologies have rarely been used in dementia care. This Perspectives article highlights the need for affordable and appropriate technologies to assist future dementia care, outlines some of the technologies currently available and describes the many challenges to integration of such technologies. Finally, guidelines are suggested for the development and implementation of new technologies in dementia care.","The promise of technology in the future of dementia care. Dementia is a leading cause of disability, and the prevalence of dementia is steadily increasing. Although people with dementia are living longer lives in the community, without adequate support for their declining physical and psychological needs, the majority of these individuals end up in nursing homes. With no cure in sight, and in the context of population ageing, we must consider how to care for these individuals in the future. Technologies that augment existing care can maintain a person comfortably in their community, maximize individual autonomy and promote social participation. However, to date, such technologies have rarely been used in dementia care. This Perspectives article highlights the need for affordable and appropriate technologies to assist future dementia care, outlines some of the technologies currently available and describes the many challenges to integration of such technologies. Finally, guidelines are suggested for the development and implementation of new technologies in dementia care."
0,Identification of the A293 (AVE1231) binding site in the cardiac two-pore-domain potassium channel TASK-1: A common low affinity antiarrhythmic drug binding site,"Background/Aims: The two-pore-domain potassium channel TASK-1 regulates atrial action potential duration. Due to the atrium-specific expression of TASK-1 in the human heart and the functional upregulation of TASK-1 currents in atrial fibrillation (AF), TASK-1 represents a promising target for the treatment of AF. Therefore, detailed knowledge of the molecular determinants of TASK-1 inhibition may help to identify new drugs for the future therapy of AF. In the current study, the molecular determinants of TASK-1 inhibition by the potent and antiarrhythmic compound A293 (AVE1231) were studied in detail. Methods: Alanine-scanning mutagenesis together with two-electrode voltage-clamp recordings were combined with in silico docking experiments. Results: Here, we have identified Q126 located in the M2 segment together with L239 and N240 of the M4 segment as amino acids essential for the A293-mediated inhibition of TASK-1. These data indicate a binding site which is different to that of A1899 for which also residues of the pore signature sequence and the late M4 segments are essential. Using in silico docking experiments, we propose a binding site at the lower end of the cytosolic pore, located at the entry to lateral side fenestrations of TASK-1. Strikingly, TASK-1 inhibition by the low affinity antiarrhythmic TASK-1 blockers propafenone, amiodarone and carvedilol was also strongly diminished by mutations at this novel binding site. Conclusion: We have identified the A293 binding site in the central cavity of TASK-1 and propose that this site might represent a conserved site of action for many low affinity antiarrhythmic TASK-1 blockers.","Identification of the A293 (AVE1231) binding site in the cardiac two-pore-domain potassium channel TASK-1: A common low affinity antiarrhythmic drug binding site. Background/Aims: The two-pore-domain potassium channel TASK-1 regulates atrial action potential duration. Due to the atrium-specific expression of TASK-1 in the human heart and the functional upregulation of TASK-1 currents in atrial fibrillation (AF), TASK-1 represents a promising target for the treatment of AF. Therefore, detailed knowledge of the molecular determinants of TASK-1 inhibition may help to identify new drugs for the future therapy of AF. In the current study, the molecular determinants of TASK-1 inhibition by the potent and antiarrhythmic compound A293 (AVE1231) were studied in detail. Methods: Alanine-scanning mutagenesis together with two-electrode voltage-clamp recordings were combined with in silico docking experiments. Results: Here, we have identified Q126 located in the M2 segment together with L239 and N240 of the M4 segment as amino acids essential for the A293-mediated inhibition of TASK-1. These data indicate a binding site which is different to that of A1899 for which also residues of the pore signature sequence and the late M4 segments are essential. Using in silico docking experiments, we propose a binding site at the lower end of the cytosolic pore, located at the entry to lateral side fenestrations of TASK-1. Strikingly, TASK-1 inhibition by the low affinity antiarrhythmic TASK-1 blockers propafenone, amiodarone and carvedilol was also strongly diminished by mutations at this novel binding site. Conclusion: We have identified the A293 binding site in the central cavity of TASK-1 and propose that this site might represent a conserved site of action for many low affinity antiarrhythmic TASK-1 blockers."
0,How to Train Your Genome,,
0,Human non-REM sleep and the mean global BOLD signal,"A hallmark of non-rapid eye movement (REM) sleep is the decreased brain activity as measured by global reductions in cerebral blood flow, oxygen metabolism, and glucose metabolism. It is unknown whether the blood oxygen level dependent (BOLD) signal undergoes similar changes. Here we show that, in contrast to the decreases in blood flow and metabolism, the mean global BOLD signal increases with sleep depth in a regionally non-uniform manner throughout gray matter. We relate our findings to the circulatory and metabolic processes influencing the BOLD signal and conclude that because oxygen consumption decreases proportionately more than blood flow in sleep, the resulting decrease in paramagnetic deoxyhemoglobin accounts for the increase in mean global BOLD signal.","Human non-REM sleep and the mean global BOLD signal. A hallmark of non-rapid eye movement (REM) sleep is the decreased brain activity as measured by global reductions in cerebral blood flow, oxygen metabolism, and glucose metabolism. It is unknown whether the blood oxygen level dependent (BOLD) signal undergoes similar changes. Here we show that, in contrast to the decreases in blood flow and metabolism, the mean global BOLD signal increases with sleep depth in a regionally non-uniform manner throughout gray matter. We relate our findings to the circulatory and metabolic processes influencing the BOLD signal and conclude that because oxygen consumption decreases proportionately more than blood flow in sleep, the resulting decrease in paramagnetic deoxyhemoglobin accounts for the increase in mean global BOLD signal."
0,"Re: Bernard H. Bochner, Guido Dalbagni, Karim H. Marzouk, et al. Randomized Trial Comparing Open Radical Cystectomy and Robot-assisted Laparoscopic Radical Cystectomy: Oncologic Outcomes. Eur Urol 2018;74:465-71: Can the Pattern of Cancer Recurrence Truly be Assigned to the Surgical Modality?",,
0,Educational attainment and cardiovascular disease in the United States: A quasi-experimental instrumental variables analysis,,
0,Assessing the effectiveness of artificial intelligence methods for melanoma: A retrospective review,"BACKGROUND: Artificial intelligence methods for the classification of melanoma have been studied extensively. However, few studies compare these methods under the same standards. OBJECTIVE: To seek the best artificial intelligence method for diagnosis of melanoma. METHODS: The contrast test used 2200 dermoscopic images. Image segmentations, feature extractions, and classifications were performed in sequence for evaluation of traditional machine learning algorithms. The recent popular convolutional neural network frameworks were used for transfer learning training classification. RESULTS: The region growing algorithm has the best segmentation performance, with an intersection over union of 70.06% and a false-positive rate of 17.67%. Classification performance was better with logistic regression, with a sensitivity of 76.36% and a specificity of 87.04%. The Inception V3 model (Google, Mountain View, CA) worked best in deep learning algorithms: the accuracy was 93.74%, the sensitivity was 94.36%, and the specificity was 85.64%. LIMITATIONS: There was no division in the severity of melanoma samples used in this experiment. The data set was relatively small for deep learning. CONCLUSION: The performance of traditional machine learning is satisfactory for the small data set of melanoma dermoscopic images, and the potential for deep learning in the future big data era is enormous.","Assessing the effectiveness of artificial intelligence methods for melanoma: A retrospective review. BACKGROUND: Artificial intelligence methods for the classification of melanoma have been studied extensively. However, few studies compare these methods under the same standards. OBJECTIVE: To seek the best artificial intelligence method for diagnosis of melanoma. METHODS: The contrast test used 2200 dermoscopic images. Image segmentations, feature extractions, and classifications were performed in sequence for evaluation of traditional machine learning algorithms. The recent popular convolutional neural network frameworks were used for transfer learning training classification. RESULTS: The region growing algorithm has the best segmentation performance, with an intersection over union of 70.06% and a false-positive rate of 17.67%. Classification performance was better with logistic regression, with a sensitivity of 76.36% and a specificity of 87.04%. The Inception V3 model (Google, Mountain View, CA) worked best in deep learning algorithms: the accuracy was 93.74%, the sensitivity was 94.36%, and the specificity was 85.64%. LIMITATIONS: There was no division in the severity of melanoma samples used in this experiment. The data set was relatively small for deep learning. CONCLUSION: The performance of traditional machine learning is satisfactory for the small data set of melanoma dermoscopic images, and the potential for deep learning in the future big data era is enormous."
0,Identification of a novel glycolysis-related gene signature that can predict the survival of patients with lung adenocarcinoma,"Lung cancer is one of the most malignant cancers worldwide, and lung adenocarcinoma (LUAD) is the most common histologic subtype. Thousands of biomarkers related to the survival and prognosis of patients with this cancer type have been investigated through database mining; however, the prediction effect of a single gene biomarker is not satisfactorily specific or sensitive. Thus, the present study aimed to develop a novel gene signature of prognostic values for patients with LUAD. Using a data-mining method, we performed expression profiling of 1145 mRNAs in large cohorts with LUAD (nÂ =Â 511) from The Cancer Genome Atlas database. Using the Gene Set Enrichment Analysis, we selected 198 genes related to GLYCOLYSIS, which is the most important enrichment gene set. Moreover, these genes were identified using Cox proportional regression modeling. We established a risk score staging system to predict the outcome of patients with LUAD and subsequently identified four genes (AGRN, AKR1A1, DDIT4, and HMMR) that were closely related to the prognosis of patients with LUAD. The identified genes allowed us to classify patients into the high-risk group (with poor outcome) and low-risk group (with better outcome). Compared with other clinical factors, the risk score has a better performance in predicting the outcome of patients with LUAD, particularly in the early stage of LUAD. In conclusion, we developed a four-gene signature related to glycolysis by utilizing the Cox regression model and a risk staging model for LUAD, which might prove valuable for the clinical management of patients with LUAD.","Identification of a novel glycolysis-related gene signature that can predict the survival of patients with lung adenocarcinoma. Lung cancer is one of the most malignant cancers worldwide, and lung adenocarcinoma (LUAD) is the most common histologic subtype. Thousands of biomarkers related to the survival and prognosis of patients with this cancer type have been investigated through database mining; however, the prediction effect of a single gene biomarker is not satisfactorily specific or sensitive. Thus, the present study aimed to develop a novel gene signature of prognostic values for patients with LUAD. Using a data-mining method, we performed expression profiling of 1145 mRNAs in large cohorts with LUAD (nÂ =Â 511) from The Cancer Genome Atlas database. Using the Gene Set Enrichment Analysis, we selected 198 genes related to GLYCOLYSIS, which is the most important enrichment gene set. Moreover, these genes were identified using Cox proportional regression modeling. We established a risk score staging system to predict the outcome of patients with LUAD and subsequently identified four genes (AGRN, AKR1A1, DDIT4, and HMMR) that were closely related to the prognosis of patients with LUAD. The identified genes allowed us to classify patients into the high-risk group (with poor outcome) and low-risk group (with better outcome). Compared with other clinical factors, the risk score has a better performance in predicting the outcome of patients with LUAD, particularly in the early stage of LUAD. In conclusion, we developed a four-gene signature related to glycolysis by utilizing the Cox regression model and a risk staging model for LUAD, which might prove valuable for the clinical management of patients with LUAD."
0,"Odanacatib for the treatment of postmenopausal osteoporosis: results of the LOFT multicentre, randomised, double-blind, placebo-controlled trial and LOFT Extension study","Background: Odanacatib, a cathepsin K inhibitor, reduces bone resorption while maintaining bone formation. Previous work has shown that odanacatib increases bone mineral density in postmenopausal women with low bone mass. We aimed to investigate the efficacy and safety of odanacatib to reduce fracture risk in postmenopausal women with osteoporosis. Methods: The Long-term Odanacatib Fracture Trial (LOFT) was a multicentre, randomised, double-blind, placebo-controlled, event-driven study at 388 outpatient clinics in 40 countries. Eligible participants were women aged at least 65 years who were postmenopausal for 5 years or more, with a femoral neck or total hip bone mineral density T-score between âˆ’2Â·5 and âˆ’4Â·0 if no previous radiographic vertebral fracture, or between âˆ’1Â·5 and âˆ’4Â·0 with a previous vertebral fracture. Women with a previous hip fracture, more than one vertebral fracture, or a T-score of less than âˆ’4Â·0 at the total hip or femoral neck were not eligible unless they were unable or unwilling to use approved osteoporosis treatment. Participants were randomly assigned (1:1) to either oral odanacatib (50 mg once per week) or matching placebo. Randomisation was done using an interactive voice recognition system after stratification for previous radiographic vertebral fracture, and treatment was masked to study participants, investigators and their staff, and sponsor personnel. If the study completed before 5 years of double-blind treatment, consenting participants could enrol in a double-blind extension study (LOFT Extension), continuing their original treatment assignment for up to 5 years from randomisation. Primary endpoints were incidence of vertebral fractures as assessed using radiographs collected at baseline, 6 and 12 months, yearly, and at final study visit in participants for whom evaluable radiograph images were available at baseline and at least one other timepoint, and hip and non-vertebral fractures adjudicated as being a result of osteoporosis as assessed by clinical history and radiograph. Safety was assessed in participants who received at least one dose of study drug. The adjudicated cardiovascular safety endpoints were a composite of cardiovascular death, myocardial infarction, or stroke, and new-onset atrial fibrillation or flutter. Individual cardiovascular endpoints and death were also assessed. LOFT and LOFT Extension are registered with ClinicalTrials.gov (number NCT00529373) and the European Clinical Trials Database (EudraCT number 2007-002693-66). Findings: Between Sept 14, 2007, and Nov 17, 2009, we randomly assigned 16 071 evaluable patients to treatment: 8043 to odanacatib and 8028 to placebo. After a median follow-up of 36Â·5 months (IQR 34Â·43â€“40Â·15) 4297 women assigned to odanacatib and 3960 assigned to placebo enrolled in LOFT Extension (total median follow-up 47Â·6 months, IQR 35Â·45â€“60Â·06). In LOFT, cumulative incidence of primary outcomes for odanacatib versus placebo were: radiographic vertebral fractures 3Â·7% (251/6770) versus 7Â·8% (542/6910), hazard ratio (HR) 0Â·46, 95% CI 0Â·40â€“0Â·53; hip fractures 0Â·8% (65/8043) versus 1Â·6% (125/8028), 0Â·53, 0Â·39â€“0Â·71; non-vertebral fractures 5Â·1% (412/8043) versus 6Â·7% (541/8028), 0Â·77, 0Â·68â€“0Â·87; all p<0Â·0001. Combined results from LOFT plus LOFT Extension for cumulative incidence of primary outcomes for odanacatib versus placebo were: radiographic vertebral fractures 4Â·9% (341/6909) versus 9Â·6% (675/7011), HR 0Â·48, 95% CI 0Â·42â€“0Â·55; hip fractures 1Â·1% (86/8043) versus 2Â·0% (162/8028), 0Â·52, 0Â·40â€“0Â·67; non-vertebral fractures 6Â·4% (512/8043) versus 8Â·4% (675/8028), 0Â·74, 0Â·66â€“0Â·83; all p<0Â·0001. In LOFT, the composite cardiovascular endpoint of cardiovascular death, myocardial infarction, or stroke occurred in 273 (3Â·4%) of 8043 patients in the odanacatib group versus 245 (3Â·1%) of 8028 in the placebo group (HR 1Â·12, 95% CI 0Â·95â€“1Â·34; p=0Â·18). New-onset atrial fibrillation or flutter occurred in 112 (1Â·4%) of 8043 patients in the odanacatib group versus 96 (1Â·2% of 8028 in the placebo group (HR 1Â·18, 0Â·90â€“1Â·55; p=0Â·24). Odanacatib was associated with an increased risk of stroke (1Â·7% [136/8043] vs 1Â·3% [104/8028], HR 1Â·32, 1Â·02â€“1Â·70; p=0Â·034), but not myocardial infarction (0Â·7% [60/8043] vs 0Â·9% [74/8028], HR 0Â·82, 0Â·58â€“1Â·15; p=0Â·26). The HR for all-cause mortality was 1Â·13 (5Â·0% [401/8043] vs 4Â·4% [356/8028], 0Â·98â€“1Â·30; p=0Â·10). When data from LOFT Extension were included, the composite of cardiovascular death, myocardial infarction, or stroke occurred in significantly more patients in the odanacatib group than in the placebo group (401 [5Â·0%] of 8043 vs 343 [4Â·3%] of 8028, HR 1Â·17, 1Â·02â€“1Â·36; p=0Â·029, as did stroke (2Â·3% [187/8043] vs 1Â·7% [137/8028], HR 1Â·37, 1Â·10â€“1Â·71; p=0Â·0051). Interpretation: Odanacatib reduced the risk of fracture, but was associated with an increased risk of cardiovascular events, specifically stroke, in postmenopausal women with osteoporosis. Based on the overall balance between benefit and risk, the study's sponsor decided that they would no longer pursue development of odanacatib for treatment of osteoporosis. Funding: Merck Sharp & Dohme Corp, a subsidiary of Merck & Co, Inc, Kenilworth, NJ, USA.","Odanacatib for the treatment of postmenopausal osteoporosis: results of the LOFT multicentre, randomised, double-blind, placebo-controlled trial and LOFT Extension study. Background: Odanacatib, a cathepsin K inhibitor, reduces bone resorption while maintaining bone formation. Previous work has shown that odanacatib increases bone mineral density in postmenopausal women with low bone mass. We aimed to investigate the efficacy and safety of odanacatib to reduce fracture risk in postmenopausal women with osteoporosis. Methods: The Long-term Odanacatib Fracture Trial (LOFT) was a multicentre, randomised, double-blind, placebo-controlled, event-driven study at 388 outpatient clinics in 40 countries. Eligible participants were women aged at least 65 years who were postmenopausal for 5 years or more, with a femoral neck or total hip bone mineral density T-score between âˆ’2Â·5 and âˆ’4Â·0 if no previous radiographic vertebral fracture, or between âˆ’1Â·5 and âˆ’4Â·0 with a previous vertebral fracture. Women with a previous hip fracture, more than one vertebral fracture, or a T-score of less than âˆ’4Â·0 at the total hip or femoral neck were not eligible unless they were unable or unwilling to use approved osteoporosis treatment. Participants were randomly assigned (1:1) to either oral odanacatib (50 mg once per week) or matching placebo. Randomisation was done using an interactive voice recognition system after stratification for previous radiographic vertebral fracture, and treatment was masked to study participants, investigators and their staff, and sponsor personnel. If the study completed before 5 years of double-blind treatment, consenting participants could enrol in a double-blind extension study (LOFT Extension), continuing their original treatment assignment for up to 5 years from randomisation. Primary endpoints were incidence of vertebral fractures as assessed using radiographs collected at baseline, 6 and 12 months, yearly, and at final study visit in participants for whom evaluable radiograph images were available at baseline and at least one other timepoint, and hip and non-vertebral fractures adjudicated as being a result of osteoporosis as assessed by clinical history and radiograph. Safety was assessed in participants who received at least one dose of study drug. The adjudicated cardiovascular safety endpoints were a composite of cardiovascular death, myocardial infarction, or stroke, and new-onset atrial fibrillation or flutter. Individual cardiovascular endpoints and death were also assessed. LOFT and LOFT Extension are registered with ClinicalTrials.gov (number NCT00529373) and the European Clinical Trials Database (EudraCT number 2007-002693-66). Findings: Between Sept 14, 2007, and Nov 17, 2009, we randomly assigned 16 071 evaluable patients to treatment: 8043 to odanacatib and 8028 to placebo. After a median follow-up of 36Â·5 months (IQR 34Â·43â€“40Â·15) 4297 women assigned to odanacatib and 3960 assigned to placebo enrolled in LOFT Extension (total median follow-up 47Â·6 months, IQR 35Â·45â€“60Â·06). In LOFT, cumulative incidence of primary outcomes for odanacatib versus placebo were: radiographic vertebral fractures 3Â·7% (251/6770) versus 7Â·8% (542/6910), hazard ratio (HR) 0Â·46, 95% CI 0Â·40â€“0Â·53; hip fractures 0Â·8% (65/8043) versus 1Â·6% (125/8028), 0Â·53, 0Â·39â€“0Â·71; non-vertebral fractures 5Â·1% (412/8043) versus 6Â·7% (541/8028), 0Â·77, 0Â·68â€“0Â·87; all p<0Â·0001. Combined results from LOFT plus LOFT Extension for cumulative incidence of primary outcomes for odanacatib versus placebo were: radiographic vertebral fractures 4Â·9% (341/6909) versus 9Â·6% (675/7011), HR 0Â·48, 95% CI 0Â·42â€“0Â·55; hip fractures 1Â·1% (86/8043) versus 2Â·0% (162/8028), 0Â·52, 0Â·40â€“0Â·67; non-vertebral fractures 6Â·4% (512/8043) versus 8Â·4% (675/8028), 0Â·74, 0Â·66â€“0Â·83; all p<0Â·0001. In LOFT, the composite cardiovascular endpoint of cardiovascular death, myocardial infarction, or stroke occurred in 273 (3Â·4%) of 8043 patients in the odanacatib group versus 245 (3Â·1%) of 8028 in the placebo group (HR 1Â·12, 95% CI 0Â·95â€“1Â·34; p=0Â·18). New-onset atrial fibrillation or flutter occurred in 112 (1Â·4%) of 8043 patients in the odanacatib group versus 96 (1Â·2% of 8028 in the placebo group (HR 1Â·18, 0Â·90â€“1Â·55; p=0Â·24). Odanacatib was associated with an increased risk of stroke (1Â·7% [136/8043] vs 1Â·3% [104/8028], HR 1Â·32, 1Â·02â€“1Â·70; p=0Â·034), but not myocardial infarction (0Â·7% [60/8043] vs 0Â·9% [74/8028], HR 0Â·82, 0Â·58â€“1Â·15; p=0Â·26). The HR for all-cause mortality was 1Â·13 (5Â·0% [401/8043] vs 4Â·4% [356/8028], 0Â·98â€“1Â·30; p=0Â·10). When data from LOFT Extension were included, the composite of cardiovascular death, myocardial infarction, or stroke occurred in significantly more patients in the odanacatib group than in the placebo group (401 [5Â·0%] of 8043 vs 343 [4Â·3%] of 8028, HR 1Â·17, 1Â·02â€“1Â·36; p=0Â·029, as did stroke (2Â·3% [187/8043] vs 1Â·7% [137/8028], HR 1Â·37, 1Â·10â€“1Â·71; p=0Â·0051). Interpretation: Odanacatib reduced the risk of fracture, but was associated with an increased risk of cardiovascular events, specifically stroke, in postmenopausal women with osteoporosis. Based on the overall balance between benefit and risk, the study's sponsor decided that they would no longer pursue development of odanacatib for treatment of osteoporosis. Funding: Merck Sharp & Dohme Corp, a subsidiary of Merck & Co, Inc, Kenilworth, NJ, USA."
0,In silico evidence of de novo interactions between ribosomal and Epstein - Barr virus proteins,"Background: Association of Epstein-Barr virus (EBV) encoded latent gene products with host ribosomal proteins (RPs) has not been fully explored, despite their involvement in the aetiology of several human cancers. To gain an insight into their plausible interactions, we employed a computational approach that encompasses structural alignment, gene ontology analysis, pathway analysis, and molecular docking. Results: In this study, the alignment analysis based on structural similarity allows the prediction of 48 potential interactions between 27 human RPs and the EBV proteins EBNA1, LMP1, LMP2A, and LMP2B. Gene ontology analysis of the putative protein-protein interactions (PPIs) reveals their probable involvement in RNA binding, ribosome biogenesis, metabolic and biosynthetic processes, and gene regulation. Pathway analysis shows their possible participation in viral infection strategies (viral translation), as well as oncogenesis (Wnt and EGFR signalling pathways). Finally, our molecular docking assay predicts the functional interactions of EBNA1 with four RPs individually: EBNA1-eS10, EBNA1-eS25, EBNA1-uL10 and EBNA1-uL11. Conclusion: These interactions have never been revealed previously via either experimental or in silico approach. We envisage that the calculated interactions between the ribosomal and EBV proteins herein would provide a hypothetical model for future experimental studies on the functional relationship between ribosomal proteins and EBV infection.","In silico evidence of de novo interactions between ribosomal and Epstein - Barr virus proteins. Background: Association of Epstein-Barr virus (EBV) encoded latent gene products with host ribosomal proteins (RPs) has not been fully explored, despite their involvement in the aetiology of several human cancers. To gain an insight into their plausible interactions, we employed a computational approach that encompasses structural alignment, gene ontology analysis, pathway analysis, and molecular docking. Results: In this study, the alignment analysis based on structural similarity allows the prediction of 48 potential interactions between 27 human RPs and the EBV proteins EBNA1, LMP1, LMP2A, and LMP2B. Gene ontology analysis of the putative protein-protein interactions (PPIs) reveals their probable involvement in RNA binding, ribosome biogenesis, metabolic and biosynthetic processes, and gene regulation. Pathway analysis shows their possible participation in viral infection strategies (viral translation), as well as oncogenesis (Wnt and EGFR signalling pathways). Finally, our molecular docking assay predicts the functional interactions of EBNA1 with four RPs individually: EBNA1-eS10, EBNA1-eS25, EBNA1-uL10 and EBNA1-uL11. Conclusion: These interactions have never been revealed previously via either experimental or in silico approach. We envisage that the calculated interactions between the ribosomal and EBV proteins herein would provide a hypothetical model for future experimental studies on the functional relationship between ribosomal proteins and EBV infection."
0,Health system costs for individual and comorbid noncommunicable diseases: An analysis of publicly funded health events from New Zealand,"BACKGROUND: There is little systematic assessment of how total health expenditure is distributed across diseases and comorbidities. The objective of this study was to use statistical methods to disaggregate all publicly funded health expenditure by disease and comorbidities in order to answer three research questions: (1) What is health expenditure by disease phase for noncommunicable diseases (NCDs) in New Zealand? (2) Is the cost of having two NCDs more or less than that expected given the independent costs of each NCD? (3) How is total health spending disaggregated by NCDs across age and by sex? METHODS AND FINDINGS: We used linked data for all adult New Zealanders for publicly funded events, including hospitalisation, outpatient, pharmaceutical, laboratory testing, and primary care from 1 July 2007 to 30 June 2014. These data include 18.9 million person-years and $26.4 billion in spending (US$ 2016). We used case definition algorithms to identify if a person had any of six NCDs (cancer, cardiovascular disease [CVD], diabetes, musculoskeletal, neurological, and a chronic lung/liver/kidney [LLK] disease). Indicator variables were used to identify the presence of any of the 15 possible comorbidity pairings of these six NCDs. Regression was used to estimate excess annual health expenditure per person. Cause deletion methods were used to estimate total population expenditure by disease. A majority (59%) of health expenditure was attributable to NCDs. Expenditure due to diseases was generally highest in the year of diagnosis and year of death. A person having two diseases simultaneously generally had greater health expenditure than the expected sum of having the diseases separately, for all 15 comorbidity pairs except the CVD-cancer pair. For example, a 60-64-year-old female with none of the six NCDs had $633 per annum expenditure. If she had both CVD and chronic LLK, additional expenditure for CVD separately was $6,443/$839/$9,225 for the first year of diagnosis/prevalent years/last year of life if dying of CVD; additional expenditure for chronic LLK separately was $6,443/$1,291/$9,051; and the additional comorbidity expenditure of having both CVD and LLK was $2,456 (95% confidence interval [CI] $2,238-$2,674). The pattern was similar for males (e.g., additional comorbidity expenditure for a 60-64-year-old male with CVD and chronic LLK was $2,498 [95% CI $2,264-$2,632]). In addition to this, the excess comorbidity costs for a person with two diseases was greater at younger ages, e.g., excess expenditure for 45-49-year-old males with CVD and chronic LLK was 10 times higher than for 75-79-year-old males and six times higher for females. At the population level, 23.8% of total health expenditure was attributable to higher costs of having one of the 15 comorbidity pairs over and above the six NCDs separately; of the remaining expenditure, CVD accounted for 18.7%, followed by musculoskeletal (16.2%), neurological (14.4%), cancer (14.1%), chronic LLK disease (7.4%), and diabetes (5.5%). Major limitations included incomplete linkage to all costed events (although these were largely non-NCD events) and missing private expenditure. CONCLUSIONS: The costs of having two NCDs simultaneously is typically superadditive, and more so for younger adults. Neurological and musculoskeletal diseases contributed the largest health system costs, in accord with burden of disease studies finding that they contribute large morbidity. Just as burden of disease methodology has advanced the understanding of disease burden, there is a need to create disease-based costing studies that facilitate the disaggregation of health budgets at a national level.","Health system costs for individual and comorbid noncommunicable diseases: An analysis of publicly funded health events from New Zealand. BACKGROUND: There is little systematic assessment of how total health expenditure is distributed across diseases and comorbidities. The objective of this study was to use statistical methods to disaggregate all publicly funded health expenditure by disease and comorbidities in order to answer three research questions: (1) What is health expenditure by disease phase for noncommunicable diseases (NCDs) in New Zealand? (2) Is the cost of having two NCDs more or less than that expected given the independent costs of each NCD? (3) How is total health spending disaggregated by NCDs across age and by sex? METHODS AND FINDINGS: We used linked data for all adult New Zealanders for publicly funded events, including hospitalisation, outpatient, pharmaceutical, laboratory testing, and primary care from 1 July 2007 to 30 June 2014. These data include 18.9 million person-years and $26.4 billion in spending (US$ 2016). We used case definition algorithms to identify if a person had any of six NCDs (cancer, cardiovascular disease [CVD], diabetes, musculoskeletal, neurological, and a chronic lung/liver/kidney [LLK] disease). Indicator variables were used to identify the presence of any of the 15 possible comorbidity pairings of these six NCDs. Regression was used to estimate excess annual health expenditure per person. Cause deletion methods were used to estimate total population expenditure by disease. A majority (59%) of health expenditure was attributable to NCDs. Expenditure due to diseases was generally highest in the year of diagnosis and year of death. A person having two diseases simultaneously generally had greater health expenditure than the expected sum of having the diseases separately, for all 15 comorbidity pairs except the CVD-cancer pair. For example, a 60-64-year-old female with none of the six NCDs had $633 per annum expenditure. If she had both CVD and chronic LLK, additional expenditure for CVD separately was $6,443/$839/$9,225 for the first year of diagnosis/prevalent years/last year of life if dying of CVD; additional expenditure for chronic LLK separately was $6,443/$1,291/$9,051; and the additional comorbidity expenditure of having both CVD and LLK was $2,456 (95% confidence interval [CI] $2,238-$2,674). The pattern was similar for males (e.g., additional comorbidity expenditure for a 60-64-year-old male with CVD and chronic LLK was $2,498 [95% CI $2,264-$2,632]). In addition to this, the excess comorbidity costs for a person with two diseases was greater at younger ages, e.g., excess expenditure for 45-49-year-old males with CVD and chronic LLK was 10 times higher than for 75-79-year-old males and six times higher for females. At the population level, 23.8% of total health expenditure was attributable to higher costs of having one of the 15 comorbidity pairs over and above the six NCDs separately; of the remaining expenditure, CVD accounted for 18.7%, followed by musculoskeletal (16.2%), neurological (14.4%), cancer (14.1%), chronic LLK disease (7.4%), and diabetes (5.5%). Major limitations included incomplete linkage to all costed events (although these were largely non-NCD events) and missing private expenditure. CONCLUSIONS: The costs of having two NCDs simultaneously is typically superadditive, and more so for younger adults. Neurological and musculoskeletal diseases contributed the largest health system costs, in accord with burden of disease studies finding that they contribute large morbidity. Just as burden of disease methodology has advanced the understanding of disease burden, there is a need to create disease-based costing studies that facilitate the disaggregation of health budgets at a national level."
0,Detecting early-warning signals of influenza outbreak based on dynamic network marker,"The seasonal outbreaks of influenza infection cause globally respiratory illness, or even death in all age groups. Given early-warning signals preceding the influenza outbreak, timely intervention such as vaccination and isolation management effectively decrease the morbidity. However, it is usually a difficult task to achieve the real-time prediction of influenza outbreak due to its complexity intertwining both biological systems and social systems. By exploring rich dynamical and high-dimensional information, our dynamic network marker/biomarker (DNM/DNB) method opens a new way to identify the tipping point prior to the catastrophic transition into an influenza pandemics. In order to detect the early-warning signals before the influenza outbreak by applying DNM method, the historical information of clinic hospitalization caused by influenza infection between years 2009 and 2016 were extracted and assembled from public records of Tokyo and Hokkaido, Japan. The early-warning signal, with an average of 4-week window lead prior to each seasonal outbreak of influenza, was provided by DNM-based on the hospitalization records, providing an opportunity to apply proactive strategies to prevent or delay the onset of influenza outbreak. Moreover, the study on the dynamical changes of hospitalization in local district networks unveils the influenza transmission dynamics or landscape in network level.","Detecting early-warning signals of influenza outbreak based on dynamic network marker. The seasonal outbreaks of influenza infection cause globally respiratory illness, or even death in all age groups. Given early-warning signals preceding the influenza outbreak, timely intervention such as vaccination and isolation management effectively decrease the morbidity. However, it is usually a difficult task to achieve the real-time prediction of influenza outbreak due to its complexity intertwining both biological systems and social systems. By exploring rich dynamical and high-dimensional information, our dynamic network marker/biomarker (DNM/DNB) method opens a new way to identify the tipping point prior to the catastrophic transition into an influenza pandemics. In order to detect the early-warning signals before the influenza outbreak by applying DNM method, the historical information of clinic hospitalization caused by influenza infection between years 2009 and 2016 were extracted and assembled from public records of Tokyo and Hokkaido, Japan. The early-warning signal, with an average of 4-week window lead prior to each seasonal outbreak of influenza, was provided by DNM-based on the hospitalization records, providing an opportunity to apply proactive strategies to prevent or delay the onset of influenza outbreak. Moreover, the study on the dynamical changes of hospitalization in local district networks unveils the influenza transmission dynamics or landscape in network level."
0,Prioritization of SNPs in y+LAT-1 culpable of Lysinuric protein intolerance and their mutational impacts using protein-protein docking and molecular dynamics simulation studies,"Lysinuric protein intolerance (LPI) is a rare, yet inimical, genetic disorder characterized by the paucity of essential dibasic amino acids in the cells. Amino acid transporter y+LAT-1 interacts with 4F2 cell-surface antigen heavy chain to transport the required dibasic amino acids. Mutation in y+LAT-1 is rumored to cause LPI. However, the underlying pathological mechanism is unknown, and, in this analysis, we investigate the impact of point mutation in y+LAT-1's interaction with 4F2 cell-surface antigen heavy chain in causing LPI. Using an efficient and extensive computational pipeline, we have isolated M50K and L334R single-nucleotide polymorphisms to be the most deleterious mutations in y+LAT-1s. Docking of mutant y+LAT-1 with 4F2 cell-surface antigen heavy chain showed decreased interaction compared with native y+LAT-1. Further, molecular dynamic simulation analysis reveals that the protein molecules increase in size, become more flexible, and alter their secondary structure upon mutation. We believe that these conformational changes because of mutation could be the reason for decreased interaction with 4F2 cell-surface antigen heavy chain causing LPI. Our analysis gives pathological insights about LPI and helps researchers to better understand the disease mechanism and develop an effective treatment strategy.","Prioritization of SNPs in y+LAT-1 culpable of Lysinuric protein intolerance and their mutational impacts using protein-protein docking and molecular dynamics simulation studies. Lysinuric protein intolerance (LPI) is a rare, yet inimical, genetic disorder characterized by the paucity of essential dibasic amino acids in the cells. Amino acid transporter y+LAT-1 interacts with 4F2 cell-surface antigen heavy chain to transport the required dibasic amino acids. Mutation in y+LAT-1 is rumored to cause LPI. However, the underlying pathological mechanism is unknown, and, in this analysis, we investigate the impact of point mutation in y+LAT-1's interaction with 4F2 cell-surface antigen heavy chain in causing LPI. Using an efficient and extensive computational pipeline, we have isolated M50K and L334R single-nucleotide polymorphisms to be the most deleterious mutations in y+LAT-1s. Docking of mutant y+LAT-1 with 4F2 cell-surface antigen heavy chain showed decreased interaction compared with native y+LAT-1. Further, molecular dynamic simulation analysis reveals that the protein molecules increase in size, become more flexible, and alter their secondary structure upon mutation. We believe that these conformational changes because of mutation could be the reason for decreased interaction with 4F2 cell-surface antigen heavy chain causing LPI. Our analysis gives pathological insights about LPI and helps researchers to better understand the disease mechanism and develop an effective treatment strategy."
0,Predicting Response to Triamcinolone in Severe Asthma by Machine Learning. Solving the Enigma,,
0,Medical education trends for future physicians in the era of advanced technology and artificial intelligence: an integrative review,"BACKGROUND: Medical education must adapt to different health care contexts, including digitalized health care systems and a digital generation of students in a hyper-connected world. The aims of this study are to identify and synthesize the values that medical educators need to implement in the curricula and to introduce representative educational programs. METHODS: An integrative review was conducted to combine data from various research designs. We searched for articles on PubMed, Scopus, Web of Science, and EBSCO ERIC between 2011 and 2017. Key search terms were ""undergraduate medical education,"" ""future,"" ""twenty-first century,"" ""millennium,"" ""curriculum,"" ""teaching,"" ""learning,"" and ""assessment."" We screened and extracted them according to inclusion and exclusion criteria from titles and abstracts. All authors read the full texts and discussed them to reach a consensus about the themes and subthemes. Data appraisal was performed using a modified Hawker 's evaluation form. RESULTS: Among the 7616 abstracts initially identified, 28 full-text articles were selected to reflect medical education trends and suggest suitable educational programs. The integrative themes and subthemes of future medical education are as follows: 1) a humanistic approach to patient safety that involves encouraging humanistic doctors and facilitating collaboration; 2) early experience and longitudinal integration by early exposure to patient-oriented integration and longitudinal integrated clerkships; 3) going beyond hospitals toward society by responding to changing community needs and showing respect for diversity; and 4) student-driven learning with advanced technology through active learning with individualization, social interaction, and resource accessibility. CONCLUSIONS: This review integrated the trends in undergraduate medical education in readiness for the anticipated changes in medical environments. The detailed programs introduced in this study could be useful for medical educators in the development of curricula. Further research is required to integrate the educational trends into graduate and continuing medical education, and to investigate the status or effects of innovative educational programs in each medical school or environment.","Medical education trends for future physicians in the era of advanced technology and artificial intelligence: an integrative review. BACKGROUND: Medical education must adapt to different health care contexts, including digitalized health care systems and a digital generation of students in a hyper-connected world. The aims of this study are to identify and synthesize the values that medical educators need to implement in the curricula and to introduce representative educational programs. METHODS: An integrative review was conducted to combine data from various research designs. We searched for articles on PubMed, Scopus, Web of Science, and EBSCO ERIC between 2011 and 2017. Key search terms were ""undergraduate medical education,"" ""future,"" ""twenty-first century,"" ""millennium,"" ""curriculum,"" ""teaching,"" ""learning,"" and ""assessment."" We screened and extracted them according to inclusion and exclusion criteria from titles and abstracts. All authors read the full texts and discussed them to reach a consensus about the themes and subthemes. Data appraisal was performed using a modified Hawker 's evaluation form. RESULTS: Among the 7616 abstracts initially identified, 28 full-text articles were selected to reflect medical education trends and suggest suitable educational programs. The integrative themes and subthemes of future medical education are as follows: 1) a humanistic approach to patient safety that involves encouraging humanistic doctors and facilitating collaboration; 2) early experience and longitudinal integration by early exposure to patient-oriented integration and longitudinal integrated clerkships; 3) going beyond hospitals toward society by responding to changing community needs and showing respect for diversity; and 4) student-driven learning with advanced technology through active learning with individualization, social interaction, and resource accessibility. CONCLUSIONS: This review integrated the trends in undergraduate medical education in readiness for the anticipated changes in medical environments. The detailed programs introduced in this study could be useful for medical educators in the development of curricula. Further research is required to integrate the educational trends into graduate and continuing medical education, and to investigate the status or effects of innovative educational programs in each medical school or environment."
0,Rationale and Application of the Protocol S Anti-Vascular Endothelial Growth Factor Algorithm for Proliferative Diabetic Retinopathy,"PURPOSE: To present the rationale, guidelines, and results of ranibizumab treatment for proliferative diabetic retinopathy (PDR) in Diabetic Retinopathy Clinical Research Network (DRCR.net) Protocol S. DESIGN: Post hoc analyses from a randomized clinical trial. PARTICIPANTS: Three hundred five participants (394 study eyes) having PDR without prior panretinal photocoagulation (PRP). METHODS: Intravitreous ranibizumab (0.5 mg) versus PRP for PDR. Ranbizumab-assigned eyes (n = 191) received monthly injections for 6 months unless resolution was achieved after 4 injections. After 6 months, injections could be deferred if neovascularization was stable over 3 consecutive visits (sustained stability). If neovascularization worsened, monthly treatment resumed. Panretinal photocoagulation could be initiated for failure or futility criteria. MAIN OUTCOME MEASURES: Neovascularization status through 2 years. RESULTS: At 1 month, 19% (35 of 188) of ranibizumab-assigned eyes showed complete neovascularization resolution and an additional 60% (113) showed improvement. At 6 months, 52% (80 of 153) showed neovascularization resolution, 3% (4) were improved, 37% (56) were stable, and 8% (13) had worsened since the last visit. Among eyes with versus without resolved neovascularization at 6 months, the median (interquartile range) number of injections between 6 months and 2 years was 4 (1-7; n = 73) versus 7 (4-11; n = 67; P < 0.001). Injections were deferred in 68 of 73 eyes (93%) meeting sustained stability at least once during the study; 62% (42 of 68) resumed injections within 16 weeks after deferral. At 2 years, 43% (66 of 154) showed neovascularization resolution, 5% (7) showed improvement, 23% (36) were stable, and 27% (42) had worsened since the last visit. Only 3 eyes met criteria for failure or futility through 2 years. CONCLUSIONS: The DRCR.net treatment algorithm for PDR can provide excellent clinical outcomes through 2 years for patients initiating anti-vascular endothelial growth factor (VEGF) therapy for PDR. When choosing between anti-VEGF and PRP as first-line therapy for PDR, treatment decisions should be guided by consideration of the relative advantages of each therapeutic method and anticipated patient compliance with follow-up and treatment recommendations.","Rationale and Application of the Protocol S Anti-Vascular Endothelial Growth Factor Algorithm for Proliferative Diabetic Retinopathy. PURPOSE: To present the rationale, guidelines, and results of ranibizumab treatment for proliferative diabetic retinopathy (PDR) in Diabetic Retinopathy Clinical Research Network (DRCR.net) Protocol S. DESIGN: Post hoc analyses from a randomized clinical trial. PARTICIPANTS: Three hundred five participants (394 study eyes) having PDR without prior panretinal photocoagulation (PRP). METHODS: Intravitreous ranibizumab (0.5 mg) versus PRP for PDR. Ranbizumab-assigned eyes (n = 191) received monthly injections for 6 months unless resolution was achieved after 4 injections. After 6 months, injections could be deferred if neovascularization was stable over 3 consecutive visits (sustained stability). If neovascularization worsened, monthly treatment resumed. Panretinal photocoagulation could be initiated for failure or futility criteria. MAIN OUTCOME MEASURES: Neovascularization status through 2 years. RESULTS: At 1 month, 19% (35 of 188) of ranibizumab-assigned eyes showed complete neovascularization resolution and an additional 60% (113) showed improvement. At 6 months, 52% (80 of 153) showed neovascularization resolution, 3% (4) were improved, 37% (56) were stable, and 8% (13) had worsened since the last visit. Among eyes with versus without resolved neovascularization at 6 months, the median (interquartile range) number of injections between 6 months and 2 years was 4 (1-7; n = 73) versus 7 (4-11; n = 67; P < 0.001). Injections were deferred in 68 of 73 eyes (93%) meeting sustained stability at least once during the study; 62% (42 of 68) resumed injections within 16 weeks after deferral. At 2 years, 43% (66 of 154) showed neovascularization resolution, 5% (7) showed improvement, 23% (36) were stable, and 27% (42) had worsened since the last visit. Only 3 eyes met criteria for failure or futility through 2 years. CONCLUSIONS: The DRCR.net treatment algorithm for PDR can provide excellent clinical outcomes through 2 years for patients initiating anti-vascular endothelial growth factor (VEGF) therapy for PDR. When choosing between anti-VEGF and PRP as first-line therapy for PDR, treatment decisions should be guided by consideration of the relative advantages of each therapeutic method and anticipated patient compliance with follow-up and treatment recommendations."
0,Defective Fibrillar Collagen Organization by Fibroblasts Contributes to Airway Remodeling in Asthma,,
0,Plasma protein patterns as comprehensive indicators of health,,
0,Contemporary Diagnosis and Management of Patients With Myocardial Infarction in the Absence of Obstructive Coronary Artery Disease: A Scientific Statement From the American Heart Association,"Myocardial infarction in the absence of obstructive coronary artery disease is found in approximately 5% to 6% of all patients with acute infarction who are referred for coronary angiography. There are a variety of causes that can result in this clinical condition. As such, it is important that patients are appropriately diagnosed and an evaluation to uncover the correct cause is performed so that, when possible, specific therapies to treat the underlying cause can be prescribed. This statement provides a formal and updated definition for the broadly labelled term MINOCA (incorporating the definition of acute myocardial infarction from the newly released ""Fourth Universal Definition of Myocardial Infarction"") and provides a clinically useful framework and algorithms for the diagnostic evaluation and management of patients with myocardial infarction in the absence of obstructive coronary artery disease.","Contemporary Diagnosis and Management of Patients With Myocardial Infarction in the Absence of Obstructive Coronary Artery Disease: A Scientific Statement From the American Heart Association. Myocardial infarction in the absence of obstructive coronary artery disease is found in approximately 5% to 6% of all patients with acute infarction who are referred for coronary angiography. There are a variety of causes that can result in this clinical condition. As such, it is important that patients are appropriately diagnosed and an evaluation to uncover the correct cause is performed so that, when possible, specific therapies to treat the underlying cause can be prescribed. This statement provides a formal and updated definition for the broadly labelled term MINOCA (incorporating the definition of acute myocardial infarction from the newly released ""Fourth Universal Definition of Myocardial Infarction"") and provides a clinically useful framework and algorithms for the diagnostic evaluation and management of patients with myocardial infarction in the absence of obstructive coronary artery disease."
0,Emergency front of neck access in children: a new learning approach in a rabbit model,,
0,Artificial Intelligence for Mammography and Digital Breast Tomosynthesis: Current Concepts and Future Perspectives,"Although computer-aided diagnosis (CAD) is widely used in mammography, conventional CAD programs that use prompts to indicate potential cancers on the mammograms have not led to an improvement in diagnostic accuracy. Because of the advances in machine learning, especially with use of deep (multilayered) convolutional neural networks, artificial intelligence has undergone a transformation that has improved the quality of the predictions of the models. Recently, such deep learning algorithms have been applied to mammography and digital breast tomosynthesis (DBT). In this review, the authors explain how deep learning works in the context of mammography and DBT and define the important technical challenges. Subsequently, they discuss the current status and future perspectives of artificial intelligence-based clinical applications for mammography, DBT, and radiomics. Available algorithms are advanced and approach the performance of radiologists-especially for cancer detection and risk prediction at mammography. However, clinical validation is largely lacking, and it is not clear how the power of deep learning should be used to optimize practice. Further development of deep learning models is necessary for DBT, and this requires collection of larger databases. It is expected that deep learning will eventually have an important role in DBT, including the generation of synthetic images.","Artificial Intelligence for Mammography and Digital Breast Tomosynthesis: Current Concepts and Future Perspectives. Although computer-aided diagnosis (CAD) is widely used in mammography, conventional CAD programs that use prompts to indicate potential cancers on the mammograms have not led to an improvement in diagnostic accuracy. Because of the advances in machine learning, especially with use of deep (multilayered) convolutional neural networks, artificial intelligence has undergone a transformation that has improved the quality of the predictions of the models. Recently, such deep learning algorithms have been applied to mammography and digital breast tomosynthesis (DBT). In this review, the authors explain how deep learning works in the context of mammography and DBT and define the important technical challenges. Subsequently, they discuss the current status and future perspectives of artificial intelligence-based clinical applications for mammography, DBT, and radiomics. Available algorithms are advanced and approach the performance of radiologists-especially for cancer detection and risk prediction at mammography. However, clinical validation is largely lacking, and it is not clear how the power of deep learning should be used to optimize practice. Further development of deep learning models is necessary for DBT, and this requires collection of larger databases. It is expected that deep learning will eventually have an important role in DBT, including the generation of synthetic images."
0,Emerging Applications of Artificial Intelligence in Neuro-Oncology,"Due to the exponential growth of computational algorithms, artificial intelligence (AI) methods are poised to improve the precision of diagnostic and therapeutic methods in medicine. The field of radiomics in neuro-oncology has been and will likely continue to be at the forefront of this revolution. A variety of AI methods applied to conventional and advanced neuro-oncology MRI data can already delineate infiltrating margins of diffuse gliomas, differentiate pseudoprogression from true progression, and predict recurrence and survival better than methods used in daily clinical practice. Radiogenomics will also advance our understanding of cancer biology, allowing noninvasive sampling of the molecular environment with high spatial resolution and providing a systems-level understanding of underlying heterogeneous cellular and molecular processes. By providing in vivo markers of spatial and molecular heterogeneity, these AI-based radiomic and radiogenomic tools have the potential to stratify patients into more precise initial diagnostic and therapeutic pathways and enable better dynamic treatment monitoring in this era of personalized medicine. Although substantial challenges remain, radiologic practice is set to change considerably as AI technology is further developed and validated for clinical use.","Emerging Applications of Artificial Intelligence in Neuro-Oncology. Due to the exponential growth of computational algorithms, artificial intelligence (AI) methods are poised to improve the precision of diagnostic and therapeutic methods in medicine. The field of radiomics in neuro-oncology has been and will likely continue to be at the forefront of this revolution. A variety of AI methods applied to conventional and advanced neuro-oncology MRI data can already delineate infiltrating margins of diffuse gliomas, differentiate pseudoprogression from true progression, and predict recurrence and survival better than methods used in daily clinical practice. Radiogenomics will also advance our understanding of cancer biology, allowing noninvasive sampling of the molecular environment with high spatial resolution and providing a systems-level understanding of underlying heterogeneous cellular and molecular processes. By providing in vivo markers of spatial and molecular heterogeneity, these AI-based radiomic and radiogenomic tools have the potential to stratify patients into more precise initial diagnostic and therapeutic pathways and enable better dynamic treatment monitoring in this era of personalized medicine. Although substantial challenges remain, radiologic practice is set to change considerably as AI technology is further developed and validated for clinical use."
0,Ferulic acid inhibits interleukin 17-dependent expression of nodal pathogenic mediators in fibroblast-like synoviocytes of rheumatoid arthritis,"Interleukin 17 (IL-17), a proinflammatory cytokine produced by T helper (Th) 17 cells, potentially controls fibroblast-like synoviocytes (FLS)-mediated disease activity of rheumatoid arthritis (RA) via IL-17/ IL-17 receptor type A (IL-17RA)/signal transducer and activator of transcription 3 (STAT-3) signaling cascade. This has suggested that targeting IL-17 signaling could serve as an important strategy to treat FLS-mediated RA progression. Ferulic acid (FA), a key polyphenol, attenuates the development of gouty arthritis and cancer through its anti-inflammatory effects, but its therapeutic efficiency on IL-17 signaling in FLS-mediated RA pathogenesis remains unknown. In the current study, FA markedly inhibited the IL-17-mediated expression of its specific transmembrane receptor IL-17RA in FLS isolated from adjuvant-induced arthritis (AA) rats. Importantly, FA dramatically suppressed the IL-17-mediated expression of toll-like receptor 3 (TLR-3), cysteine-rich angiogenic inducer 61 (Cyr61), IL-23, granulocyte-macrophage colony stimulating factor (GM-CSF) in AA-FLS via the inhibition of IL-17/IL-17RA/STAT-3 signaling cascade. In addition, FA significantly decreased the formation of osteoclast cells and bone resorption potential in a coculture system consisting of IL-17 treated AA-FLS and rat bone marrow derived monocytes/macrophages. Furthermore, FA remarkably inhibited the IL-17-mediated expression of receptor activator of nuclear factor Îº-Î’ ligandÂ (RANKL) and increased the expression of osteoprotegerinÂ (OPG) in AA-FLS via the regulation of IL-17/IL-17RA/STAT-3 signaling cascade. The therapeutic efficiency of FA on IL-17 signaling was further confirmed by knockdown of IL-17RA using small interfering RNA or blocking of STAT-3 activation with S3I-201. The molecular docking analysis revealed that FA manifests significant ligand efficiency toward IL-17RA, STAT-3, IL-23, and RANKL proteins. This study provides new evidence that FA can be used as a potential therapeutic agent for inhibiting IL-17-mediated disease severity and bone erosion in RA.","Ferulic acid inhibits interleukin 17-dependent expression of nodal pathogenic mediators in fibroblast-like synoviocytes of rheumatoid arthritis. Interleukin 17 (IL-17), a proinflammatory cytokine produced by T helper (Th) 17 cells, potentially controls fibroblast-like synoviocytes (FLS)-mediated disease activity of rheumatoid arthritis (RA) via IL-17/ IL-17 receptor type A (IL-17RA)/signal transducer and activator of transcription 3 (STAT-3) signaling cascade. This has suggested that targeting IL-17 signaling could serve as an important strategy to treat FLS-mediated RA progression. Ferulic acid (FA), a key polyphenol, attenuates the development of gouty arthritis and cancer through its anti-inflammatory effects, but its therapeutic efficiency on IL-17 signaling in FLS-mediated RA pathogenesis remains unknown. In the current study, FA markedly inhibited the IL-17-mediated expression of its specific transmembrane receptor IL-17RA in FLS isolated from adjuvant-induced arthritis (AA) rats. Importantly, FA dramatically suppressed the IL-17-mediated expression of toll-like receptor 3 (TLR-3), cysteine-rich angiogenic inducer 61 (Cyr61), IL-23, granulocyte-macrophage colony stimulating factor (GM-CSF) in AA-FLS via the inhibition of IL-17/IL-17RA/STAT-3 signaling cascade. In addition, FA significantly decreased the formation of osteoclast cells and bone resorption potential in a coculture system consisting of IL-17 treated AA-FLS and rat bone marrow derived monocytes/macrophages. Furthermore, FA remarkably inhibited the IL-17-mediated expression of receptor activator of nuclear factor Îº-Î’ ligandÂ (RANKL) and increased the expression of osteoprotegerinÂ (OPG) in AA-FLS via the regulation of IL-17/IL-17RA/STAT-3 signaling cascade. The therapeutic efficiency of FA on IL-17 signaling was further confirmed by knockdown of IL-17RA using small interfering RNA or blocking of STAT-3 activation with S3I-201. The molecular docking analysis revealed that FA manifests significant ligand efficiency toward IL-17RA, STAT-3, IL-23, and RANKL proteins. This study provides new evidence that FA can be used as a potential therapeutic agent for inhibiting IL-17-mediated disease severity and bone erosion in RA."
0,Tuberculosis case detection by trained inmate peer educators in a resource-limited prison setting in Ethiopia: a cluster-randomised trial,,
0,The Extended Supervised Learning Event (ESLE): Assessing Nontechnical Skills in Emergency Medicine Trainees in the Workplace,"STUDY OBJECTIVE: The contribution of emergency medicine clinicians' nontechnical skills in providing safe, high-quality care in the emergency department (ED) is well known. In 2015, the UK Royal College of Emergency Medicine introduced explicit validated descriptors of nontechnical skills needed to function effectively in the ED. A new nontechnical skills assessment tool that provided a score for 12 domains of nontechnical skills and detailed narrative feedback, the Extended Supervised Learning Event (ESLE), was introduced and was mandated as part of the Royal College of Emergency Medicine assessment schedule. We aim to evaluate the psychometric reliability of the ESLE in its first year of use. METHODS: ESLEs were mandated for all UK emergency medicine trainees in the final 4 years of a 6-year national training program from August 2015. The completed assessments were uploaded to the Royal College of Emergency Medicine e-portfolio. All assessments recorded in the Royal College of Emergency Medicine e-portfolio database between August 2015 and August 2016 were anonymized and analyzed for psychometric reliability, using generalizability theory. Decision analysis was used to model the effect of altering the number of episodes and assessors on reliability. RESULTS: A total of 1,390 ESLEs were analyzed. The majority (62%) of the variation in nontechnical skills scores was attributable to the trainee's ability. The circumstances of the event (eg, case complexity, workload) accounted for 21% and the stringency or leniency of assessors the remaining 16%. Decision analysis suggests that 3 ESLEs by 2 or more assessors, as currently recommended in the Royal College of Emergency Medicine curriculum, provide an assessment with a reliability coefficient of 0.8. CONCLUSION: Board-certified-equivalent emergency medicine supervisors are able to provide reliable assessments of emergency medicine trainees' nontechnical skills in the workplace by using the ESLE.","The Extended Supervised Learning Event (ESLE): Assessing Nontechnical Skills in Emergency Medicine Trainees in the Workplace. STUDY OBJECTIVE: The contribution of emergency medicine clinicians' nontechnical skills in providing safe, high-quality care in the emergency department (ED) is well known. In 2015, the UK Royal College of Emergency Medicine introduced explicit validated descriptors of nontechnical skills needed to function effectively in the ED. A new nontechnical skills assessment tool that provided a score for 12 domains of nontechnical skills and detailed narrative feedback, the Extended Supervised Learning Event (ESLE), was introduced and was mandated as part of the Royal College of Emergency Medicine assessment schedule. We aim to evaluate the psychometric reliability of the ESLE in its first year of use. METHODS: ESLEs were mandated for all UK emergency medicine trainees in the final 4 years of a 6-year national training program from August 2015. The completed assessments were uploaded to the Royal College of Emergency Medicine e-portfolio. All assessments recorded in the Royal College of Emergency Medicine e-portfolio database between August 2015 and August 2016 were anonymized and analyzed for psychometric reliability, using generalizability theory. Decision analysis was used to model the effect of altering the number of episodes and assessors on reliability. RESULTS: A total of 1,390 ESLEs were analyzed. The majority (62%) of the variation in nontechnical skills scores was attributable to the trainee's ability. The circumstances of the event (eg, case complexity, workload) accounted for 21% and the stringency or leniency of assessors the remaining 16%. Decision analysis suggests that 3 ESLEs by 2 or more assessors, as currently recommended in the Royal College of Emergency Medicine curriculum, provide an assessment with a reliability coefficient of 0.8. CONCLUSION: Board-certified-equivalent emergency medicine supervisors are able to provide reliable assessments of emergency medicine trainees' nontechnical skills in the workplace by using the ESLE."
0,Evolution of Aesthetic Dentistry,,
0,Standardized and Validated Training Programs for Robot-assisted Laparoscopy: The Challenge of the Future,,
0,Health Care in 2030: Will Artificial Intelligence Replace Physicians?,,
0,A Genome-wide Functional Signature Ontology Map and Applications to Natural Product Mechanism of Action Discovery,"Gene expression signature-based inference of functional connectivity within and between genetic perturbations, chemical perturbations, and disease status can lead to the development of actionable hypotheses for gene function, chemical modes of action, and disease treatment strategies. Here, we report a FuSiOn-based genome-wide integration of hypomorphic cellular phenotypes that enables functional annotation of gene network topology, assignment of mechanistic hypotheses to genes of unknown function, and detection of cooperativity among cell regulatory systems. Dovetailing genetic perturbation data with chemical perturbation phenotypes allowed simultaneous generation of mechanism of action hypotheses for thousands of uncharacterized natural products fractions (NPFs). The predicted mechanism of actions span a broad spectrum of cellular mechanisms, many of which are not currently recognized as â€œdruggable.â€ To enable use of FuSiOn as a hypothesis generation resource, all associations and analyses are available within an open source web-based GUI (http://fusion.yuhs.ac).","A Genome-wide Functional Signature Ontology Map and Applications to Natural Product Mechanism of Action Discovery. Gene expression signature-based inference of functional connectivity within and between genetic perturbations, chemical perturbations, and disease status can lead to the development of actionable hypotheses for gene function, chemical modes of action, and disease treatment strategies. Here, we report a FuSiOn-based genome-wide integration of hypomorphic cellular phenotypes that enables functional annotation of gene network topology, assignment of mechanistic hypotheses to genes of unknown function, and detection of cooperativity among cell regulatory systems. Dovetailing genetic perturbation data with chemical perturbation phenotypes allowed simultaneous generation of mechanism of action hypotheses for thousands of uncharacterized natural products fractions (NPFs). The predicted mechanism of actions span a broad spectrum of cellular mechanisms, many of which are not currently recognized as â€œdruggable.â€ To enable use of FuSiOn as a hypothesis generation resource, all associations and analyses are available within an open source web-based GUI (http://fusion.yuhs.ac)."
0,Deep Learning for Chest Radiography in the Emergency Department,,
0,Mitochondrial homeostasis in aml and gasping for response in resistance to bcl2 blockade,"Understanding resistance to BCL2 inhibition is a critical scientific and clinical challenge. In this issue of Cancer Discovery, two laboratories use unbiased approaches of large loss-of-function CRISPR/Cas 9 screens to discover targetable liabilities in cell signaling and metabolism to acute myeloid leukemia resistant to BCL2 inhibition.","Mitochondrial homeostasis in aml and gasping for response in resistance to bcl2 blockade. Understanding resistance to BCL2 inhibition is a critical scientific and clinical challenge. In this issue of Cancer Discovery, two laboratories use unbiased approaches of large loss-of-function CRISPR/Cas 9 screens to discover targetable liabilities in cell signaling and metabolism to acute myeloid leukemia resistant to BCL2 inhibition."
0,"Fast Adipogenesis Tracking System (FATS) - A robust, high-throughput, automation-ready adipogenesis quantification technique 10 Technology 1004 Medical Biotechnology","Adipogenesis is essential in in vitro experimentation to assess differentiation capability of stem cells, and therefore, its accurate measurement is important. Quantitative analysis of adipogenic levels, however, is challenging and often susceptible to errors due to non-specific reading or manual estimation by observers. To this end, we developed a novel adipocyte quantification algorithm, named Fast Adipogenesis Tracking System (FATS), based on computer vision libraries. The FATS algorithm is versatile and capable of accurately detecting and quantifying percentage of cells undergoing adipogenic and browning differentiation even under difficult conditions such as the presence of large cell clumps or high cell densities. The algorithm was tested on various cell lines including 3T3-L1 cells, adipose-derived mesenchymal stem cells (ASCs), and induced pluripotent stem cell (iPSC)-derived cells. The FATS algorithm is particularly useful for adipogenic measurement of embryoid bodies derived from pluripotent stem cells and was capable of accurately distinguishing adipogenic cells from false-positive stains. We then demonstrate the effectiveness of the FATS algorithm for screening of nuclear receptor ligands that affect adipogenesis in the high-throughput manner. Together, the FATS offer a universal and automated image-based method to quantify adipocyte differentiation of different cell lines in both standard and high-throughput workflows.","Fast Adipogenesis Tracking System (FATS) - A robust, high-throughput, automation-ready adipogenesis quantification technique 10 Technology 1004 Medical Biotechnology. Adipogenesis is essential in in vitro experimentation to assess differentiation capability of stem cells, and therefore, its accurate measurement is important. Quantitative analysis of adipogenic levels, however, is challenging and often susceptible to errors due to non-specific reading or manual estimation by observers. To this end, we developed a novel adipocyte quantification algorithm, named Fast Adipogenesis Tracking System (FATS), based on computer vision libraries. The FATS algorithm is versatile and capable of accurately detecting and quantifying percentage of cells undergoing adipogenic and browning differentiation even under difficult conditions such as the presence of large cell clumps or high cell densities. The algorithm was tested on various cell lines including 3T3-L1 cells, adipose-derived mesenchymal stem cells (ASCs), and induced pluripotent stem cell (iPSC)-derived cells. The FATS algorithm is particularly useful for adipogenic measurement of embryoid bodies derived from pluripotent stem cells and was capable of accurately distinguishing adipogenic cells from false-positive stains. We then demonstrate the effectiveness of the FATS algorithm for screening of nuclear receptor ligands that affect adipogenesis in the high-throughput manner. Together, the FATS offer a universal and automated image-based method to quantify adipocyte differentiation of different cell lines in both standard and high-throughput workflows."
0,Predicting dark respiration rates of wheat leaves from hyperspectral reflectance,"Greater availability of leaf dark respiration (Rdark ) data could facilitate breeding efforts to raise crop yield and improve global carbon cycle modelling. However, the availability of Rdark data is limited because it is cumbersome, time consuming, or destructive to measure. We report a non-destructive and high-throughput method of estimating Rdark from leaf hyperspectral reflectance data that was derived from leaf Rdark measured by a destructive high-throughput oxygen consumption technique. We generated a large dataset of leaf Rdark for wheat (1380 samples) from 90 genotypes, multiple growth stages, and growth conditions to generate models for Rdark . Leaf Rdark (per unit leaf area, fresh mass, dry mass or nitrogen, N) varied 7- to 15-fold among individual plants, whereas traits known to scale with Rdark , leaf N, and leaf mass per area (LMA) only varied twofold to fivefold. Our models predicted leaf Rdark , N, and LMA with r2 values of 0.50-0.63, 0.91, and 0.75, respectively, and relative bias of 17-18% for Rdark and 7-12% for N and LMA. Our results suggest that hyperspectral model prediction of wheat leaf Rdark is largely independent of leaf N and LMA. Potential drivers of hyperspectral signatures of Rdark are discussed.","Predicting dark respiration rates of wheat leaves from hyperspectral reflectance. Greater availability of leaf dark respiration (Rdark ) data could facilitate breeding efforts to raise crop yield and improve global carbon cycle modelling. However, the availability of Rdark data is limited because it is cumbersome, time consuming, or destructive to measure. We report a non-destructive and high-throughput method of estimating Rdark from leaf hyperspectral reflectance data that was derived from leaf Rdark measured by a destructive high-throughput oxygen consumption technique. We generated a large dataset of leaf Rdark for wheat (1380 samples) from 90 genotypes, multiple growth stages, and growth conditions to generate models for Rdark . Leaf Rdark (per unit leaf area, fresh mass, dry mass or nitrogen, N) varied 7- to 15-fold among individual plants, whereas traits known to scale with Rdark , leaf N, and leaf mass per area (LMA) only varied twofold to fivefold. Our models predicted leaf Rdark , N, and LMA with r2 values of 0.50-0.63, 0.91, and 0.75, respectively, and relative bias of 17-18% for Rdark and 7-12% for N and LMA. Our results suggest that hyperspectral model prediction of wheat leaf Rdark is largely independent of leaf N and LMA. Potential drivers of hyperspectral signatures of Rdark are discussed."
0,A Roadmap for Foundational Research on Artificial Intelligence in Medical Imaging: From the 2018 NIH/RSNA/ACR/The Academy Workshop,"Imaging research laboratories are rapidly creating machine learning systems that achieve expert human performance using open-source methods and tools. These artificial intelligence systems are being developed to improve medical image reconstruction, noise reduction, quality assurance, triage, segmentation, computer-aided detection, computer-aided classification, and radiogenomics. In August 2018, a meeting was held in Bethesda, Maryland, at the National Institutes of Health to discuss the current state of the art and knowledge gaps and to develop a roadmap for future research initiatives. Key research priorities include: 1, new image reconstruction methods that efficiently produce images suitable for human interpretation from source data; 2, automated image labeling and annotation methods, including information extraction from the imaging report, electronic phenotyping, and prospective structured image reporting; 3, new machine learning methods for clinical imaging data, such as tailored, pretrained model architectures, and federated machine learning methods; 4, machine learning methods that can explain the advice they provide to human users (so-called explainable artificial intelligence); and 5, validated methods for image de-identification and data sharing to facilitate wide availability of clinical imaging data sets. This research roadmap is intended to identify and prioritize these needs for academic research laboratories, funding agencies, professional societies, and industry.","A Roadmap for Foundational Research on Artificial Intelligence in Medical Imaging: From the 2018 NIH/RSNA/ACR/The Academy Workshop. Imaging research laboratories are rapidly creating machine learning systems that achieve expert human performance using open-source methods and tools. These artificial intelligence systems are being developed to improve medical image reconstruction, noise reduction, quality assurance, triage, segmentation, computer-aided detection, computer-aided classification, and radiogenomics. In August 2018, a meeting was held in Bethesda, Maryland, at the National Institutes of Health to discuss the current state of the art and knowledge gaps and to develop a roadmap for future research initiatives. Key research priorities include: 1, new image reconstruction methods that efficiently produce images suitable for human interpretation from source data; 2, automated image labeling and annotation methods, including information extraction from the imaging report, electronic phenotyping, and prospective structured image reporting; 3, new machine learning methods for clinical imaging data, such as tailored, pretrained model architectures, and federated machine learning methods; 4, machine learning methods that can explain the advice they provide to human users (so-called explainable artificial intelligence); and 5, validated methods for image de-identification and data sharing to facilitate wide availability of clinical imaging data sets. This research roadmap is intended to identify and prioritize these needs for academic research laboratories, funding agencies, professional societies, and industry."
0,Antibiotic Lethality Is Impacted by Nutrient Availabilities: New Insights from Machine Learning,"In this issue of Cell, Yang, Wright et al. describe a machine learning approach that that can provide mechanistic insight from chemical screens. They use this approach to uncover how the nutritional availability for Escherichia coli impacts lethality toward three widely used antibiotics.","Antibiotic Lethality Is Impacted by Nutrient Availabilities: New Insights from Machine Learning. In this issue of Cell, Yang, Wright et al. describe a machine learning approach that that can provide mechanistic insight from chemical screens. They use this approach to uncover how the nutritional availability for Escherichia coli impacts lethality toward three widely used antibiotics."
0,Molecular Mechanism for Ligand Recognition and Subtype Selectivity of Î±2C Adrenergic Receptor,"Adrenergic G-protein-coupled receptors (GPCRs) mediate different cellular signaling pathways in the presence of endogenous catecholamines and play important roles in both physiological and pathological conditions. Extensive studies have been carried out to investigate the structure and function of Î² adrenergic receptors (Î²ARs). However, the structure of Î± adrenergic receptors (Î±ARs) remains to be determined. Here, we report the structure of the human Î±2C adrenergic receptor (Î±2CAR) with the non-selective antagonist, RS79948, at 2.8 Ã…. Our structure, mutations, modeling, and functional experiments indicate that a Î±2CAR-specific D206ECL2-R409ECL3-Y4056.58 network plays a role in determining Î±2 adrenergic subtype selectivity. Furthermore, our results show that a specific loosened helix at the top of TM4 in Î±2CAR is involved in receptor activation. Together, our structure of human Î±2CAR-RS79948 provides key insight into the mechanism underlying the Î±2 adrenergic receptor activation and subtype selectivity.","Molecular Mechanism for Ligand Recognition and Subtype Selectivity of Î±2C Adrenergic Receptor. Adrenergic G-protein-coupled receptors (GPCRs) mediate different cellular signaling pathways in the presence of endogenous catecholamines and play important roles in both physiological and pathological conditions. Extensive studies have been carried out to investigate the structure and function of Î² adrenergic receptors (Î²ARs). However, the structure of Î± adrenergic receptors (Î±ARs) remains to be determined. Here, we report the structure of the human Î±2C adrenergic receptor (Î±2CAR) with the non-selective antagonist, RS79948, at 2.8 Ã…. Our structure, mutations, modeling, and functional experiments indicate that a Î±2CAR-specific D206ECL2-R409ECL3-Y4056.58 network plays a role in determining Î±2 adrenergic subtype selectivity. Furthermore, our results show that a specific loosened helix at the top of TM4 in Î±2CAR is involved in receptor activation. Together, our structure of human Î±2CAR-RS79948 provides key insight into the mechanism underlying the Î±2 adrenergic receptor activation and subtype selectivity."
0,Emergency Computed Tomography Predicts Caustic Esophageal Stricture Formation,,
0,Structural bioinformatics insights into the CARD-CARD interaction mediated by the mitochondrial antiviral-signaling protein of black carp,"The innate immune system offers the first line of defense against invading microbial pathogens through the recognition of conserved pathogen-associated molecular patterns (PAMPs) by pattern recognition receptors (PRRs). The host innate immune system through PRRs, the sensors for PAMPs, induces the production of cytokines. Among different families of PRRs, the retinoic acid-inducible gene I (RIG-I)-like receptors (RLRs), and its mitochondrial adaptor ie, the mitochondrial antiviral-signaling (MAVS) protein, are crucial for RLR-triggered interferon (IFN) antiviral immunity. Recent studies have shownÂ that the N-terminal caspase recruitment domain (CARD) and transmembrane domain playÂ a pivotal role in oligomerization of black carp MAVS (BcMAVS), crucial for the host innate immune response against viral invasion. In this study, we have used molecular modeling, docking, and molecular dynamics (MD) simulation approaches to shed molecular insights into the oligomerization mechanism of BcMAVSCARD. MD simulation and interaction analysis portrayed that the type-I surface patches of BcMAVS CARDÂ makeÂ the major contributionÂ to the interaction. Moreover, the evidence from surface patches and critical residues involved in the said interaction is found to be similar toÂ that of the human counterpart and requires further investigation for legitimacy. Altogether, our study provided crucial information on oligomerization of BcMAVS CARDs and might be helpful for clarifying the innate immune response against pathogens and downstream signaling in fishes.","Structural bioinformatics insights into the CARD-CARD interaction mediated by the mitochondrial antiviral-signaling protein of black carp. The innate immune system offers the first line of defense against invading microbial pathogens through the recognition of conserved pathogen-associated molecular patterns (PAMPs) by pattern recognition receptors (PRRs). The host innate immune system through PRRs, the sensors for PAMPs, induces the production of cytokines. Among different families of PRRs, the retinoic acid-inducible gene I (RIG-I)-like receptors (RLRs), and its mitochondrial adaptor ie, the mitochondrial antiviral-signaling (MAVS) protein, are crucial for RLR-triggered interferon (IFN) antiviral immunity. Recent studies have shownÂ that the N-terminal caspase recruitment domain (CARD) and transmembrane domain playÂ a pivotal role in oligomerization of black carp MAVS (BcMAVS), crucial for the host innate immune response against viral invasion. In this study, we have used molecular modeling, docking, and molecular dynamics (MD) simulation approaches to shed molecular insights into the oligomerization mechanism of BcMAVSCARD. MD simulation and interaction analysis portrayed that the type-I surface patches of BcMAVS CARDÂ makeÂ the major contributionÂ to the interaction. Moreover, the evidence from surface patches and critical residues involved in the said interaction is found to be similar toÂ that of the human counterpart and requires further investigation for legitimacy. Altogether, our study provided crucial information on oligomerization of BcMAVS CARDs and might be helpful for clarifying the innate immune response against pathogens and downstream signaling in fishes."
0,Vedolizumab trough level monitoring in inflammatory bowel disease: a state-of-the-art overview,,
0,Deep learning: new computational modelling techniques for genomics,"As a data-driven science, genomics largely utilizes machine learning to capture dependencies in data and derive novel biological hypotheses. However, the ability to extract new insights from the exponentially increasing volume of genomics data requires more expressive machine learning models. By effectively leveraging large data sets, deep learning has transformed fields such as computer vision and natural language processing. Now, it is becoming the method of choice for many genomics modelling tasks, including predicting the impact of genetic variation on gene regulatory mechanisms such as DNA accessibility and splicing.","Deep learning: new computational modelling techniques for genomics. As a data-driven science, genomics largely utilizes machine learning to capture dependencies in data and derive novel biological hypotheses. However, the ability to extract new insights from the exponentially increasing volume of genomics data requires more expressive machine learning models. By effectively leveraging large data sets, deep learning has transformed fields such as computer vision and natural language processing. Now, it is becoming the method of choice for many genomics modelling tasks, including predicting the impact of genetic variation on gene regulatory mechanisms such as DNA accessibility and splicing."
0,Association Between Age and Familial Risk for Alcoholism on Functional Connectivity in Adolescence,,
0,Effects of Bakuchiol on chondrocyte proliferation via the PI3K-Akt and ERK1/2 pathways mediated by the estrogen receptor for promotion of the regeneration of knee articular cartilage defects,"Objectives: Cartilaginous tissue degradation occurs because of the lack of survival of chondrocytes. Here, we ascertained whether bakuchiol (BAK) has the capability of activating chondrocyte proliferation. Materials and methods: The effect of BAK on the proliferation of rat chondrocytes at a concentration of 10 and 20Â Âµmol/L was investigated. The molecular mechanisms involving target binding and signalling pathways were elucidated by RNA-sequencing, qPCR, molecular docking and Western blotting. Matrigel mixed with bakuchiol was implanted locally into rat knee articular cartilage defects to verify the activation of chondrocytes due to bakuchiol in vivo. Results: Bakuchiol implantation resulted in the activation of rat chondrocyte proliferation in a dose-dependent manner. RNA-sequencing revealed 107 differentially expressed genes (DEGs) with 75 that were up-regulated and 32 that were down-regulated, indicating increased activation of the PI3K-Akt and cell cycle pathways. Activation of the phosphorylation of Akt, ERK1/2 and their inhibitors blocked the proliferative effect of bakuchiol treatment, confirming its direct involvement in these signal transduction pathways. Molecular docking and siRNA silencing revealed that estrogen receptor-Î± (ERÎ±) was the target of bakuchiol in terms of its cell proliferative effect via PI3K activation. Two weeks after implantation of bakuchiol, the appearance and physiological structure of the articular cartilage was more integrated with abundant chondrocytes and cartilage matrix compared to that of the control. Conclusions: Bakuchiol demonstrated significant bioactivity towards chondrocyte proliferation via the PI3K-Akt and ERK1/2 pathways mediated by estrogen receptor activation and exhibited enhanced promotion of the remodelling of injured cartilage.","Effects of Bakuchiol on chondrocyte proliferation via the PI3K-Akt and ERK1/2 pathways mediated by the estrogen receptor for promotion of the regeneration of knee articular cartilage defects. Objectives: Cartilaginous tissue degradation occurs because of the lack of survival of chondrocytes. Here, we ascertained whether bakuchiol (BAK) has the capability of activating chondrocyte proliferation. Materials and methods: The effect of BAK on the proliferation of rat chondrocytes at a concentration of 10 and 20Â Âµmol/L was investigated. The molecular mechanisms involving target binding and signalling pathways were elucidated by RNA-sequencing, qPCR, molecular docking and Western blotting. Matrigel mixed with bakuchiol was implanted locally into rat knee articular cartilage defects to verify the activation of chondrocytes due to bakuchiol in vivo. Results: Bakuchiol implantation resulted in the activation of rat chondrocyte proliferation in a dose-dependent manner. RNA-sequencing revealed 107 differentially expressed genes (DEGs) with 75 that were up-regulated and 32 that were down-regulated, indicating increased activation of the PI3K-Akt and cell cycle pathways. Activation of the phosphorylation of Akt, ERK1/2 and their inhibitors blocked the proliferative effect of bakuchiol treatment, confirming its direct involvement in these signal transduction pathways. Molecular docking and siRNA silencing revealed that estrogen receptor-Î± (ERÎ±) was the target of bakuchiol in terms of its cell proliferative effect via PI3K activation. Two weeks after implantation of bakuchiol, the appearance and physiological structure of the articular cartilage was more integrated with abundant chondrocytes and cartilage matrix compared to that of the control. Conclusions: Bakuchiol demonstrated significant bioactivity towards chondrocyte proliferation via the PI3K-Akt and ERK1/2 pathways mediated by estrogen receptor activation and exhibited enhanced promotion of the remodelling of injured cartilage."
0,In vivo liquid biopsy using Cytophone platform for photoacoustic detection of circulating tumor cells in patients with melanoma,,
0,USA aid policy and induced abortion in sub-Saharan Africa: an analysis of the Mexico City Policy,"Background: The Mexico City Policy, first announced by US President Ronald Reagan and since lifted and reinstated by presidents along partisan lines, prohibits US foreign assistance to any organisation that performs or provides counselling on abortion. Many organisations affected by this policy are also providers of modern contraception. If the policy reduces these organisations' ability to supply modern contraceptives, it could have the unintended consequence of increasing abortion rates. Methods: We empirically examined patterns of modern contraception use, pregnancies, and abortion among women in 26 countries in sub-Saharan Africa in response to the reinstatement and subsequent repeal of the Mexico City Policy across three presidential administrations (William Clinton, George W Bush, and Barack Obama). We combine individual-level data on pregnancies and abortions from 743 691 women, country-year data on modern contraception use, and annual data on development assistance for family planning and reproductive health in a difference-in-difference framework to examine relative changes in use of modern contraception, pregnancy, and abortion in response to the policy. Findings: We found that when the Mexico City Policy was in effect (2001â€“08), abortion rates rose among women in countries highly exposed to the policy by 4Â·8 abortions per 10 000 woman-years (95% CI 1Â·5 to 8Â·1, p=0Â·0041) relative to women in low-exposure countries and relative to periods when the policy was rescinded in 1995â€“2000 and 2009â€“14, a rise of approximately 40%. We found a symmetric reduction in use of modern contraception by 3Â·15 percentage points (relative decrease of 13Â·5%; 95% CI âˆ’4Â·9 to âˆ’1Â·4; p=0Â·0006) and increase in pregnancies by 3Â·2 percentage points (relative increase of 12%; 95% CI 1Â·6 to 4Â·8; p<0Â·0001) while the policy was enacted. Interpretation: Our findings suggest that curbing US assistance to family planning organisations, especially those that consider abortion as a method of family planning, increases abortion prevalence in sub-Saharan African countries most affected by the policy. Funding: The William and Flora Hewlett Foundation, the Doris Duke Charitable Foundation, the David and Lucile Packard Foundation, and the Stanford Earth Dean's Fellowship.","USA aid policy and induced abortion in sub-Saharan Africa: an analysis of the Mexico City Policy. Background: The Mexico City Policy, first announced by US President Ronald Reagan and since lifted and reinstated by presidents along partisan lines, prohibits US foreign assistance to any organisation that performs or provides counselling on abortion. Many organisations affected by this policy are also providers of modern contraception. If the policy reduces these organisations' ability to supply modern contraceptives, it could have the unintended consequence of increasing abortion rates. Methods: We empirically examined patterns of modern contraception use, pregnancies, and abortion among women in 26 countries in sub-Saharan Africa in response to the reinstatement and subsequent repeal of the Mexico City Policy across three presidential administrations (William Clinton, George W Bush, and Barack Obama). We combine individual-level data on pregnancies and abortions from 743 691 women, country-year data on modern contraception use, and annual data on development assistance for family planning and reproductive health in a difference-in-difference framework to examine relative changes in use of modern contraception, pregnancy, and abortion in response to the policy. Findings: We found that when the Mexico City Policy was in effect (2001â€“08), abortion rates rose among women in countries highly exposed to the policy by 4Â·8 abortions per 10 000 woman-years (95% CI 1Â·5 to 8Â·1, p=0Â·0041) relative to women in low-exposure countries and relative to periods when the policy was rescinded in 1995â€“2000 and 2009â€“14, a rise of approximately 40%. We found a symmetric reduction in use of modern contraception by 3Â·15 percentage points (relative decrease of 13Â·5%; 95% CI âˆ’4Â·9 to âˆ’1Â·4; p=0Â·0006) and increase in pregnancies by 3Â·2 percentage points (relative increase of 12%; 95% CI 1Â·6 to 4Â·8; p<0Â·0001) while the policy was enacted. Interpretation: Our findings suggest that curbing US assistance to family planning organisations, especially those that consider abortion as a method of family planning, increases abortion prevalence in sub-Saharan African countries most affected by the policy. Funding: The William and Flora Hewlett Foundation, the Doris Duke Charitable Foundation, the David and Lucile Packard Foundation, and the Stanford Earth Dean's Fellowship."
0,"Re: Robot-assisted Radical Cystectomy Versus Open Radical Cystectomy in Patients with Bladder Cancer (RAZOR): An Open-label, Randomised, Phase 3, Non-inferiority Trial",,
0,"Low-dose ritonavir-boosted darunavir once daily versus ritonavir-boosted lopinavir for participants with less than 50 HIV RNA copies per mL (WRHI 052): a randomised, open-label, phase 3, non-inferiority trial",,
0,Histone Octamer Structure Is Altered Early in ISW2 ATP-Dependent Nucleosome Remodeling,Nucleosomes are the fundamental building blocks of chromatin that regulate DNA access and are composed of histone octamers. ATP-dependent chromatin remodelers like ISW2 regulate chromatin access by translationally moving nucleosomes to different DNA regions. We find that histone octamers are more pliable than previously assumed and distorted by ISW2 early in remodeling before DNA enters nucleosomes and the ATPase motor moves processively on nucleosomal DNA. Uncoupling the ATPase activity of ISW2 from nucleosome movement with deletion of the SANT domain from the C terminus of the Isw2 catalytic subunit traps remodeling intermediates in which the histone octamer structure is changed. We find restricting histone movement by chemical crosslinking also traps remodeling intermediates resembling those seen early in ISW2 remodeling with loss of the SANT domain. Other evidence shows histone octamers are intrinsically prone to changing their conformation and can be distorted merely by H3-H4 tetramer disulfide crosslinking. Hada et al. show that ISW2 catalyzes DNA movement through nucleosomes with a short stretch of DNA persistently exiting the nucleosome before DNA enters nucleosomes. A consequence of asynchronous DNA movement is distortion of the histone octamer structure in which the rotational phasing of DNA is maintained on the octamer surface.,Histone Octamer Structure Is Altered Early in ISW2 ATP-Dependent Nucleosome Remodeling. Nucleosomes are the fundamental building blocks of chromatin that regulate DNA access and are composed of histone octamers. ATP-dependent chromatin remodelers like ISW2 regulate chromatin access by translationally moving nucleosomes to different DNA regions. We find that histone octamers are more pliable than previously assumed and distorted by ISW2 early in remodeling before DNA enters nucleosomes and the ATPase motor moves processively on nucleosomal DNA. Uncoupling the ATPase activity of ISW2 from nucleosome movement with deletion of the SANT domain from the C terminus of the Isw2 catalytic subunit traps remodeling intermediates in which the histone octamer structure is changed. We find restricting histone movement by chemical crosslinking also traps remodeling intermediates resembling those seen early in ISW2 remodeling with loss of the SANT domain. Other evidence shows histone octamers are intrinsically prone to changing their conformation and can be distorted merely by H3-H4 tetramer disulfide crosslinking. Hada et al. show that ISW2 catalyzes DNA movement through nucleosomes with a short stretch of DNA persistently exiting the nucleosome before DNA enters nucleosomes. A consequence of asynchronous DNA movement is distortion of the histone octamer structure in which the rotational phasing of DNA is maintained on the octamer surface.
0,Species-Level Salivary Microbial Indicators of Well-Resolved Periodontitis: A Preliminary Investigation,"Objective: To profile the salivary microbiomes of a Hong Kong Chinese cohort at a species-level resolution and determine species that discriminated clinically resolved periodontitis from periodontally healthy cases. Methods: Salivary microbiomes of 35 Hong Kong Chinese subjects' under routine supportive dental care were analyzed. All subjects had been treated for any dental caries or periodontal disease with all restorative treatment completed at least 1 year ago and had â‰¤3 residual pockets. They were categorized based on a past diagnosis of chronic periodontitis into â€œhealthyâ€ (H) or â€œperiodontitisâ€ (P) categories. Unstimulated whole saliva was collected, genomic DNA was isolated, and high throughput Illumina MiSeq sequencing of 16S rRNA (V3-V4) gene amplicons was performed. The sequences were assigned taxonomy at the species level by using a BLASTN based algorithm that used a combined reference database of HOMD RefSeqV14.51, HOMD RefSeqExtended V1.1 and GreenGeneGold. Species-level OTUs were subjected to downstream analysis in QIIME and R. For P and H group comparisons, community diversity measures were compared, differentially abundant species were determined using DESeq2, and disease indicator species were determined using multi-level pattern analysis within the R package â€œindicspecies.â€ Results: P subjects were significantly older than H subjects (p = 0.003) but not significantly different in their BOP scores (p = 0.82). No significant differences were noted in alpha diversity measures after adjusting for age, gender, and BOP or in the beta diversity estimates. Four species; Treponema sp. oral taxon 237, TM7 sp. Oral Taxon A56, Prevotella sp. oral taxon 314, Prevotella sp. oral taxon 304, and Capnocytophaga leadbetteri were significantly more abundant in P than in the H group. Indicator species analysis showed 7 significant indicators species of P group. Fusobacterium sp oral taxon 370 was the sole positive indicator of P group (positive predictive value = 0.9, p = 0.04). Significant indicators of the H category were Leptotrichia buccalis, Corynebacterium matruchotii, Leptotrichia hofstadii, and Streptococcus intermedius. Conclusion: This exploratory study showed salivary microbial species could discriminate treated, well-maintained chronic periodontitis from healthy controls with similar gingival inflammation levels. The findings suggest that certain salivary microbiome features may identify periodontitis-susceptible individuals despite clinical disease resolution.","Species-Level Salivary Microbial Indicators of Well-Resolved Periodontitis: A Preliminary Investigation. Objective: To profile the salivary microbiomes of a Hong Kong Chinese cohort at a species-level resolution and determine species that discriminated clinically resolved periodontitis from periodontally healthy cases. Methods: Salivary microbiomes of 35 Hong Kong Chinese subjects' under routine supportive dental care were analyzed. All subjects had been treated for any dental caries or periodontal disease with all restorative treatment completed at least 1 year ago and had â‰¤3 residual pockets. They were categorized based on a past diagnosis of chronic periodontitis into â€œhealthyâ€ (H) or â€œperiodontitisâ€ (P) categories. Unstimulated whole saliva was collected, genomic DNA was isolated, and high throughput Illumina MiSeq sequencing of 16S rRNA (V3-V4) gene amplicons was performed. The sequences were assigned taxonomy at the species level by using a BLASTN based algorithm that used a combined reference database of HOMD RefSeqV14.51, HOMD RefSeqExtended V1.1 and GreenGeneGold. Species-level OTUs were subjected to downstream analysis in QIIME and R. For P and H group comparisons, community diversity measures were compared, differentially abundant species were determined using DESeq2, and disease indicator species were determined using multi-level pattern analysis within the R package â€œindicspecies.â€ Results: P subjects were significantly older than H subjects (p = 0.003) but not significantly different in their BOP scores (p = 0.82). No significant differences were noted in alpha diversity measures after adjusting for age, gender, and BOP or in the beta diversity estimates. Four species; Treponema sp. oral taxon 237, TM7 sp. Oral Taxon A56, Prevotella sp. oral taxon 314, Prevotella sp. oral taxon 304, and Capnocytophaga leadbetteri were significantly more abundant in P than in the H group. Indicator species analysis showed 7 significant indicators species of P group. Fusobacterium sp oral taxon 370 was the sole positive indicator of P group (positive predictive value = 0.9, p = 0.04). Significant indicators of the H category were Leptotrichia buccalis, Corynebacterium matruchotii, Leptotrichia hofstadii, and Streptococcus intermedius. Conclusion: This exploratory study showed salivary microbial species could discriminate treated, well-maintained chronic periodontitis from healthy controls with similar gingival inflammation levels. The findings suggest that certain salivary microbiome features may identify periodontitis-susceptible individuals despite clinical disease resolution."
0,Artificial Intelligence and Surgical Decision-Making,"Importance: Surgeons make complex, high-stakes decisions under time constraints and uncertainty, with significant effect on patient outcomes. This review describes the weaknesses of traditional clinical decision-support systems and proposes that artificial intelligence should be used to augment surgical decision-making. Observations: Surgical decision-making is dominated by hypothetical-deductive reasoning, individual judgment, and heuristics. These factors can lead to bias, error, and preventable harm. Traditional predictive analytics and clinical decision-support systems are intended to augment surgical decision-making, but their clinical utility is compromised by time-consuming manual data management and suboptimal accuracy. These challenges can be overcome by automated artificial intelligence models fed by livestreaming electronic health record data with mobile device outputs. This approach would require data standardization, advances in model interpretability, careful implementation and monitoring, attention to ethical challenges involving algorithm bias and accountability for errors, and preservation of bedside assessment and human intuition in the decision-making process. Conclusions and Relevance: Integration of artificial intelligence with surgical decision-making has the potential to transform care by augmenting the decision to operate, informed consent process, identification and mitigation of modifiable risk factors, decisions regarding postoperative management, and shared decisions regarding resource use.","Artificial Intelligence and Surgical Decision-Making. Importance: Surgeons make complex, high-stakes decisions under time constraints and uncertainty, with significant effect on patient outcomes. This review describes the weaknesses of traditional clinical decision-support systems and proposes that artificial intelligence should be used to augment surgical decision-making. Observations: Surgical decision-making is dominated by hypothetical-deductive reasoning, individual judgment, and heuristics. These factors can lead to bias, error, and preventable harm. Traditional predictive analytics and clinical decision-support systems are intended to augment surgical decision-making, but their clinical utility is compromised by time-consuming manual data management and suboptimal accuracy. These challenges can be overcome by automated artificial intelligence models fed by livestreaming electronic health record data with mobile device outputs. This approach would require data standardization, advances in model interpretability, careful implementation and monitoring, attention to ethical challenges involving algorithm bias and accountability for errors, and preservation of bedside assessment and human intuition in the decision-making process. Conclusions and Relevance: Integration of artificial intelligence with surgical decision-making has the potential to transform care by augmenting the decision to operate, informed consent process, identification and mitigation of modifiable risk factors, decisions regarding postoperative management, and shared decisions regarding resource use."
0,Stimulus- and goal-oriented frameworks for understanding natural vision,,
0,"In pregnant women, the pregnancy-adapted YEARS algorithm ruled out PE, with a low rate of VTE at 3 months",,
0,Clustering analysis of microRNA and mRNA expression data from TCGA using maximum edge-weighted matching algorithms,"Background: microRNA (miRNA) is a short RNA (âˆ¼ 22 nt) that regulates gene expression at the posttranscriptional level. Aberration of miRNA expressions could affect their targeting mRNAs involved in cancer-related signaling pathways. We conduct clustering analysis of miRNA and mRNA using expression data from the Cancer Genome Atlas (TCGA). We combine the Hungarian algorithm and blossom algorithm in graph theory. Data analysis is done using programming language R and Python. Methods: We first quantify edge-weights of the miRNA-mRNA pairs by combining their expression correlation coefficient in tumor (T-CC) and correlation coefficient in normal (N-CC). We thereby introduce a bipartite graph partition procedure to identify cluster candidates. Specifically, we propose six weight formulas to quantify the change of miRNA-mRNA expression T-CC relative to N-CC, and apply the traditional hierarchical clustering to subjectively evaluate the different weight formulas of miRNA-mRNA pairs. Among these six different weight formulas, we choose the optimal one, which we define as the integrated mean value weights, to represent the connections between miRNA and mRNAs. Then the Hungarian algorithm and the blossom algorithm are employed on the miRNA-mRNA bipartite graph to passively determine the clusters. The combination of Hungarian and the blossom algorithms is dubbed maximum weighted merger method (MWMM). Results: MWMM identifies clusters of different sizes that meet the mathematical criterion that internal connections inside a cluster are relatively denser than external connections outside the cluster and biological criterion that the intra-cluster Gene Ontology (GO) term similarities are larger than the inter-cluster GO term similarities. MWMM is developed using breast invasive carcinoma (BRCA) as training data set, but can also applies to other cancer type data sets. MWMM shows advantage in GO term similarity in most cancer types, when compared to other algorithms. Conclusions: miRNAs and mRNAs that are likely to be affected by common underlying causal factors in cancer can be clustered by MWMM approach and potentially be used as candidate biomarkers for different cancer types and provide clues for targets of precision medicine in cancer treatment.","Clustering analysis of microRNA and mRNA expression data from TCGA using maximum edge-weighted matching algorithms. Background: microRNA (miRNA) is a short RNA (âˆ¼ 22 nt) that regulates gene expression at the posttranscriptional level. Aberration of miRNA expressions could affect their targeting mRNAs involved in cancer-related signaling pathways. We conduct clustering analysis of miRNA and mRNA using expression data from the Cancer Genome Atlas (TCGA). We combine the Hungarian algorithm and blossom algorithm in graph theory. Data analysis is done using programming language R and Python. Methods: We first quantify edge-weights of the miRNA-mRNA pairs by combining their expression correlation coefficient in tumor (T-CC) and correlation coefficient in normal (N-CC). We thereby introduce a bipartite graph partition procedure to identify cluster candidates. Specifically, we propose six weight formulas to quantify the change of miRNA-mRNA expression T-CC relative to N-CC, and apply the traditional hierarchical clustering to subjectively evaluate the different weight formulas of miRNA-mRNA pairs. Among these six different weight formulas, we choose the optimal one, which we define as the integrated mean value weights, to represent the connections between miRNA and mRNAs. Then the Hungarian algorithm and the blossom algorithm are employed on the miRNA-mRNA bipartite graph to passively determine the clusters. The combination of Hungarian and the blossom algorithms is dubbed maximum weighted merger method (MWMM). Results: MWMM identifies clusters of different sizes that meet the mathematical criterion that internal connections inside a cluster are relatively denser than external connections outside the cluster and biological criterion that the intra-cluster Gene Ontology (GO) term similarities are larger than the inter-cluster GO term similarities. MWMM is developed using breast invasive carcinoma (BRCA) as training data set, but can also applies to other cancer type data sets. MWMM shows advantage in GO term similarity in most cancer types, when compared to other algorithms. Conclusions: miRNAs and mRNAs that are likely to be affected by common underlying causal factors in cancer can be clustered by MWMM approach and potentially be used as candidate biomarkers for different cancer types and provide clues for targets of precision medicine in cancer treatment."
0,DoubletFinder: Doublet Detection in Single-Cell RNA Sequencing Data Using Artificial Nearest Neighbors,"Single-cell RNA sequencing (scRNA-seq) data are commonly affected by technical artifacts known as â€œdoublets,â€ which limit cell throughput and lead to spurious biological conclusions. Here, we present a computational doublet detection toolâ€”DoubletFinderâ€”that identifies doublets using only gene expression data. DoubletFinder predicts doublets according to each real cell's proximity in gene expression space to artificial doublets created by averaging the transcriptional profile of randomly chosen cell pairs. We first use scRNA-seq datasets where the identity of doublets is known to show that DoubletFinder identifies doublets formed from transcriptionally distinct cells. When these doublets are removed, the identification of differentially expressed genes is enhanced. Second, we provide a method for estimating DoubletFinder input parameters, allowing its application across scRNA-seq datasets with diverse distributions of cell types. Lastly, we present â€œbest practicesâ€ for DoubletFinder applications and illustrate that DoubletFinder is insensitive to an experimentally validated kidney cell type with â€œhybridâ€ expression features.","DoubletFinder: Doublet Detection in Single-Cell RNA Sequencing Data Using Artificial Nearest Neighbors. Single-cell RNA sequencing (scRNA-seq) data are commonly affected by technical artifacts known as â€œdoublets,â€ which limit cell throughput and lead to spurious biological conclusions. Here, we present a computational doublet detection toolâ€”DoubletFinderâ€”that identifies doublets using only gene expression data. DoubletFinder predicts doublets according to each real cell's proximity in gene expression space to artificial doublets created by averaging the transcriptional profile of randomly chosen cell pairs. We first use scRNA-seq datasets where the identity of doublets is known to show that DoubletFinder identifies doublets formed from transcriptionally distinct cells. When these doublets are removed, the identification of differentially expressed genes is enhanced. Second, we provide a method for estimating DoubletFinder input parameters, allowing its application across scRNA-seq datasets with diverse distributions of cell types. Lastly, we present â€œbest practicesâ€ for DoubletFinder applications and illustrate that DoubletFinder is insensitive to an experimentally validated kidney cell type with â€œhybridâ€ expression features."
0,Evidence for the impact of BAG3 on electrophysiological activity of primary culture of neonatal cardiomyocytes,"Homeostasis of proteins involved in contractility of individual cardiomyocytes and those coupling adjacent cells is of critical importance as any abnormalities in cardiac electrical conduction may result in cardiac irregular activity and heart failure. Bcl2-associated athanogene 3 (BAG3) is a stress-induced protein whose role in stabilizing myofibril proteins as well as protein quality control pathways, especially in the cardiac tissue, has captured much attention. Mutations of BAG3 have been implicated in the pathogenesis of cardiac complications such as dilated cardiomyopathy. In this study, we have used an in vitro model of neonatal rat ventricular cardiomyocytes to investigate potential impacts of BAG3 on electrophysiological activity by employing the microelectrode array (MEA) technology. Our MEA data showed that BAG3 plays an important role in the cardiac signal generation as reduced levels of BAG3 led to lower signal frequency and amplitude. Our analysis also revealed that BAG3 is essential to the signal propagation throughout the myocardium, as the MEA data-based conduction velocity, connectivity degree, activation time, and synchrony were adversely affected by BAG3 knockdown. Moreover, BAG3 deficiency was demonstrated to be connected with the emergence of independently beating clusters of cardiomyocytes. On the other hand, BAG3 overexpression improved the activity of cardiomyocytes in terms of electrical signal amplitude and connectivity degree. Overall, by providing more in-depth analyses and characterization of electrophysiological parameters, this study reveals that BAG3 is of critical importance for electrical activity of neonatal cardiomyocytes.","Evidence for the impact of BAG3 on electrophysiological activity of primary culture of neonatal cardiomyocytes. Homeostasis of proteins involved in contractility of individual cardiomyocytes and those coupling adjacent cells is of critical importance as any abnormalities in cardiac electrical conduction may result in cardiac irregular activity and heart failure. Bcl2-associated athanogene 3 (BAG3) is a stress-induced protein whose role in stabilizing myofibril proteins as well as protein quality control pathways, especially in the cardiac tissue, has captured much attention. Mutations of BAG3 have been implicated in the pathogenesis of cardiac complications such as dilated cardiomyopathy. In this study, we have used an in vitro model of neonatal rat ventricular cardiomyocytes to investigate potential impacts of BAG3 on electrophysiological activity by employing the microelectrode array (MEA) technology. Our MEA data showed that BAG3 plays an important role in the cardiac signal generation as reduced levels of BAG3 led to lower signal frequency and amplitude. Our analysis also revealed that BAG3 is essential to the signal propagation throughout the myocardium, as the MEA data-based conduction velocity, connectivity degree, activation time, and synchrony were adversely affected by BAG3 knockdown. Moreover, BAG3 deficiency was demonstrated to be connected with the emergence of independently beating clusters of cardiomyocytes. On the other hand, BAG3 overexpression improved the activity of cardiomyocytes in terms of electrical signal amplitude and connectivity degree. Overall, by providing more in-depth analyses and characterization of electrophysiological parameters, this study reveals that BAG3 is of critical importance for electrical activity of neonatal cardiomyocytes."
0,Characterization of Parkinson's disease using blood-based biomarkers: A multicohort proteomic analysis,"BACKGROUND: Parkinson's disease (PD) is a progressive neurodegenerative disease affecting about 5 million people worldwide with no disease-modifying therapies. We sought blood-based biomarkers in order to provide molecular characterization of individuals with PD for diagnostic confirmation and prediction of progression. METHODS AND FINDINGS: In 141 plasma samples (96 PD, 45 neurologically normal control [NC] individuals; 45.4% female, mean age 70.0 years) from a longitudinally followed Discovery Cohort based at the University of Pennsylvania (UPenn), we measured levels of 1,129 proteins using an aptamer-based platform. We modeled protein plasma concentration (log10 of relative fluorescence units [RFUs]) as the effect of treatment group (PD versus NC), age at plasma collection, sex, and the levodopa equivalent daily dose (LEDD), deriving first-pass candidate protein biomarkers based on p-value for PD versus NC. These candidate proteins were then ranked by Stability Selection. We confirmed findings from our Discovery Cohort in a Replication Cohort of 317 individuals (215 PD, 102 NC; 47.9% female, mean age 66.7 years) from the multisite, longitudinally followed National Institute of Neurological Disorders and Stroke Parkinson's Disease Biomarker Program (PDBP) Cohort. Analytical approach in the Replication Cohort mirrored the approach in the Discovery Cohort: each protein plasma concentration (log10 of RFU) was modeled as the effect of group (PD versus NC), age at plasma collection, sex, clinical site, and batch. Of the top 10 proteins from the Discovery Cohort ranked by Stability Selection, four associations were replicated in the Replication Cohort. These blood-based biomarkers were bone sialoprotein (BSP, Discovery false discovery rate [FDR]-corrected p = 2.82 x 10-2, Replication FDR-corrected p = 1.03 x 10-4), osteomodulin (OMD, Discovery FDR-corrected p = 2.14 x 10-2, Replication FDR-corrected p = 9.14 x 10-5), aminoacylase-1 (ACY1, Discovery FDR-corrected p = 1.86 x 10-3, Replication FDR-corrected p = 2.18 x 10-2), and growth hormone receptor (GHR, Discovery FDR-corrected p = 3.49 x 10-4, Replication FDR-corrected p = 2.97 x 10-3). Measures of these proteins were not significantly affected by differences in sample handling, and they did not change comparing plasma samples from 10 PD participants sampled both on versus off dopaminergic medication. Plasma measures of OMD, ACY1, and GHR differed in PD versus NC but did not differ between individuals with amyotrophic lateral sclerosis (ALS, n = 59) versus NC. In the Discovery Cohort, individuals with baseline levels of GHR and ACY1 in the lowest tertile were more likely to progress to mild cognitive impairment (MCI) or dementia in Cox proportional hazards analyses adjusting for age, sex, and disease duration (hazard ratio [HR] 2.27 [95% CI 1.04-5.0, p = 0.04] for GHR, and HR 3.0 [95% CI 1.24-7.0, p = 0.014] for ACY1). GHR's association with cognitive decline was confirmed in the Replication Cohort (HR 3.6 [95% CI 1.20-11.1, p = 0.02]). The main limitations of this study were its reliance on the aptamer-based platform for protein measurement and limited follow-up time available for some cohorts. CONCLUSIONS: In this study, we found that the blood-based biomarkers BSP, OMD, ACY1, and GHR robustly associated with PD across multiple clinical sites. Our findings suggest that biomarkers based on a peripheral blood sample may be developed for both disease characterization and prediction of future disease progression in PD.","Characterization of Parkinson's disease using blood-based biomarkers: A multicohort proteomic analysis. BACKGROUND: Parkinson's disease (PD) is a progressive neurodegenerative disease affecting about 5 million people worldwide with no disease-modifying therapies. We sought blood-based biomarkers in order to provide molecular characterization of individuals with PD for diagnostic confirmation and prediction of progression. METHODS AND FINDINGS: In 141 plasma samples (96 PD, 45 neurologically normal control [NC] individuals; 45.4% female, mean age 70.0 years) from a longitudinally followed Discovery Cohort based at the University of Pennsylvania (UPenn), we measured levels of 1,129 proteins using an aptamer-based platform. We modeled protein plasma concentration (log10 of relative fluorescence units [RFUs]) as the effect of treatment group (PD versus NC), age at plasma collection, sex, and the levodopa equivalent daily dose (LEDD), deriving first-pass candidate protein biomarkers based on p-value for PD versus NC. These candidate proteins were then ranked by Stability Selection. We confirmed findings from our Discovery Cohort in a Replication Cohort of 317 individuals (215 PD, 102 NC; 47.9% female, mean age 66.7 years) from the multisite, longitudinally followed National Institute of Neurological Disorders and Stroke Parkinson's Disease Biomarker Program (PDBP) Cohort. Analytical approach in the Replication Cohort mirrored the approach in the Discovery Cohort: each protein plasma concentration (log10 of RFU) was modeled as the effect of group (PD versus NC), age at plasma collection, sex, clinical site, and batch. Of the top 10 proteins from the Discovery Cohort ranked by Stability Selection, four associations were replicated in the Replication Cohort. These blood-based biomarkers were bone sialoprotein (BSP, Discovery false discovery rate [FDR]-corrected p = 2.82 x 10-2, Replication FDR-corrected p = 1.03 x 10-4), osteomodulin (OMD, Discovery FDR-corrected p = 2.14 x 10-2, Replication FDR-corrected p = 9.14 x 10-5), aminoacylase-1 (ACY1, Discovery FDR-corrected p = 1.86 x 10-3, Replication FDR-corrected p = 2.18 x 10-2), and growth hormone receptor (GHR, Discovery FDR-corrected p = 3.49 x 10-4, Replication FDR-corrected p = 2.97 x 10-3). Measures of these proteins were not significantly affected by differences in sample handling, and they did not change comparing plasma samples from 10 PD participants sampled both on versus off dopaminergic medication. Plasma measures of OMD, ACY1, and GHR differed in PD versus NC but did not differ between individuals with amyotrophic lateral sclerosis (ALS, n = 59) versus NC. In the Discovery Cohort, individuals with baseline levels of GHR and ACY1 in the lowest tertile were more likely to progress to mild cognitive impairment (MCI) or dementia in Cox proportional hazards analyses adjusting for age, sex, and disease duration (hazard ratio [HR] 2.27 [95% CI 1.04-5.0, p = 0.04] for GHR, and HR 3.0 [95% CI 1.24-7.0, p = 0.014] for ACY1). GHR's association with cognitive decline was confirmed in the Replication Cohort (HR 3.6 [95% CI 1.20-11.1, p = 0.02]). The main limitations of this study were its reliance on the aptamer-based platform for protein measurement and limited follow-up time available for some cohorts. CONCLUSIONS: In this study, we found that the blood-based biomarkers BSP, OMD, ACY1, and GHR robustly associated with PD across multiple clinical sites. Our findings suggest that biomarkers based on a peripheral blood sample may be developed for both disease characterization and prediction of future disease progression in PD."
0,Licochalcone D directly targets JAK2 to induced apoptosis in human oral squamous cell carcinoma,"Licochalcone (LC) families have been reported to have a wide range of biological function such as antioxidant, antibacterial, antiviral, and anticancer effects. Although various beneficial effects of LCD were revealed, its anticancer effect in human oral squamous cancer has not been identified. To examine the signaling pathway of LCDâ€™s anticancer effect, we determined whether LCD has physical interaction with Janus kinase (JAK2)/signal transducer and activator of transcription-3 (STAT3) signaling, which is critical in promoting cancer cell survival and proliferation. Our results demonstrated that LCD inhibited the kinase activity of JAK2, soft agar colony formation, and the proliferation of HN22 and HSC4 cells. LCD also induced mitochondrial apoptotic events such as altered mitochondrial membrane potential and reactive oxygen speciesÂ production. LCD increased the expression of apoptosis-associated proteins in oral squamous cell carcinoma (OSCC) cells. Finally, the xenograft study showed that LCD significantly inhibited HN22 tumor growth. Immunohistochemical data supported that LCD suppressed p-JAK2 and p-STAT3 expression and induced cleaved-caspase-3 expression. These results indicate that the anticancer effect of LCD is due to the direct targeting of JAK2 kinase. Therefore, LCD can be used for therapeutic application against OSCC.","Licochalcone D directly targets JAK2 to induced apoptosis in human oral squamous cell carcinoma. Licochalcone (LC) families have been reported to have a wide range of biological function such as antioxidant, antibacterial, antiviral, and anticancer effects. Although various beneficial effects of LCD were revealed, its anticancer effect in human oral squamous cancer has not been identified. To examine the signaling pathway of LCDâ€™s anticancer effect, we determined whether LCD has physical interaction with Janus kinase (JAK2)/signal transducer and activator of transcription-3 (STAT3) signaling, which is critical in promoting cancer cell survival and proliferation. Our results demonstrated that LCD inhibited the kinase activity of JAK2, soft agar colony formation, and the proliferation of HN22 and HSC4 cells. LCD also induced mitochondrial apoptotic events such as altered mitochondrial membrane potential and reactive oxygen speciesÂ production. LCD increased the expression of apoptosis-associated proteins in oral squamous cell carcinoma (OSCC) cells. Finally, the xenograft study showed that LCD significantly inhibited HN22 tumor growth. Immunohistochemical data supported that LCD suppressed p-JAK2 and p-STAT3 expression and induced cleaved-caspase-3 expression. These results indicate that the anticancer effect of LCD is due to the direct targeting of JAK2 kinase. Therefore, LCD can be used for therapeutic application against OSCC."
0,Correlation structure of grid cells is preserved during sleep,,
0,Phenotype and Variant Spectrum in the LAMB3 Form of Amelogenesis Imperfecta,,
0,"Zerumbone, a cyclic sesquiterpene, exerts antimitotic activity in HeLa cells through tubulin binding and exhibits synergistic activity with vinblastine and paclitaxel",Objectives: The aim of this study was to elucidate the antimitotic mechanism of zerumbone and to investigate its effect on the HeLa cells in combination with other mitotic blockers. Materials and methods: HeLa cells and fluorescence microscopy were used to analyse the effect of zerumbone on cancer cell lines. Cellular internalization of zerumbone was investigated using FITC-labelled zerumbone. The interaction of zerumbone with tubulin was characterized using fluorescence spectroscopy. The Chou and Talalay equation was used to calculate the combination index. Results: Zerumbone selectively inhibited the proliferation of HeLa cells with an IC50 of 14.2Â Â±Â 0.5Â Î¼mol/L through enhanced cellular uptake compared to the normal cell line L929. It induced a strong mitotic block with cells exhibiting bipolar spindles at the IC50 and monopolar spindles at 30Â Î¼mol/L. Docking analysis indicated that tubulin is the principal target of zerumbone. In vitro studies indicated that it bound to goat brain tubulin with a Kd of 4Â Î¼mol/L and disrupted the assembly of tubulin into microtubules. Zerumbone and colchicine had partially overlapping binding site on tubulin. Zerumbone synergistically enhanced the anti-proliferative activity of vinblastine and paclitaxel through augmented mitotic block. Conclusion: Our data suggest that disruption of microtubule assembly dynamics is one of the mechanisms of the anti-cancer activity of zerumbone and it can be used in combination therapy targeting cell division.,"Zerumbone, a cyclic sesquiterpene, exerts antimitotic activity in HeLa cells through tubulin binding and exhibits synergistic activity with vinblastine and paclitaxel. Objectives: The aim of this study was to elucidate the antimitotic mechanism of zerumbone and to investigate its effect on the HeLa cells in combination with other mitotic blockers. Materials and methods: HeLa cells and fluorescence microscopy were used to analyse the effect of zerumbone on cancer cell lines. Cellular internalization of zerumbone was investigated using FITC-labelled zerumbone. The interaction of zerumbone with tubulin was characterized using fluorescence spectroscopy. The Chou and Talalay equation was used to calculate the combination index. Results: Zerumbone selectively inhibited the proliferation of HeLa cells with an IC50 of 14.2Â Â±Â 0.5Â Î¼mol/L through enhanced cellular uptake compared to the normal cell line L929. It induced a strong mitotic block with cells exhibiting bipolar spindles at the IC50 and monopolar spindles at 30Â Î¼mol/L. Docking analysis indicated that tubulin is the principal target of zerumbone. In vitro studies indicated that it bound to goat brain tubulin with a Kd of 4Â Î¼mol/L and disrupted the assembly of tubulin into microtubules. Zerumbone and colchicine had partially overlapping binding site on tubulin. Zerumbone synergistically enhanced the anti-proliferative activity of vinblastine and paclitaxel through augmented mitotic block. Conclusion: Our data suggest that disruption of microtubule assembly dynamics is one of the mechanisms of the anti-cancer activity of zerumbone and it can be used in combination therapy targeting cell division."
0,Modeling DNA Unwinding by AddAB Helicaseâ€“Nuclease and Modulation by Chi Sequences: Comparison with AdnAB and RecBCD,"Introduction: AddAB enzyme is a helicaseâ€“nuclease complex that initiates recombinational repair of double-stranded DNA breaks. It catalyzes processive DNA unwinding and concomitant resection of the unwound strands, which are modulated by the recognition of a recombination hotspot called Chi in the 3â€²-terminated strand. Despite extensive structural, biochemical and single molecule studies, the detailed molecular mechanism of DNA unwinding by the complex and modulation by Chi sequence remains unclear. Methods: A model of DNA unwinding by the AddAB complex and modulation by Chi recognition was presented, based on which the dynamics of AddAB complex was studied analytically. Results: The theoretical results explain well the available experimental data on effect of DNA sequence on velocity, effect of Chi recognition on velocity, static disorder peculiar to the AddAB complex, and dynamics of pausing of wild-type and mutant AddAB complexes occurring at Chi or Chi-like sequence. Predictions were provided. Comparisons of AddAB complex with other helicaseâ€“nuclease complexes such as RecBCD and AdnAB were made. Conclusions: The study has strong implications for the molecular mechanism of DNA unwinding by the AddAB complex. The intriguing issues are addressed of why Chi recognition is an inefficient process, how AddAB complex pauses upon recognizing Chi sequence, how the paused state transits to the translocating state, why the mutant AddAB with a stronger affinity to Chi sequence has a shorter pausing lifetime, why the pausing lifetime is sensitive to the solution temperature, and so on.","Modeling DNA Unwinding by AddAB Helicaseâ€“Nuclease and Modulation by Chi Sequences: Comparison with AdnAB and RecBCD. Introduction: AddAB enzyme is a helicaseâ€“nuclease complex that initiates recombinational repair of double-stranded DNA breaks. It catalyzes processive DNA unwinding and concomitant resection of the unwound strands, which are modulated by the recognition of a recombination hotspot called Chi in the 3â€²-terminated strand. Despite extensive structural, biochemical and single molecule studies, the detailed molecular mechanism of DNA unwinding by the complex and modulation by Chi sequence remains unclear. Methods: A model of DNA unwinding by the AddAB complex and modulation by Chi recognition was presented, based on which the dynamics of AddAB complex was studied analytically. Results: The theoretical results explain well the available experimental data on effect of DNA sequence on velocity, effect of Chi recognition on velocity, static disorder peculiar to the AddAB complex, and dynamics of pausing of wild-type and mutant AddAB complexes occurring at Chi or Chi-like sequence. Predictions were provided. Comparisons of AddAB complex with other helicaseâ€“nuclease complexes such as RecBCD and AdnAB were made. Conclusions: The study has strong implications for the molecular mechanism of DNA unwinding by the AddAB complex. The intriguing issues are addressed of why Chi recognition is an inefficient process, how AddAB complex pauses upon recognizing Chi sequence, how the paused state transits to the translocating state, why the mutant AddAB with a stronger affinity to Chi sequence has a shorter pausing lifetime, why the pausing lifetime is sensitive to the solution temperature, and so on."
0,A rapid triage test for active pulmonary tuberculosis in adult patients with persistent cough,,
0,Unravelling cellular relationships during development and regeneration using genetic lineage tracing,,
0,Identification of key gene modules and pathways of human glioma through coexpression network,"Glioma causes great harm to people worldwide. Systemic coexpression analysis of this disease could be beneficial for the identification and development of new prognostic and predictive markers in the clinical management of glioma. In this study, we extracted data sets from the Gene Expression Omnibus data set by using â€œgliomaâ€ as the keyword. Then, a coexpression module was constructed with the help of Weighted Gene Coexpression Network Analysis software. Besides, Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analyses were performed on the genes in these modules. As a result, the critical modules and target genes were identified. Eight coexpression modules were constructed using the 4,000 genes with a high expression value of the total 141 glioma samples. The result of the analysis of the interaction among these modules showed that there was a high scale independence degree among them. The GO and KEGG enrichment analyses showed that there was a significant difference in the enriched terms and degree among these eight modules, and module 5 was identified as the most important module. Besides, the pathways it was enriched in, hsa04510: Focal adhesion and hsa04610: Complement and coagulation cascades, were determined as the most important pathways. In summary, module 5 and the pathways it was enriched in, hsa04510: Focal adhesion and has 04610: Complement and coagulation cascades, have the potential to serve as biomarkers for patients with glioma.","Identification of key gene modules and pathways of human glioma through coexpression network. Glioma causes great harm to people worldwide. Systemic coexpression analysis of this disease could be beneficial for the identification and development of new prognostic and predictive markers in the clinical management of glioma. In this study, we extracted data sets from the Gene Expression Omnibus data set by using â€œgliomaâ€ as the keyword. Then, a coexpression module was constructed with the help of Weighted Gene Coexpression Network Analysis software. Besides, Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analyses were performed on the genes in these modules. As a result, the critical modules and target genes were identified. Eight coexpression modules were constructed using the 4,000 genes with a high expression value of the total 141 glioma samples. The result of the analysis of the interaction among these modules showed that there was a high scale independence degree among them. The GO and KEGG enrichment analyses showed that there was a significant difference in the enriched terms and degree among these eight modules, and module 5 was identified as the most important module. Besides, the pathways it was enriched in, hsa04510: Focal adhesion and hsa04610: Complement and coagulation cascades, were determined as the most important pathways. In summary, module 5 and the pathways it was enriched in, hsa04510: Focal adhesion and has 04610: Complement and coagulation cascades, have the potential to serve as biomarkers for patients with glioma."
0,DeepNEU: Artificially Induced Stem Cell (aiPSC) and Differentiated Skeletal Muscle Cell (aiSkMC) Simulations of Infantile Onset POMPE Disease (IOPD) for Potential Biomarker Identification and Drug Discovery,"Infantile onset Pompe disease (IOPD) is a rare and lethal genetic disorder caused by the deletion of the acid alpha-glucosidase (GAA) gene. This gene encodes an essential lysosomal enzyme that converts glycogen to glucose. While enzyme replacement therapy helps some, our understanding of disease pathophysiology is limited. In this project we develop computer simulated stem cells (aiPSC) and differentiated skeletal muscle cells (aiSkMC) to empower IOPD research and drug discovery. Our Artificial Intelligence (AI) platform, DeepNEU v3.6 was used to generate aiPSC and aiSkMC simulations with and without GAA expression. These simulations were validated using peer reviewed results from the recent literature. Once the aiSkMC simulations (IOPD and WT) were validated they were used to evaluate calcium homeostasis and mitochondrial function in IOPD. Lastly, we used aiSkMC IOPD simulations to identify known and novel biomarkers and potential therapeutic targets. The aiSkMC simulations of IOPD correctly predicted genotypic and phenotypic features that were reported in recent literature. The probability that these features were accurately predicted by chance alone using the binomial test is 0.0025. The aiSkMC IOPD simulation correctly identified L-type calcium channels (VDCC) as a biomarker and confirmed the positive effects of calcium channel blockade (CCB) on calcium homeostasis and mitochondrial function. These published data were extended by the aiSkMC simulations to identify calpain(s) as a novel potential biomarker and therapeutic target for IOPD. This is the first time that computer simulations of iPSC and differentiated skeletal muscle cells have been used to study IOPD. The simulations are robust and accurate based on available published literature. We also demonstrated that the IOPD simulations can be used for potential biomarker identification leading to targeted drug discovery. We will continue to explore the potential for calpain inhibitors with and without CCB as effective therapy for IOPD.","DeepNEU: Artificially Induced Stem Cell (aiPSC) and Differentiated Skeletal Muscle Cell (aiSkMC) Simulations of Infantile Onset POMPE Disease (IOPD) for Potential Biomarker Identification and Drug Discovery. Infantile onset Pompe disease (IOPD) is a rare and lethal genetic disorder caused by the deletion of the acid alpha-glucosidase (GAA) gene. This gene encodes an essential lysosomal enzyme that converts glycogen to glucose. While enzyme replacement therapy helps some, our understanding of disease pathophysiology is limited. In this project we develop computer simulated stem cells (aiPSC) and differentiated skeletal muscle cells (aiSkMC) to empower IOPD research and drug discovery. Our Artificial Intelligence (AI) platform, DeepNEU v3.6 was used to generate aiPSC and aiSkMC simulations with and without GAA expression. These simulations were validated using peer reviewed results from the recent literature. Once the aiSkMC simulations (IOPD and WT) were validated they were used to evaluate calcium homeostasis and mitochondrial function in IOPD. Lastly, we used aiSkMC IOPD simulations to identify known and novel biomarkers and potential therapeutic targets. The aiSkMC simulations of IOPD correctly predicted genotypic and phenotypic features that were reported in recent literature. The probability that these features were accurately predicted by chance alone using the binomial test is 0.0025. The aiSkMC IOPD simulation correctly identified L-type calcium channels (VDCC) as a biomarker and confirmed the positive effects of calcium channel blockade (CCB) on calcium homeostasis and mitochondrial function. These published data were extended by the aiSkMC simulations to identify calpain(s) as a novel potential biomarker and therapeutic target for IOPD. This is the first time that computer simulations of iPSC and differentiated skeletal muscle cells have been used to study IOPD. The simulations are robust and accurate based on available published literature. We also demonstrated that the IOPD simulations can be used for potential biomarker identification leading to targeted drug discovery. We will continue to explore the potential for calpain inhibitors with and without CCB as effective therapy for IOPD."
0,Arrhythmic risk stratification in post-myocardial infarction patients with preserved ejection fraction: the PRESERVE EF study,,
0,P53-mediated PI3K/AKT/mTOR pathway played a role in PtoXDPT-induced EMT inhibition in liver cancer cell lines,"Epithelial-mesenchymal transition (EMT) involves metastasis and drug resistance; thus, a new EMT reversing agent is required. It has shown that wild-type p53 can reverse EMT back to epithelial characteristics, and iron chelator acting as a p53 inducer has been demonstrated. Moreover, recent study revealed that etoposide could also inhibit EMT. Therefore, combination of etoposide with iron chelator might achieve better inhibition of EMT. To this end, we prepared di-2-pyridineketone hydrazone dithiocarbamate S-propionate podophyllotoxin ester (PtoxDpt) that combined the podophyllotoxin (Ptox) structural unit (etoposide) with the dithiocarbamate unit (iron chelator) through the hybridization strategy. The resulting PtoxDpt inherited characteristics from parent structural units, acting as both the p53 inducer and topoisomerase II inhibitor. In addition, the PtoxDpt exhibited significant inhibition in migration and invasion, which correlated with downregulation of matrix metalloproteinase (MMP). More importantly, PtoxDpt could inhibit EMT in the absence or presence of TGF-Î²1, concomitant to the ROS production, and the additional evidence revealed that PtoxDpt downregulated AKT/mTOR through upregulation of p53, indicating that PtoxDpt induced EMT inhibition through the p53/PI3K/AKT/mTOR pathway.","P53-mediated PI3K/AKT/mTOR pathway played a role in PtoXDPT-induced EMT inhibition in liver cancer cell lines. Epithelial-mesenchymal transition (EMT) involves metastasis and drug resistance; thus, a new EMT reversing agent is required. It has shown that wild-type p53 can reverse EMT back to epithelial characteristics, and iron chelator acting as a p53 inducer has been demonstrated. Moreover, recent study revealed that etoposide could also inhibit EMT. Therefore, combination of etoposide with iron chelator might achieve better inhibition of EMT. To this end, we prepared di-2-pyridineketone hydrazone dithiocarbamate S-propionate podophyllotoxin ester (PtoxDpt) that combined the podophyllotoxin (Ptox) structural unit (etoposide) with the dithiocarbamate unit (iron chelator) through the hybridization strategy. The resulting PtoxDpt inherited characteristics from parent structural units, acting as both the p53 inducer and topoisomerase II inhibitor. In addition, the PtoxDpt exhibited significant inhibition in migration and invasion, which correlated with downregulation of matrix metalloproteinase (MMP). More importantly, PtoxDpt could inhibit EMT in the absence or presence of TGF-Î²1, concomitant to the ROS production, and the additional evidence revealed that PtoxDpt downregulated AKT/mTOR through upregulation of p53, indicating that PtoxDpt induced EMT inhibition through the p53/PI3K/AKT/mTOR pathway."
0,Neuronal Dynamics Regulating Brain and Behavioral State Transitions,,
0,Assessing Cancer Risk from Mammograms: Deep Learning Is Superior to Conventional Risk Models,,
0,Beyond plasticity: the dynamic impact of electrical synapses on neural circuits,,
0,Caries Detection with Near-Infrared Transillumination Using Deep Learning,"Dental caries is the most prevalent chronic condition worldwide. Early detection can significantly improve treatment outcomes and reduce the need for invasive procedures. Recently, near-infrared transillumination (TI) imaging has been shown to be effective for the detection of early stage lesions. In this work, we present a deep learning model for the automated detection and localization of dental lesions in TI images. Our method is based on a convolutional neural network (CNN) trained on a semantic segmentation task. We use various strategies to mitigate issues related to training data scarcity, class imbalance, and overfitting. With only 185 training samples, our model achieved an overall mean intersection-over-union (IOU) score of 72.7% on a 5-class segmentation task and specifically an IOU score of 49.5% and 49.0% for proximal and occlusal carious lesions, respectively. In addition, we constructed a simplified task, in which regions of interest were evaluated for the binary presence or absence of carious lesions. For this task, our model achieved an area under the receiver operating characteristic curve of 83.6% and 85.6% for occlusal and proximal lesions, respectively. Our work demonstrates that a deep learning approach for the analysis of dental images holds promise for increasing the speed and accuracy of caries detection, supporting the diagnoses of dental practitioners, and improving patient outcomes.","Caries Detection with Near-Infrared Transillumination Using Deep Learning. Dental caries is the most prevalent chronic condition worldwide. Early detection can significantly improve treatment outcomes and reduce the need for invasive procedures. Recently, near-infrared transillumination (TI) imaging has been shown to be effective for the detection of early stage lesions. In this work, we present a deep learning model for the automated detection and localization of dental lesions in TI images. Our method is based on a convolutional neural network (CNN) trained on a semantic segmentation task. We use various strategies to mitigate issues related to training data scarcity, class imbalance, and overfitting. With only 185 training samples, our model achieved an overall mean intersection-over-union (IOU) score of 72.7% on a 5-class segmentation task and specifically an IOU score of 49.5% and 49.0% for proximal and occlusal carious lesions, respectively. In addition, we constructed a simplified task, in which regions of interest were evaluated for the binary presence or absence of carious lesions. For this task, our model achieved an area under the receiver operating characteristic curve of 83.6% and 85.6% for occlusal and proximal lesions, respectively. Our work demonstrates that a deep learning approach for the analysis of dental images holds promise for increasing the speed and accuracy of caries detection, supporting the diagnoses of dental practitioners, and improving patient outcomes."
0,Survival Advantage of Laparoscopic Versus Open Resection For Colorectal Liver Metastases: A Meta-analysis of Individual Patient Data From Randomized Trials and Propensity-score Matched Studies,,
0,Mapping Spatiotemporal Microproteomics Landscape in Experimental Model of Traumatic Brain Injury Unveils a link to Parkinsonâ€™s Disease,"Traumatic brain injury (TBI) represents a major health concerns with no clinically-approved FDA drug available for therapeutic intervention. Several genomics and neuroproteomics studies have been employed to decipher the underlying pathological mechanisms involved that can serve as potential neurotherapeutic targets and unveil a possible underlying relation of TBI to other secondary neurological disorders. In this work, we present a novel high throughput systems biology approach using a spatially resolved microproteomics platform conducted on different brain regions in an experimental rat model of moderate of controlled cortical injury (CCI) at a temporal pattern postinjury (1 day, 3 days, 7 days, and 10 days). Mapping the spatiotemporal landscape of signature markers in TBI revealed an overexpression of major protein families known to be implicated in Parkinsonâ€™s disease (PD) such as GPR158, HGMB1, synaptotagmin and glutamate decarboxylase in the ipsilateral substantia nigra. In silico bioinformatics docking experiments indicated the potential correlation between TBI and PD through alpha-synuclein. In an in vitro model, stimulation with palmitoylcarnitine triggered an inflammatory response in macrophages and a regeneration processes in astrocytes which also further confirmed the in vivo TBI proteomics data. Taken together, this is the first study to assess the microproteomics landscape in TBI, mainly in the substantia nigra, thus revealing a potential predisposition for PD or Parkinsonism post-TBI.","Mapping Spatiotemporal Microproteomics Landscape in Experimental Model of Traumatic Brain Injury Unveils a link to Parkinsonâ€™s Disease. Traumatic brain injury (TBI) represents a major health concerns with no clinically-approved FDA drug available for therapeutic intervention. Several genomics and neuroproteomics studies have been employed to decipher the underlying pathological mechanisms involved that can serve as potential neurotherapeutic targets and unveil a possible underlying relation of TBI to other secondary neurological disorders. In this work, we present a novel high throughput systems biology approach using a spatially resolved microproteomics platform conducted on different brain regions in an experimental rat model of moderate of controlled cortical injury (CCI) at a temporal pattern postinjury (1 day, 3 days, 7 days, and 10 days). Mapping the spatiotemporal landscape of signature markers in TBI revealed an overexpression of major protein families known to be implicated in Parkinsonâ€™s disease (PD) such as GPR158, HGMB1, synaptotagmin and glutamate decarboxylase in the ipsilateral substantia nigra. In silico bioinformatics docking experiments indicated the potential correlation between TBI and PD through alpha-synuclein. In an in vitro model, stimulation with palmitoylcarnitine triggered an inflammatory response in macrophages and a regeneration processes in astrocytes which also further confirmed the in vivo TBI proteomics data. Taken together, this is the first study to assess the microproteomics landscape in TBI, mainly in the substantia nigra, thus revealing a potential predisposition for PD or Parkinsonism post-TBI."
0,Antidepressants May Affect Pain Neural Network,,
0,"Should Health Care Demand Interpretable Artificial Intelligence or Accept ""Black Box"" Medicine?",,
0,Study on therapeutic action and mechanism of TMZ Combined with RITA against glioblastoma,"Background/Aims: Glioblastoma multiforme (GBM) is a malignant and aggressive central nervous system (CNS) tumor with high mortality and low survival rate. Effective treatment of GMB is a challenge worldwide. Temozolomide (TMZ) is a drug used to treat GBM, while the survival period of GBM patients with positive treatment remains less than 15 months. Reactivating p53 and Inducing Tumor Apoptosis (RITA) is a novel potential anti-cancer small molecular drug. Thus, it is essential to discover novel targets or develop effective drugs combination strategy to treat GBM. Methods: The U87 cells and U251 cells (p53 mutated) were treated with DMSO and 1, 5,10, 20 Î¼M RITA, TMZ, RITA+TMA or PFT-Î±. The cell proliferation was measured using the MTS cell proliferation assay. The cell apoptosis was analyzed by Annexin V-FITC/PI Apoptosis Detection Kit. The key protein expression level was evaluated by WB. Molecular docking and molecular dynamics (MD) simulation methods were applied to simulate the interaction between RITA and ASK1. Results: Herein, we found that combination RITA and TMZ effectively inhibited the proliferation of U87 cells and promoted the apoptosis of U87 cells. Then the mechanism of RITA and TMZ treating GBM were further studied by detecting the expression of the proteins associating with p53 pathway, such as ASK1, Bax, and so on. RITA bound to the amino acids residues in the activation domain of the ASK1, then induced the conformation change of ASK1 receptor, activated ASK1 and caused a series of signal transduction, further resulted in the physiological effects. Conclusion: Taken together, the RITA suppressed the cell proliferation in glioblastoma via targeting ASK1.","Study on therapeutic action and mechanism of TMZ Combined with RITA against glioblastoma. Background/Aims: Glioblastoma multiforme (GBM) is a malignant and aggressive central nervous system (CNS) tumor with high mortality and low survival rate. Effective treatment of GMB is a challenge worldwide. Temozolomide (TMZ) is a drug used to treat GBM, while the survival period of GBM patients with positive treatment remains less than 15 months. Reactivating p53 and Inducing Tumor Apoptosis (RITA) is a novel potential anti-cancer small molecular drug. Thus, it is essential to discover novel targets or develop effective drugs combination strategy to treat GBM. Methods: The U87 cells and U251 cells (p53 mutated) were treated with DMSO and 1, 5,10, 20 Î¼M RITA, TMZ, RITA+TMA or PFT-Î±. The cell proliferation was measured using the MTS cell proliferation assay. The cell apoptosis was analyzed by Annexin V-FITC/PI Apoptosis Detection Kit. The key protein expression level was evaluated by WB. Molecular docking and molecular dynamics (MD) simulation methods were applied to simulate the interaction between RITA and ASK1. Results: Herein, we found that combination RITA and TMZ effectively inhibited the proliferation of U87 cells and promoted the apoptosis of U87 cells. Then the mechanism of RITA and TMZ treating GBM were further studied by detecting the expression of the proteins associating with p53 pathway, such as ASK1, Bax, and so on. RITA bound to the amino acids residues in the activation domain of the ASK1, then induced the conformation change of ASK1 receptor, activated ASK1 and caused a series of signal transduction, further resulted in the physiological effects. Conclusion: Taken together, the RITA suppressed the cell proliferation in glioblastoma via targeting ASK1."
0,An algorithm for patients with intracranial pressure monitoring: filling the gap between evidence and practice,,
0,Implementation of an intensive adherence intervention in patients with second-line antiretroviral therapy failure in four west African countries with little access to genotypic resistance testing: a prospective cohort study,,
0,Systematic expression analysis of ligand-receptor pairs reveals important cell-to-cell interactions inside glioma,"Background: Glioma is the most commonly diagnosed malignant and aggressive brain cancer in adults. Traditional researches mainly explored the expression profile of glioma at cell-population level, but ignored the heterogeneity and interactions of among glioma cells. Methods: Here, we firstly analyzed the single-cell RNA-seq (scRNA-seq) data of 6341 glioma cells using manifold learning and identified neoplastic and healthy cells infiltrating in tumor microenvironment. We systematically revealed cell-to-cell interactions inside gliomas based on corresponding scRNA-seq and TCGA RNA-seq data. Results: A total of 16 significantly correlated autocrine ligand-receptor signal pairs inside neoplastic cells were identified based on the scRNA-seq and TCGA data of glioma. Furthermore, we explored the intercellular communications between cancer stem-like cells (CSCs) and macrophages, and identified 66 ligand-receptor pairs, some of which could significantly affect prognostic outcomes. An efficient machine learning model was constructed to accurately predict the prognosis of glioma patients based on the ligand-receptor interactions. Conclusion: Collectively, our study not only reveals functionally important cell-to-cell interactions inside glioma, but also detects potentially prognostic markers for predicting the survival of glioma patients.","Systematic expression analysis of ligand-receptor pairs reveals important cell-to-cell interactions inside glioma. Background: Glioma is the most commonly diagnosed malignant and aggressive brain cancer in adults. Traditional researches mainly explored the expression profile of glioma at cell-population level, but ignored the heterogeneity and interactions of among glioma cells. Methods: Here, we firstly analyzed the single-cell RNA-seq (scRNA-seq) data of 6341 glioma cells using manifold learning and identified neoplastic and healthy cells infiltrating in tumor microenvironment. We systematically revealed cell-to-cell interactions inside gliomas based on corresponding scRNA-seq and TCGA RNA-seq data. Results: A total of 16 significantly correlated autocrine ligand-receptor signal pairs inside neoplastic cells were identified based on the scRNA-seq and TCGA data of glioma. Furthermore, we explored the intercellular communications between cancer stem-like cells (CSCs) and macrophages, and identified 66 ligand-receptor pairs, some of which could significantly affect prognostic outcomes. An efficient machine learning model was constructed to accurately predict the prognosis of glioma patients based on the ligand-receptor interactions. Conclusion: Collectively, our study not only reveals functionally important cell-to-cell interactions inside glioma, but also detects potentially prognostic markers for predicting the survival of glioma patients."
0,Asp305Gly mutation improved the activity and stability of the styrene monooxygenase for efficient epoxide production in Pseudomonas putida KT2440,"Background: Styrene monooxygenase (SMO) catalyzes the first step of aromatic alkene degradation yielding the corresponding epoxides. Because of its broad spectrum of substrates, the enzyme harbors a great potential for an application in medicine and chemical industries. Results: In this study, we achieved higher enzymatic activity and better stability towards styrene by enlarging the ligand entrance tunnel and improving the hydrophobicity through error-prone PCR and site-saturation mutagenesis. It was found that Asp305 (D305) hindered the entrance of the FAD cofactor according to the model analysis. Therefore, substitution with amino acids possessing shorter side chains, like glycine, opened the entrance tunnel and resulted in up to 2.7 times higher activity compared to the wild-type enzyme. The half-lives of thermal inactivation for the variant D305G at 60 Â°C was 28.9 h compared to only 3.2 h of the wild type SMO. Moreover, overexpression of SMO in Pseudomonas putida KT2440 with NADH regeneration was carried out in order to improve biotransformation efficiency for epoxide production. A hexadecane/buffer (v/v) biphasic system was applied in order to minimize the inactivation effect of high substrate concentrations on the SMO enzyme. Finally, SMO activities of 190 U/g CDW were measured and a total amount of 20.5 mM (S)-styrene oxide were obtained after 8 h. Conclusions: This study offers an alternative strategy for improved SMO expression and provides an efficient biocatalytic system for epoxide production via engineering the entrance tunnel of the enzyme's active site.","Asp305Gly mutation improved the activity and stability of the styrene monooxygenase for efficient epoxide production in Pseudomonas putida KT2440. Background: Styrene monooxygenase (SMO) catalyzes the first step of aromatic alkene degradation yielding the corresponding epoxides. Because of its broad spectrum of substrates, the enzyme harbors a great potential for an application in medicine and chemical industries. Results: In this study, we achieved higher enzymatic activity and better stability towards styrene by enlarging the ligand entrance tunnel and improving the hydrophobicity through error-prone PCR and site-saturation mutagenesis. It was found that Asp305 (D305) hindered the entrance of the FAD cofactor according to the model analysis. Therefore, substitution with amino acids possessing shorter side chains, like glycine, opened the entrance tunnel and resulted in up to 2.7 times higher activity compared to the wild-type enzyme. The half-lives of thermal inactivation for the variant D305G at 60 Â°C was 28.9 h compared to only 3.2 h of the wild type SMO. Moreover, overexpression of SMO in Pseudomonas putida KT2440 with NADH regeneration was carried out in order to improve biotransformation efficiency for epoxide production. A hexadecane/buffer (v/v) biphasic system was applied in order to minimize the inactivation effect of high substrate concentrations on the SMO enzyme. Finally, SMO activities of 190 U/g CDW were measured and a total amount of 20.5 mM (S)-styrene oxide were obtained after 8 h. Conclusions: This study offers an alternative strategy for improved SMO expression and provides an efficient biocatalytic system for epoxide production via engineering the entrance tunnel of the enzyme's active site."
0,miR-22-5p and miR-29a-5p Are Reliable Reference Genes for Analyzing Extracellular Vesicle-Associated miRNAs in Adipose-Derived Mesenchymal Stem Cells and Are Stable under Inflammatory Priming Mimicking Osteoarthritis Condition,"During the last two decades, mesenchymal stem cells (MSCs) gained a place of privilege in the field of regenerative medicine. Recently, extracellular vesicles (EVs) have been identified as major mediators of MSCs immunosuppressive as well as pro-regenerative activities in many disease models, including inflammatory/degenerative conditions as joint diseases and osteoarthritis. In order to shed light on EVs potential, a rigorous profiling of embedded proteins, lipids and nucleic acids (mRNA/miRNA) is mandatory. Nevertheless, reliable strategies to efficiently score miRNA cargo and modulation under diverse experimental conditions or treatments are missing. The aim of this work was to identify reliable reference genes (RGs) to analyze miRNA content in EVs secreted by adipose-derived MSCs (ASCs) and verify their consistency under inflammatory conditions that were proposed to enhance ASC-EVs immunomodulatory and regenerative potential. RefFinder algorithm, that integrates the currently available major computational programs (geNorm, NormFinder, BestKeeper, and Delta Ct method), allowed to identify miR-22-5p and miR-29a-5p as the most stable RGs. Notably, both miRNAs maintained the highest stability when EVs isolated from IFNg-treated ASCs were included in the analysis. In addition, considerable effects of suboptimal RGs choice on the reliable quantification of miRNAs involved at different levels (tissue homeostasis or macrophage polarization) in the osteoarthritis phenotype, and thus considered as promising therapeutic molecule, have clearly been demonstrated. In conclusion, a proper normalization method is not only needed for research purposes but also mandatory to characterize clinical products and predict their therapeutic potential, especially in the emerging field of MSCs derived-EVs as new tools for regenerative medicine.","miR-22-5p and miR-29a-5p Are Reliable Reference Genes for Analyzing Extracellular Vesicle-Associated miRNAs in Adipose-Derived Mesenchymal Stem Cells and Are Stable under Inflammatory Priming Mimicking Osteoarthritis Condition. During the last two decades, mesenchymal stem cells (MSCs) gained a place of privilege in the field of regenerative medicine. Recently, extracellular vesicles (EVs) have been identified as major mediators of MSCs immunosuppressive as well as pro-regenerative activities in many disease models, including inflammatory/degenerative conditions as joint diseases and osteoarthritis. In order to shed light on EVs potential, a rigorous profiling of embedded proteins, lipids and nucleic acids (mRNA/miRNA) is mandatory. Nevertheless, reliable strategies to efficiently score miRNA cargo and modulation under diverse experimental conditions or treatments are missing. The aim of this work was to identify reliable reference genes (RGs) to analyze miRNA content in EVs secreted by adipose-derived MSCs (ASCs) and verify their consistency under inflammatory conditions that were proposed to enhance ASC-EVs immunomodulatory and regenerative potential. RefFinder algorithm, that integrates the currently available major computational programs (geNorm, NormFinder, BestKeeper, and Delta Ct method), allowed to identify miR-22-5p and miR-29a-5p as the most stable RGs. Notably, both miRNAs maintained the highest stability when EVs isolated from IFNg-treated ASCs were included in the analysis. In addition, considerable effects of suboptimal RGs choice on the reliable quantification of miRNAs involved at different levels (tissue homeostasis or macrophage polarization) in the osteoarthritis phenotype, and thus considered as promising therapeutic molecule, have clearly been demonstrated. In conclusion, a proper normalization method is not only needed for research purposes but also mandatory to characterize clinical products and predict their therapeutic potential, especially in the emerging field of MSCs derived-EVs as new tools for regenerative medicine."
0,The Learning Curve for Robot-assisted Partial Nephrectomy: There is Much Beyond a Trifecta,,
0,Pharma blockchains AI for drug development,,
0,Quantitative MRI of Diffuse Liver Disease: Current Applications and Future Directions,"As radiologic technology advances, quantitative imaging is becoming more prevalent in clinical practice. This article reviews quantitative hepatic MRI, specifically involving fat and iron deposition, by demonstrating how they were iteratively improved. These iterative improvements involved incorporating more knowledge about the physiology of liver disease and MRI physics to reduce the adverse effects caused by confounding factors. The relevant foundations of MRI physics and liver pathophysiology are briefly reviewed, followed by the various improvements made by expanding on this foundational knowledge. Results from the literature are then discussed within this context, validating the improvement of these resultant methods into clinically robust and useful techniques. Fibrosis quantification, which has been more difficult to robustly perform in clinical practice, is similarly reviewed in an online appendix, with proposals for future multiparametric directions to improve performance on the basis of the insights gained from fat and iron quantification in the liver.","Quantitative MRI of Diffuse Liver Disease: Current Applications and Future Directions. As radiologic technology advances, quantitative imaging is becoming more prevalent in clinical practice. This article reviews quantitative hepatic MRI, specifically involving fat and iron deposition, by demonstrating how they were iteratively improved. These iterative improvements involved incorporating more knowledge about the physiology of liver disease and MRI physics to reduce the adverse effects caused by confounding factors. The relevant foundations of MRI physics and liver pathophysiology are briefly reviewed, followed by the various improvements made by expanding on this foundational knowledge. Results from the literature are then discussed within this context, validating the improvement of these resultant methods into clinically robust and useful techniques. Fibrosis quantification, which has been more difficult to robustly perform in clinical practice, is similarly reviewed in an online appendix, with proposals for future multiparametric directions to improve performance on the basis of the insights gained from fat and iron quantification in the liver."
0,A network embedding model for pathogenic genes prediction by multi-path random walking on heterogeneous network,"Background: Prediction of pathogenic genes is crucial for disease prevention, diagnosis, and treatment. But traditional genetic localization methods are often technique-difficulty and time-consuming. With the development of computer science, computational biology has gradually become one of the main methods for finding candidate pathogenic genes. Methods: We propose a pathogenic genes prediction method based on network embedding which is called Multipath2vec. Firstly, we construct an heterogeneous network which is called GP-network. It is constructed based on three kinds of relationships between genes and phenotypes, including correlations between phenotypes, interactions between genes and known gene-phenotype pairs. Then in order to embedding the network better, we design the multi-path to guide random walk in GP-network. The multi-path includes multiple paths between genes and phenotypes which can capture complex structural information of heterogeneous network. Finally, we use the learned vector representation of each phenotype and protein to calculate the similarities and rank according to the similarities between candidate genes and the target phenotype. Results: We implemented Multipath2vec and four baseline approaches (i.e., CATAPULT, PRINCE, Deepwalk and Metapath2vec) on many-genes gene-phenotype data, single-gene gene-phenotype data and whole gene-phenotype data. Experimental results show that Multipath2vec outperformed the state-of-the-art baselines in pathogenic genes prediction task. Conclusions: We propose Multipath2vec that can be utilized to predict pathogenic genes and experimental results show the higher accuracy of pathogenic genes prediction.","A network embedding model for pathogenic genes prediction by multi-path random walking on heterogeneous network. Background: Prediction of pathogenic genes is crucial for disease prevention, diagnosis, and treatment. But traditional genetic localization methods are often technique-difficulty and time-consuming. With the development of computer science, computational biology has gradually become one of the main methods for finding candidate pathogenic genes. Methods: We propose a pathogenic genes prediction method based on network embedding which is called Multipath2vec. Firstly, we construct an heterogeneous network which is called GP-network. It is constructed based on three kinds of relationships between genes and phenotypes, including correlations between phenotypes, interactions between genes and known gene-phenotype pairs. Then in order to embedding the network better, we design the multi-path to guide random walk in GP-network. The multi-path includes multiple paths between genes and phenotypes which can capture complex structural information of heterogeneous network. Finally, we use the learned vector representation of each phenotype and protein to calculate the similarities and rank according to the similarities between candidate genes and the target phenotype. Results: We implemented Multipath2vec and four baseline approaches (i.e., CATAPULT, PRINCE, Deepwalk and Metapath2vec) on many-genes gene-phenotype data, single-gene gene-phenotype data and whole gene-phenotype data. Experimental results show that Multipath2vec outperformed the state-of-the-art baselines in pathogenic genes prediction task. Conclusions: We propose Multipath2vec that can be utilized to predict pathogenic genes and experimental results show the higher accuracy of pathogenic genes prediction."
0,Polygenic risk-tailored screening for prostate cancer: A benefit-harm and cost-effectiveness modelling study,"Background The United States Preventive Services Task Force supports individualised decision-making for prostate-specific antigen (PSA)-based screening in men aged 55-69. Knowing how the potential benefits and harms of screening vary by an individual's risk of developing prostate cancer could inform decision-making about screening at both an individual and population level. This modelling study examined the benefit-harm tradeoffs and the cost-effectiveness of a risk-tailored screening programme compared to age-based and no screening. Methods and findings A life-table model, projecting age-specific prostate cancer incidence and mortality, was developed of a hypothetical cohort of 4.48 million men in England aged 55 to 69 years with follow-up to age 90. Risk thresholds were based on age and polygenic profile. We compared no screening, age-based screening (quadrennial PSA testing from 55 to 69), and risk-tailored screening (men aged 55 to 69 years with a 10-year absolute risk greater than a threshold receive quadrennial PSA testing from the age they reach the risk threshold). The analysis was undertaken from the health service perspective, including direct costs borne by the health system for risk assessment, screening, diagnosis, and treatment. We used probabilistic sensitivity analyses to account for parameter uncertainty and discounted future costs and benefits at 3.5% per year. Our analysis should be considered cautiously in light of limitations related to our model's cohort-based structure and the uncertainty of input parameters in mathematical models. Compared to no screening over 35 years follow-up, age-based screening prevented the most deaths from prostate cancer (39,272, 95% uncertainty interval [UI]: 16,792-59,685) at the expense of 94,831 (95% UI: 84,827-105,630) overdiagnosed cancers. Age-based screening was the least cost-effective strategy studied. The greatest number of quality-adjusted life-years (QALYs) was generated by risk-based screening at a 10-year absolute risk threshold of 4%. At this threshold, risk-based screening led to one-third fewer overdiagnosed cancers (64,384, 95% UI: 57,382-72,050) but averted 6.3% fewer (9,695, 95% UI: 2,853-15,851) deaths from prostate cancer by comparison with age-based screening. Relative to no screening, risk-based screening at a 4% 10-year absolute risk threshold was cost-effective in 48.4% and 57.4% of the simulations at willingness-to-pay thresholds of GBPÂ£20,000 (US$26,000) and Â£30,000 ($39,386) per QALY, respectively. The cost-effectiveness of risk-tailored screening improved as the threshold rose. Conclusions Based on the results of this modelling study, offering screening to men at higher risk could potentially reduce overdiagnosis and improve the benefit-harm tradeoff and the cost-effectiveness of a prostate cancer screening program. The optimal threshold will depend on societal judgements of the appropriate balance of benefits-harms and cost-effectiveness.","Polygenic risk-tailored screening for prostate cancer: A benefit-harm and cost-effectiveness modelling study. Background The United States Preventive Services Task Force supports individualised decision-making for prostate-specific antigen (PSA)-based screening in men aged 55-69. Knowing how the potential benefits and harms of screening vary by an individual's risk of developing prostate cancer could inform decision-making about screening at both an individual and population level. This modelling study examined the benefit-harm tradeoffs and the cost-effectiveness of a risk-tailored screening programme compared to age-based and no screening. Methods and findings A life-table model, projecting age-specific prostate cancer incidence and mortality, was developed of a hypothetical cohort of 4.48 million men in England aged 55 to 69 years with follow-up to age 90. Risk thresholds were based on age and polygenic profile. We compared no screening, age-based screening (quadrennial PSA testing from 55 to 69), and risk-tailored screening (men aged 55 to 69 years with a 10-year absolute risk greater than a threshold receive quadrennial PSA testing from the age they reach the risk threshold). The analysis was undertaken from the health service perspective, including direct costs borne by the health system for risk assessment, screening, diagnosis, and treatment. We used probabilistic sensitivity analyses to account for parameter uncertainty and discounted future costs and benefits at 3.5% per year. Our analysis should be considered cautiously in light of limitations related to our model's cohort-based structure and the uncertainty of input parameters in mathematical models. Compared to no screening over 35 years follow-up, age-based screening prevented the most deaths from prostate cancer (39,272, 95% uncertainty interval [UI]: 16,792-59,685) at the expense of 94,831 (95% UI: 84,827-105,630) overdiagnosed cancers. Age-based screening was the least cost-effective strategy studied. The greatest number of quality-adjusted life-years (QALYs) was generated by risk-based screening at a 10-year absolute risk threshold of 4%. At this threshold, risk-based screening led to one-third fewer overdiagnosed cancers (64,384, 95% UI: 57,382-72,050) but averted 6.3% fewer (9,695, 95% UI: 2,853-15,851) deaths from prostate cancer by comparison with age-based screening. Relative to no screening, risk-based screening at a 4% 10-year absolute risk threshold was cost-effective in 48.4% and 57.4% of the simulations at willingness-to-pay thresholds of GBPÂ£20,000 (US$26,000) and Â£30,000 ($39,386) per QALY, respectively. The cost-effectiveness of risk-tailored screening improved as the threshold rose. Conclusions Based on the results of this modelling study, offering screening to men at higher risk could potentially reduce overdiagnosis and improve the benefit-harm tradeoff and the cost-effectiveness of a prostate cancer screening program. The optimal threshold will depend on societal judgements of the appropriate balance of benefits-harms and cost-effectiveness."
0,Modulatory effects of different exercise modalities on the functional connectivity of the periaqueductal grey and ventral tegmental area in patients with knee osteoarthritis: a randomised multimodal magnetic resonance imaging study,,
0,Different pharmacological properties of GLUT9a and GLUT9b: Potential implications in preeclampsia,"Background/Aims: Glucose transporter 9 (GLUT9/SLC2A9) is the major regulator of uric acid homeostasis in humans. Hyperuricemia due to impaired regulation by GLUT9 in pregnancy is closely associated with preeclampsia. While GLUT9 is expressed in two alternative splice variants, GLUT9a and GLUT9b, with different subcellular localizations, no functional differences of the two splice variants are known to date. The aim of this study was to investigate the function of both GLUT9 isoforms. Methods: To characterize the different pharmacological properties of GLUT9a and GLUT9b electrophysiological studies of these isoforms and their modified variants, i.e. NmodGLUT9a and NmodGLUT9b, were performed using a Xenopus laevis oocytes model. Currents were measured by an electrode voltage clamp system. Results: Functional experiments unveiled that uric acid transport mediated by GLUT9a but not GLUT9b is chloride-dependent: Replacing chloride by different anions resulted in a 3.43Â±0.63-fold increase of GLUT9a-but not GLUT9b-mediated currents. However, replacement by iodide resulted in a loss of current for GLUT9a but not GLUT9b. Iodide inhibits GLUT9a with an IC50 of35.1Â±6.7Î¼M. Modification of the N-terminal domain leads to a shift of the iodide IC50 to 1200Â±228Î¼M. Using molecular docking studies, we identified two positively charged residues H23 and R31 in the N-terminal domain of hGLUT9a which can explain the observed functional differences. Conclusion: To the best of our knowledge, this is the first study showing that the N-terminal domain of hGLUT9a has a unique regulatory function and the potential to interact with small negatively charged ions like iodide. These findings may have significant implications in our understanding of hyperuricemia-associated diseases, specifically during pregnancy.","Different pharmacological properties of GLUT9a and GLUT9b: Potential implications in preeclampsia. Background/Aims: Glucose transporter 9 (GLUT9/SLC2A9) is the major regulator of uric acid homeostasis in humans. Hyperuricemia due to impaired regulation by GLUT9 in pregnancy is closely associated with preeclampsia. While GLUT9 is expressed in two alternative splice variants, GLUT9a and GLUT9b, with different subcellular localizations, no functional differences of the two splice variants are known to date. The aim of this study was to investigate the function of both GLUT9 isoforms. Methods: To characterize the different pharmacological properties of GLUT9a and GLUT9b electrophysiological studies of these isoforms and their modified variants, i.e. NmodGLUT9a and NmodGLUT9b, were performed using a Xenopus laevis oocytes model. Currents were measured by an electrode voltage clamp system. Results: Functional experiments unveiled that uric acid transport mediated by GLUT9a but not GLUT9b is chloride-dependent: Replacing chloride by different anions resulted in a 3.43Â±0.63-fold increase of GLUT9a-but not GLUT9b-mediated currents. However, replacement by iodide resulted in a loss of current for GLUT9a but not GLUT9b. Iodide inhibits GLUT9a with an IC50 of35.1Â±6.7Î¼M. Modification of the N-terminal domain leads to a shift of the iodide IC50 to 1200Â±228Î¼M. Using molecular docking studies, we identified two positively charged residues H23 and R31 in the N-terminal domain of hGLUT9a which can explain the observed functional differences. Conclusion: To the best of our knowledge, this is the first study showing that the N-terminal domain of hGLUT9a has a unique regulatory function and the potential to interact with small negatively charged ions like iodide. These findings may have significant implications in our understanding of hyperuricemia-associated diseases, specifically during pregnancy."
0,Phase contrast mapping MRI measurements of global cerebral blood flow across different perfusion states â€“ A direct comparison with 15O-H2O positron emission tomography using a hybrid PET/MR system,"Phase-contrast mapping (PCM) magnetic resonance imaging (MRI) provides easy-access non-invasive quantification of global cerebral blood flow (gCBF) but its accuracy in altered perfusion states is not established. We aimed to compare paired PCM MRI and 15O-H2O positron emission tomography (PET) measurements of gCBF in different perfusion states in a single scanning session. Duplicate combined gCBF PCM-MRI and 15O-H2O PET measurements were performed in the resting condition, during hyperventilation and after acetazolamide administration (post-ACZ) using a 3T hybrid PET/MR system. A total of 62 paired gCBF measurements were acquired in 14 healthy young male volunteers. Average gCBF in resting state measured by PCM-MRI and 15O-H2O PET were 58.5 Â± 10.7 and 38.6 Â± 5.7 mL/100 g/min, respectively, during hyperventilation 33 Â± 8.6 and 24.7 Â± 5.8 mL/100 g/min, respectively, and post-ACZ 89.6 Â± 27.1 and 57.3 Â± 9.6 mL/100 g/min, respectively. On average, gCBF measured by PCM-MRI was 49% higher compared to 15O-H2O PET. A strong correlation between the two methods across all states was observed (R2 = 0.72, p < 0.001). Blandâ€“Altman analysis suggested a perfusion dependent relative bias resulting in higher relative difference at higher CBF values. In conclusion, measurements of gCBF by PCM-MRI in healthy volunteers show a strong correlation with 15O-H2O PET, but are associated with a large and non-linear perfusion-dependent difference.","Phase contrast mapping MRI measurements of global cerebral blood flow across different perfusion states â€“ A direct comparison with 15O-H2O positron emission tomography using a hybrid PET/MR system. Phase-contrast mapping (PCM) magnetic resonance imaging (MRI) provides easy-access non-invasive quantification of global cerebral blood flow (gCBF) but its accuracy in altered perfusion states is not established. We aimed to compare paired PCM MRI and 15O-H2O positron emission tomography (PET) measurements of gCBF in different perfusion states in a single scanning session. Duplicate combined gCBF PCM-MRI and 15O-H2O PET measurements were performed in the resting condition, during hyperventilation and after acetazolamide administration (post-ACZ) using a 3T hybrid PET/MR system. A total of 62 paired gCBF measurements were acquired in 14 healthy young male volunteers. Average gCBF in resting state measured by PCM-MRI and 15O-H2O PET were 58.5 Â± 10.7 and 38.6 Â± 5.7 mL/100 g/min, respectively, during hyperventilation 33 Â± 8.6 and 24.7 Â± 5.8 mL/100 g/min, respectively, and post-ACZ 89.6 Â± 27.1 and 57.3 Â± 9.6 mL/100 g/min, respectively. On average, gCBF measured by PCM-MRI was 49% higher compared to 15O-H2O PET. A strong correlation between the two methods across all states was observed (R2 = 0.72, p < 0.001). Blandâ€“Altman analysis suggested a perfusion dependent relative bias resulting in higher relative difference at higher CBF values. In conclusion, measurements of gCBF by PCM-MRI in healthy volunteers show a strong correlation with 15O-H2O PET, but are associated with a large and non-linear perfusion-dependent difference."
0,Machine Learning and the Cancer-Diagnosis Problem - No Gold Standard,,
0,Kaempferol attenuates liver fibrosis by inhibiting activin receptorâ€“like kinase 5,"Liver fibrosis is a common public health problem. Patients with liver fibrosis are more likely to develop cirrhosis, or hepatocellular carcinoma (HCC) as a more serious consequence. Numerous therapeutic approaches have emerged, but the final clinical outcome remains unsatisfactory. Here, we discovered a flavonoid natural product kaempferol that could dramatically ameliorate liver fibrosis formation. Our data showed that intraperitoneal injection of kaempferol could significantly decrease the necroinflammatory scores and collagen deposition in the liver tissue. In addition, serum alanine aminotransferase (ALT), aspartate aminotransferase (AST), laminin (LN) and hyaluronic acid (HA) levels were significantly down-regulated in kaempferol treatment group compared with those in the control group. Our study also demonstrated that kaempferol markedly inhibited the synthesis of collagen and activation of hepatic stellate cells (HSCs) both in vivo and in vitro. Furthermore, the results of Western blotting revealed that kaempferol could down-regulate Smad2/3 phosphorylation dose-dependently. These bioactivities of kaempferol may result from its targeted binding to the ATP-binding pocket of activin receptorâ€“like kinase 5 (ALK5), as suggested by the molecular docking study and LanthaScreen Eu kinase binding assay. Above all, our data indicate that kaempferol may prove to be a novel agent for the treatment of liver fibrosis or other fibroproliferative diseases.","Kaempferol attenuates liver fibrosis by inhibiting activin receptorâ€“like kinase 5. Liver fibrosis is a common public health problem. Patients with liver fibrosis are more likely to develop cirrhosis, or hepatocellular carcinoma (HCC) as a more serious consequence. Numerous therapeutic approaches have emerged, but the final clinical outcome remains unsatisfactory. Here, we discovered a flavonoid natural product kaempferol that could dramatically ameliorate liver fibrosis formation. Our data showed that intraperitoneal injection of kaempferol could significantly decrease the necroinflammatory scores and collagen deposition in the liver tissue. In addition, serum alanine aminotransferase (ALT), aspartate aminotransferase (AST), laminin (LN) and hyaluronic acid (HA) levels were significantly down-regulated in kaempferol treatment group compared with those in the control group. Our study also demonstrated that kaempferol markedly inhibited the synthesis of collagen and activation of hepatic stellate cells (HSCs) both in vivo and in vitro. Furthermore, the results of Western blotting revealed that kaempferol could down-regulate Smad2/3 phosphorylation dose-dependently. These bioactivities of kaempferol may result from its targeted binding to the ATP-binding pocket of activin receptorâ€“like kinase 5 (ALK5), as suggested by the molecular docking study and LanthaScreen Eu kinase binding assay. Above all, our data indicate that kaempferol may prove to be a novel agent for the treatment of liver fibrosis or other fibroproliferative diseases."
0,The Inhibitory Effect of GlmU Acetyltransferase Inhibitor TPSA on Mycobacterium tuberculosis May Be Affected Due to Its Methylation by Methyltransferase Rv0560c,"Mycobacterium tuberculosis bifunctional enzyme GlmU is a novel target for anti-TB drugs and is involved in glycosyl donor UDP-N-acetylglucosamine biosynthesis. Here, we found that TPSA (2-[5-(2-{[4-(2-thienyl)-2-pyrimidinyl]sulfanyl}acetyl)-2-thienyl]acetic acid) was a novel inhibitor for GlmU acetyltransferase activity (IC50: 5.3 Î¼M). The interaction sites of GlmU and TPSA by molecular docking were confirmed by site-directed mutagenesis. TPSA showed an inhibitory effect on Mtb H37Ra growth and intracellular H37Ra in macrophage cells (MIC: 66.5 Î¼M). To investigate why TPSA at a higher concentration (66.5 Î¼M) was able to inhibit H37Ra growth, proteome and transcriptome of H37Ra treated with TPSA were analyzed. The expression of two methyltransferases MRA_0565 (Rv0558) and MRA_0567 (Rv0560c) were markedly increased. TPSA was pre-incubated with purified Rv0558 and Rv0560c in the presence of S-adenosylmethionine (methyl donor) respectively, resulting in its decreased inhibitory effect of GlmU on acetyltransferase activity. The inhibition of TPSA on growth of H37Ra with overexpressed Rv0558 and Rv0560c was reduced. These implied that methyltransferases could modify TPSA. The methylation of TPSA catalyzed by Rv0560c was subsequently confirmed by LC-MS. Therefore, TPSA as a GlmU acetyltransferase activity inhibitor may offer a structural basis for new anti-tuberculosis drugs. TPSA needs to be modified further by some groups to prevent its methylation by methyltransferases.","The Inhibitory Effect of GlmU Acetyltransferase Inhibitor TPSA on Mycobacterium tuberculosis May Be Affected Due to Its Methylation by Methyltransferase Rv0560c. Mycobacterium tuberculosis bifunctional enzyme GlmU is a novel target for anti-TB drugs and is involved in glycosyl donor UDP-N-acetylglucosamine biosynthesis. Here, we found that TPSA (2-[5-(2-{[4-(2-thienyl)-2-pyrimidinyl]sulfanyl}acetyl)-2-thienyl]acetic acid) was a novel inhibitor for GlmU acetyltransferase activity (IC50: 5.3 Î¼M). The interaction sites of GlmU and TPSA by molecular docking were confirmed by site-directed mutagenesis. TPSA showed an inhibitory effect on Mtb H37Ra growth and intracellular H37Ra in macrophage cells (MIC: 66.5 Î¼M). To investigate why TPSA at a higher concentration (66.5 Î¼M) was able to inhibit H37Ra growth, proteome and transcriptome of H37Ra treated with TPSA were analyzed. The expression of two methyltransferases MRA_0565 (Rv0558) and MRA_0567 (Rv0560c) were markedly increased. TPSA was pre-incubated with purified Rv0558 and Rv0560c in the presence of S-adenosylmethionine (methyl donor) respectively, resulting in its decreased inhibitory effect of GlmU on acetyltransferase activity. The inhibition of TPSA on growth of H37Ra with overexpressed Rv0558 and Rv0560c was reduced. These implied that methyltransferases could modify TPSA. The methylation of TPSA catalyzed by Rv0560c was subsequently confirmed by LC-MS. Therefore, TPSA as a GlmU acetyltransferase activity inhibitor may offer a structural basis for new anti-tuberculosis drugs. TPSA needs to be modified further by some groups to prevent its methylation by methyltransferases."
0,Mechanistic elucidation of amphetamine metabolism by tyramine oxidase from human gut microbiota using molecular dynamics simulations,"The human gut harbors diverse bacterial species in the gut, which play an important role in the metabolism of foodÂ and host health.Â Recent studies have also revealed their role in altering the pharmacological propertiesÂ and efficacy of oral drugs through promiscuous metabolism. However, the atomistic details of the enzyme-drug interactions of gut bacterial enzymes which can potentially carry out the metabolism of drug molecules are still scarce. A well-known example is the FDA drug amphetamine (a central nervous system stimulant), which has been predicted to undergo promiscuous metabolism by gut bacteria. Therefore, to understand the atomistic details and energy landscape of the gut microbial enzyme-mediated metabolism of this drug, molecular dynamics studies were performed. It was observed that amphetamine binds to tyramine oxidase from the Escherichia coli strain present in the human gut microbiota at the binding site harboring polar and nonpolar amino acids. The stability analysis of amphetamine at the binding site showed that the binding is stable and the free energy for the binding of amphetamine was found to be ~ âˆ’51.71 kJ/mol. The insights provided by this study on promiscuous metabolism of amphetamine by a gut enzyme will be very useful to improve the efficacy of the drug.","Mechanistic elucidation of amphetamine metabolism by tyramine oxidase from human gut microbiota using molecular dynamics simulations. The human gut harbors diverse bacterial species in the gut, which play an important role in the metabolism of foodÂ and host health.Â Recent studies have also revealed their role in altering the pharmacological propertiesÂ and efficacy of oral drugs through promiscuous metabolism. However, the atomistic details of the enzyme-drug interactions of gut bacterial enzymes which can potentially carry out the metabolism of drug molecules are still scarce. A well-known example is the FDA drug amphetamine (a central nervous system stimulant), which has been predicted to undergo promiscuous metabolism by gut bacteria. Therefore, to understand the atomistic details and energy landscape of the gut microbial enzyme-mediated metabolism of this drug, molecular dynamics studies were performed. It was observed that amphetamine binds to tyramine oxidase from the Escherichia coli strain present in the human gut microbiota at the binding site harboring polar and nonpolar amino acids. The stability analysis of amphetamine at the binding site showed that the binding is stable and the free energy for the binding of amphetamine was found to be ~ âˆ’51.71 kJ/mol. The insights provided by this study on promiscuous metabolism of amphetamine by a gut enzyme will be very useful to improve the efficacy of the drug."
0,Optomotor Swimming in Larval Zebrafish Is Driven by Global Whole-Field Visual Motion and Local Light-Dark Transitions,"Kist and Portugues use reverse correlation in an optomotor behavioral assay in larval zebrafish to identify the stereotypic filter that elicits swimming. It consists of a forward-moving local light-dark transition alongside global whole-field motion. The luminance profile strongly affects behavioral parameters, and filter-specific activity is spread across the brain.","Optomotor Swimming in Larval Zebrafish Is Driven by Global Whole-Field Visual Motion and Local Light-Dark Transitions. Kist and Portugues use reverse correlation in an optomotor behavioral assay in larval zebrafish to identify the stereotypic filter that elicits swimming. It consists of a forward-moving local light-dark transition alongside global whole-field motion. The luminance profile strongly affects behavioral parameters, and filter-specific activity is spread across the brain."
0,Advances in clinical MRI technology,"Advances in MRI technologies have the potential to detect, characterize, and monitor a wide variety of diseases.","Advances in clinical MRI technology. Advances in MRI technologies have the potential to detect, characterize, and monitor a wide variety of diseases."
0,IFN-gamma enhances cell-mediated cytotoxicity against keratinocytes via JAK2/STAT1 in lichen planus,,
0,A graph-cut approach for pulmonary artery-vein segmentation in noncontrast CT images,"Lung vessel segmentation has been widely explored by the biomedical image processing community; however, the differentiation of arterial from venous irrigation is still a challenge. Pulmonary arteryâ€“vein (AV) segmentation using computed tomography (CT) is growing in importance owing to its undeniable utility in multiple cardiopulmonary pathological states, especially those implying vascular remodelling, allowing the study of both flow systems separately. We present a new framework to approach the separation of tree-like structures using local information and a specifically designed graph-cut methodology that ensures connectivity as well as the spatial and directional consistency of the derived subtrees. This framework has been applied to the pulmonary AV classification using a random forest (RF) pre-classifier to exploit the local anatomical differences of arteries and veins. The evaluation of the system was performed using 192 bronchopulmonary segment phantoms, 48 anthropomorphic pulmonary CT phantoms, and 26 lungs from noncontrast CT images with precise voxel-based reference standards obtained by manually labelling the vessel trees. The experiments reveal a relevant improvement in the accuracy (âˆ¼ 20%) of the vessel particle classification with the proposed framework with respect to using only the pre-classification based on local information applied to the whole area of the lung under study. The results demonstrated the accurate differentiation between arteries and veins in both clinical and synthetic cases, specifically when the image quality can guarantee a good airway segmentation, which opens a huge range of possibilities in the clinical study of cardiopulmonary diseases.","A graph-cut approach for pulmonary artery-vein segmentation in noncontrast CT images. Lung vessel segmentation has been widely explored by the biomedical image processing community; however, the differentiation of arterial from venous irrigation is still a challenge. Pulmonary arteryâ€“vein (AV) segmentation using computed tomography (CT) is growing in importance owing to its undeniable utility in multiple cardiopulmonary pathological states, especially those implying vascular remodelling, allowing the study of both flow systems separately. We present a new framework to approach the separation of tree-like structures using local information and a specifically designed graph-cut methodology that ensures connectivity as well as the spatial and directional consistency of the derived subtrees. This framework has been applied to the pulmonary AV classification using a random forest (RF) pre-classifier to exploit the local anatomical differences of arteries and veins. The evaluation of the system was performed using 192 bronchopulmonary segment phantoms, 48 anthropomorphic pulmonary CT phantoms, and 26 lungs from noncontrast CT images with precise voxel-based reference standards obtained by manually labelling the vessel trees. The experiments reveal a relevant improvement in the accuracy (âˆ¼ 20%) of the vessel particle classification with the proposed framework with respect to using only the pre-classification based on local information applied to the whole area of the lung under study. The results demonstrated the accurate differentiation between arteries and veins in both clinical and synthetic cases, specifically when the image quality can guarantee a good airway segmentation, which opens a huge range of possibilities in the clinical study of cardiopulmonary diseases."
0,Genetic Control of Expression and Splicing in Developing Human Brain Informs Disease Mechanisms,,
0,Agreement and Predictors of Discordance of 6 Visual Field Progression Algorithms,"PURPOSE: To determine the agreement of 6 established visual field (VF) progression algorithms in a large dataset of VFs from multiple institutions and to determine predictors of discordance among these algorithms. DESIGN: Retrospective longitudinal cohort study. PARTICIPANTS: Visual fields from 5 major eye care institutions in the United States were analyzed, including a subset of eyes with at least 5 Swedish interactive threshold algorithm standard 24-2 VFs that met our reliability criteria. Of a total of 831 240 VFs, a subset of 90 713 VFs from 13 156 eyes of 8499 patients met the inclusion criteria. METHODS: Six commonly used VF progression algorithms (mean deviation [MD] slope, VF index slope, Advanced Glaucoma Intervention Study, Collaborative Initial Glaucoma Treatment Study, pointwise linear regression, and permutation of pointwise linear regression) were applied to this cohort, and each eye was determined to be stable or progressing using each measure. Agreement between individual algorithms was tested using Cohen's kappa coefficient. Bivariate and multivariate analyses were used to determine predictors of discordance (3 algorithms progressing and 3 algorithms stable). MAIN OUTCOME MEASURES: Agreement and discordance between algorithms. RESULTS: Individual algorithms showed poor to moderate agreement with each other when compared directly (kappa range, 0.12-0.52). Based on at least 4 algorithms, 11.7% of eyes progressed. Major predictors of discordance or lack of agreement among algorithms were more depressed initial MD (P < 0.01) and older age at first available VF (P < 0.01). A greater number of VFs (P < 0.01), more years of follow-up (P < 0.01), and eye care institution (P = 0.03) also were associated with discordance. CONCLUSIONS: This extremely large comparative series demonstrated that existing algorithms have limited agreement and that agreement varies with clinical parameters, including institution. These issues underscore the challenges to the clinical use and application of progression algorithms and of applying big-data results to individual practices.","Agreement and Predictors of Discordance of 6 Visual Field Progression Algorithms. PURPOSE: To determine the agreement of 6 established visual field (VF) progression algorithms in a large dataset of VFs from multiple institutions and to determine predictors of discordance among these algorithms. DESIGN: Retrospective longitudinal cohort study. PARTICIPANTS: Visual fields from 5 major eye care institutions in the United States were analyzed, including a subset of eyes with at least 5 Swedish interactive threshold algorithm standard 24-2 VFs that met our reliability criteria. Of a total of 831 240 VFs, a subset of 90 713 VFs from 13 156 eyes of 8499 patients met the inclusion criteria. METHODS: Six commonly used VF progression algorithms (mean deviation [MD] slope, VF index slope, Advanced Glaucoma Intervention Study, Collaborative Initial Glaucoma Treatment Study, pointwise linear regression, and permutation of pointwise linear regression) were applied to this cohort, and each eye was determined to be stable or progressing using each measure. Agreement between individual algorithms was tested using Cohen's kappa coefficient. Bivariate and multivariate analyses were used to determine predictors of discordance (3 algorithms progressing and 3 algorithms stable). MAIN OUTCOME MEASURES: Agreement and discordance between algorithms. RESULTS: Individual algorithms showed poor to moderate agreement with each other when compared directly (kappa range, 0.12-0.52). Based on at least 4 algorithms, 11.7% of eyes progressed. Major predictors of discordance or lack of agreement among algorithms were more depressed initial MD (P < 0.01) and older age at first available VF (P < 0.01). A greater number of VFs (P < 0.01), more years of follow-up (P < 0.01), and eye care institution (P = 0.03) also were associated with discordance. CONCLUSIONS: This extremely large comparative series demonstrated that existing algorithms have limited agreement and that agreement varies with clinical parameters, including institution. These issues underscore the challenges to the clinical use and application of progression algorithms and of applying big-data results to individual practices."
0,"Re: Dipen J. Parekh, Isidinha M. Reis, Erik P. Castle, et al. Robot-assisted Radical Cystectomy Versus Open Radical Cystectomy in Patients with Bladder Cancer (RAZOR): An Open-label, Randomised, Phase 3, Non-inferiority Trial. Lancet 2018;391:2525-36: Blood Loss in Robot-assisted Radical Cystectomy: An Important Patient Benefit",,
0,The circadian regulation of food intake,,
0,Looking back and thinking forwards-15 years of cardiology and cardiovascular research,,
0,Artificial Intelligence and the Future of Surgical Robotics,,
0,Toward a clearer picture of health,,
0,Delirium detection in older acute medical inpatients: a multicentre prospective comparative diagnostic test accuracy study of the 4AT and the confusion assessment method,"BACKGROUND: Delirium affects > 15% of hospitalised patients but is grossly underdetected, contributing to poor care. The 4 'A's Test (4AT, www.the4AT.com ) is a short delirium assessment tool designed for routine use without special training. The primary objective was to assess the accuracy of the 4AT for delirium detection. The secondary objective was to compare the 4AT with another commonly used delirium assessment tool, the Confusion Assessment Method (CAM). METHODS: This was a prospective diagnostic test accuracy study set in emergency departments or acute medical wards involving acute medical patients aged >/= 70. All those without acutely life-threatening illness or coma were eligible. Patients underwent (1) reference standard delirium assessment based on DSM-IV criteria and (2) were randomised to either the index test (4AT, scores 0-12; prespecified score of > 3 considered positive) or the comparator (CAM; scored positive or negative), in a random order, using computer-generated pseudo-random numbers, stratified by study site, with block allocation. Reference standard and 4AT or CAM assessments were performed by pairs of independent raters blinded to the results of the other assessment. RESULTS: Eight hundred forty-three individuals were randomised: 21 withdrew, 3 lost contact, 32 indeterminate diagnosis, 2 missing outcome, and 785 were included in the analysis. Mean age was 81.4 (SD 6.4) years. 12.1% (95/785) had delirium by reference standard assessment, 14.3% (56/392) by 4AT, and 4.7% (18/384) by CAM. The 4AT had an area under the receiver operating characteristic curve of 0.90 (95% CI 0.84-0.96). The 4AT had a sensitivity of 76% (95% CI 61-87%) and a specificity of 94% (95% CI 92-97%). The CAM had a sensitivity of 40% (95% CI 26-57%) and a specificity of 100% (95% CI 98-100%). CONCLUSIONS: The 4AT is a short, pragmatic tool which can help improving detection rates of delirium in routine clinical care. TRIAL REGISTRATION: International standard randomised controlled trial number (ISRCTN) 53388093 . Date applied 30/05/2014; date assigned 02/06/2014.","Delirium detection in older acute medical inpatients: a multicentre prospective comparative diagnostic test accuracy study of the 4AT and the confusion assessment method. BACKGROUND: Delirium affects > 15% of hospitalised patients but is grossly underdetected, contributing to poor care. The 4 'A's Test (4AT, www.the4AT.com ) is a short delirium assessment tool designed for routine use without special training. The primary objective was to assess the accuracy of the 4AT for delirium detection. The secondary objective was to compare the 4AT with another commonly used delirium assessment tool, the Confusion Assessment Method (CAM). METHODS: This was a prospective diagnostic test accuracy study set in emergency departments or acute medical wards involving acute medical patients aged >/= 70. All those without acutely life-threatening illness or coma were eligible. Patients underwent (1) reference standard delirium assessment based on DSM-IV criteria and (2) were randomised to either the index test (4AT, scores 0-12; prespecified score of > 3 considered positive) or the comparator (CAM; scored positive or negative), in a random order, using computer-generated pseudo-random numbers, stratified by study site, with block allocation. Reference standard and 4AT or CAM assessments were performed by pairs of independent raters blinded to the results of the other assessment. RESULTS: Eight hundred forty-three individuals were randomised: 21 withdrew, 3 lost contact, 32 indeterminate diagnosis, 2 missing outcome, and 785 were included in the analysis. Mean age was 81.4 (SD 6.4) years. 12.1% (95/785) had delirium by reference standard assessment, 14.3% (56/392) by 4AT, and 4.7% (18/384) by CAM. The 4AT had an area under the receiver operating characteristic curve of 0.90 (95% CI 0.84-0.96). The 4AT had a sensitivity of 76% (95% CI 61-87%) and a specificity of 94% (95% CI 92-97%). The CAM had a sensitivity of 40% (95% CI 26-57%) and a specificity of 100% (95% CI 98-100%). CONCLUSIONS: The 4AT is a short, pragmatic tool which can help improving detection rates of delirium in routine clinical care. TRIAL REGISTRATION: International standard randomised controlled trial number (ISRCTN) 53388093 . Date applied 30/05/2014; date assigned 02/06/2014."
0,Substitution of venous for arterial blood sampling in the determination of regional rates of cerebral protein synthesis with L-[1-11C]leucine PET: A validation study,"We developed and validated a method to estimate input functions for determination of regional rates of cerebral protein synthesis (rCPS) with L-[1-11C]leucine PET without arterial sampling. The method is based on a population-derived input function (PDIF) approach, with venous samples for calibration. Population input functions were constructed from arterial blood data measured in 25 healthy 18â€“24-year-old males who underwent L-[1-11C]leucine PET scans while awake. To validate the approach, three additional groups of 18â€“27-year-old males underwent L-[1-11C]leucine PET scans with both arterial and venous blood sampling: 13 awake healthy volunteers, 10 sedated healthy volunteers, and 5 sedated subjects with fragile X syndrome. Rate constants of the L-[1-11C]leucine kinetic model were estimated voxel-wise with measured arterial input functions and with venous-calibrated PDIFs. Venous plasma leucine measurements were used with venous-calibrated PDIFs for rCPS computation. rCPS determined with PDIFs calibrated with 30â€“60 min venous samples had small errors (RMSE: 4â€“9%), and no statistically significant differences were found in any group when compared to rCPS determined with arterial input functions. We conclude that in young adult males, PDIFs calibrated with 30â€“60 min venous samples can be used in place of arterial input functions for determination of rCPS with L-[1-11C]leucine PET.","Substitution of venous for arterial blood sampling in the determination of regional rates of cerebral protein synthesis with L-[1-11C]leucine PET: A validation study. We developed and validated a method to estimate input functions for determination of regional rates of cerebral protein synthesis (rCPS) with L-[1-11C]leucine PET without arterial sampling. The method is based on a population-derived input function (PDIF) approach, with venous samples for calibration. Population input functions were constructed from arterial blood data measured in 25 healthy 18â€“24-year-old males who underwent L-[1-11C]leucine PET scans while awake. To validate the approach, three additional groups of 18â€“27-year-old males underwent L-[1-11C]leucine PET scans with both arterial and venous blood sampling: 13 awake healthy volunteers, 10 sedated healthy volunteers, and 5 sedated subjects with fragile X syndrome. Rate constants of the L-[1-11C]leucine kinetic model were estimated voxel-wise with measured arterial input functions and with venous-calibrated PDIFs. Venous plasma leucine measurements were used with venous-calibrated PDIFs for rCPS computation. rCPS determined with PDIFs calibrated with 30â€“60 min venous samples had small errors (RMSE: 4â€“9%), and no statistically significant differences were found in any group when compared to rCPS determined with arterial input functions. We conclude that in young adult males, PDIFs calibrated with 30â€“60 min venous samples can be used in place of arterial input functions for determination of rCPS with L-[1-11C]leucine PET."
0,Review of Clinical Applications for Virtual Monoenergetic Dual-Energy CT,"In this article, the authors discuss the technical background and summarize the current body of literature regarding virtual monoenergetic (VM) images derived from dual-energy CT data, which can be reconstructed between 40 and 200 keV. Substantially improved iodine attenuation at lower kiloelectron volt levels and reduced beam-hardening artifacts at higher kiloelectron volt levels have been demonstrated from all major manufacturers of dual-energy CT units. Improved contrast attenuation with VM imaging at lower kiloelectron volt levels enables better delineation and diagnostic accuracy in the detection of various vascular or oncologic abnormalities. Low-kiloelectron-volt VM imaging may be useful for salvaging CT studies with suboptimal contrast material delivery or providing additional information on the arterial vasculature obtained from venous phase acquisitions. For patients with renal impairment, substantial reductions in the use of iodinated contrast material can be achieved by using lower-energy VM imaging. The authors recommend routine reconstruction of VM images at 50 keV when using dual-energy CT to exploit the increased contrast properties. For reduction of beam-hardening artifacts, VM imaging at 120 keV is useful for the initial assessment.","Review of Clinical Applications for Virtual Monoenergetic Dual-Energy CT. In this article, the authors discuss the technical background and summarize the current body of literature regarding virtual monoenergetic (VM) images derived from dual-energy CT data, which can be reconstructed between 40 and 200 keV. Substantially improved iodine attenuation at lower kiloelectron volt levels and reduced beam-hardening artifacts at higher kiloelectron volt levels have been demonstrated from all major manufacturers of dual-energy CT units. Improved contrast attenuation with VM imaging at lower kiloelectron volt levels enables better delineation and diagnostic accuracy in the detection of various vascular or oncologic abnormalities. Low-kiloelectron-volt VM imaging may be useful for salvaging CT studies with suboptimal contrast material delivery or providing additional information on the arterial vasculature obtained from venous phase acquisitions. For patients with renal impairment, substantial reductions in the use of iodinated contrast material can be achieved by using lower-energy VM imaging. The authors recommend routine reconstruction of VM images at 50 keV when using dual-energy CT to exploit the increased contrast properties. For reduction of beam-hardening artifacts, VM imaging at 120 keV is useful for the initial assessment."
0,Availability bias and artificial intelligence,,
0,"Diagnostic value of the urine lipoarabinomannan assay in HIV-positive, ambulatory patients with CD4 below 200 cells/mu l in 2 low-resource settings: A prospective observational study",,
0,Mechanical power normalized to predicted body weight as a predictor of mortality in patients with acute respiratory distress syndrome,,
0,HiSSI: High-order SNP-SNP interactions detection based on efficient significant pattern and differential evolution,"Background: Detecting single nucleotide polymorphism (SNP) interactions is an important and challenging task in genome-wide association studies (GWAS). Various efforts have been devoted to detect SNP interactions. However, the large volume of SNP datasets results in such a big number of high-order SNP combinations that restrict the power of detecting interactions. Methods: In this paper, to combat with this challenge, we propose a two-stage approach (called HiSSI) to detect high-order SNP-SNP interactions. In the screening stage, HiSSI employs a statistically significant pattern that takes into account family wise error rate, to control false positives and to effectively screen two-locus combinations candidate set. In the searching stage, HiSSI applies two different search strategies (exhaustive search and heuristic search based on differential evolution along with Ï‡ 2-test) on candidate pairwise SNP combinations to detect high-order SNP interactions. Results: Extensive experiments on simulated datasets are conducted to evaluate HiSSI and recently proposed and related approaches on both two-locus and three-locus disease models. A real genome-wide dataset: Breast cancer dataset collected from the Wellcome Trust Case Control Consortium (WTCCC) is also used to test HiSSI. Conclusions: Simulated experiments on both two-locus and three-locus disease models show that HiSSI is more powerful than other related approaches. Real experiment on breast cancer dataset, in which HiSSI detects some significantly two-locus and three-locus interactions associated with breast cancer, again corroborate the effectiveness of HiSSI in high-order SNP-SNP interaction identification.","HiSSI: High-order SNP-SNP interactions detection based on efficient significant pattern and differential evolution. Background: Detecting single nucleotide polymorphism (SNP) interactions is an important and challenging task in genome-wide association studies (GWAS). Various efforts have been devoted to detect SNP interactions. However, the large volume of SNP datasets results in such a big number of high-order SNP combinations that restrict the power of detecting interactions. Methods: In this paper, to combat with this challenge, we propose a two-stage approach (called HiSSI) to detect high-order SNP-SNP interactions. In the screening stage, HiSSI employs a statistically significant pattern that takes into account family wise error rate, to control false positives and to effectively screen two-locus combinations candidate set. In the searching stage, HiSSI applies two different search strategies (exhaustive search and heuristic search based on differential evolution along with Ï‡ 2-test) on candidate pairwise SNP combinations to detect high-order SNP interactions. Results: Extensive experiments on simulated datasets are conducted to evaluate HiSSI and recently proposed and related approaches on both two-locus and three-locus disease models. A real genome-wide dataset: Breast cancer dataset collected from the Wellcome Trust Case Control Consortium (WTCCC) is also used to test HiSSI. Conclusions: Simulated experiments on both two-locus and three-locus disease models show that HiSSI is more powerful than other related approaches. Real experiment on breast cancer dataset, in which HiSSI detects some significantly two-locus and three-locus interactions associated with breast cancer, again corroborate the effectiveness of HiSSI in high-order SNP-SNP interaction identification."
0,Nimbolide protects against endotoxin-induced acute respiratory distress syndrome by inhibiting TNF-Î± mediated NF-ÎºB and HDAC-3 nuclear translocation,"Acute respiratory distress syndrome (ARDS) is characterized by an excessive acute inflammatory response in lung parenchyma, which ultimately leads to refractory hypoxemia. One of the earliest abnormalities seen in lung injury is the elevated levels of inflammatory cytokines, among them, the soluble tumor necrosis factor (TNF-Î±) has a key role, which exerts cytotoxicity in epithelial and endothelial cells thus exacerbates edema. The bacterial lipopolysaccharide (LPS) was used both in vitro (RAW 264.7, THP-1, MLE-12, A549, and BEAS-2B) and in vivo (C57BL/6 mice), as it activates a plethora of overlapping inflammatory signaling pathways involved in ARDS. Nimbolide is a chemical constituent of Azadirachta indica, which contains multiple biological properties, while its role in ARDS is elusive. Herein, we have investigated the protective effects of nimbolide in abrogating the complications associated with ARDS. We showed that nimbolide markedly suppressed the nitrosative-oxidative stress, inflammatory cytokines, and chemokines expression by suppressing iNOS, myeloperoxidase, and nitrotyrosine expression. Moreover, nimbolide mitigated the migration of neutrophils and mast cells whilst normalizing the LPS-induced hypothermia. Also, nimbolide modulated the expression of epigenetic regulators with multiple HDAC inhibitory activity by suppressing the nuclear translocation of NF-ÎºB and HDAC-3. We extended our studies usingÂ molecular docking studies, which demonstrated a strong interaction between nimbolide and TNF-Î±. Additionally, we showed that treatment with nimbolide increased GSH, Nrf-2, SOD-1, and HO-1 protein expression; concomitantly abrogated the LPS-triggered TNF-Î±, p38 MAPK, mTOR, and GSK-3Î² protein expression. Collectively, these results indicate that TNF-Î±-regulated NF-ÎºB and HDAC-3 crosstalk was ameliorated by nimbolide with promising anti-nitrosative, antioxidant, and anti-inflammatory properties in LPS-induced ARDS.","Nimbolide protects against endotoxin-induced acute respiratory distress syndrome by inhibiting TNF-Î± mediated NF-ÎºB and HDAC-3 nuclear translocation. Acute respiratory distress syndrome (ARDS) is characterized by an excessive acute inflammatory response in lung parenchyma, which ultimately leads to refractory hypoxemia. One of the earliest abnormalities seen in lung injury is the elevated levels of inflammatory cytokines, among them, the soluble tumor necrosis factor (TNF-Î±) has a key role, which exerts cytotoxicity in epithelial and endothelial cells thus exacerbates edema. The bacterial lipopolysaccharide (LPS) was used both in vitro (RAW 264.7, THP-1, MLE-12, A549, and BEAS-2B) and in vivo (C57BL/6 mice), as it activates a plethora of overlapping inflammatory signaling pathways involved in ARDS. Nimbolide is a chemical constituent of Azadirachta indica, which contains multiple biological properties, while its role in ARDS is elusive. Herein, we have investigated the protective effects of nimbolide in abrogating the complications associated with ARDS. We showed that nimbolide markedly suppressed the nitrosative-oxidative stress, inflammatory cytokines, and chemokines expression by suppressing iNOS, myeloperoxidase, and nitrotyrosine expression. Moreover, nimbolide mitigated the migration of neutrophils and mast cells whilst normalizing the LPS-induced hypothermia. Also, nimbolide modulated the expression of epigenetic regulators with multiple HDAC inhibitory activity by suppressing the nuclear translocation of NF-ÎºB and HDAC-3. We extended our studies usingÂ molecular docking studies, which demonstrated a strong interaction between nimbolide and TNF-Î±. Additionally, we showed that treatment with nimbolide increased GSH, Nrf-2, SOD-1, and HO-1 protein expression; concomitantly abrogated the LPS-triggered TNF-Î±, p38 MAPK, mTOR, and GSK-3Î² protein expression. Collectively, these results indicate that TNF-Î±-regulated NF-ÎºB and HDAC-3 crosstalk was ameliorated by nimbolide with promising anti-nitrosative, antioxidant, and anti-inflammatory properties in LPS-induced ARDS."
0,ACPA-negative RA divided into clinical subsets,,
0,Effective learning is accompanied by high-dimensional and efficient representations of neural activity,,
0,Heterogeneous network embedding enabling accurate disease association predictions,"Background: It is significant to identificate complex biological mechanisms of various diseases in biomedical research. Recently, the growing generation of tremendous amount of data in genomics, epigenomics, metagenomics, proteomics, metabolomics, nutriomics, etc., has resulted in the rise of systematic biological means of exploring complex diseases. However, the disparity between the production of the multiple data and our capability of analyzing data has been broaden gradually. Furthermore, we observe that networks can represent many of the above-mentioned data, and founded on the vector representations learned by network embedding methods, entities which are in close proximity but at present do not actually possess direct links are very likely to be related, therefore they are promising candidate subjects for biological investigation. Results: We incorporate six public biological databases to construct a heterogeneous biological network containing three categories of entities (i.e., genes, diseases, miRNAs) and multiple types of edges (i.e., the known relationships). To tackle the inherent heterogeneity, we develop a heterogeneous network embedding model for mapping the network into a low dimensional vector space in which the relationships between entities are preserved well. And in order to assess the effectiveness of our method, we conduct gene-disease as well as miRNA-disease associations predictions, results of which show the superiority of our novel method over several state-of-the-arts. Furthermore, many associations predicted by our method are verified in the latest real-world dataset. Conclusions: We propose a novel heterogeneous network embedding method which can adequately take advantage of the abundant contextual information and structures of heterogeneous network. Moreover, we illustrate the performance of the proposed method on directing studies in biology, which can assist in identifying new hypotheses in biological investigation.","Heterogeneous network embedding enabling accurate disease association predictions. Background: It is significant to identificate complex biological mechanisms of various diseases in biomedical research. Recently, the growing generation of tremendous amount of data in genomics, epigenomics, metagenomics, proteomics, metabolomics, nutriomics, etc., has resulted in the rise of systematic biological means of exploring complex diseases. However, the disparity between the production of the multiple data and our capability of analyzing data has been broaden gradually. Furthermore, we observe that networks can represent many of the above-mentioned data, and founded on the vector representations learned by network embedding methods, entities which are in close proximity but at present do not actually possess direct links are very likely to be related, therefore they are promising candidate subjects for biological investigation. Results: We incorporate six public biological databases to construct a heterogeneous biological network containing three categories of entities (i.e., genes, diseases, miRNAs) and multiple types of edges (i.e., the known relationships). To tackle the inherent heterogeneity, we develop a heterogeneous network embedding model for mapping the network into a low dimensional vector space in which the relationships between entities are preserved well. And in order to assess the effectiveness of our method, we conduct gene-disease as well as miRNA-disease associations predictions, results of which show the superiority of our novel method over several state-of-the-arts. Furthermore, many associations predicted by our method are verified in the latest real-world dataset. Conclusions: We propose a novel heterogeneous network embedding method which can adequately take advantage of the abundant contextual information and structures of heterogeneous network. Moreover, we illustrate the performance of the proposed method on directing studies in biology, which can assist in identifying new hypotheses in biological investigation."
0,Accurate and rapid screening model for potential diabetes mellitus,"BACKGROUND: Prediction or early diagnosis of diabetes is crucial for populations with high risk of diabetes. METHODS: In this study, we assessed the ability of five popular classifiers (J48, AdaboostM1, SMO, Bayes Net, and NaÃ¯ve Bayes) to identify individuals with diabetes based on nine non-invasive and easily obtained clinical features, including age, gender, body mass index (BMI), hypertension, history of cardiovascular disease or stroke, family history of diabetes, physical activity, work stress, and salty food preference. A total of 4205 data entries were obtained from annual physical examination reports for adults in the Shengjing Hospital of China Medical University during January-April 2017. Weka data mining software was used to identify the best algorithm for diabetes classification. RESULTS: The results indicate that decision tree classifier J48 has the best performance (accuracyâ€‰=â€‰0.9503, precisionâ€‰=â€‰0.950, recallâ€‰=â€‰0.950, F-measureâ€‰=â€‰0.948, and AUCâ€‰=â€‰0.964). The decision tree structure shows that age is the most significant feature, followed by family history of diabetes, work stress, BMI, salty food preference, physical activity, hypertension, gender, and history of cardiovascular disease or stroke. CONCLUSIONS: Our study shows that decision tree analyses can be applied to screen individuals for early diabetes risk without the need for invasive tests. This procedure will be particularly useful in developing regions with high epidemiological risk and poor socioeconomic status, and enable clinical practitioners to rapidly screen patients for increased risk of diabetes. The key features in the tree structure could further facilitate diabetes prevention through targeted community interventions, which can potentially improve early diabetes diagnosis and reduce burdens on the healthcare system.","Accurate and rapid screening model for potential diabetes mellitus. BACKGROUND: Prediction or early diagnosis of diabetes is crucial for populations with high risk of diabetes. METHODS: In this study, we assessed the ability of five popular classifiers (J48, AdaboostM1, SMO, Bayes Net, and NaÃ¯ve Bayes) to identify individuals with diabetes based on nine non-invasive and easily obtained clinical features, including age, gender, body mass index (BMI), hypertension, history of cardiovascular disease or stroke, family history of diabetes, physical activity, work stress, and salty food preference. A total of 4205 data entries were obtained from annual physical examination reports for adults in the Shengjing Hospital of China Medical University during January-April 2017. Weka data mining software was used to identify the best algorithm for diabetes classification. RESULTS: The results indicate that decision tree classifier J48 has the best performance (accuracyâ€‰=â€‰0.9503, precisionâ€‰=â€‰0.950, recallâ€‰=â€‰0.950, F-measureâ€‰=â€‰0.948, and AUCâ€‰=â€‰0.964). The decision tree structure shows that age is the most significant feature, followed by family history of diabetes, work stress, BMI, salty food preference, physical activity, hypertension, gender, and history of cardiovascular disease or stroke. CONCLUSIONS: Our study shows that decision tree analyses can be applied to screen individuals for early diabetes risk without the need for invasive tests. This procedure will be particularly useful in developing regions with high epidemiological risk and poor socioeconomic status, and enable clinical practitioners to rapidly screen patients for increased risk of diabetes. The key features in the tree structure could further facilitate diabetes prevention through targeted community interventions, which can potentially improve early diabetes diagnosis and reduce burdens on the healthcare system."
0,Lack of Association Between CTLA-4 Genetic Polymorphisms and Noncardiac Gastric Cancer in a Chinese Population,"Cytotoxic T lymphocyte antigen 4 (CTLA-4) is a key negative immunoregulatory molecule with characteristics of gene polymorphisms. Genetically predisposed CTLA-4 alteration in humans was associated with gastric cancer (GC) development. To explore the association of CTLA-4 polymorphism with susceptibility of noncardiac GC (NCGC), 490 NCGC patients and 1476 control individuals were studied. Four CTLA-4 polymorphisms were genotyped with SNPscan genotyping assays and the haplotypes were constructed with SHESIS software. Frequencies of the CTLA-4 haplotypes were estimated using an expectation-maximization algorithm. The CTLA-4 polymorphism genotype distribution and allele frequencies were not significantly different between the NCGC patients and the control subjects. The CTLA-4 haplotypes did not exhibit a significantly increased risk for NCGC patients. Adjusting status of age, sex, smoking status, alcohol use, and body mass index could not moderate any of the relationships. Data suggested that CTLA-4 polymorphisms (rs3087243, rs16840252, rs733618, and rs231775) were not significantly associated with the risk of NCGC in this Chinese population studied.","Lack of Association Between CTLA-4 Genetic Polymorphisms and Noncardiac Gastric Cancer in a Chinese Population. Cytotoxic T lymphocyte antigen 4 (CTLA-4) is a key negative immunoregulatory molecule with characteristics of gene polymorphisms. Genetically predisposed CTLA-4 alteration in humans was associated with gastric cancer (GC) development. To explore the association of CTLA-4 polymorphism with susceptibility of noncardiac GC (NCGC), 490 NCGC patients and 1476 control individuals were studied. Four CTLA-4 polymorphisms were genotyped with SNPscan genotyping assays and the haplotypes were constructed with SHESIS software. Frequencies of the CTLA-4 haplotypes were estimated using an expectation-maximization algorithm. The CTLA-4 polymorphism genotype distribution and allele frequencies were not significantly different between the NCGC patients and the control subjects. The CTLA-4 haplotypes did not exhibit a significantly increased risk for NCGC patients. Adjusting status of age, sex, smoking status, alcohol use, and body mass index could not moderate any of the relationships. Data suggested that CTLA-4 polymorphisms (rs3087243, rs16840252, rs733618, and rs231775) were not significantly associated with the risk of NCGC in this Chinese population studied."
0,Long-Lasting Rescue of Network and Cognitive Dysfunction in a Genetic Schizophrenia Model,,
0,Digital Phenotyping With Mobile and Wearable Devices: Advanced Symptom Measurement in Child and Adolescent Depression,,
0,"Comparison of the Harms, Advantages, and Costs Associated With Alternative Guidelines for the Evaluation of Hematuria",,
0,A single-cell atlas of mouse brain macrophages reveals unique transcriptional identities shaped by ontogeny and tissue environment,,
0,Identification and validation of l-asparaginase as a potential metabolic target against Mycobacterium tuberculosis,"Multidrug-resistant Mycobacterium tuberculosis (Mtb) has emerged as a major health challenge, necessitating the search for new molecular targets. A secretory amidohydrolase, l-asparaginase of Mtb (MtA), originally implicated in nitrogen assimilation and neutralization of acidic microenvironment inside human alveolar macrophages, has been proposed as a crucial metabolic enzyme. To investigate whether this enzyme could serve as a potential drug target, it was studied for structural details and active siteâ€“specific inhibitors were tested on cultured Mycobacterial strain. The structural details of MtA obtained through comparative modeling and molecular dynamics simulations provided insights about the orchestration of an alternate reaction mechanism at the active site. This was contrary to the critical Tyr flipping mechanism reported in other asparaginases. We report the novel finding of Tyr to Val replacement in catalytic triad I along with the structural reorganization of a Î²-hairpin loop upon substrate binding in MtA active site. Further,Â 5 MtA-specific, active-siteâ€“based inhibitors were obtained by following a rigorous differential screening protocol. When tested on Mycobacterium culture, 3 of these, M3 (ZINC 4740895), M26 (ZINC 33535), and doxorubicin showed promising results with inhibitory concentrations (IC 50) of 431, 100, and 56 ÂµM, respectively. Based on our findings and considering stark differences with human asparaginase, we project MtA as a promising molecular target against which the selected inhibitors may be used to counteract Mtb infection effectively.","Identification and validation of l-asparaginase as a potential metabolic target against Mycobacterium tuberculosis. Multidrug-resistant Mycobacterium tuberculosis (Mtb) has emerged as a major health challenge, necessitating the search for new molecular targets. A secretory amidohydrolase, l-asparaginase of Mtb (MtA), originally implicated in nitrogen assimilation and neutralization of acidic microenvironment inside human alveolar macrophages, has been proposed as a crucial metabolic enzyme. To investigate whether this enzyme could serve as a potential drug target, it was studied for structural details and active siteâ€“specific inhibitors were tested on cultured Mycobacterial strain. The structural details of MtA obtained through comparative modeling and molecular dynamics simulations provided insights about the orchestration of an alternate reaction mechanism at the active site. This was contrary to the critical Tyr flipping mechanism reported in other asparaginases. We report the novel finding of Tyr to Val replacement in catalytic triad I along with the structural reorganization of a Î²-hairpin loop upon substrate binding in MtA active site. Further,Â 5 MtA-specific, active-siteâ€“based inhibitors were obtained by following a rigorous differential screening protocol. When tested on Mycobacterium culture, 3 of these, M3 (ZINC 4740895), M26 (ZINC 33535), and doxorubicin showed promising results with inhibitory concentrations (IC 50) of 431, 100, and 56 ÂµM, respectively. Based on our findings and considering stark differences with human asparaginase, we project MtA as a promising molecular target against which the selected inhibitors may be used to counteract Mtb infection effectively."
0,Algorithm-Aided Prediction of Patient Preferences - An Ethics Sneak Peek,,
0,AI can help to diagnose AF during sinus rhythm,,
0,"Microglial Morphometric Parameters Correlate With the Expression Level of IL-1Î², and Allow Identifying Different Activated Morphotypes","Microglia are the resident macrophages in the brain. Traditionally, two forms of microglia have been described: one considered as a resting/surveillant state in which cells have a highly branched morphology, and another considered as an activated state in which they acquire a de-ramified or amoeboid form. However, many studies describe intermediate microglial morphologies which emerge during pathological processes. Since microglial form and function are closely related, it is of interest to correlate microglial morphology with the extent of its activation. To address this issue, we used a rat model of neuroinflammation consisting in a single injection of the enzyme neuraminidase (NA) within the lateral ventricle. Sections from NA-injected animals were co-immunolabeled with the microglial marker IBA1 and the cytokine IL-1Î², which highlight features of the cellâ€™s shape and inflammatory activation, respectively. Activated (IL-1Î² positive) microglial cells were sampled from the dorsal hypothalamus nearby the third ventricle. Images of single microglial cells were processed in two different ways to obtain (1) an accurate measure of the level of expression of IL-1Î² (indicating the degree of activation), and (2) a set of 15 morphological parameters to quantitatively and objectively describe the cellâ€™s shape. A simple regression analysis revealed a dependence of most of the morphometric parameters on IL-1Î² expression, demonstrating that the morphology of microglial cells changes progressively with the degree of activation. Moreover, a hierarchical cluster analysis pointed out four different morphotypes of activated microglia, which are characterized not only by morphological parameters values, but also by specific IL-1Î² expression levels. Thus, these results demonstrate in an objective manner that the activation of microglial cells is a gradual process, and correlates with their morphological change. Even so, it is still possible to categorize activated cells according to their morphometric parameters, each category presenting a different activation degree. The physiological relevance of those activated morphotypes is an issue worth to be assessed in the future.","Microglial Morphometric Parameters Correlate With the Expression Level of IL-1Î², and Allow Identifying Different Activated Morphotypes. Microglia are the resident macrophages in the brain. Traditionally, two forms of microglia have been described: one considered as a resting/surveillant state in which cells have a highly branched morphology, and another considered as an activated state in which they acquire a de-ramified or amoeboid form. However, many studies describe intermediate microglial morphologies which emerge during pathological processes. Since microglial form and function are closely related, it is of interest to correlate microglial morphology with the extent of its activation. To address this issue, we used a rat model of neuroinflammation consisting in a single injection of the enzyme neuraminidase (NA) within the lateral ventricle. Sections from NA-injected animals were co-immunolabeled with the microglial marker IBA1 and the cytokine IL-1Î², which highlight features of the cellâ€™s shape and inflammatory activation, respectively. Activated (IL-1Î² positive) microglial cells were sampled from the dorsal hypothalamus nearby the third ventricle. Images of single microglial cells were processed in two different ways to obtain (1) an accurate measure of the level of expression of IL-1Î² (indicating the degree of activation), and (2) a set of 15 morphological parameters to quantitatively and objectively describe the cellâ€™s shape. A simple regression analysis revealed a dependence of most of the morphometric parameters on IL-1Î² expression, demonstrating that the morphology of microglial cells changes progressively with the degree of activation. Moreover, a hierarchical cluster analysis pointed out four different morphotypes of activated microglia, which are characterized not only by morphological parameters values, but also by specific IL-1Î² expression levels. Thus, these results demonstrate in an objective manner that the activation of microglial cells is a gradual process, and correlates with their morphological change. Even so, it is still possible to categorize activated cells according to their morphometric parameters, each category presenting a different activation degree. The physiological relevance of those activated morphotypes is an issue worth to be assessed in the future."
0,Distinct cortical-amygdala projections drive reward value encoding and retrieval,,
0,"Three-dimensional visualisation of the fetal heart using prenatal MRI with motion-corrected slice-volume registration: a prospective, single-centre cohort study",,
0,Hierarchical segmentation using equivalence test (HiSET): Application to DCE image sequences,"Dynamical contrast enhanced (DCE) imaging allows non invasive access to tissue micro-vascularization. It appears as a promising tool to build imaging biomarkers for diagnostic, prognosis or anti-angiogenesis treatment monitoring of cancer. However, quantitative analysis of DCE image sequences suffers from low signal to noise ratio (SNR). SNR may be improved by averaging functional information in a large region of interest when it is functionally homogeneous. We propose a novel method for automatic segmentation of DCE image sequences into functionally homogeneous regions, called DCE-HiSET. Using an observation model which depends on one parameter a and is justified a posteriori, DCE-HiSET is a hierarchical clustering algorithm. It uses the p-value of a multiple equivalence test as dissimilarity measure and consists of two steps. The first exploits the spatial neighborhood structure to reduce complexity and takes advantage of the regularity of anatomical features, while the second recovers (spatially) disconnected homogeneous structures at a larger (global) scale. Given a minimal expected homogeneity discrepancy for the multiple equivalence test, both steps stop automatically by controlling the Type I error. This provides an adaptive choice for the number of clusters. Assuming that the DCE image sequence is functionally piecewise constant with signals on each piece sufficiently separated, we prove that DCE-HiSET will retrieve the exact partition with high probability as soon as the number of images in the sequence is large enough. The minimal expected homogeneity discrepancy appears as the tuning parameter controlling the size of the segmentation. DCE-HiSET has been implemented in C++ for 2D and 3D image sequences with competitive speed.","Hierarchical segmentation using equivalence test (HiSET): Application to DCE image sequences. Dynamical contrast enhanced (DCE) imaging allows non invasive access to tissue micro-vascularization. It appears as a promising tool to build imaging biomarkers for diagnostic, prognosis or anti-angiogenesis treatment monitoring of cancer. However, quantitative analysis of DCE image sequences suffers from low signal to noise ratio (SNR). SNR may be improved by averaging functional information in a large region of interest when it is functionally homogeneous. We propose a novel method for automatic segmentation of DCE image sequences into functionally homogeneous regions, called DCE-HiSET. Using an observation model which depends on one parameter a and is justified a posteriori, DCE-HiSET is a hierarchical clustering algorithm. It uses the p-value of a multiple equivalence test as dissimilarity measure and consists of two steps. The first exploits the spatial neighborhood structure to reduce complexity and takes advantage of the regularity of anatomical features, while the second recovers (spatially) disconnected homogeneous structures at a larger (global) scale. Given a minimal expected homogeneity discrepancy for the multiple equivalence test, both steps stop automatically by controlling the Type I error. This provides an adaptive choice for the number of clusters. Assuming that the DCE image sequence is functionally piecewise constant with signals on each piece sufficiently separated, we prove that DCE-HiSET will retrieve the exact partition with high probability as soon as the number of images in the sequence is large enough. The minimal expected homogeneity discrepancy appears as the tuning parameter controlling the size of the segmentation. DCE-HiSET has been implemented in C++ for 2D and 3D image sequences with competitive speed."
0,Diagnosis of Pulmonary Hypertension with Cardiac MRI: Derivation and Validation of Regression Models,,
0,Repetitive motion compensation for real time intraoperative video processing,"In this paper, we present a motion compensation algorithm dedicated to video processing during neurosurgery. After craniotomy, the brain surface undergoes a repetitive motion due to the cardiac pulsation. This motion as well as potential video camera motion prevent accurate video analysis. We propose a dedicated motion model where the brain deformation is described using a linear basis learned from a few initial frames of the video. As opposed to other works using linear basis for the flow, the camera motion is explicitly accounted in the transformation model. Despite the nonlinear nature of our model, all the motion parameters are robustly estimated all at once, using only one singular value decomposition (SVD), making our procedure computationally efficient. A Lagrangian specification of the flow field ensures the stability of the method. Experiments on in vivo data are presented to evaluate the capacity of the method to cope with occlusion or camera motion. The method we propose satisfies the intraoperative constraints: it is robust to surgical tools occlusions, it works in real time, and it is able to handle large camera viewpoint changes.","Repetitive motion compensation for real time intraoperative video processing. In this paper, we present a motion compensation algorithm dedicated to video processing during neurosurgery. After craniotomy, the brain surface undergoes a repetitive motion due to the cardiac pulsation. This motion as well as potential video camera motion prevent accurate video analysis. We propose a dedicated motion model where the brain deformation is described using a linear basis learned from a few initial frames of the video. As opposed to other works using linear basis for the flow, the camera motion is explicitly accounted in the transformation model. Despite the nonlinear nature of our model, all the motion parameters are robustly estimated all at once, using only one singular value decomposition (SVD), making our procedure computationally efficient. A Lagrangian specification of the flow field ensures the stability of the method. Experiments on in vivo data are presented to evaluate the capacity of the method to cope with occlusion or camera motion. The method we propose satisfies the intraoperative constraints: it is robust to surgical tools occlusions, it works in real time, and it is able to handle large camera viewpoint changes."
0,Protective role of epigallocatechin-3-gallate in NADPH oxidase-MMP2-Spm-Cer-S1P signalling axis mediated ET-1 induced pulmonary artery smooth muscle cell proliferation,"The signalling pathway involving MMP-2 and sphingosine-1-phosphate (S1P) in endothelin-1 (ET-1) induced pulmonary artery smooth muscle cell (PASMC) proliferation is not clearly known. We, therefore, investigated the role of NADPH oxidase derived O2.--mediated modulation of MMP2-sphingomyeline-ceramide-S1P signalling axis in ET-1 induced increase in proliferation of PASMCs. Additionally, protective role of the tea cathechin, epigallocatechin-3-gallate (EGCG), if any, in this scenario has also been explored. ET-1 markedly increased NADPH oxidase and MMP-2 activities and proliferation of bovine pulmonary artery smooth muscle cells (BPASMCs). ET-1 also caused significant increase in sphingomyelinase (SMase) activity, ERK1/2 and sphingosine kinase (SPHK) phosphorylations, and S1P level in the cells. EGCG inhibited ET-1 induced increase in SMase activity, ERK1/2 and SPHK phosphorylations, S1P level and the SMC proliferation. EGCG also attenuated ET-1 induced activation of MMP-2 by inhibiting NADPH oxidase activity upon inhibiting the association of the NADPH oxidase components, p47phox and p67phox in the cell membrane. Molecular docking study revealed a marked binding affinity of p47phox with the galloyl group of EGCG. Overall, our study suggest that ET-1 induced proliferation of the PASMCs occurs via NADPH oxidase-MMP2- Spm- Cer-S1P signalling axis, and EGCG attenuates ET-1 induced increase in proliferation of the cells by inhibiting NADPH oxidase activity.","Protective role of epigallocatechin-3-gallate in NADPH oxidase-MMP2-Spm-Cer-S1P signalling axis mediated ET-1 induced pulmonary artery smooth muscle cell proliferation. The signalling pathway involving MMP-2 and sphingosine-1-phosphate (S1P) in endothelin-1 (ET-1) induced pulmonary artery smooth muscle cell (PASMC) proliferation is not clearly known. We, therefore, investigated the role of NADPH oxidase derived O2.--mediated modulation of MMP2-sphingomyeline-ceramide-S1P signalling axis in ET-1 induced increase in proliferation of PASMCs. Additionally, protective role of the tea cathechin, epigallocatechin-3-gallate (EGCG), if any, in this scenario has also been explored. ET-1 markedly increased NADPH oxidase and MMP-2 activities and proliferation of bovine pulmonary artery smooth muscle cells (BPASMCs). ET-1 also caused significant increase in sphingomyelinase (SMase) activity, ERK1/2 and sphingosine kinase (SPHK) phosphorylations, and S1P level in the cells. EGCG inhibited ET-1 induced increase in SMase activity, ERK1/2 and SPHK phosphorylations, S1P level and the SMC proliferation. EGCG also attenuated ET-1 induced activation of MMP-2 by inhibiting NADPH oxidase activity upon inhibiting the association of the NADPH oxidase components, p47phox and p67phox in the cell membrane. Molecular docking study revealed a marked binding affinity of p47phox with the galloyl group of EGCG. Overall, our study suggest that ET-1 induced proliferation of the PASMCs occurs via NADPH oxidase-MMP2- Spm- Cer-S1P signalling axis, and EGCG attenuates ET-1 induced increase in proliferation of the cells by inhibiting NADPH oxidase activity."
0,Decentralized care with generic direct-acting antivirals in the management of chronic hepatitis C in a public health care setting,,
0,The Genetics of Pneumothorax,,
0,"Long-term primary results of accelerated partial breast irradiation after breast-conserving surgery for early-stage breast cancer: a randomised, phase 3, equivalence trial",,
0,Programmed Death-1 or Programmed Death Ligand-1 Blockade in Patients with Platinum-resistant Metastatic Urothelial Cancer: A Systematic Review and Meta-analysis,,
0,Predicting drug response of tumors from integrated genomic profiles by deep neural networks,"Background: The study of high-throughput genomic profiles from a pharmacogenomics viewpoint has provided unprecedented insights into the oncogenic features modulating drug response. A recent study screened for the response of a thousand human cancer cell lines to a wide collection of anti-cancer drugs and illuminated the link between cellular genotypes and vulnerability. However, due to essential differences between cell lines and tumors, to date the translation into predicting drug response in tumors remains challenging. Recently, advances in deep learning have revolutionized bioinformatics and introduced new techniques to the integration of genomic data. Its application on pharmacogenomics may fill the gap between genomics and drug response and improve the prediction of drug response in tumors. Results: We proposed a deep learning model to predict drug response (DeepDR) based on mutation and expression profiles of a cancer cell or a tumor. The model contains three deep neural networks (DNNs), i) a mutation encoder pre-trained using a large pan-cancer dataset (The Cancer Genome Atlas; TCGA) to abstract core representations of high-dimension mutation data, ii) a pre-trained expression encoder, and iii) a drug response predictor network integrating the first two subnetworks. Given a pair of mutation and expression profiles, the model predicts IC50 values of 265 drugs. We trained and tested the model on a dataset of 622 cancer cell lines and achieved an overall prediction performance of mean squared error at 1.96 (log-scale IC50 values). The performance was superior in prediction error or stability than two classical methods (linear regression and support vector machine) and four analog DNN models of DeepDR, including DNNs built without TCGA pre-training, partly replaced by principal components, and built on individual types of input data. We then applied the model to predict drug response of 9059 tumors of 33 cancer types. Using per-cancer and pan-cancer settings, the model predicted both known, including EGFR inhibitors in non-small cell lung cancer and tamoxifen in ER+ breast cancer, and novel drug targets, such as vinorelbine for TTN-mutated tumors. The comprehensive analysis further revealed the molecular mechanisms underlying the resistance to a chemotherapeutic drug docetaxel in a pan-cancer setting and the anti-cancer potential of a novel agent, CX-5461, in treating gliomas and hematopoietic malignancies. Conclusions: Here we present, as far as we know, the first DNN model to translate pharmacogenomics features identified from in vitro drug screening to predict the response of tumors. The results covered both well-studied and novel mechanisms of drug resistance and drug targets. Our model and findings improve the prediction of drug response and the identification of novel therapeutic options.","Predicting drug response of tumors from integrated genomic profiles by deep neural networks. Background: The study of high-throughput genomic profiles from a pharmacogenomics viewpoint has provided unprecedented insights into the oncogenic features modulating drug response. A recent study screened for the response of a thousand human cancer cell lines to a wide collection of anti-cancer drugs and illuminated the link between cellular genotypes and vulnerability. However, due to essential differences between cell lines and tumors, to date the translation into predicting drug response in tumors remains challenging. Recently, advances in deep learning have revolutionized bioinformatics and introduced new techniques to the integration of genomic data. Its application on pharmacogenomics may fill the gap between genomics and drug response and improve the prediction of drug response in tumors. Results: We proposed a deep learning model to predict drug response (DeepDR) based on mutation and expression profiles of a cancer cell or a tumor. The model contains three deep neural networks (DNNs), i) a mutation encoder pre-trained using a large pan-cancer dataset (The Cancer Genome Atlas; TCGA) to abstract core representations of high-dimension mutation data, ii) a pre-trained expression encoder, and iii) a drug response predictor network integrating the first two subnetworks. Given a pair of mutation and expression profiles, the model predicts IC50 values of 265 drugs. We trained and tested the model on a dataset of 622 cancer cell lines and achieved an overall prediction performance of mean squared error at 1.96 (log-scale IC50 values). The performance was superior in prediction error or stability than two classical methods (linear regression and support vector machine) and four analog DNN models of DeepDR, including DNNs built without TCGA pre-training, partly replaced by principal components, and built on individual types of input data. We then applied the model to predict drug response of 9059 tumors of 33 cancer types. Using per-cancer and pan-cancer settings, the model predicted both known, including EGFR inhibitors in non-small cell lung cancer and tamoxifen in ER+ breast cancer, and novel drug targets, such as vinorelbine for TTN-mutated tumors. The comprehensive analysis further revealed the molecular mechanisms underlying the resistance to a chemotherapeutic drug docetaxel in a pan-cancer setting and the anti-cancer potential of a novel agent, CX-5461, in treating gliomas and hematopoietic malignancies. Conclusions: Here we present, as far as we know, the first DNN model to translate pharmacogenomics features identified from in vitro drug screening to predict the response of tumors. The results covered both well-studied and novel mechanisms of drug resistance and drug targets. Our model and findings improve the prediction of drug response and the identification of novel therapeutic options."
0,"Pyrazinamide resistance and mutations L19R, R140H, and E144K in Pyrazinamidase of Mycobacterium tuberculosis","Pyrazinamide (PZA) is an important component of first-line antituberculosis drugs activated by Mycobacterium tuberculosis pyrazinamidase (PZase) into its active form pyrazinoic acid. Mutations in the pncA gene have been recognized as the major cause of PZA resistance. We detected some novel mutations, Leucine19Arginine (L19R), Arginine140Histidine (R140H), andÂ Glutamic acid144 Lysine (E144K), in the pncA gene of PZA-resistant isolates in our wet lab PZA drug susceptibility testing and sequencing. As the molecular mechanism of resistance of these variants has not been reported earlier, we have performed multiple analyses to unveil different mechanisms of resistance because of PZase mutations L19R, R140H, and E144K. The mutants and native PZase structures were subjected to comprehensive computational molecular dynamics (MD) simulations at 100 nanoseconds in apo and drug-bound form. Mutants and native PZase binding pocket were compared to observe the consequence of mutations on the binding pocket size. Hydrogen bonding, Gibbs free energy, and natural ligand Fe +2 effect were also analyzed between native and mutants. A significant variation between native and mutant PZase structure activity was observed. The native PZase protein docking score was found to be the maximum, showing strong binding affinity in comparison with mutants. MD simulations explored the effect of the variants on the biological function of PZase. Hydrogen bonding, metal ion Fe +2 deviation, and fluctuation also seemed to be affected because of the mutations L19R, R140H, and E144K. The variants L19R, R140H, and E144K play a significant role in PZA resistance, altering the overall activity of native PZase, including metal ion Fe +2 displacement and free energy. This study offers valuable evidence for better management of drug-resistant tuberculosis.","Pyrazinamide resistance and mutations L19R, R140H, and E144K in Pyrazinamidase of Mycobacterium tuberculosis. Pyrazinamide (PZA) is an important component of first-line antituberculosis drugs activated by Mycobacterium tuberculosis pyrazinamidase (PZase) into its active form pyrazinoic acid. Mutations in the pncA gene have been recognized as the major cause of PZA resistance. We detected some novel mutations, Leucine19Arginine (L19R), Arginine140Histidine (R140H), andÂ Glutamic acid144 Lysine (E144K), in the pncA gene of PZA-resistant isolates in our wet lab PZA drug susceptibility testing and sequencing. As the molecular mechanism of resistance of these variants has not been reported earlier, we have performed multiple analyses to unveil different mechanisms of resistance because of PZase mutations L19R, R140H, and E144K. The mutants and native PZase structures were subjected to comprehensive computational molecular dynamics (MD) simulations at 100 nanoseconds in apo and drug-bound form. Mutants and native PZase binding pocket were compared to observe the consequence of mutations on the binding pocket size. Hydrogen bonding, Gibbs free energy, and natural ligand Fe +2 effect were also analyzed between native and mutants. A significant variation between native and mutant PZase structure activity was observed. The native PZase protein docking score was found to be the maximum, showing strong binding affinity in comparison with mutants. MD simulations explored the effect of the variants on the biological function of PZase. Hydrogen bonding, metal ion Fe +2 deviation, and fluctuation also seemed to be affected because of the mutations L19R, R140H, and E144K. The variants L19R, R140H, and E144K play a significant role in PZA resistance, altering the overall activity of native PZase, including metal ion Fe +2 displacement and free energy. This study offers valuable evidence for better management of drug-resistant tuberculosis."
0,Artificial intelligence aims to improve cancer screenings in Kenya,,
0,(-)-Epigallocatechin-3-gallate derivatives combined with cisplatin exhibit synergistic inhibitory effects on non-small-cell lung cancer cells,"Background: Non-small-cell lung cancer (NSCLC) is the leading cause of cancer-related death worldwide. The inhibition of epidermal growth factor receptor (EGFR) signaling by tyrosine kinase inhibitors or monoclonal antibodies plays a key role in NSCLC treatment. Unfortunately, these treatment strategies are limited by eventual resistance and cell lines with differential EGFR status. Therefore, new therapeutic strategies for NSCLC are urgently required. Methods: To improve the stability and absorption of (-)-epigallocatechin-3-gallate (EGCG), we synthesized a series of EGCG derivatives. The antitumor activities of EGCG derivatives with or without cisplatin were investigated in vitro and vivo. Cell proliferation, cell cycle distribution and apoptosis were measured in NSCLC cell lines and in vivo in a NCI-H441 xenograft model. Results: We found that the EGCG derivatives inhibited cell viability and colony formation, caused cell cycle redistribution, and induced apoptosis. More importantly, the combination of the EGCG derivative and cisplatin led to increased growth inhibition, caused cell cycle redistribution, and enhanced the apoptosis rate compared to either compound alone. Consistent with the experiments in vitro, EGCG derivatives plus cisplatin significantly reduced tumor growth. Conclusions: The combination treatment was found to inhibit the EGFR signaling pathway and decrease the expression of p-EGFR, p-AKT, and p-ERK in vitro and vivo. Our results suggest that compound 3 is a novel potential compound for NSCLC patients.","(-)-Epigallocatechin-3-gallate derivatives combined with cisplatin exhibit synergistic inhibitory effects on non-small-cell lung cancer cells. Background: Non-small-cell lung cancer (NSCLC) is the leading cause of cancer-related death worldwide. The inhibition of epidermal growth factor receptor (EGFR) signaling by tyrosine kinase inhibitors or monoclonal antibodies plays a key role in NSCLC treatment. Unfortunately, these treatment strategies are limited by eventual resistance and cell lines with differential EGFR status. Therefore, new therapeutic strategies for NSCLC are urgently required. Methods: To improve the stability and absorption of (-)-epigallocatechin-3-gallate (EGCG), we synthesized a series of EGCG derivatives. The antitumor activities of EGCG derivatives with or without cisplatin were investigated in vitro and vivo. Cell proliferation, cell cycle distribution and apoptosis were measured in NSCLC cell lines and in vivo in a NCI-H441 xenograft model. Results: We found that the EGCG derivatives inhibited cell viability and colony formation, caused cell cycle redistribution, and induced apoptosis. More importantly, the combination of the EGCG derivative and cisplatin led to increased growth inhibition, caused cell cycle redistribution, and enhanced the apoptosis rate compared to either compound alone. Consistent with the experiments in vitro, EGCG derivatives plus cisplatin significantly reduced tumor growth. Conclusions: The combination treatment was found to inhibit the EGFR signaling pathway and decrease the expression of p-EGFR, p-AKT, and p-ERK in vitro and vivo. Our results suggest that compound 3 is a novel potential compound for NSCLC patients."
0,Heterogeneous information network based clustering for precision traditional Chinese medicine,"BACKGROUND: Traditional Chinese medicine (TCM) is a highly important complement to modern medicine and is widely practiced in China and in many other countries. The work of Chinese medicine is subject to the two factors of the inheritance and development of clinical experience of famous Chinese medicine practitioners and the difficulty in improving the service capacity of basic Chinese medicine practitioners. Heterogeneous information networks (HINs) are a kind of graphical model for integrating and modeling real-world information. Through HINs, we can integrate and model the large-scale heterogeneous TCM data into structured graph data and use this as a basis for analysis. METHODS: Mining categorizations from TCM data is an important task for precision medicine. In this paper, we propose a novel structured learning model to solve the problem of formula regularity, a pivotal task in prescription optimization. We integrate clustering with ranking in a heterogeneous information network. RESULTS: The results from experiments on the Pharmacopoeia of the People's Republic of China (ChP) demonstrate the effectiveness and accuracy of the proposed model for discovering useful categorizations of formulas. CONCLUSIONS: We use heterogeneous information networks to model TCM data and propose a TCM-HIN. Combining the heterogeneous graph with the probability graph, we proposed the TCM-Clus algorithm, which combines clustering with ranking and classifies traditional Chinese medicine prescriptions. The results of the categorizations can help Chinese medicine practitioners to make clinical decision.","Heterogeneous information network based clustering for precision traditional Chinese medicine. BACKGROUND: Traditional Chinese medicine (TCM) is a highly important complement to modern medicine and is widely practiced in China and in many other countries. The work of Chinese medicine is subject to the two factors of the inheritance and development of clinical experience of famous Chinese medicine practitioners and the difficulty in improving the service capacity of basic Chinese medicine practitioners. Heterogeneous information networks (HINs) are a kind of graphical model for integrating and modeling real-world information. Through HINs, we can integrate and model the large-scale heterogeneous TCM data into structured graph data and use this as a basis for analysis. METHODS: Mining categorizations from TCM data is an important task for precision medicine. In this paper, we propose a novel structured learning model to solve the problem of formula regularity, a pivotal task in prescription optimization. We integrate clustering with ranking in a heterogeneous information network. RESULTS: The results from experiments on the Pharmacopoeia of the People's Republic of China (ChP) demonstrate the effectiveness and accuracy of the proposed model for discovering useful categorizations of formulas. CONCLUSIONS: We use heterogeneous information networks to model TCM data and propose a TCM-HIN. Combining the heterogeneous graph with the probability graph, we proposed the TCM-Clus algorithm, which combines clustering with ranking and classifies traditional Chinese medicine prescriptions. The results of the categorizations can help Chinese medicine practitioners to make clinical decision."
0,Characterization of proteome variation during modern maize breeding,"The success of modern maize breeding has been demonstrated by remarkable increases in productivity with tremendous modification of agricultural phenotypes over the last century. Although the underlying genetic changes of the maize adaptation from tropical to temperate regions have been extensively studied, our knowledge is limited regarding the accordance of protein and mRNA expression levels accompanying such adaptation. Here we conducted an integrative analysis of proteomic and transcriptomic changes in a maize association panel. The minimum extent of correlation between protein and RNA levels suggests that variation in mRNA expression is often not indicative of protein expression at a population scale. This is corroborated by the observation that mRNA- and protein-based coexpression networks are relatively independent of each other, and many pQTLs arise without the presence of corresponding eQTLs. Importantly, compared with transcriptome, the subtypes categorized by the proteome show a markedly high accuracy to resemble the genomic subpopulation. These findings suggest that proteome evolved under a greater evolutionary constraint than transcriptome during maize adaptation from tropical to temperate regions. Overall, the integrated multi-omics analysis provides a functional context to interpret gene expression variation during modern maize breeding.","Characterization of proteome variation during modern maize breeding. The success of modern maize breeding has been demonstrated by remarkable increases in productivity with tremendous modification of agricultural phenotypes over the last century. Although the underlying genetic changes of the maize adaptation from tropical to temperate regions have been extensively studied, our knowledge is limited regarding the accordance of protein and mRNA expression levels accompanying such adaptation. Here we conducted an integrative analysis of proteomic and transcriptomic changes in a maize association panel. The minimum extent of correlation between protein and RNA levels suggests that variation in mRNA expression is often not indicative of protein expression at a population scale. This is corroborated by the observation that mRNA- and protein-based coexpression networks are relatively independent of each other, and many pQTLs arise without the presence of corresponding eQTLs. Importantly, compared with transcriptome, the subtypes categorized by the proteome show a markedly high accuracy to resemble the genomic subpopulation. These findings suggest that proteome evolved under a greater evolutionary constraint than transcriptome during maize adaptation from tropical to temperate regions. Overall, the integrated multi-omics analysis provides a functional context to interpret gene expression variation during modern maize breeding."
0,"Estimation of global insulin use for type 2 diabetes, 2018-30: a microsimulation analysis",,
0,Ethics of Artificial Intelligence in Radiology: Summary of the Joint European and North American Multisociety Statement,"This is a condensed summary of an international multisociety statement on ethics of artificial intelligence (AI) in radiology produced by the ACR, European Society of Radiology, RSNA, Society for Imaging Informatics in Medicine, European Society of Medical Imaging Informatics, Canadian Association of Radiologists, and American Association of Physicists in Medicine. AI has great potential to increase efficiency and accuracy throughout radiology, but it also carries inherent pitfalls and biases. Widespread use of AI-based intelligent and autonomous systems in radiology can increase the risk of systemic errors with high consequence and highlights complex ethical and societal issues. Currently, there is little experience using AI for patient care in diverse clinical settings. Extensive research is needed to understand how to best deploy AI in clinical practice. This statement highlights our consensus that ethical use of AI in radiology should promote well-being, minimize harm, and ensure that the benefits and harms are distributed among stakeholders in a just manner. We believe AI should respect human rights and freedoms, including dignity and privacy. It should be designed for maximum transparency and dependability. Ultimate responsibility and accountability for AI remains with its human designers and operators for the foreseeable future. The radiology community should start now to develop codes of ethics and practice for AI that promote any use that helps patients and the common good and should block use of radiology data and algorithms for financial gain without those two attributes. This article is a simultaneous joint publication in Radiology, Journal of the American College of Radiology, Canadian Association of Radiologists Journal, and Insights into Imaging. Published under a CC BY-NC-ND 4.0 license. Online supplemental material is available for this article.","Ethics of Artificial Intelligence in Radiology: Summary of the Joint European and North American Multisociety Statement. This is a condensed summary of an international multisociety statement on ethics of artificial intelligence (AI) in radiology produced by the ACR, European Society of Radiology, RSNA, Society for Imaging Informatics in Medicine, European Society of Medical Imaging Informatics, Canadian Association of Radiologists, and American Association of Physicists in Medicine. AI has great potential to increase efficiency and accuracy throughout radiology, but it also carries inherent pitfalls and biases. Widespread use of AI-based intelligent and autonomous systems in radiology can increase the risk of systemic errors with high consequence and highlights complex ethical and societal issues. Currently, there is little experience using AI for patient care in diverse clinical settings. Extensive research is needed to understand how to best deploy AI in clinical practice. This statement highlights our consensus that ethical use of AI in radiology should promote well-being, minimize harm, and ensure that the benefits and harms are distributed among stakeholders in a just manner. We believe AI should respect human rights and freedoms, including dignity and privacy. It should be designed for maximum transparency and dependability. Ultimate responsibility and accountability for AI remains with its human designers and operators for the foreseeable future. The radiology community should start now to develop codes of ethics and practice for AI that promote any use that helps patients and the common good and should block use of radiology data and algorithms for financial gain without those two attributes. This article is a simultaneous joint publication in Radiology, Journal of the American College of Radiology, Canadian Association of Radiologists Journal, and Insights into Imaging. Published under a CC BY-NC-ND 4.0 license. Online supplemental material is available for this article."
0,Interaction with hyaluronan matrix and miRNA cargo as contributors for in vitro potential of mesenchymal stem cell-derived extracellular vesicles in a model of human osteoarthritic synoviocytes,"Background: Osteoarthritis (OA) is the most prevalent joint disease, and to date, no options for effective tissue repair and restoration are available. With the aim of developing new therapies, the impact of mesenchymal stem cells (MSCs) has been explored, and the efficacy of MSCs started to be deciphered. A strong paracrine capacity relying on both secreted and vesicle-embedded (EVs) protein or nucleic acid-based factors has been proposed as the principal mechanism that contributes to tissue repair. This work investigated the mechanism of internalization of extracellular vesicles (EVs) released by adipose-derived MSCs (ASCs) and the role of shuttled miRNAs in the restoration of homeostasis in an in vitro model of human fibroblast-like synoviocytes (FLSs) from OA patients. Methods: ASC-EVs were isolated by differential centrifugation and validated by flow cytometry and nanoparticle tracking analysis. ASC-EVs with increased hyaluronan (HA) receptor CD44 levels were obtained culturing ASCs on HA-coated plastic surfaces. OA FLSs with intact or digested HA matrix were co-cultured with fluorescent ASC-EVs, and incorporation scored by flow cytometry and ELISA. ASC-EV complete miRNome was deciphered by high-throughput screening. In inflamed OA FLSs, genes and pathways potentially regulated by ASC-EV miRNA were predicted by bioinformatics. OA FLSs stimulated with IL-1Î² at physiological levels (25 pg/mL) were treated with ASC-EVs, and expression of inflammation and OA-related genes was measured by qRT-PCR over a 10-day time frame with modulated candidates verified by ELISA. Results: The data showed that HA is involved in ASC-EV internalization in FLSs. Indeed, both removal of HA matrix presence on FLSs and modulation of CD44 levels on EVs affected their recruitment. Bioinformatics analysis of EV-embedded miRNAs showed their ability to potentially regulate the main pathways strictly associated with synovial inflammation in OA. In this frame, ASC-EVs reduced the expression of pro-inflammatory cytokines and chemokines in a chronic model of FLS inflammation. Conclusions: Given their ability to affect FLS behavior in a model of chronic inflammation through direct interaction with HA matrix and miRNA release, ASC-EVs confirm their role as a novel therapeutic option for osteoarthritic joints.","Interaction with hyaluronan matrix and miRNA cargo as contributors for in vitro potential of mesenchymal stem cell-derived extracellular vesicles in a model of human osteoarthritic synoviocytes. Background: Osteoarthritis (OA) is the most prevalent joint disease, and to date, no options for effective tissue repair and restoration are available. With the aim of developing new therapies, the impact of mesenchymal stem cells (MSCs) has been explored, and the efficacy of MSCs started to be deciphered. A strong paracrine capacity relying on both secreted and vesicle-embedded (EVs) protein or nucleic acid-based factors has been proposed as the principal mechanism that contributes to tissue repair. This work investigated the mechanism of internalization of extracellular vesicles (EVs) released by adipose-derived MSCs (ASCs) and the role of shuttled miRNAs in the restoration of homeostasis in an in vitro model of human fibroblast-like synoviocytes (FLSs) from OA patients. Methods: ASC-EVs were isolated by differential centrifugation and validated by flow cytometry and nanoparticle tracking analysis. ASC-EVs with increased hyaluronan (HA) receptor CD44 levels were obtained culturing ASCs on HA-coated plastic surfaces. OA FLSs with intact or digested HA matrix were co-cultured with fluorescent ASC-EVs, and incorporation scored by flow cytometry and ELISA. ASC-EV complete miRNome was deciphered by high-throughput screening. In inflamed OA FLSs, genes and pathways potentially regulated by ASC-EV miRNA were predicted by bioinformatics. OA FLSs stimulated with IL-1Î² at physiological levels (25 pg/mL) were treated with ASC-EVs, and expression of inflammation and OA-related genes was measured by qRT-PCR over a 10-day time frame with modulated candidates verified by ELISA. Results: The data showed that HA is involved in ASC-EV internalization in FLSs. Indeed, both removal of HA matrix presence on FLSs and modulation of CD44 levels on EVs affected their recruitment. Bioinformatics analysis of EV-embedded miRNAs showed their ability to potentially regulate the main pathways strictly associated with synovial inflammation in OA. In this frame, ASC-EVs reduced the expression of pro-inflammatory cytokines and chemokines in a chronic model of FLS inflammation. Conclusions: Given their ability to affect FLS behavior in a model of chronic inflammation through direct interaction with HA matrix and miRNA release, ASC-EVs confirm their role as a novel therapeutic option for osteoarthritic joints."
0,Clinical validation of the FLIP algorithm and the SAF score in patients with non-alcoholic fatty liver disease,"BACKGROUND & AIMS: Histological classifications used to diagnose/stage non-alcoholic fatty liver disease (NAFLD) are based on morphology, with undetermined clinical correlates and relevance. We assessed the clinical relevance of the fatty liver inhibition of progression (FLIP) algorithm and the steatosis, activity, and fibrosis (SAF) scoring system. METHODS: One hundred and forty consecutive patients with suspected NAFLD and a separate validation cohort of 78 patients enrolled in a therapeutic trial, all with central reading of liver biopsy, were included. FLIP and SAF were used to categorize patients with non-alcoholic steatohepatitis (NASH), non-NASH NAFLD (NAFL), or non-NAFLD. The SAF activity score assessed hepatocyte ballooning and lobular inflammation; a histologically severe disease was defined as a SAF activity score of >/=3 and/or bridging fibrosis or cirrhosis. Clinical, biochemical, and metabolic data were analyzed in relation to histology. RESULTS: Patients with NASH according to the FLIP algorithm had a clinical profile distinct from those with NAFL, with a higher prevalence of metabolic risk factors (increased body mass index [BMI], central obesity, serum glucose, and glycated hemoglobin), more severe insulin resistance (fasting insulin and homeostasis model assessment for insulin resistance [HOMA-IR] values), and higher levels of aminotransferases. Similar findings were documented for patients with severe disease vs. those without. Positive linear trends existed between NASH or severe disease and increasing BMI and HOMA-IR. There was a strong association between liver fibrosis and NASH or SAF-defined scores of activity. Patients with either significant or bridging fibrosis overwhelmingly had NASH, and bridging fibrosis most often coexisted with severe activity. CONCLUSIONS: The FLIP algorithm/SAF score, although based on purely morphological grounds, are clinically relevant, as they identify patients with distinct clinical and biological profiles of disease severity. Disease activity in NAFLD is associated with fibrosis severity. LAY SUMMARY: The examination of liver tissue under the microscope (histology) serves to define the type and severity of non-alcoholic fatty liver disease morphologically, and is also used to determine improvement in therapeutic or natural history clinical trials. The FLIP algorithm/SAF classification is a new histological classification well validated on morphological but not clinical grounds. Here, we demonstrate that different disease categories defined by the FLIP/SAF classification correspond to entities of different clinical and biological severity. We also show a strong association between the activity of steatohepatitis (defined histologically) and the amount of fibrotic scar.","Clinical validation of the FLIP algorithm and the SAF score in patients with non-alcoholic fatty liver disease. BACKGROUND & AIMS: Histological classifications used to diagnose/stage non-alcoholic fatty liver disease (NAFLD) are based on morphology, with undetermined clinical correlates and relevance. We assessed the clinical relevance of the fatty liver inhibition of progression (FLIP) algorithm and the steatosis, activity, and fibrosis (SAF) scoring system. METHODS: One hundred and forty consecutive patients with suspected NAFLD and a separate validation cohort of 78 patients enrolled in a therapeutic trial, all with central reading of liver biopsy, were included. FLIP and SAF were used to categorize patients with non-alcoholic steatohepatitis (NASH), non-NASH NAFLD (NAFL), or non-NAFLD. The SAF activity score assessed hepatocyte ballooning and lobular inflammation; a histologically severe disease was defined as a SAF activity score of >/=3 and/or bridging fibrosis or cirrhosis. Clinical, biochemical, and metabolic data were analyzed in relation to histology. RESULTS: Patients with NASH according to the FLIP algorithm had a clinical profile distinct from those with NAFL, with a higher prevalence of metabolic risk factors (increased body mass index [BMI], central obesity, serum glucose, and glycated hemoglobin), more severe insulin resistance (fasting insulin and homeostasis model assessment for insulin resistance [HOMA-IR] values), and higher levels of aminotransferases. Similar findings were documented for patients with severe disease vs. those without. Positive linear trends existed between NASH or severe disease and increasing BMI and HOMA-IR. There was a strong association between liver fibrosis and NASH or SAF-defined scores of activity. Patients with either significant or bridging fibrosis overwhelmingly had NASH, and bridging fibrosis most often coexisted with severe activity. CONCLUSIONS: The FLIP algorithm/SAF score, although based on purely morphological grounds, are clinically relevant, as they identify patients with distinct clinical and biological profiles of disease severity. Disease activity in NAFLD is associated with fibrosis severity. LAY SUMMARY: The examination of liver tissue under the microscope (histology) serves to define the type and severity of non-alcoholic fatty liver disease morphologically, and is also used to determine improvement in therapeutic or natural history clinical trials. The FLIP algorithm/SAF classification is a new histological classification well validated on morphological but not clinical grounds. Here, we demonstrate that different disease categories defined by the FLIP/SAF classification correspond to entities of different clinical and biological severity. We also show a strong association between the activity of steatohepatitis (defined histologically) and the amount of fibrotic scar."
0,DoubletDecon: Deconvoluting Doublets from Single-Cell RNA-Sequencing Data,"Methods for single-cell RNA sequencing (scRNA-seq) have greatly advanced in recent years. While droplet- and well-based methods have increased the capture frequency of cells for scRNA-seq, these technologies readily produce technical artifacts, such as doublet cell captures. Doublets occurring between distinct cell types can appear as hybrid scRNA-seq profiles, but do not have distinct transcriptomes from individual cell states. We introduce DoubletDecon, an approach that detects doublets with a combination of deconvolution analyses and the identification of unique cell-state gene expression. We demonstrate the ability of DoubletDecon to identify synthetic, mixed-species, genetic, and cell-hashing cell doublets from scRNA-seq datasets of varying cellular complexity with a high sensitivity relative to alternative approaches. Importantly, this algorithm prevents the prediction of valid mixed-lineage and transitional cell states as doublets by considering their unique gene expression. DoubletDecon has an easy-to-use graphical user interface and is compatible with diverse species and unsupervised population detection algorithms.","DoubletDecon: Deconvoluting Doublets from Single-Cell RNA-Sequencing Data. Methods for single-cell RNA sequencing (scRNA-seq) have greatly advanced in recent years. While droplet- and well-based methods have increased the capture frequency of cells for scRNA-seq, these technologies readily produce technical artifacts, such as doublet cell captures. Doublets occurring between distinct cell types can appear as hybrid scRNA-seq profiles, but do not have distinct transcriptomes from individual cell states. We introduce DoubletDecon, an approach that detects doublets with a combination of deconvolution analyses and the identification of unique cell-state gene expression. We demonstrate the ability of DoubletDecon to identify synthetic, mixed-species, genetic, and cell-hashing cell doublets from scRNA-seq datasets of varying cellular complexity with a high sensitivity relative to alternative approaches. Importantly, this algorithm prevents the prediction of valid mixed-lineage and transitional cell states as doublets by considering their unique gene expression. DoubletDecon has an easy-to-use graphical user interface and is compatible with diverse species and unsupervised population detection algorithms."
0,Identification of natural compound inhibitors against PfDXR: A hybrid structure-based molecular modeling approach and molecular dynamics simulation studies,"In the present contribution, multicomplex-based pharmacophore studies were carried out on the structural proteome of Plasmodium falciparum 1-deoxy-D-xylulose-5-phosphate reductoisomerase. Among the constructed models, a representative model with complementary features, accountable for the inhibition was used as a primary filter for the screening of database molecules. Auxiliary evaluations of the screened molecules were performed via drug-likeness and molecular docking studies. Subsequently, the stability of the docked inhibitors was envisioned by molecular dynamics simulations, principle component analysis, and molecular mechanics-Poisson-Boltzmann surface area-based free binding energy calculations. The stability assessment of the hits was done by comparing with the reference (beta-substituted fosmidomycin analog, LC5) to prioritize more potent candidates. All the complexes showed stable dynamic behavior while three of them displayed higher binding free energy compared with the reference. The work resulted in the identification of the compounds with diverse scaffolds, which could be used as initial leads for the design of novel PfDXR inhibitors.","Identification of natural compound inhibitors against PfDXR: A hybrid structure-based molecular modeling approach and molecular dynamics simulation studies. In the present contribution, multicomplex-based pharmacophore studies were carried out on the structural proteome of Plasmodium falciparum 1-deoxy-D-xylulose-5-phosphate reductoisomerase. Among the constructed models, a representative model with complementary features, accountable for the inhibition was used as a primary filter for the screening of database molecules. Auxiliary evaluations of the screened molecules were performed via drug-likeness and molecular docking studies. Subsequently, the stability of the docked inhibitors was envisioned by molecular dynamics simulations, principle component analysis, and molecular mechanics-Poisson-Boltzmann surface area-based free binding energy calculations. The stability assessment of the hits was done by comparing with the reference (beta-substituted fosmidomycin analog, LC5) to prioritize more potent candidates. All the complexes showed stable dynamic behavior while three of them displayed higher binding free energy compared with the reference. The work resulted in the identification of the compounds with diverse scaffolds, which could be used as initial leads for the design of novel PfDXR inhibitors."
0,ClearF: A supervised feature scoring method to find biomarkers using class-wise embedding and reconstruction,"Background: Feature selection or scoring methods for the detection of biomarkers are essential in bioinformatics. Various feature selection methods have been developed for the detection of biomarkers, and several studies have employed information-theoretic approaches. However, most of these methods generally require a long processing time. In addition, information-theoretic methods discretize continuous features, which is a drawback that can lead to the loss of information. Results: In this paper, a novel supervised feature scoring method named ClearF is proposed. The proposed method is suitable for continuous-valued data, which is similar to the principle of feature selection using mutual information, with the added advantage of a reduced computation time. The proposed score calculation is motivated by the association between the reconstruction error and the information-theoretic measurement. Our method is based on class-wise low-dimensional embedding and the resulting reconstruction error. Given multi-class datasets such as a case-control study dataset, low-dimensional embedding is first applied to each class to obtain a compressed representation of the class, and also for the entire dataset. Reconstruction is then performed to calculate the error of each feature and the final score for each feature is defined in terms of the reconstruction errors. The correlation between the information theoretic measurement and the proposed method is demonstrated using a simulation. For performance validation, we compared the classification performance of the proposed method with those of various algorithms on benchmark datasets. Conclusions: The proposed method showed higher accuracy and lower execution time than the other established methods. Moreover, an experiment was conducted on the TCGA breast cancer dataset, and it was confirmed that the genes with the highest scores were highly associated with subtypes of breast cancer.","ClearF: A supervised feature scoring method to find biomarkers using class-wise embedding and reconstruction. Background: Feature selection or scoring methods for the detection of biomarkers are essential in bioinformatics. Various feature selection methods have been developed for the detection of biomarkers, and several studies have employed information-theoretic approaches. However, most of these methods generally require a long processing time. In addition, information-theoretic methods discretize continuous features, which is a drawback that can lead to the loss of information. Results: In this paper, a novel supervised feature scoring method named ClearF is proposed. The proposed method is suitable for continuous-valued data, which is similar to the principle of feature selection using mutual information, with the added advantage of a reduced computation time. The proposed score calculation is motivated by the association between the reconstruction error and the information-theoretic measurement. Our method is based on class-wise low-dimensional embedding and the resulting reconstruction error. Given multi-class datasets such as a case-control study dataset, low-dimensional embedding is first applied to each class to obtain a compressed representation of the class, and also for the entire dataset. Reconstruction is then performed to calculate the error of each feature and the final score for each feature is defined in terms of the reconstruction errors. The correlation between the information theoretic measurement and the proposed method is demonstrated using a simulation. For performance validation, we compared the classification performance of the proposed method with those of various algorithms on benchmark datasets. Conclusions: The proposed method showed higher accuracy and lower execution time than the other established methods. Moreover, an experiment was conducted on the TCGA breast cancer dataset, and it was confirmed that the genes with the highest scores were highly associated with subtypes of breast cancer."
0,"End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography (vol 25, pg 954, 2019)",,
0,Small molecule nAS-E targeting cAMP response element binding protein (CREB) and CREB-binding protein interaction inhibits breast cancer bone metastasis,"Bone is the most common metastatic site for breast cancer. The excessive osteoclast activity in the metastatic bone lesions often produces osteolysis. The cyclic-AMP (cAMP)-response element binding protein (CREB) serves a variety of biological functions including the transformation and immortalization of breast cancer cells. In addition, evidence has shown that CREB plays a key role in osteoclastgenesis and bone resorption. Small organic molecules with good pharmacokinetic properties and specificity, targeting CREB-CBP (CREB-binding protein) interaction to inhibit CREB-mediated gene transcription have attracted more considerations as cancer therapeutics. We recently identified naphthol AS-E (nAS-E) as a cell-permeable inhibitor of CREB-mediated gene transcription through inhibiting CREB-CBP interaction. In this study, we tested the effect of nAS-E on breast cancer cell proliferation, survival, migration as well as osteoclast formation and bone resorption in vitro for the first time. Our results demonstrated that nAS-E inhibited breast cancer cell proliferation, migration, survival and suppressed osteoclast differentiation as well as bone resorption through inhibiting CREB-CBP interaction. In addition, the in vivo effect of nAS-E in protecting against breast cancer-induced osteolysis was evaluated. Our results indicated that nAS-E could reverse bone loss induced by MDA-MB-231 tumour. These results suggest that small molecules targeting CREB-CBP interaction to inhibit CREB-mediated gene transcription might be a potential approach for the treatment of breast cancer bone metastasis.","Small molecule nAS-E targeting cAMP response element binding protein (CREB) and CREB-binding protein interaction inhibits breast cancer bone metastasis. Bone is the most common metastatic site for breast cancer. The excessive osteoclast activity in the metastatic bone lesions often produces osteolysis. The cyclic-AMP (cAMP)-response element binding protein (CREB) serves a variety of biological functions including the transformation and immortalization of breast cancer cells. In addition, evidence has shown that CREB plays a key role in osteoclastgenesis and bone resorption. Small organic molecules with good pharmacokinetic properties and specificity, targeting CREB-CBP (CREB-binding protein) interaction to inhibit CREB-mediated gene transcription have attracted more considerations as cancer therapeutics. We recently identified naphthol AS-E (nAS-E) as a cell-permeable inhibitor of CREB-mediated gene transcription through inhibiting CREB-CBP interaction. In this study, we tested the effect of nAS-E on breast cancer cell proliferation, survival, migration as well as osteoclast formation and bone resorption in vitro for the first time. Our results demonstrated that nAS-E inhibited breast cancer cell proliferation, migration, survival and suppressed osteoclast differentiation as well as bone resorption through inhibiting CREB-CBP interaction. In addition, the in vivo effect of nAS-E in protecting against breast cancer-induced osteolysis was evaluated. Our results indicated that nAS-E could reverse bone loss induced by MDA-MB-231 tumour. These results suggest that small molecules targeting CREB-CBP interaction to inhibit CREB-mediated gene transcription might be a potential approach for the treatment of breast cancer bone metastasis."
0,Understanding clinical variables to improve empirical antibiotic therapy for UTI,,
0,Artificial intelligence in gastrointestinal endoscopy: how intelligent can it get?,,
0,Robot-assisted versus open cystectomy in the RAZOR trial,,
0,Influence of coupled hemodynamics-arterial wall interaction on compliance in a realistic pulmonary artery with variable intravascular wall properties,,
0,Use of a molecular classifier to identify usual interstitial pneumonia in conventional transbronchial lung biopsy samples: a prospective validation study,,
0,Graph algorithms for condensing and consolidating gene set analysis results,"Gene set analysis plays a critical role in the functional interpretation of omics data. Although this is typically done for one omics experiment at a time, there is an increasing need to combine gene set analysis results from multiple experiments performed on the same or different omics platforms, such as in multi-omics studies. Integrating results from multiple experiments is challenging, and annotation redundancy between gene sets further obscures clear conclusions. We propose to use a weighted set cover algorithm to reduce redundancy of gene sets identified in a single experiment. Next, we use affinity propagation to consolidate similar gene sets identified from multiple experiments into clusters and to automatically determine the most representative gene set for each cluster. Using three examples from over representation analysis and gene set enrichment analysis, we showed that weighted set cover outperformed a previously published set cover method and reduced the number of gene sets by 52-77%. Focusing on overlapping genes between the list of input genes and the enriched gene sets in over-representation analysis and leading-edge genes in gene set enrichment analysis further reduced the number of gene sets. A use case combining enrichment analysis results from RNASeq and proteomics data comparing basal and luminal A breast cancer samples highlighted the known difference in proliferation and DNA damage response. Finally, we used these algorithms for a pan-cancer survival analysis. Our analysis clearly revealed prognosis-related pathways common to multiple cancer types or specific to individual cancer types, as well as pathways associated with prognosis in different directions in different cancer types. We implemented these two algorithms in an R package, Sumer, which generates tables and static and interactive plots for exploration and publication.","Graph algorithms for condensing and consolidating gene set analysis results. Gene set analysis plays a critical role in the functional interpretation of omics data. Although this is typically done for one omics experiment at a time, there is an increasing need to combine gene set analysis results from multiple experiments performed on the same or different omics platforms, such as in multi-omics studies. Integrating results from multiple experiments is challenging, and annotation redundancy between gene sets further obscures clear conclusions. We propose to use a weighted set cover algorithm to reduce redundancy of gene sets identified in a single experiment. Next, we use affinity propagation to consolidate similar gene sets identified from multiple experiments into clusters and to automatically determine the most representative gene set for each cluster. Using three examples from over representation analysis and gene set enrichment analysis, we showed that weighted set cover outperformed a previously published set cover method and reduced the number of gene sets by 52-77%. Focusing on overlapping genes between the list of input genes and the enriched gene sets in over-representation analysis and leading-edge genes in gene set enrichment analysis further reduced the number of gene sets. A use case combining enrichment analysis results from RNASeq and proteomics data comparing basal and luminal A breast cancer samples highlighted the known difference in proliferation and DNA damage response. Finally, we used these algorithms for a pan-cancer survival analysis. Our analysis clearly revealed prognosis-related pathways common to multiple cancer types or specific to individual cancer types, as well as pathways associated with prognosis in different directions in different cancer types. We implemented these two algorithms in an R package, Sumer, which generates tables and static and interactive plots for exploration and publication."
0,Principles for Integrative Structural Biology Studies,"Integrative structure determination is a powerful approach to modeling the structures of biological systems based on data produced by multiple experimental and theoretical methods, with implications for our understanding of cellular biology and drug discovery. This Primer introduces the theory and methods of integrative approaches, emphasizing the kinds of data that can be effectively included in developing models and using the nuclear pore complex as an example to illustrate the practice and challenges involved. These guidelines are intended to aid the researcher in understanding and applying integrative structural methods to systems of their interest and thus take advantage of this rapidly evolving field.","Principles for Integrative Structural Biology Studies. Integrative structure determination is a powerful approach to modeling the structures of biological systems based on data produced by multiple experimental and theoretical methods, with implications for our understanding of cellular biology and drug discovery. This Primer introduces the theory and methods of integrative approaches, emphasizing the kinds of data that can be effectively included in developing models and using the nuclear pore complex as an example to illustrate the practice and challenges involved. These guidelines are intended to aid the researcher in understanding and applying integrative structural methods to systems of their interest and thus take advantage of this rapidly evolving field."
0,Publisher Correction: Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network,"In the version of this article originally published, the x axis labels in Fig. 1a were incorrect. The labels originally were 'Specificity,' but should have been '1 - Specificity.' Also, the x axis label in Fig. 2b was incorrect. It was originally 'DNN predicted label,' but should have been 'Average cardiologist label.' The errors have been corrected in the PDF and HTML versions of this article.","Publisher Correction: Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network. In the version of this article originally published, the x axis labels in Fig. 1a were incorrect. The labels originally were 'Specificity,' but should have been '1 - Specificity.' Also, the x axis label in Fig. 2b was incorrect. It was originally 'DNN predicted label,' but should have been 'Average cardiologist label.' The errors have been corrected in the PDF and HTML versions of this article."
0,Molecular profiling of cancer patients enables personalized combination therapy: the I-PREDICT study,,
0,Retinal fingerprints for precision profiling of cardiovascular risk,,
0,"Applications of FLIKA, a Python-based image processing and analysis platform, for studying local events of cellular calcium signaling","The patterning of cytosolic Ca2+ signals underlies their ubiquitous ability to specifically regulate numerous cellular processes. Advances in fluorescence microscopy have made it possible to image these signals with unprecedented temporal and spatial resolution. However, this is a double-edged sword, as the resulting enormous data sets necessitate development of software to automate image processing and analysis. Here, we describe Flika, an open source, graphical user interface program written in the Python environment that contains a suite of built-in image processing tools to enable intuitive visualization of image data and analysis. We illustrate the utility and power of Flika by three applications for studying cellular Ca2+ signaling: a script for measuring single-cell global Ca2+ signals; a plugin for the detection, localization and analysis of subcellular Ca2+ puffs; and a script that implements a novel approach for fluctuation analysis of transient, local Ca2+ fluorescence signals. This article is part of a Special Issue entitled: ECS Meeting edited by Claus Heizmann, Joachim Krebs and Jacques Haiech.","Applications of FLIKA, a Python-based image processing and analysis platform, for studying local events of cellular calcium signaling. The patterning of cytosolic Ca2+ signals underlies their ubiquitous ability to specifically regulate numerous cellular processes. Advances in fluorescence microscopy have made it possible to image these signals with unprecedented temporal and spatial resolution. However, this is a double-edged sword, as the resulting enormous data sets necessitate development of software to automate image processing and analysis. Here, we describe Flika, an open source, graphical user interface program written in the Python environment that contains a suite of built-in image processing tools to enable intuitive visualization of image data and analysis. We illustrate the utility and power of Flika by three applications for studying cellular Ca2+ signaling: a script for measuring single-cell global Ca2+ signals; a plugin for the detection, localization and analysis of subcellular Ca2+ puffs; and a script that implements a novel approach for fluctuation analysis of transient, local Ca2+ fluorescence signals. This article is part of a Special Issue entitled: ECS Meeting edited by Claus Heizmann, Joachim Krebs and Jacques Haiech."
0,Bidirectional Control of Autophagy by BECN1 BARA Domain Dynamics,"Membrane targeting of the BECN1-containing class III PI 3-kinase (PI3KC3) complexes is pivotal to the regulation of autophagy. The interaction of PI3KC3 complex II and its ubiquitously expressed inhibitor, Rubicon, was mapped to the first Î² sheet of the BECN1 BARA domain and the UVRAG BARA2 domain by hydrogen-deuterium exchange and cryo-EM. These data suggest that the BARA Î² sheet 1 unfolds to directly engage the membrane. This mechanism was confirmed using protein engineering, giant unilamellar vesicle assays, and molecular simulations. Using this mechanism, a BECN1 Î² sheet-1 derived peptide activates both PI3KC3 complexes I and II, while HIV-1 Nef inhibits complex II. These data reveal how BECN1 switches on and off PI3KC3 binding to membranes. The observations explain how PI3KC3 inhibition by Rubicon, activation by autophagy-inducing BECN1 peptides, and inhibition by HIV-1 Nef are mediated by the switchable ability of the BECN1 BARA domain to partially unfold and insert into membranes.","Bidirectional Control of Autophagy by BECN1 BARA Domain Dynamics. Membrane targeting of the BECN1-containing class III PI 3-kinase (PI3KC3) complexes is pivotal to the regulation of autophagy. The interaction of PI3KC3 complex II and its ubiquitously expressed inhibitor, Rubicon, was mapped to the first Î² sheet of the BECN1 BARA domain and the UVRAG BARA2 domain by hydrogen-deuterium exchange and cryo-EM. These data suggest that the BARA Î² sheet 1 unfolds to directly engage the membrane. This mechanism was confirmed using protein engineering, giant unilamellar vesicle assays, and molecular simulations. Using this mechanism, a BECN1 Î² sheet-1 derived peptide activates both PI3KC3 complexes I and II, while HIV-1 Nef inhibits complex II. These data reveal how BECN1 switches on and off PI3KC3 binding to membranes. The observations explain how PI3KC3 inhibition by Rubicon, activation by autophagy-inducing BECN1 peptides, and inhibition by HIV-1 Nef are mediated by the switchable ability of the BECN1 BARA domain to partially unfold and insert into membranes."
0,Human 3D cellular model of hypoxic brain injury of prematurity,,
0,"Dolutegravir plus lamivudine versus dolutegravir plus tenofovir disoproxil fumarate and emtricitabine in antiretroviral-naive adults with HIV-1 infection (GEMINI-1 and GEMINI-2): week 48 results from two multicentre, double-blind, randomised, non-inferiority, phase 3 trials",,
0,Gliomasphere marker combinatorics: multidimensional flow cytometry detects CD44+/CD133+/ITGA6+/CD36+ signature,"Glioblastoma is the most dangerous brain cancer. One reason for glioblastoma's aggressiveness are glioblastoma stem-like cells. To target them, a number of markers have been proposed (CD133, CD44, CD15, A2B5, CD36, CXCR4, IL6R, L1CAM, and ITGA6). A comprehensive study of co-expression patterns of them has, however, not been performed so far. Here, we mapped the multidimensional co-expression profile of these stemness-associated molecules. Gliomaspheres â€“ an established model of glioblastoma stem-like cells â€“ were used. Seven different gliomasphere systems were subjected to multicolor flow cytometry measuring the nine markers CD133, CD44, CD15, A2B5, CD36, CXCR4, IL6R, L1CAM, and ITGA6 all simultaneously based on a novel 9-marker multicolor panel developed for this study. The viSNE dimensionality reduction algorithm was applied for analysis. All gliomaspheres were found to express at least five different glioblastoma stem-like cell markers. Multi-dimensional analysis showed that all studied gliomaspheres consistently harbored a cell population positive for the molecular signature CD44+/CD133+/ITGA6+/CD36+. Glioblastoma patients with an enrichment of this combination had a significantly worse survival outcome when analyzing the two largest available The Cancer Genome Atlas datasets (MIT/Harvard Affymetrix: PÂ =Â 0.0015, University of North Carolina Agilent: PÂ =Â 0.0322). In sum, we detected a previously unknown marker combination â€“ demonstrating feasibility, usefulness, and importance of high-dimensional gliomasphere marker combinatorics.","Gliomasphere marker combinatorics: multidimensional flow cytometry detects CD44+/CD133+/ITGA6+/CD36+ signature. Glioblastoma is the most dangerous brain cancer. One reason for glioblastoma's aggressiveness are glioblastoma stem-like cells. To target them, a number of markers have been proposed (CD133, CD44, CD15, A2B5, CD36, CXCR4, IL6R, L1CAM, and ITGA6). A comprehensive study of co-expression patterns of them has, however, not been performed so far. Here, we mapped the multidimensional co-expression profile of these stemness-associated molecules. Gliomaspheres â€“ an established model of glioblastoma stem-like cells â€“ were used. Seven different gliomasphere systems were subjected to multicolor flow cytometry measuring the nine markers CD133, CD44, CD15, A2B5, CD36, CXCR4, IL6R, L1CAM, and ITGA6 all simultaneously based on a novel 9-marker multicolor panel developed for this study. The viSNE dimensionality reduction algorithm was applied for analysis. All gliomaspheres were found to express at least five different glioblastoma stem-like cell markers. Multi-dimensional analysis showed that all studied gliomaspheres consistently harbored a cell population positive for the molecular signature CD44+/CD133+/ITGA6+/CD36+. Glioblastoma patients with an enrichment of this combination had a significantly worse survival outcome when analyzing the two largest available The Cancer Genome Atlas datasets (MIT/Harvard Affymetrix: PÂ =Â 0.0015, University of North Carolina Agilent: PÂ =Â 0.0322). In sum, we detected a previously unknown marker combination â€“ demonstrating feasibility, usefulness, and importance of high-dimensional gliomasphere marker combinatorics."
0,First-trimester maternal serum alpha fetoprotein is associated with ischemic placental disease,,
0,"FA-97, a New Synthetic Caffeic Acid Phenethyl Ester Derivative, Protects against Oxidative Stress-Mediated Neuronal Cell Apoptosis and Scopolamine-Induced Cognitive Impairment by Activating Nrf2/HO-1 Signaling","Alzheimer's disease (AD) is an age-related neurodegenerative disorder with cognitive deficits, which is becoming markedly more common in the world. Currently, the exact cause of AD is still unclear, and no curative therapy is available for preventing or mitigating the disease progression. Caffeic acid phenethyl ester (CAPE), a natural phenolic compound derived from honeybee hive propolis, has been reported as a potential therapeutic agent against AD, while its application is limited due to the low water solubility and poor bioavailability. Here, caffeic acid phenethyl ester 4-O-glucoside (FA-97) is synthesized. We validate that FA-97 attenuates H2O2-induced apoptosis in SH-SY5Y and PC12 cells and suppresses H2O2-induced oxidative stress by inhibiting the ROS level, malondialdehyde (MDA) level, and protein carbonylation level, as well as induces cellular glutathione (GSH) and superoxide dismutase (SOD). Mechanistically, FA-97 promotes the nuclear translocation and transcriptional activity of Nrf2 associated with the upregulated expression of HO-1 and NQO-1. The prime importance of Nrf2 activation in the neuroprotective and antioxidant effects of FA-97 is verified by Nrf2 siRNA transfection. In addition, FA-97 prevents scopolamine- (SCOP-) induced learning and memory impairments in vivo via reducing neuronal apoptosis and protecting against cholinergic system dysfunction in the hippocampus and cortex. Moreover, the increased MDA level and low total antioxidant capacity in SCOP-treated mouse brains are reversed by FA-97, with the increased expression of HO-1, NQO-1, and nuclear Nrf2. In conclusion, FA-97 protects against oxidative stress-mediated neuronal cell apoptosis and SCOP-induced cognitive impairment by activating Nrf2/HO-1 signaling, which might be developed as a therapeutic drug for AD.","FA-97, a New Synthetic Caffeic Acid Phenethyl Ester Derivative, Protects against Oxidative Stress-Mediated Neuronal Cell Apoptosis and Scopolamine-Induced Cognitive Impairment by Activating Nrf2/HO-1 Signaling. Alzheimer's disease (AD) is an age-related neurodegenerative disorder with cognitive deficits, which is becoming markedly more common in the world. Currently, the exact cause of AD is still unclear, and no curative therapy is available for preventing or mitigating the disease progression. Caffeic acid phenethyl ester (CAPE), a natural phenolic compound derived from honeybee hive propolis, has been reported as a potential therapeutic agent against AD, while its application is limited due to the low water solubility and poor bioavailability. Here, caffeic acid phenethyl ester 4-O-glucoside (FA-97) is synthesized. We validate that FA-97 attenuates H2O2-induced apoptosis in SH-SY5Y and PC12 cells and suppresses H2O2-induced oxidative stress by inhibiting the ROS level, malondialdehyde (MDA) level, and protein carbonylation level, as well as induces cellular glutathione (GSH) and superoxide dismutase (SOD). Mechanistically, FA-97 promotes the nuclear translocation and transcriptional activity of Nrf2 associated with the upregulated expression of HO-1 and NQO-1. The prime importance of Nrf2 activation in the neuroprotective and antioxidant effects of FA-97 is verified by Nrf2 siRNA transfection. In addition, FA-97 prevents scopolamine- (SCOP-) induced learning and memory impairments in vivo via reducing neuronal apoptosis and protecting against cholinergic system dysfunction in the hippocampus and cortex. Moreover, the increased MDA level and low total antioxidant capacity in SCOP-treated mouse brains are reversed by FA-97, with the increased expression of HO-1, NQO-1, and nuclear Nrf2. In conclusion, FA-97 protects against oxidative stress-mediated neuronal cell apoptosis and SCOP-induced cognitive impairment by activating Nrf2/HO-1 signaling, which might be developed as a therapeutic drug for AD."
0,New approaches for detecting cancer with circulating cell-free DNA,,
0,Comparative genome-scale metabolic modeling of metallo-beta-lactamase-producing multidrug-resistant klebsiella pneumoniae clinical isolates,"The emergence and spread of metallo-beta-lactamase-producing multidrug-resistant (MDR) Klebsiella pneumoniae is a serious public health threat, which is further complicated by the increased prevalence of colistin resistance. The link between antimicrobial resistance acquired by strains of Klebsiella and their unique metabolic capabilities has not been determined. Here, we reconstruct genome-scale metabolic models for 22 K. pneumoniae strains with various resistance profiles to different antibiotics, including two strains exhibiting colistin resistance isolated from Cairo, Egypt. We use the models to predict growth capabilities on 265 different sole carbon, nitrogen, sulfur, and phosphorus sources for all 22 strains. Alternate nitrogen source utilization of glutamate, arginine, histidine, and ethanolamine among others provided discriminatory power for identifying resistance to amikacin, tetracycline, and gentamicin. Thus, genome-scale model based predictions of growth capabilities on alternative substrates may lead to construction of classification trees that are indicative of antibiotic resistance in Klebsiella isolates.","Comparative genome-scale metabolic modeling of metallo-beta-lactamase-producing multidrug-resistant klebsiella pneumoniae clinical isolates. The emergence and spread of metallo-beta-lactamase-producing multidrug-resistant (MDR) Klebsiella pneumoniae is a serious public health threat, which is further complicated by the increased prevalence of colistin resistance. The link between antimicrobial resistance acquired by strains of Klebsiella and their unique metabolic capabilities has not been determined. Here, we reconstruct genome-scale metabolic models for 22 K. pneumoniae strains with various resistance profiles to different antibiotics, including two strains exhibiting colistin resistance isolated from Cairo, Egypt. We use the models to predict growth capabilities on 265 different sole carbon, nitrogen, sulfur, and phosphorus sources for all 22 strains. Alternate nitrogen source utilization of glutamate, arginine, histidine, and ethanolamine among others provided discriminatory power for identifying resistance to amikacin, tetracycline, and gentamicin. Thus, genome-scale model based predictions of growth capabilities on alternative substrates may lead to construction of classification trees that are indicative of antibiotic resistance in Klebsiella isolates."
0,Substantial Cardiovascular Morbidity in Adults With Lower-Complexity Congenital Heart Disease,"BACKGROUND: Although lower-complexity cardiac malformations constitute the majority of adult congenital heart disease (ACHD), the long-term risks of adverse cardiovascular events and relationship with conventional risk factors in this population are poorly understood. We aimed to quantify the risk of adverse cardiovascular events associated with lower-complexity ACHD that is unmeasured by conventional risk factors. METHODS: A multitiered classification algorithm was used to select individuals with lower-complexity ACHD and individuals without ACHD for comparison among >500 000 British adults in the UK Biobank. ACHD diagnoses were subclassified as isolated aortic valve and noncomplex defects. Time-to-event analyses were conducted for the primary end points of fatal or nonfatal acute coronary syndrome, ischemic stroke, heart failure, and atrial fibrillation and a secondary combined end point for major adverse cardiovascular events. Maximum follow-up time for the study period was 22 years with retrospectively and prospectively collected data from the UK Biobank. RESULTS: We identified 2006 individuals with lower-complexity ACHD and 497 983 unexposed individuals in the UK Biobank (median age at enrollment, 58 [interquartile range, 51-63] years). Of the ACHD-exposed group, 59% were male, 51% were current or former smokers, 30% were obese, and 69%, 41%, and 7% were diagnosed or treated for hypertension, hyperlipidemia, and diabetes mellitus, respectively. After adjustment for 12 measured cardiovascular risk factors, ACHD remained strongly associated with the primary end points, with hazard ratios ranging from 2.0 (95% CI, 1.5-2.8; P<0.001) for acute coronary syndrome to 13.0 (95% CI, 9.4-18.1; P<0.001) for heart failure. ACHD-exposed individuals with </=2 cardiovascular risk factors had a 29% age-adjusted incidence rate of major adverse cardiovascular events, in contrast to 13% in individuals without ACHD with >/=5 risk factors. CONCLUSIONS: Individuals with lower-complexity ACHD had a higher burden of adverse cardiovascular events relative to the general population that was unaccounted for by conventional cardiovascular risk factors. These findings highlight the need for closer surveillance of patients with mild to moderate ACHD and further investigation into management and mechanisms of cardiovascular risk unique to this growing population of high-risk adults.","Substantial Cardiovascular Morbidity in Adults With Lower-Complexity Congenital Heart Disease. BACKGROUND: Although lower-complexity cardiac malformations constitute the majority of adult congenital heart disease (ACHD), the long-term risks of adverse cardiovascular events and relationship with conventional risk factors in this population are poorly understood. We aimed to quantify the risk of adverse cardiovascular events associated with lower-complexity ACHD that is unmeasured by conventional risk factors. METHODS: A multitiered classification algorithm was used to select individuals with lower-complexity ACHD and individuals without ACHD for comparison among >500 000 British adults in the UK Biobank. ACHD diagnoses were subclassified as isolated aortic valve and noncomplex defects. Time-to-event analyses were conducted for the primary end points of fatal or nonfatal acute coronary syndrome, ischemic stroke, heart failure, and atrial fibrillation and a secondary combined end point for major adverse cardiovascular events. Maximum follow-up time for the study period was 22 years with retrospectively and prospectively collected data from the UK Biobank. RESULTS: We identified 2006 individuals with lower-complexity ACHD and 497 983 unexposed individuals in the UK Biobank (median age at enrollment, 58 [interquartile range, 51-63] years). Of the ACHD-exposed group, 59% were male, 51% were current or former smokers, 30% were obese, and 69%, 41%, and 7% were diagnosed or treated for hypertension, hyperlipidemia, and diabetes mellitus, respectively. After adjustment for 12 measured cardiovascular risk factors, ACHD remained strongly associated with the primary end points, with hazard ratios ranging from 2.0 (95% CI, 1.5-2.8; P<0.001) for acute coronary syndrome to 13.0 (95% CI, 9.4-18.1; P<0.001) for heart failure. ACHD-exposed individuals with </=2 cardiovascular risk factors had a 29% age-adjusted incidence rate of major adverse cardiovascular events, in contrast to 13% in individuals without ACHD with >/=5 risk factors. CONCLUSIONS: Individuals with lower-complexity ACHD had a higher burden of adverse cardiovascular events relative to the general population that was unaccounted for by conventional cardiovascular risk factors. These findings highlight the need for closer surveillance of patients with mild to moderate ACHD and further investigation into management and mechanisms of cardiovascular risk unique to this growing population of high-risk adults."
0,Deceased Pediatric Donor Livers: How Current Policy Drives Allocation and Transplantation,,
0,Large-Scale Assessment of a Smartwatch to Identify Atrial Fibrillation,"BACKGROUND: Optical sensors on wearable devices can detect irregular pulses. The ability of a smartwatch application (app) to identify atrial fibrillation during typical use is unknown. METHODS: Participants without atrial fibrillation (as reported by the participants themselves) used a smartphone (Apple iPhone) app to consent to monitoring. If a smartwatch-based irregular pulse notification algorithm identified possible atrial fibrillation, a telemedicine visit was initiated and an electrocardiography (ECG) patch was mailed to the participant, to be worn for up to 7 days. Surveys were administered 90 days after notification of the irregular pulse and at the end of the study. The main objectives were to estimate the proportion of notified participants with atrial fibrillation shown on an ECG patch and the positive predictive value of irregular pulse intervals with a targeted confidence interval width of 0.10. RESULTS: We recruited 419,297 participants over 8 months. Over a median of 117 days of monitoring, 2161 participants (0.52%) received notifications of irregular pulse. Among the 450 participants who returned ECG patches containing data that could be analyzed - which had been applied, on average, 13 days after notification - atrial fibrillation was present in 34% (97.5% confidence interval [CI], 29 to 39) overall and in 35% (97.5% CI, 27 to 43) of participants 65 years of age or older. Among participants who were notified of an irregular pulse, the positive predictive value was 0.84 (95% CI, 0.76 to 0.92) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular pulse notification and 0.71 (97.5% CI, 0.69 to 0.74) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular tachogram. Of 1376 notified participants who returned a 90-day survey, 57% contacted health care providers outside the study. There were no reports of serious app-related adverse events. CONCLUSIONS: The probability of receiving an irregular pulse notification was low. Among participants who received notification of an irregular pulse, 34% had atrial fibrillation on subsequent ECG patch readings and 84% of notifications were concordant with atrial fibrillation. This siteless (no on-site visits were required for the participants), pragmatic study design provides a foundation for large-scale pragmatic studies in which outcomes or adherence can be reliably assessed with user-owned devices. (Funded by Apple; Apple Heart Study ClinicalTrials.gov number, NCT03335800.).","Large-Scale Assessment of a Smartwatch to Identify Atrial Fibrillation. BACKGROUND: Optical sensors on wearable devices can detect irregular pulses. The ability of a smartwatch application (app) to identify atrial fibrillation during typical use is unknown. METHODS: Participants without atrial fibrillation (as reported by the participants themselves) used a smartphone (Apple iPhone) app to consent to monitoring. If a smartwatch-based irregular pulse notification algorithm identified possible atrial fibrillation, a telemedicine visit was initiated and an electrocardiography (ECG) patch was mailed to the participant, to be worn for up to 7 days. Surveys were administered 90 days after notification of the irregular pulse and at the end of the study. The main objectives were to estimate the proportion of notified participants with atrial fibrillation shown on an ECG patch and the positive predictive value of irregular pulse intervals with a targeted confidence interval width of 0.10. RESULTS: We recruited 419,297 participants over 8 months. Over a median of 117 days of monitoring, 2161 participants (0.52%) received notifications of irregular pulse. Among the 450 participants who returned ECG patches containing data that could be analyzed - which had been applied, on average, 13 days after notification - atrial fibrillation was present in 34% (97.5% confidence interval [CI], 29 to 39) overall and in 35% (97.5% CI, 27 to 43) of participants 65 years of age or older. Among participants who were notified of an irregular pulse, the positive predictive value was 0.84 (95% CI, 0.76 to 0.92) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular pulse notification and 0.71 (97.5% CI, 0.69 to 0.74) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular tachogram. Of 1376 notified participants who returned a 90-day survey, 57% contacted health care providers outside the study. There were no reports of serious app-related adverse events. CONCLUSIONS: The probability of receiving an irregular pulse notification was low. Among participants who received notification of an irregular pulse, 34% had atrial fibrillation on subsequent ECG patch readings and 84% of notifications were concordant with atrial fibrillation. This siteless (no on-site visits were required for the participants), pragmatic study design provides a foundation for large-scale pragmatic studies in which outcomes or adherence can be reliably assessed with user-owned devices. (Funded by Apple; Apple Heart Study ClinicalTrials.gov number, NCT03335800.)."
0,Promising radiotherapy classifier for early breast cancer,,
0,Using second-person neuroscience to elucidate the mechanisms of social interaction,,
0,Can activity monitors predict outcomes in patients with heart failure? A systematic review,,
0,Sensitivity and specificity of an algorithm based on medico-administrative data to identify hospitalized patients with major bleeding presenting to an emergency department,"BACKGROUND: Validation studies on an ICD-10-based algorithm to identify major bleeding events are scarce, and mostly focused on positive predictive values. OBJECTIVE: To evaluate the sensitivity and specificity of an ICD-10-based algorithm in adult patients referred to hospital. METHODS: This was a cross-sectional, retrospective analysis. Among all hospital stays of adult patients referred to Rennes University Hospital, France, through the emergency ward in 2014, we identified major bleeding events according to an index test based on a list of ICD-10 diagnoses. As a reference, a two-step process was applied: firstly, a computerized request for electronic health records from the emergency ward, using several hemorrhage-related diagnostic codes and specific emergency therapies so as to discard stays with a very low probability of bleeding; secondly, a chart review of selected records was conducted by a medical expert blinded to the index test results and each hospital stay was classified into one of two exclusive categories: major bleeding or no major bleeding, according to pre-specified criteria. RESULTS: Out of 16,012 hospital stays, the reference identified 736 major bleeding events and left 15,276 stays considered as without the target condition. The index test identified 637 bleeding events: 293 intracranial hemorrhages, 197 gastrointestinal hemorrhages and 147 other bleeding events. Overall, sensitivity was 65% (95%CI, 62 to 69), and specificity was 99.0%. We observed differential sensitivity and specificity across bleeding types, with the highest values for intracranial hemorrhage. Positive predictive values ranged from 59% for ""other"" bleeding events, to 71% (95%CI, 65 to 78) for gastrointestinal hemorrhage, and 96% for intracranial hemorrhage. CONCLUSIONS: Low sensitivity and differential measures of accuracy across bleeding types support the need for specific data collection and medical validation rather than using an ICD-10-based algorithm for assessing the incidence of major bleeding.","Sensitivity and specificity of an algorithm based on medico-administrative data to identify hospitalized patients with major bleeding presenting to an emergency department. BACKGROUND: Validation studies on an ICD-10-based algorithm to identify major bleeding events are scarce, and mostly focused on positive predictive values. OBJECTIVE: To evaluate the sensitivity and specificity of an ICD-10-based algorithm in adult patients referred to hospital. METHODS: This was a cross-sectional, retrospective analysis. Among all hospital stays of adult patients referred to Rennes University Hospital, France, through the emergency ward in 2014, we identified major bleeding events according to an index test based on a list of ICD-10 diagnoses. As a reference, a two-step process was applied: firstly, a computerized request for electronic health records from the emergency ward, using several hemorrhage-related diagnostic codes and specific emergency therapies so as to discard stays with a very low probability of bleeding; secondly, a chart review of selected records was conducted by a medical expert blinded to the index test results and each hospital stay was classified into one of two exclusive categories: major bleeding or no major bleeding, according to pre-specified criteria. RESULTS: Out of 16,012 hospital stays, the reference identified 736 major bleeding events and left 15,276 stays considered as without the target condition. The index test identified 637 bleeding events: 293 intracranial hemorrhages, 197 gastrointestinal hemorrhages and 147 other bleeding events. Overall, sensitivity was 65% (95%CI, 62 to 69), and specificity was 99.0%. We observed differential sensitivity and specificity across bleeding types, with the highest values for intracranial hemorrhage. Positive predictive values ranged from 59% for ""other"" bleeding events, to 71% (95%CI, 65 to 78) for gastrointestinal hemorrhage, and 96% for intracranial hemorrhage. CONCLUSIONS: Low sensitivity and differential measures of accuracy across bleeding types support the need for specific data collection and medical validation rather than using an ICD-10-based algorithm for assessing the incidence of major bleeding."
0,Identification of Cancer-associated metabolic vulnerabilities by modeling multi-objective optimality in metabolism,"Background: Cancer cells undergo global reprogramming of cellular metabolism to satisfy demands of energy and biomass during proliferation and metastasis. Computational modeling of genome-scale metabolic models is an effective approach for designing new therapeutics targeting dysregulated cancer metabolism by identifying metabolic enzymes crucial for satisfying metabolic goals of cancer cells, but nearly all previous studies neglect the existence of metabolic demands other than biomass synthesis and trade-offs between these contradicting metabolic demands. It is thus necessary to develop computational models covering multiple metabolic objectives to study cancer metabolism and identify novel metabolic targets. Methods: We developed a multi-objective optimization model for cancer cell metabolism at genome-scale and an integrated, data-driven workflow for analyzing the Pareto optimality of this model in achieving multiple metabolic goals and identifying metabolic enzymes crucial for maintaining cancer-associated metabolic phenotypes. Using this workflow, we constructed cell line-specific models for a panel of cancer cell lines and identified lists of metabolic targets promoting or suppressing cancer cell proliferation or the Warburg Effect. The targets were then validated using knockdown and over-expression experiments in cultured cancer cell lines. Results: We found that the multi-objective optimization model correctly predicted phenotypes including cell growth rates, essentiality of metabolic genes and cell line specific sensitivities to metabolic perturbations. To our surprise, metabolic enzymes promoting proliferation substantially overlapped with those suppressing the Warburg Effect, suggesting that simply targeting the overlapping enzymes may lead to complicated outcomes. We also identified lists of metabolic enzymes important for maintaining rapid proliferation or high Warburg Effect while having little effect on the other. The importance of these enzymes in cancer metabolism predicted by the model was validated by their association with cancer patient survival and knockdown and overexpression experiments in a variety of cancer cell lines. Conclusions: These results confirm this multi-objective optimization model as a novel and effective approach for studying trade-off between metabolic demands of cancer cells and identifying cancer-associated metabolic vulnerabilities, and suggest novel metabolic targets for cancer treatment. Graphical abstract: [Figure not available: See fulltext.]","Identification of Cancer-associated metabolic vulnerabilities by modeling multi-objective optimality in metabolism. Background: Cancer cells undergo global reprogramming of cellular metabolism to satisfy demands of energy and biomass during proliferation and metastasis. Computational modeling of genome-scale metabolic models is an effective approach for designing new therapeutics targeting dysregulated cancer metabolism by identifying metabolic enzymes crucial for satisfying metabolic goals of cancer cells, but nearly all previous studies neglect the existence of metabolic demands other than biomass synthesis and trade-offs between these contradicting metabolic demands. It is thus necessary to develop computational models covering multiple metabolic objectives to study cancer metabolism and identify novel metabolic targets. Methods: We developed a multi-objective optimization model for cancer cell metabolism at genome-scale and an integrated, data-driven workflow for analyzing the Pareto optimality of this model in achieving multiple metabolic goals and identifying metabolic enzymes crucial for maintaining cancer-associated metabolic phenotypes. Using this workflow, we constructed cell line-specific models for a panel of cancer cell lines and identified lists of metabolic targets promoting or suppressing cancer cell proliferation or the Warburg Effect. The targets were then validated using knockdown and over-expression experiments in cultured cancer cell lines. Results: We found that the multi-objective optimization model correctly predicted phenotypes including cell growth rates, essentiality of metabolic genes and cell line specific sensitivities to metabolic perturbations. To our surprise, metabolic enzymes promoting proliferation substantially overlapped with those suppressing the Warburg Effect, suggesting that simply targeting the overlapping enzymes may lead to complicated outcomes. We also identified lists of metabolic enzymes important for maintaining rapid proliferation or high Warburg Effect while having little effect on the other. The importance of these enzymes in cancer metabolism predicted by the model was validated by their association with cancer patient survival and knockdown and overexpression experiments in a variety of cancer cell lines. Conclusions: These results confirm this multi-objective optimization model as a novel and effective approach for studying trade-off between metabolic demands of cancer cells and identifying cancer-associated metabolic vulnerabilities, and suggest novel metabolic targets for cancer treatment. Graphical abstract: [Figure not available: See fulltext.]"
0,Deep learning for cardiovascular medicine: a practical primer,"Deep learning (DL) is a branch of machine learning (ML) showing increasing promise in medicine, to assist in data classification, novel disease phenotyping and complex decision making. Deep learning is a form of ML typically implemented via multi-layered neural networks. Deep learning has accelerated by recent advances in computer hardware and algorithms and is increasingly applied in e-commerce, finance, and voice and image recognition to learn and classify complex datasets. The current medical literature shows both strengths and limitations of DL. Strengths of DL include its ability to automate medical image interpretation, enhance clinical decision-making, identify novel phenotypes, and select better treatment pathways in complex diseases. Deep learning may be well-suited to cardiovascular medicine in which haemodynamic and electrophysiological indices are increasingly captured on a continuous basis by wearable devices as well as image segmentation in cardiac imaging. However, DL also has significant weaknesses including difficulties in interpreting its models (the 'black-box' criticism), its need for extensive adjudicated ('labelled') data in training, lack of standardization in design, lack of data-efficiency in training, limited applicability to clinical trials, and other factors. Thus, the optimal clinical application of DL requires careful formulation of solvable problems, selection of most appropriate DL algorithms and data, and balanced interpretation of results. This review synthesizes the current state of DL for cardiovascular clinicians and investigators, and provides technical context to appreciate the promise, pitfalls, near-term challenges, and opportunities for this exciting new area.","Deep learning for cardiovascular medicine: a practical primer. Deep learning (DL) is a branch of machine learning (ML) showing increasing promise in medicine, to assist in data classification, novel disease phenotyping and complex decision making. Deep learning is a form of ML typically implemented via multi-layered neural networks. Deep learning has accelerated by recent advances in computer hardware and algorithms and is increasingly applied in e-commerce, finance, and voice and image recognition to learn and classify complex datasets. The current medical literature shows both strengths and limitations of DL. Strengths of DL include its ability to automate medical image interpretation, enhance clinical decision-making, identify novel phenotypes, and select better treatment pathways in complex diseases. Deep learning may be well-suited to cardiovascular medicine in which haemodynamic and electrophysiological indices are increasingly captured on a continuous basis by wearable devices as well as image segmentation in cardiac imaging. However, DL also has significant weaknesses including difficulties in interpreting its models (the 'black-box' criticism), its need for extensive adjudicated ('labelled') data in training, lack of standardization in design, lack of data-efficiency in training, limited applicability to clinical trials, and other factors. Thus, the optimal clinical application of DL requires careful formulation of solvable problems, selection of most appropriate DL algorithms and data, and balanced interpretation of results. This review synthesizes the current state of DL for cardiovascular clinicians and investigators, and provides technical context to appreciate the promise, pitfalls, near-term challenges, and opportunities for this exciting new area."
0,Neuronal network activity controls microglial process surveillance in awake mice via norepinephrine signaling,,
0,Activating alpha 4 beta 2 Nicotinic Acetylcholine Receptors Alleviates Fentanyl-induced Respiratory Depression in Rats,,
0,Incorporating adjustments for variability in control group response rates in network meta-analysis: a case study of biologics for rheumatoid arthritis,"BACKGROUND: The importance of adjusting for cross-study heterogeneity in control group response rates when conducting network meta-analyses (NMA) was demonstrated using a case study involving a comparison of biologics for the treatment of moderate-to-severe rheumatoid arthritis. METHODS: Bayesian NMAs were conducted for American College of Rheumatology (ACR) 50 treatment response based upon a set of randomized controlled trials (RCTs) identified by a recently completed systematic review of the literature. In addition to the performance of an unadjusted NMA, a model adjusting for cross-study heterogeneity of control group response rates using meta-regression was fit to the data. Model fit was evaluated, and findings from both analyses were compared with regard to clinical interpretations. RESULTS: ACR 50 response data from a total of 51 RCTs and 16,223 patients were analyzed. Inspection of cross-study variability in control group response rates identified considerable differences between studies. NMA incorporating adjustment for this variability was associated with an average change of 38.1% in the magnitude of the ORs between treatment comparisons, and over 64% of the odds ratio changed by 15% or more. Important changes in the clinical interpretations drawn from treatment comparisons were identified with this improved modeling approach. CONCLUSIONS: In comparing biologics for moderate to severe rheumatoid arthritis, failure to adjust for cross-trial differences in the control arm response rates in NMA can lead to biased estimates of comparative efficacy between treatments.","Incorporating adjustments for variability in control group response rates in network meta-analysis: a case study of biologics for rheumatoid arthritis. BACKGROUND: The importance of adjusting for cross-study heterogeneity in control group response rates when conducting network meta-analyses (NMA) was demonstrated using a case study involving a comparison of biologics for the treatment of moderate-to-severe rheumatoid arthritis. METHODS: Bayesian NMAs were conducted for American College of Rheumatology (ACR) 50 treatment response based upon a set of randomized controlled trials (RCTs) identified by a recently completed systematic review of the literature. In addition to the performance of an unadjusted NMA, a model adjusting for cross-study heterogeneity of control group response rates using meta-regression was fit to the data. Model fit was evaluated, and findings from both analyses were compared with regard to clinical interpretations. RESULTS: ACR 50 response data from a total of 51 RCTs and 16,223 patients were analyzed. Inspection of cross-study variability in control group response rates identified considerable differences between studies. NMA incorporating adjustment for this variability was associated with an average change of 38.1% in the magnitude of the ORs between treatment comparisons, and over 64% of the odds ratio changed by 15% or more. Important changes in the clinical interpretations drawn from treatment comparisons were identified with this improved modeling approach. CONCLUSIONS: In comparing biologics for moderate to severe rheumatoid arthritis, failure to adjust for cross-trial differences in the control arm response rates in NMA can lead to biased estimates of comparative efficacy between treatments."
0,State of the Art in Abdominal CT: The Limits of Iterative Reconstruction Algorithms,"The development and widespread adoption of iterative reconstruction (IR) algorithms for CT have greatly facilitated the contemporary practice of radiation dose reduction during abdominal CT examinations. IR mitigates the increased image noise typically associated with reduced radiation dose levels, thereby maintaining subjective image quality and diagnostic confidence for a variety of clinical tasks. Mounting evidence, however, points to important limitations of this method involving radiologists' ability to perform low-contrast diagnostic tasks, such as the detection of liver metastases or pancreatic masses. Radiologists need to be aware that use of IR can result in a decline of spatial resolution for low-contrast structures and degradation of low-contrast detectability when radiation dose reductions exceed approximately 25%. This article will review the principles of IR algorithm technology, describe the various commercial implementations of IR in CT, and review published studies that have evaluated the ability of IR to preserve diagnostic performance for low-contrast diagnostic tasks. In addition, future developments in CT noise reduction techniques and methods to rigorously evaluate their diagnostic performance will be discussed.","State of the Art in Abdominal CT: The Limits of Iterative Reconstruction Algorithms. The development and widespread adoption of iterative reconstruction (IR) algorithms for CT have greatly facilitated the contemporary practice of radiation dose reduction during abdominal CT examinations. IR mitigates the increased image noise typically associated with reduced radiation dose levels, thereby maintaining subjective image quality and diagnostic confidence for a variety of clinical tasks. Mounting evidence, however, points to important limitations of this method involving radiologists' ability to perform low-contrast diagnostic tasks, such as the detection of liver metastases or pancreatic masses. Radiologists need to be aware that use of IR can result in a decline of spatial resolution for low-contrast structures and degradation of low-contrast detectability when radiation dose reductions exceed approximately 25%. This article will review the principles of IR algorithm technology, describe the various commercial implementations of IR in CT, and review published studies that have evaluated the ability of IR to preserve diagnostic performance for low-contrast diagnostic tasks. In addition, future developments in CT noise reduction techniques and methods to rigorously evaluate their diagnostic performance will be discussed."
0,Evolving Images for Visual Neurons Using a Deep Generative Network Reveals Coding Principles and Neuronal Preferences,"What specific features should visual neurons encode, given the infinity of real-world images and the limited number of neurons available to represent them? We investigated neuronal selectivity in monkey inferotemporal cortex via the vast hypothesis space of a generative deep neural network, avoiding assumptions about features or semantic categories. A genetic algorithm searched this space for stimuli that maximized neuronal firing. This led to the evolution of rich synthetic images of objects with complex combinations of shapes, colors, and textures, sometimes resembling animals or familiar people, other times revealing novel patterns that did not map to any clear semantic category. These results expand our conception of the dictionary of features encoded in the cortex, and the approach can potentially reveal the internal representations of any system whose input can be captured by a generative model.","Evolving Images for Visual Neurons Using a Deep Generative Network Reveals Coding Principles and Neuronal Preferences. What specific features should visual neurons encode, given the infinity of real-world images and the limited number of neurons available to represent them? We investigated neuronal selectivity in monkey inferotemporal cortex via the vast hypothesis space of a generative deep neural network, avoiding assumptions about features or semantic categories. A genetic algorithm searched this space for stimuli that maximized neuronal firing. This led to the evolution of rich synthetic images of objects with complex combinations of shapes, colors, and textures, sometimes resembling animals or familiar people, other times revealing novel patterns that did not map to any clear semantic category. These results expand our conception of the dictionary of features encoded in the cortex, and the approach can potentially reveal the internal representations of any system whose input can be captured by a generative model."
0,Inhibition of PI3K/Akt/NF-ÎºB signaling with leonurine for ameliorating the progression of osteoarthritis: In vitro and in vivo studies,"Osteoarthritis (OA) is characterized as the degeneration and destruction of articular cartilage. In recent decades, leonurine (LN), the main active component in medical and edible dual purpose plant Herba Leonuri, has been shown associated with potent anti-inflammatory effects in several diseases. In the current study, we examined the protective effects of LN in the inhibition of OA development as well as its underlying mechanism both in vitro and in vivo experiments. In vitro, interleukin-1 beta (IL-1Î²) induced over-production of prostaglandin E2, nitric oxide, inducible nitric oxide synthase, cyclooxygenase-2, interleukin-6 and tumor necrosis factor alpha were all inhibited significantly by the pretreatment of LN at a dose-dependent manner (5, 10, and 20 ÂµM). Moreover, the expression of thrombospondin motifs 5 (ADAMTS5) and metalloproteinase 13 (MMP13) was downregulated by LN. All these changes led to the IL-1Î² induced degradation of extracellular matrix. Mechanistically, the LN suppressed IL-1Î² induced activation of the PI3K/Akt/NF-ÎºB signaling pathway cascades. Meanwhile, it was also demonstrated in our molecular docking studies that LN had strong binding abilities to PI3K. In addition, LN was observed exerting protective effects in a surgical induced model of OA. To sum up, this study indicated LN could be applied as a promising therapeutic agent in the treatment of OA.","Inhibition of PI3K/Akt/NF-ÎºB signaling with leonurine for ameliorating the progression of osteoarthritis: In vitro and in vivo studies. Osteoarthritis (OA) is characterized as the degeneration and destruction of articular cartilage. In recent decades, leonurine (LN), the main active component in medical and edible dual purpose plant Herba Leonuri, has been shown associated with potent anti-inflammatory effects in several diseases. In the current study, we examined the protective effects of LN in the inhibition of OA development as well as its underlying mechanism both in vitro and in vivo experiments. In vitro, interleukin-1 beta (IL-1Î²) induced over-production of prostaglandin E2, nitric oxide, inducible nitric oxide synthase, cyclooxygenase-2, interleukin-6 and tumor necrosis factor alpha were all inhibited significantly by the pretreatment of LN at a dose-dependent manner (5, 10, and 20 ÂµM). Moreover, the expression of thrombospondin motifs 5 (ADAMTS5) and metalloproteinase 13 (MMP13) was downregulated by LN. All these changes led to the IL-1Î² induced degradation of extracellular matrix. Mechanistically, the LN suppressed IL-1Î² induced activation of the PI3K/Akt/NF-ÎºB signaling pathway cascades. Meanwhile, it was also demonstrated in our molecular docking studies that LN had strong binding abilities to PI3K. In addition, LN was observed exerting protective effects in a surgical induced model of OA. To sum up, this study indicated LN could be applied as a promising therapeutic agent in the treatment of OA."
0,Association of Early Interventions with Birth Outcomes and Child Linear Growth in Low-Income and Middle-Income Countries: Bayesian Network Meta-analyses of Randomized Clinical Trials,"Importance: The first 1000 days of life represent a critical window for child development. Pregnancy, exclusive breastfeeding (EBF) period (0-6 months), and complementary feeding (CF) period (6-24 months) have different growth requirements, so separate considerations for intervention strategies are needed. No synthesis to date has attempted to quantify the associations of interventions under multiple domains of micronutrient and balanced energy protein and food supplements, deworming, maternal education, water sanitation, and hygiene across these 3 life periods with birth and growth outcomes. Objective: To determine the magnitude of association of interventions with birth and growth outcomes based on randomized clinical trials (RCTs) conducted in low-income and middle-income countries (LMICs) using Bayesian network meta-analyses. Data Sources: MEDLINE, Embase, and Cochrane databases were searched from their inception up to August 14, 2018. Study Selection: Included were LMIC-based RCTs of interventions provided to pregnant women, infants (0-6 months), and children (6-24 months). Data Extraction and Synthesis: Two independent reviewers used a standardized data extraction and quality assessment form. Random-effects network meta-analyses were performed for each life period. Effect sizes are reported as odds ratios (ORs) and mean differences (MeanDiffs) for dichotomous and continuous outcomes, with 95% credible intervals (CrIs). This study calculated probabilities of interventions being superior to standard of care by at least a minimal clinically important difference. Main Outcomes and Measures: The study compared ORs on preterm birth and MeanDiffs on birth weight for pregnancy, length for age (LAZ) for EBF, and height for age (HAZ) for CF. Results: Among 302 061 participants in 169 randomized clinical trials, the network meta-analyses found several nutritional interventions that demonstrated greater association with improved birth and growth outcomes compared with standard of care. For instance, compared with standard of care, maternal supplements of multiple micronutrients showed reduced odds for preterm birth (OR, 0.54; 95% CrI, 0.27-0.97) and improved mean birth weight (MeanDiff, 0.08 kg; 95% CrI, 0.00-0.17 kg) but not LAZ during EBF (MeanDiff, -0.02; 95% CrI, -0.18 to 0.14). Supplementing infants and children with multiple micronutrients showed improved LAZ (MeanDiff, 0.20; 95% CrI, 0.03-0.35) and HAZ (MeanDiff, 0.14; 95% CrI, 0.02-0.25). The study found that pregnancy interventions generally had higher probabilities of a minimal clinically importance difference than the interventions for the EBF or CF in the first 1000 days of life. Conclusions and Relevance: These analyses highlight the importance of intervening early for child development, during pregnancy if possible. Results of this study suggest that there is a need to combine interventions from multiple domains and test for their effectiveness as a package..","Association of Early Interventions with Birth Outcomes and Child Linear Growth in Low-Income and Middle-Income Countries: Bayesian Network Meta-analyses of Randomized Clinical Trials. Importance: The first 1000 days of life represent a critical window for child development. Pregnancy, exclusive breastfeeding (EBF) period (0-6 months), and complementary feeding (CF) period (6-24 months) have different growth requirements, so separate considerations for intervention strategies are needed. No synthesis to date has attempted to quantify the associations of interventions under multiple domains of micronutrient and balanced energy protein and food supplements, deworming, maternal education, water sanitation, and hygiene across these 3 life periods with birth and growth outcomes. Objective: To determine the magnitude of association of interventions with birth and growth outcomes based on randomized clinical trials (RCTs) conducted in low-income and middle-income countries (LMICs) using Bayesian network meta-analyses. Data Sources: MEDLINE, Embase, and Cochrane databases were searched from their inception up to August 14, 2018. Study Selection: Included were LMIC-based RCTs of interventions provided to pregnant women, infants (0-6 months), and children (6-24 months). Data Extraction and Synthesis: Two independent reviewers used a standardized data extraction and quality assessment form. Random-effects network meta-analyses were performed for each life period. Effect sizes are reported as odds ratios (ORs) and mean differences (MeanDiffs) for dichotomous and continuous outcomes, with 95% credible intervals (CrIs). This study calculated probabilities of interventions being superior to standard of care by at least a minimal clinically important difference. Main Outcomes and Measures: The study compared ORs on preterm birth and MeanDiffs on birth weight for pregnancy, length for age (LAZ) for EBF, and height for age (HAZ) for CF. Results: Among 302 061 participants in 169 randomized clinical trials, the network meta-analyses found several nutritional interventions that demonstrated greater association with improved birth and growth outcomes compared with standard of care. For instance, compared with standard of care, maternal supplements of multiple micronutrients showed reduced odds for preterm birth (OR, 0.54; 95% CrI, 0.27-0.97) and improved mean birth weight (MeanDiff, 0.08 kg; 95% CrI, 0.00-0.17 kg) but not LAZ during EBF (MeanDiff, -0.02; 95% CrI, -0.18 to 0.14). Supplementing infants and children with multiple micronutrients showed improved LAZ (MeanDiff, 0.20; 95% CrI, 0.03-0.35) and HAZ (MeanDiff, 0.14; 95% CrI, 0.02-0.25). The study found that pregnancy interventions generally had higher probabilities of a minimal clinically importance difference than the interventions for the EBF or CF in the first 1000 days of life. Conclusions and Relevance: These analyses highlight the importance of intervening early for child development, during pregnancy if possible. Results of this study suggest that there is a need to combine interventions from multiple domains and test for their effectiveness as a package.."
0,"Eosinophil-guided corticosteroid therapy in patients admitted to hospital with COPD exacerbation (CORTICO-COP): a multicentre, randomised, controlled, open-label, non-inferiority trial",,
0,The power of three spatial dimensions,,
0,Satisfactory analgesia with minimal emesis in day surgeries: a randomised controlled trial of morphine versus hydromorphone,,
0,IFN-Î³ enhances cell-mediated cytotoxicity against keratinocytes via JAK2/STAT1 in lichen planus,"Lichen planus (LP) is a chronic debilitating inflammatory disease of unknown etiology affecting the skin, nails, and mucosa with no current FDA-approved treatments. It is histologically characterized by dense infiltration of T cells and epidermal keratinocyte apoptosis. Using global transcriptomic profiling of patient skin samples, we demonstrate that LP is characterized by a type II interferon (IFN) inflammatory response. The type II IFN, IFN-Î³, is demonstrated to prime keratinocytes and increase their susceptibility to CD8+ T cell-mediated cytotoxic responses through MHC class I induction in a coculture model. We show that this process is dependent on Janus kinase 2 (JAK2) and signal transducer and activator of transcription 1 (STAT1), but not JAK1 or STAT2 signaling. Last, using drug prediction algorithms, we identify JAK inhibitors as promising therapeutic agents in LP and demonstrate that the JAK1/2 inhibitor baricitinib fully protects keratinocytes against cell-mediated cytotoxic responses in vitro. In summary, this work elucidates the role and mechanisms of IFN-Î³ in LP pathogenesis and provides evidence for the therapeutic use of JAK inhibitors to limit cell-mediated cytotoxicity in patients with LP.","IFN-Î³ enhances cell-mediated cytotoxicity against keratinocytes via JAK2/STAT1 in lichen planus. Lichen planus (LP) is a chronic debilitating inflammatory disease of unknown etiology affecting the skin, nails, and mucosa with no current FDA-approved treatments. It is histologically characterized by dense infiltration of T cells and epidermal keratinocyte apoptosis. Using global transcriptomic profiling of patient skin samples, we demonstrate that LP is characterized by a type II interferon (IFN) inflammatory response. The type II IFN, IFN-Î³, is demonstrated to prime keratinocytes and increase their susceptibility to CD8+ T cell-mediated cytotoxic responses through MHC class I induction in a coculture model. We show that this process is dependent on Janus kinase 2 (JAK2) and signal transducer and activator of transcription 1 (STAT1), but not JAK1 or STAT2 signaling. Last, using drug prediction algorithms, we identify JAK inhibitors as promising therapeutic agents in LP and demonstrate that the JAK1/2 inhibitor baricitinib fully protects keratinocytes against cell-mediated cytotoxic responses in vitro. In summary, this work elucidates the role and mechanisms of IFN-Î³ in LP pathogenesis and provides evidence for the therapeutic use of JAK inhibitors to limit cell-mediated cytotoxicity in patients with LP."
0,Automated Algorithms and Retinopathy of Prematurity Treatment,,
0,A Prospective Study Identifying Predictive Factors of Cardiac Decompensation After Transjugular Intrahepatic Portosystemic Shunt: The Toulouse Algorithm,"BACKGROUND AND AIMS: Transjugular intrahepatic portosystemic shunt (TIPS) is now a standard for the treatment of portal hypertension-related complications. After the TIPS procedure, incidence and risk factors of cardiac decompensation are poorly known. The main objectives were to measure the incidence of the onset of cardiac decompensation after TIPS and identify the predictive factors. APPROACH AND RESULTS: All patients with cirrhosis treated with TIPS between May 2011 and June 2016 were considered for inclusion. They received a cardiac assessment by standard biological parameters, transthoracic echocardiography, and right heart catheterization. Patients were followed for 1 year after TIPS insertion. The main endpoint was the incidence of cardiac decompensation requiring hospitalization. One hundred seventy-four patients were treated by TIPS during the period. One hundred patients who underwent a complete cardiac evaluation were included. A cardiac decompensation occurred in 20% of the patients. The parameters associated with the occurrence of severe cardiac decompensation were a prolonged QT interval corrected (462 vs. 443 ms; P = 0.05), an elevated pre-TIPS brain natriuretic peptide (BNP) or N-terminal pro-brain natriuretic peptide (NT-proBNP) level, an elevated E/A ratio (1.5 vs. 1.0; P = 0.001) and E/e' ratio (11 vs. 7; P < 0.001), and a left atrial dilatation (40 vs. 29 mL/m(2) ; P = 0.011). The presence of aortic stenosis was also associated with cardiac decompensation. A level of BNP <40 pg/mL and NT-proBNP <125 pg/mL allowed identifying patients without risk of cardiac decompensation. Additionally, absence of diastolic dysfunction criteria at echocardiography ruled out the risk of further cardiac decompensation. CONCLUSIONS: Hospitalization for cardiac decompensation is observed in 20% of patients in the year after TIPS insertion. Combining BNP or NT-proBNP levels and echocardiographic parameters should help improve patient selection.","A Prospective Study Identifying Predictive Factors of Cardiac Decompensation After Transjugular Intrahepatic Portosystemic Shunt: The Toulouse Algorithm. BACKGROUND AND AIMS: Transjugular intrahepatic portosystemic shunt (TIPS) is now a standard for the treatment of portal hypertension-related complications. After the TIPS procedure, incidence and risk factors of cardiac decompensation are poorly known. The main objectives were to measure the incidence of the onset of cardiac decompensation after TIPS and identify the predictive factors. APPROACH AND RESULTS: All patients with cirrhosis treated with TIPS between May 2011 and June 2016 were considered for inclusion. They received a cardiac assessment by standard biological parameters, transthoracic echocardiography, and right heart catheterization. Patients were followed for 1 year after TIPS insertion. The main endpoint was the incidence of cardiac decompensation requiring hospitalization. One hundred seventy-four patients were treated by TIPS during the period. One hundred patients who underwent a complete cardiac evaluation were included. A cardiac decompensation occurred in 20% of the patients. The parameters associated with the occurrence of severe cardiac decompensation were a prolonged QT interval corrected (462 vs. 443 ms; P = 0.05), an elevated pre-TIPS brain natriuretic peptide (BNP) or N-terminal pro-brain natriuretic peptide (NT-proBNP) level, an elevated E/A ratio (1.5 vs. 1.0; P = 0.001) and E/e' ratio (11 vs. 7; P < 0.001), and a left atrial dilatation (40 vs. 29 mL/m(2) ; P = 0.011). The presence of aortic stenosis was also associated with cardiac decompensation. A level of BNP <40 pg/mL and NT-proBNP <125 pg/mL allowed identifying patients without risk of cardiac decompensation. Additionally, absence of diastolic dysfunction criteria at echocardiography ruled out the risk of further cardiac decompensation. CONCLUSIONS: Hospitalization for cardiac decompensation is observed in 20% of patients in the year after TIPS insertion. Combining BNP or NT-proBNP levels and echocardiographic parameters should help improve patient selection."
0,Deep learning and medical diagnosis - Authors' reply,,
0,Commentary: Position statement on augmented intelligence (AuI),,
0,Google's lung cancer AI: a promising tool that needs further validation,,
0,Matrine is a novel inhibitor of the TMEM16A chloride channel with antilung adenocarcinoma effects,"Calcium-activated chloride channels (CaCCs) are ion channels with key roles in physiological processes. They are abnormally expressed in various cancers, including esophageal squamous cell cancer, head and neck squamous cell carcinoma, colorectal cancer, and gastrointestinal stromal tumors. The CaCC component TMEM16A/ANO1 was recently shown to be overexpressed in lung adenocarcinoma cells and may serve as a tumorigenic protein. In this study, we determined that matrine is a potent TMEM16A inhibitor that exerts anti-lung adenocarcinoma effects. Patch clamp experiments showed that matrine inhibited TMEM16A current in a concentration-dependent manner with an IC 50 of 27.94 Â± 4.78 Î¼M. Molecular simulation and site-directed mutation experiments demonstrated that the matrine-sensitive sites of the TMEM16A channel involve the amino acids Y355, F411, and F415. Results of cell viability and wound healing assays showed that matrine significantly inhibited the proliferation and migration of LA795 cells, which exhibit high TMEM16A expression. In contrast, matrine has only weak inhibitory effect on CCD-19Lu and HeLa cells lacking TMEM16A expression. Matrine-induced effects on the proliferation and migration of LA795 cells were abrogated upon shRNA-mediated TMEM16A knockdown in LA795 cells. Finally, in vivo experiments demonstrated that matrine dramatically inhibited the growth of lung adenocarcinoma xenograft tumors in mice but did not affect mouse body weight. Collectively, these data indicate that matrine is an effective and safe TMEM16A inhibitor and that TMEM16A is the target of matrine anti-lung adenocarcinoma activity. These findings provide new insight for the development of novel treatments for lung adenocarcinoma.","Matrine is a novel inhibitor of the TMEM16A chloride channel with antilung adenocarcinoma effects. Calcium-activated chloride channels (CaCCs) are ion channels with key roles in physiological processes. They are abnormally expressed in various cancers, including esophageal squamous cell cancer, head and neck squamous cell carcinoma, colorectal cancer, and gastrointestinal stromal tumors. The CaCC component TMEM16A/ANO1 was recently shown to be overexpressed in lung adenocarcinoma cells and may serve as a tumorigenic protein. In this study, we determined that matrine is a potent TMEM16A inhibitor that exerts anti-lung adenocarcinoma effects. Patch clamp experiments showed that matrine inhibited TMEM16A current in a concentration-dependent manner with an IC 50 of 27.94 Â± 4.78 Î¼M. Molecular simulation and site-directed mutation experiments demonstrated that the matrine-sensitive sites of the TMEM16A channel involve the amino acids Y355, F411, and F415. Results of cell viability and wound healing assays showed that matrine significantly inhibited the proliferation and migration of LA795 cells, which exhibit high TMEM16A expression. In contrast, matrine has only weak inhibitory effect on CCD-19Lu and HeLa cells lacking TMEM16A expression. Matrine-induced effects on the proliferation and migration of LA795 cells were abrogated upon shRNA-mediated TMEM16A knockdown in LA795 cells. Finally, in vivo experiments demonstrated that matrine dramatically inhibited the growth of lung adenocarcinoma xenograft tumors in mice but did not affect mouse body weight. Collectively, these data indicate that matrine is an effective and safe TMEM16A inhibitor and that TMEM16A is the target of matrine anti-lung adenocarcinoma activity. These findings provide new insight for the development of novel treatments for lung adenocarcinoma."
0,Brain Structure and Function in School-Aged Children With Sluggish Cognitive Tempo Symptoms,,
0,Value of Molecular Classification for Prognostic Assessment of Adrenocortical Carcinoma,"Importance: The risk stratification of adrenocortical carcinoma (ACC) based on tumor proliferation index and stage is limited. Adjuvant therapy after surgery is recommended for most patients. Pan-genomic studies have identified distinct molecular groups closely associated with outcome. Objective: To compare the molecular classification for prognostic assessment of ACC with other known prognostic factors. Design, Setting, and Participants: In this retrospective biomarker analysis, ACC tumor samples from 368 patients who had undergone surgical tumor removal were collected from March 1, 2005, to September 30, 2015 (144 in the training cohort and 224 in the validation cohort) at 21 referral centers with a median follow-up of 35 months (interquartile range, 18-74 months). Data were analyzed from March 2016 to March 2018. Exposures: Meta-analysis of pan-genomic studies (transcriptome, methylome, chromosome alteration, and mutational profiles) was performed on the training cohort. Targeted biomarker analysis, including targeted gene expression (BUB1B and PINK1), targeted methylation (PAX5, GSTP1, PYCARD, and PAX6), and targeted next-generation sequencing, was performed on the training and validation cohorts. Main Outcomes and Measures: Disease-free survival. Cox proportional hazards regression and C indexes were used to assess the prognostic value of each model. Results: Of the 368 patients (mean [SD] age, 49 [16] years), 144 were in the training cohort (100 [69.4%] female) and 224 were in the validation cohort (142 [63.4%] female). In the training cohort, pan-genomic measures classified ACC into 3 molecular groups (A1, A2, and A3-B), with 5-year survival of 9% for group A1, 45% for group A2, and 82% for group A3-B (log-rank P <.001). Molecular class was an independent prognostic factor of recurrence in stage I to III ACC after complete surgery (hazard ratio, 55.91; 95% CI, 8.55-365.40; P <.001). The combination of European Network for the Study of Adrenal Tumors (ENSAT) stage, tumor proliferation index, and molecular class provided the most discriminant prognostic model (C index, 0.88). In the validation cohort, the molecular classification, determined by targeted biomarker measures, was confirmed as an independent prognostic factor of recurrence (hazard ratio, 5.96 [95% CI, 1.81-19.58], P =.003 for the targeted classifier combining expression, methylation, and chromosome alterations; and 2.61 [95% CI, 1.31-5.19], P =.006 for the targeted classifier combining methylation, chromosome alterations, and mutational profile). The prognostic value of the molecular markers was limited for patients with stage IV ACC. Conclusions and Relevance: The findings suggest that in localized ACC, targeted classifiers may be used as independent markers of recurrence. The determination of molecular class may improve individual prognostic assessment and thus may spare unnecessary adjuvant treatment.","Value of Molecular Classification for Prognostic Assessment of Adrenocortical Carcinoma. Importance: The risk stratification of adrenocortical carcinoma (ACC) based on tumor proliferation index and stage is limited. Adjuvant therapy after surgery is recommended for most patients. Pan-genomic studies have identified distinct molecular groups closely associated with outcome. Objective: To compare the molecular classification for prognostic assessment of ACC with other known prognostic factors. Design, Setting, and Participants: In this retrospective biomarker analysis, ACC tumor samples from 368 patients who had undergone surgical tumor removal were collected from March 1, 2005, to September 30, 2015 (144 in the training cohort and 224 in the validation cohort) at 21 referral centers with a median follow-up of 35 months (interquartile range, 18-74 months). Data were analyzed from March 2016 to March 2018. Exposures: Meta-analysis of pan-genomic studies (transcriptome, methylome, chromosome alteration, and mutational profiles) was performed on the training cohort. Targeted biomarker analysis, including targeted gene expression (BUB1B and PINK1), targeted methylation (PAX5, GSTP1, PYCARD, and PAX6), and targeted next-generation sequencing, was performed on the training and validation cohorts. Main Outcomes and Measures: Disease-free survival. Cox proportional hazards regression and C indexes were used to assess the prognostic value of each model. Results: Of the 368 patients (mean [SD] age, 49 [16] years), 144 were in the training cohort (100 [69.4%] female) and 224 were in the validation cohort (142 [63.4%] female). In the training cohort, pan-genomic measures classified ACC into 3 molecular groups (A1, A2, and A3-B), with 5-year survival of 9% for group A1, 45% for group A2, and 82% for group A3-B (log-rank P <.001). Molecular class was an independent prognostic factor of recurrence in stage I to III ACC after complete surgery (hazard ratio, 55.91; 95% CI, 8.55-365.40; P <.001). The combination of European Network for the Study of Adrenal Tumors (ENSAT) stage, tumor proliferation index, and molecular class provided the most discriminant prognostic model (C index, 0.88). In the validation cohort, the molecular classification, determined by targeted biomarker measures, was confirmed as an independent prognostic factor of recurrence (hazard ratio, 5.96 [95% CI, 1.81-19.58], P =.003 for the targeted classifier combining expression, methylation, and chromosome alterations; and 2.61 [95% CI, 1.31-5.19], P =.006 for the targeted classifier combining methylation, chromosome alterations, and mutational profile). The prognostic value of the molecular markers was limited for patients with stage IV ACC. Conclusions and Relevance: The findings suggest that in localized ACC, targeted classifiers may be used as independent markers of recurrence. The determination of molecular class may improve individual prognostic assessment and thus may spare unnecessary adjuvant treatment."
0,Modeling the safe minimum frequency of molecular monitoring for CML patients attempting treatment-free remission,,
0,"Efficient Golgi Forward Trafficking Requires GOLPH3-Driven, PI4P-Dependent Membrane Curvature","Vesicle budding for Golgi-to-plasma membrane trafficking is a key step in secretion. Proteins that induce curvature of the Golgi membrane are predicted to be required, by analogy to vesicle budding from other membranes. Here, we demonstrate that GOLPH3, upon binding to the phosphoinositide PI4P, induces curvature of synthetic membranes in vitro and the Golgi in cells. Moreover, efficient Golgi-to-plasma membrane trafficking critically depends on the ability of GOLPH3 to curve the Golgi membrane. Interestingly, uncoupling of GOLPH3 from its binding partner MYO18A results in extensive curvature of Golgi membranes, producing dramatic tubulation of the Golgi, but does not support forward trafficking. Thus, forward trafficking from the Golgi to the plasma membrane requires the ability of GOLPH3 both to induce Golgi membrane curvature and to recruit MYO18A. These data provide fundamental insight into the mechanism of Golgi trafficking and into the function of the unique Golgi secretory oncoproteins GOLPH3 and MYO18A.","Efficient Golgi Forward Trafficking Requires GOLPH3-Driven, PI4P-Dependent Membrane Curvature. Vesicle budding for Golgi-to-plasma membrane trafficking is a key step in secretion. Proteins that induce curvature of the Golgi membrane are predicted to be required, by analogy to vesicle budding from other membranes. Here, we demonstrate that GOLPH3, upon binding to the phosphoinositide PI4P, induces curvature of synthetic membranes in vitro and the Golgi in cells. Moreover, efficient Golgi-to-plasma membrane trafficking critically depends on the ability of GOLPH3 to curve the Golgi membrane. Interestingly, uncoupling of GOLPH3 from its binding partner MYO18A results in extensive curvature of Golgi membranes, producing dramatic tubulation of the Golgi, but does not support forward trafficking. Thus, forward trafficking from the Golgi to the plasma membrane requires the ability of GOLPH3 both to induce Golgi membrane curvature and to recruit MYO18A. These data provide fundamental insight into the mechanism of Golgi trafficking and into the function of the unique Golgi secretory oncoproteins GOLPH3 and MYO18A."
0,"Can AI Help Make Screening Mammography ""Lean""?",,
0,Comparative assessment of CNN architectures for classification of breast FNAC images,"Fine needle aspiration cytology (FNAC) entails using a narrow gauge (25-22 G) needle to collect a sample of a lesion for microscopic examination. It allows a minimally invasive, rapid diagnosis of tissue but does not preserve its histological architecture. FNAC is commonly used for diagnosis of breast cancer, with traditional practice being based on the subjective visual assessment of the breast cytopathology cell samples under a microscope to evaluate the state of various cytological features. Therefore, there are many challenges in maintaining consistency and reproducibility of findings. However, the advent of digital imaging and computational aid in diagnosis can improve the diagnostic accuracy and reduce the effective workload of pathologists. This paper presents a comparison of various deep convolutional neural network (CNN) based fine-tuned transfer learned classification approach for the diagnosis of the cell samples. The proposed approach has been tested using VGG16, VGG19, ResNet-50 and GoogLeNet-V3 (aka Inception V3) architectures of CNN on an image dataset of 212 images (99 benign and 113 malignant), later augmented and cleansed to 2120 images (990 benign and 1130 malignant), where the network was trained using images of 80% cell samples and tested on the rest. This paper presents a comparative assessment of the models giving a new dimension to FNAC study where GoogLeNet-V3 (fine-tuned) achieved an accuracy of 96.25% which is highly satisfactory.","Comparative assessment of CNN architectures for classification of breast FNAC images. Fine needle aspiration cytology (FNAC) entails using a narrow gauge (25-22 G) needle to collect a sample of a lesion for microscopic examination. It allows a minimally invasive, rapid diagnosis of tissue but does not preserve its histological architecture. FNAC is commonly used for diagnosis of breast cancer, with traditional practice being based on the subjective visual assessment of the breast cytopathology cell samples under a microscope to evaluate the state of various cytological features. Therefore, there are many challenges in maintaining consistency and reproducibility of findings. However, the advent of digital imaging and computational aid in diagnosis can improve the diagnostic accuracy and reduce the effective workload of pathologists. This paper presents a comparison of various deep convolutional neural network (CNN) based fine-tuned transfer learned classification approach for the diagnosis of the cell samples. The proposed approach has been tested using VGG16, VGG19, ResNet-50 and GoogLeNet-V3 (aka Inception V3) architectures of CNN on an image dataset of 212 images (99 benign and 113 malignant), later augmented and cleansed to 2120 images (990 benign and 1130 malignant), where the network was trained using images of 80% cell samples and tested on the rest. This paper presents a comparative assessment of the models giving a new dimension to FNAC study where GoogLeNet-V3 (fine-tuned) achieved an accuracy of 96.25% which is highly satisfactory."
0,Progress is an Iterative Process,,
0,Cumulative Radiation Exposures from CT Screening and Surveillance Strategies for von Hippel-Lindau-associated Solid Pancreatic Tumors,,
0,Grab recruitment by Rab27A-Rabphilin3a triggers Rab3A activation in human sperm exocytosis,"Sperm must undergo the regulated exocytosis of its dense core granule (the acrosome reaction, AR) to fertilize the egg. We have previously described that Rabs3 and 27 are organized in a RabGEF cascade within the signaling pathway elicited by exocytosis stimuli in human sperm. Here, we report the identity and the role of two molecules that link these secretory Rabs in the RabGEF cascade: Rabphilin3a and GRAB. Like Rab3 and Rab27, GRAB and Rabphilin3a are present, localize to the acrosomal region and are required for calcium-triggered exocytosis in human sperm. Sequestration of either protein with specific antibodies introduced into streptolysin O-permeabilized sperm impairs the activation of Rab3 in the acrosomal region elicited by calcium, but not that of Rab27. Biochemical and functional assays indicate that Rabphilin3a behaves as a Rab27 effector during the AR and that GRAB exhibits GEF activity toward Rab3A. Recombinant, active Rab27A pulls down Rabphilin3a and GRAB from human sperm extracts. Conversely, immobilized Rabphilin3a recruits Rab27 and GRAB; the latter promotes Rab3A activation. The enzymatic activity of GRAB toward Rab3A was also suggested by in silico and in vitro assays with purified proteins. In summary, we describe here a signaling module where Rab27A-GTP interacts with Rabphilin3a, which in turn recruits a guanine nucleotide-exchange activity toward Rab3A. This is the first description of the interaction of Rabphilin3a with a GEF. Because the machinery that drives exocytosis is highly conserved, it is tempting to hypothesize that the RabGEF cascade unveiled here might be part of the molecular mechanisms that drive exocytosis in other secretory systems.","Grab recruitment by Rab27A-Rabphilin3a triggers Rab3A activation in human sperm exocytosis. Sperm must undergo the regulated exocytosis of its dense core granule (the acrosome reaction, AR) to fertilize the egg. We have previously described that Rabs3 and 27 are organized in a RabGEF cascade within the signaling pathway elicited by exocytosis stimuli in human sperm. Here, we report the identity and the role of two molecules that link these secretory Rabs in the RabGEF cascade: Rabphilin3a and GRAB. Like Rab3 and Rab27, GRAB and Rabphilin3a are present, localize to the acrosomal region and are required for calcium-triggered exocytosis in human sperm. Sequestration of either protein with specific antibodies introduced into streptolysin O-permeabilized sperm impairs the activation of Rab3 in the acrosomal region elicited by calcium, but not that of Rab27. Biochemical and functional assays indicate that Rabphilin3a behaves as a Rab27 effector during the AR and that GRAB exhibits GEF activity toward Rab3A. Recombinant, active Rab27A pulls down Rabphilin3a and GRAB from human sperm extracts. Conversely, immobilized Rabphilin3a recruits Rab27 and GRAB; the latter promotes Rab3A activation. The enzymatic activity of GRAB toward Rab3A was also suggested by in silico and in vitro assays with purified proteins. In summary, we describe here a signaling module where Rab27A-GTP interacts with Rabphilin3a, which in turn recruits a guanine nucleotide-exchange activity toward Rab3A. This is the first description of the interaction of Rabphilin3a with a GEF. Because the machinery that drives exocytosis is highly conserved, it is tempting to hypothesize that the RabGEF cascade unveiled here might be part of the molecular mechanisms that drive exocytosis in other secretory systems."
0,Artificial intelligence in intensive care: are we there yet?,,
0,Acute Upper Airway Obstruction,,
0,Whole-genome sequencing of triple-negative breast cancers in a population-based clinical study,,
0,Informing future intensive care trials with machine learning,,
0,Bayesian bacterial detection using irregularly sampled optical endomicroscopy images,,
0,Three-dimensional US Fractional Moving Blood Volume: Validation of renal perfusion quantification,"Background: Three-dimensional (3D) fractional moving blood volume (FMBV) derived from 3D power Doppler US has been proposed for noninvasive approximation of perfusion. However, 3D FMBV has never been applied in animals against a ground truth. Purpose: To determine the correlation between 3D FMBV and the reference standard of fluorescent microspheres (FMS) for measurement of renal perfusion in a porcine model. Materials and Methods: From February 2017 to September 2017, adult pigs were administered FMS before and after measurement of renal 3D FMBV at baseline (100%) and approximately 75%, 50%, and 25% flow levels by using US machines from two different vendors. The 3D power Doppler US volumes were converted and segmented, and correlations between FMS and 3D FMBV were made with simple linear regression (r2). Similarity and reproducibility of manual segmentation were determined with the Dice similarity coefficient and 3D FMBV reproducibility (intraclass correlation coefficient [ICC]). Results: Thirteen pigs were studied with 33 flow measurements. Kidney volume (mean Dice similarity coefficient 6 standard deviation, 0.89 6 0.01) and renal segmentation (coefficient of variation = 12.6%; ICC = 0.86) were consistent. The 3D FMBV calculations had high reproducibility (ICC = 0.97; 95% confidence interval: 0.96, 0.98). The 3D FMBV per-pig correlation showed excellent correlation for US machines from both vendors (mean r2 = 0.96 [range, 0.92â€“1.0] and 0.93 [range, 0.78â€“1.0], respectively). The correlation between 3D FMBV and perfusion measured with microspheres was high for both US machines (r2 = 0.80 [P , .001] and 0.70 [P , .001], respectively). Conclusion: The strong correlation between three-dimensional (3D) fractional moving blood volume (FMBV) and fluorescent microspheres indicates that 3D FMBV shows excellent correlation to perfusion and good reproducibility.","Three-dimensional US Fractional Moving Blood Volume: Validation of renal perfusion quantification. Background: Three-dimensional (3D) fractional moving blood volume (FMBV) derived from 3D power Doppler US has been proposed for noninvasive approximation of perfusion. However, 3D FMBV has never been applied in animals against a ground truth. Purpose: To determine the correlation between 3D FMBV and the reference standard of fluorescent microspheres (FMS) for measurement of renal perfusion in a porcine model. Materials and Methods: From February 2017 to September 2017, adult pigs were administered FMS before and after measurement of renal 3D FMBV at baseline (100%) and approximately 75%, 50%, and 25% flow levels by using US machines from two different vendors. The 3D power Doppler US volumes were converted and segmented, and correlations between FMS and 3D FMBV were made with simple linear regression (r2). Similarity and reproducibility of manual segmentation were determined with the Dice similarity coefficient and 3D FMBV reproducibility (intraclass correlation coefficient [ICC]). Results: Thirteen pigs were studied with 33 flow measurements. Kidney volume (mean Dice similarity coefficient 6 standard deviation, 0.89 6 0.01) and renal segmentation (coefficient of variation = 12.6%; ICC = 0.86) were consistent. The 3D FMBV calculations had high reproducibility (ICC = 0.97; 95% confidence interval: 0.96, 0.98). The 3D FMBV per-pig correlation showed excellent correlation for US machines from both vendors (mean r2 = 0.96 [range, 0.92â€“1.0] and 0.93 [range, 0.78â€“1.0], respectively). The correlation between 3D FMBV and perfusion measured with microspheres was high for both US machines (r2 = 0.80 [P , .001] and 0.70 [P , .001], respectively). Conclusion: The strong correlation between three-dimensional (3D) fractional moving blood volume (FMBV) and fluorescent microspheres indicates that 3D FMBV shows excellent correlation to perfusion and good reproducibility."
0,Artificial Intelligence and Machine Learning in Anesthesiology,"Commercial applications of artificial intelligence and machine learning have made remarkable progress recently, particularly in areas such as image recognition, natural speech processing, language translation, textual analysis, and self-learning. Progress had historically languished in these areas, such that these skills had come to seem ineffably bound to intelligence. However, these commercial advances have performed best at single-task applications in which imperfect outputs and occasional frank errors can be tolerated.The practice of anesthesiology is different. It embodies a requirement for high reliability, and a pressured cycle of interpretation, physical action, and response rather than any single cognitive act. This review covers the basics of what is meant by artificial intelligence and machine learning for the practicing anesthesiologist, describing how decision-making behaviors can emerge from simple equations. Relevant clinical questions are introduced to illustrate how machine learning might help solve them-perhaps bringing anesthesiology into an era of machine-assisted discovery.","Artificial Intelligence and Machine Learning in Anesthesiology. Commercial applications of artificial intelligence and machine learning have made remarkable progress recently, particularly in areas such as image recognition, natural speech processing, language translation, textual analysis, and self-learning. Progress had historically languished in these areas, such that these skills had come to seem ineffably bound to intelligence. However, these commercial advances have performed best at single-task applications in which imperfect outputs and occasional frank errors can be tolerated.The practice of anesthesiology is different. It embodies a requirement for high reliability, and a pressured cycle of interpretation, physical action, and response rather than any single cognitive act. This review covers the basics of what is meant by artificial intelligence and machine learning for the practicing anesthesiologist, describing how decision-making behaviors can emerge from simple equations. Relevant clinical questions are introduced to illustrate how machine learning might help solve them-perhaps bringing anesthesiology into an era of machine-assisted discovery."
0,Management of acute radiation dermatitis: A review of the literature and proposal for treatment algorithm,"Radiation dermatitis is a common sequela of radiation therapy; up to 95% of patients will develop moderate-to-severe skin reactions. No criterion standard currently exists for the treatment of acute radiation-induced skin toxicity. It is therefore imperative to develop a greater understanding of management options available to allow clinicians to make informed decisions when managing radiation oncology patients. This literature review discusses the topical agents that have been studied for the treatment of acute radiation dermatitis, reviews their mechanisms of action, and presents a treatment algorithm for clinicians managing patients experiencing radiation dermatitis.","Management of acute radiation dermatitis: A review of the literature and proposal for treatment algorithm. Radiation dermatitis is a common sequela of radiation therapy; up to 95% of patients will develop moderate-to-severe skin reactions. No criterion standard currently exists for the treatment of acute radiation-induced skin toxicity. It is therefore imperative to develop a greater understanding of management options available to allow clinicians to make informed decisions when managing radiation oncology patients. This literature review discusses the topical agents that have been studied for the treatment of acute radiation dermatitis, reviews their mechanisms of action, and presents a treatment algorithm for clinicians managing patients experiencing radiation dermatitis."
0,Reporting of artificial intelligence prediction models,,
0,"Levofloxacin prophylaxis in patients with newly diagnosed myeloma (TEAMM): a multicentre, double-blind, placebo-controlled, randomised, phase 3 trial",,
0,New approaches for effective and safe pelvic radiotherapy in high-risk prostate cancer,,
0,A graph-based algorithm for estimating clonal haplotypes of tumor sample from sequencing data,"Background: Haplotype phasing is an important step in many bioinformatics workflows. In cancer genomics, it is suggested that reconstructing the clonal haplotypes of a tumor sample could facilitate a comprehensive understanding of its clonal architecture and further provide valuable reference in clinical diagnosis and treatment. However, the sequencing data is an admixture of reads sampled from different clonal haplotypes, which complicates the computational problem by exponentially increasing the solution-space and leads the existing algorithms to an unacceptable time-/space- complexity. In addition, the evolutionary process among clonal haplotypes further weakens those algorithms by bringing indistinguishable candidate solutions. Results: To improve the algorithmic performance of phasing clonal haplotypes, in this article, we propose MixSubHap, which is a graph-based computational pipeline working on cancer sequencing data. To reduce the computation complexity, MixSubHap adopts three bounding strategies to limit the solution space and filter out false positive candidates. It first estimates the global clonal structure by clustering the variant allelic frequencies on sampled point mutations. This offers a priori on the number of clonal haplotypes when copy-number variations are not considered. Then, it utilizes a greedy extension algorithm to approximately find the longest linkage of the locally assembled contigs. Finally, it incorporates a read-depth stripping algorithm to filter out false linkages according to the posterior estimation of tumor purity and the estimated percentage of each sub-clone in the sample. A series of experiments are conducted to verify the performance of the proposed pipeline. Conclusions: The results demonstrate that MixSubHap is able to identify about 90% on average of the preset clonal haplotypes under different simulation configurations. Especially, MixSubHap is robust when decreasing the mutation rates, in which cases the longest assembled contig could reach to 10kbps, while the accuracy of assigning a mutation to its haplotype still keeps more than 60% on average. MixSubHap is considered as a practical algorithm to reconstruct clonal haplotypes from cancer sequencing data. The source codes have been uploaded and maintained at https://github.com/YixuanWang1120/MixSubHap for academic use only.","A graph-based algorithm for estimating clonal haplotypes of tumor sample from sequencing data. Background: Haplotype phasing is an important step in many bioinformatics workflows. In cancer genomics, it is suggested that reconstructing the clonal haplotypes of a tumor sample could facilitate a comprehensive understanding of its clonal architecture and further provide valuable reference in clinical diagnosis and treatment. However, the sequencing data is an admixture of reads sampled from different clonal haplotypes, which complicates the computational problem by exponentially increasing the solution-space and leads the existing algorithms to an unacceptable time-/space- complexity. In addition, the evolutionary process among clonal haplotypes further weakens those algorithms by bringing indistinguishable candidate solutions. Results: To improve the algorithmic performance of phasing clonal haplotypes, in this article, we propose MixSubHap, which is a graph-based computational pipeline working on cancer sequencing data. To reduce the computation complexity, MixSubHap adopts three bounding strategies to limit the solution space and filter out false positive candidates. It first estimates the global clonal structure by clustering the variant allelic frequencies on sampled point mutations. This offers a priori on the number of clonal haplotypes when copy-number variations are not considered. Then, it utilizes a greedy extension algorithm to approximately find the longest linkage of the locally assembled contigs. Finally, it incorporates a read-depth stripping algorithm to filter out false linkages according to the posterior estimation of tumor purity and the estimated percentage of each sub-clone in the sample. A series of experiments are conducted to verify the performance of the proposed pipeline. Conclusions: The results demonstrate that MixSubHap is able to identify about 90% on average of the preset clonal haplotypes under different simulation configurations. Especially, MixSubHap is robust when decreasing the mutation rates, in which cases the longest assembled contig could reach to 10kbps, while the accuracy of assigning a mutation to its haplotype still keeps more than 60% on average. MixSubHap is considered as a practical algorithm to reconstruct clonal haplotypes from cancer sequencing data. The source codes have been uploaded and maintained at https://github.com/YixuanWang1120/MixSubHap for academic use only."
0,MiR-181c-5p exacerbates hypoxia/reoxygenation-induced cardiomyocyte apoptosis via targeting PTPN4,"Background. Activation of cell apoptosis is a major form of cell death during myocardial ischemia/reperfusion injury (I/RI). Therefore, examining ways to control cell apoptosis has important clinical significance for improving postischemic recovery. Clinical evidence demonstrated that miR-181c-5p was significantly upregulated in the early phase of myocardial infarction. However, whether or not miR-181c-5p mediates cardiac I/RI through cell apoptosis pathway is unknown. Thus, the present study is aimed at investigating the role and the possible mechanism of miR-181c-5p in apoptosis during I/R injury by using H9C2 cardiomyocytes. Methods and Results. The rat origin H9C2 cardiomyocytes were subjected to hypoxia/reoxygenation (H/R, 6 hours hypoxia followed by 6 hours reoxygenation) to induce cell injury. The results showed that H/R significantly increased the expression of miR-181c-5p but not miR-181c-3p in H9C2 cells. In line with this, in an in vivo rat cardiac I/RI model, miR-181c-5p expression was also significantly increased. The overexpression of miR-181c-5p by its agomir transfection significantly aggravated H/R-induced cell injury (increased lactate dehydrogenase level and reduced cell viability) and exacerbated H/R-induced cell apoptosis (greater cleaved caspases 3 expression, Bax/Bcl-2 and more TUNEL-positive cells). In contrast, inhibition of miR-181c-5p in vitro had the opposite effect. By using computational prediction algorithms, protein tyrosine phosphatase nonreceptor type 4 (PTPN4) was predicted as a potential target gene of miR-181c-5p and was verified by the luciferase reporter assay. The overexpression of miR-181c-5p significantly attenuated the mRNA and protein expression of PTPN4 in H9C2 cardiomyocytes. Moreover, knockdown of PTPN4 significantly aggravated H/R-induced enhancement of LDH level, cleaved caspase 3 expression, and apoptotic cell death, which mimicked the proapoptotic effects of miR-181c-5p in H9C2 cardiomyocytes. Conclusions. These findings suggested that miR-181c-5p exacerbates H/R-induced cardiomyocyte injury and apoptosis via targeting PTPN4 and that miR-181c-5p/PTPN4 signaling may yield novel strategies to combat myocardial I/R injury.","MiR-181c-5p exacerbates hypoxia/reoxygenation-induced cardiomyocyte apoptosis via targeting PTPN4. Background. Activation of cell apoptosis is a major form of cell death during myocardial ischemia/reperfusion injury (I/RI). Therefore, examining ways to control cell apoptosis has important clinical significance for improving postischemic recovery. Clinical evidence demonstrated that miR-181c-5p was significantly upregulated in the early phase of myocardial infarction. However, whether or not miR-181c-5p mediates cardiac I/RI through cell apoptosis pathway is unknown. Thus, the present study is aimed at investigating the role and the possible mechanism of miR-181c-5p in apoptosis during I/R injury by using H9C2 cardiomyocytes. Methods and Results. The rat origin H9C2 cardiomyocytes were subjected to hypoxia/reoxygenation (H/R, 6 hours hypoxia followed by 6 hours reoxygenation) to induce cell injury. The results showed that H/R significantly increased the expression of miR-181c-5p but not miR-181c-3p in H9C2 cells. In line with this, in an in vivo rat cardiac I/RI model, miR-181c-5p expression was also significantly increased. The overexpression of miR-181c-5p by its agomir transfection significantly aggravated H/R-induced cell injury (increased lactate dehydrogenase level and reduced cell viability) and exacerbated H/R-induced cell apoptosis (greater cleaved caspases 3 expression, Bax/Bcl-2 and more TUNEL-positive cells). In contrast, inhibition of miR-181c-5p in vitro had the opposite effect. By using computational prediction algorithms, protein tyrosine phosphatase nonreceptor type 4 (PTPN4) was predicted as a potential target gene of miR-181c-5p and was verified by the luciferase reporter assay. The overexpression of miR-181c-5p significantly attenuated the mRNA and protein expression of PTPN4 in H9C2 cardiomyocytes. Moreover, knockdown of PTPN4 significantly aggravated H/R-induced enhancement of LDH level, cleaved caspase 3 expression, and apoptotic cell death, which mimicked the proapoptotic effects of miR-181c-5p in H9C2 cardiomyocytes. Conclusions. These findings suggested that miR-181c-5p exacerbates H/R-induced cardiomyocyte injury and apoptosis via targeting PTPN4 and that miR-181c-5p/PTPN4 signaling may yield novel strategies to combat myocardial I/R injury."
0,Development and validation of serum exosomal microRNAs as diagnostic and prognostic biomarkers for hepatocellular carcinoma,"Hepatocellular carcinoma (HCC) is the second leading cause of cancer-related death worldwide. China accounts for over half of the new cases and deaths. Diagnostic imprecision and a lack of complimentary molecular biomarkers are partially responsible for this lack of progress. Herein, serum-derived exosomal microRNA (miRNA) profiling was performed on 80 patients which histologically confirmed HCC and 30 normal controls. A classification of 8 exosomal miRNAs had biologically and statistically significant differences between HCC and normal serum samples, including miR-122, miR-125b, miR-145, miR-192, miR-194, miR-29a, miR-17-5p, and miR-106a. Online algorithm showed strong independent classification accuracy (area under the curve) reached 0.535 to 0.850, separately. The significant correlation between serum exosomal miRNAs and tumor size was observed. In addition, the survival difference of HCC patients with high or low exosomal miR-106a was statistically significant using Kaplan-Meier analysis. Besides, we also measured the proliferation and invasion ability of HCC cells following exosomal miR-106a mimics or inhibitor treatment. After prediction with algorithms, mitogen-activated protein kinase and c-Jun N-terminal kinase pathways were identified associated with miR-106aâ€™s function. In summary, differentially expressed serum exosomal miRNAs can be helpful for diagnostic and prognostic of HCC.","Development and validation of serum exosomal microRNAs as diagnostic and prognostic biomarkers for hepatocellular carcinoma. Hepatocellular carcinoma (HCC) is the second leading cause of cancer-related death worldwide. China accounts for over half of the new cases and deaths. Diagnostic imprecision and a lack of complimentary molecular biomarkers are partially responsible for this lack of progress. Herein, serum-derived exosomal microRNA (miRNA) profiling was performed on 80 patients which histologically confirmed HCC and 30 normal controls. A classification of 8 exosomal miRNAs had biologically and statistically significant differences between HCC and normal serum samples, including miR-122, miR-125b, miR-145, miR-192, miR-194, miR-29a, miR-17-5p, and miR-106a. Online algorithm showed strong independent classification accuracy (area under the curve) reached 0.535 to 0.850, separately. The significant correlation between serum exosomal miRNAs and tumor size was observed. In addition, the survival difference of HCC patients with high or low exosomal miR-106a was statistically significant using Kaplan-Meier analysis. Besides, we also measured the proliferation and invasion ability of HCC cells following exosomal miR-106a mimics or inhibitor treatment. After prediction with algorithms, mitogen-activated protein kinase and c-Jun N-terminal kinase pathways were identified associated with miR-106aâ€™s function. In summary, differentially expressed serum exosomal miRNAs can be helpful for diagnostic and prognostic of HCC."
0,Changes in Whole Brain Dynamics and Connectivity Patterns during Sevoflurane- A nd Propofol-induced Unconsciousness Identified by Functional Magnetic Resonance Imaging,"Editor's Perspective What We Already Know about This Topic The extent to which alterations within specific brain networks impairs communication among networks remains unknown What This Article Tells Us That Is New In a volunteer functional magnetic resonance study, general anesthesia reduced activity within and among networks Specific between-network connectivity is necessary for consciousness Background: A key feature of the human brain is its capability to adapt flexibly to changing external stimuli. This capability can be eliminated by general anesthesia, a state characterized by unresponsiveness, amnesia, and (most likely) unconsciousness. Previous studies demonstrated decreased connectivity within the thalamus, frontoparietal, and default mode networks during general anesthesia. We hypothesized that these alterations within specific brain networks lead to a change of communication between networks and their temporal dynamics. Methods: We conducted a pooled spatial independent component analysis of resting-state functional magnetic resonance imaging data obtained from 16 volunteers during propofol and 14 volunteers during sevoflurane general anesthesia that have been previously published. Similar to previous studies, mean z-scores of the resulting spatial maps served as a measure of the activity within a network. Additionally, correlations of associated time courses served as a measure of the connectivity between networks. To analyze the temporal dynamics of between-network connectivity, we computed the correlation matrices during sliding windows of 1 min and applied k-means clustering to the matrices during both general anesthesia and wakefulness. Results: Within-network activity was decreased in the default mode, attentional, and salience networks during general anesthesia (P < 0.001, range of median changes:-0.34,-0.13). Average between-network connectivity was reduced during general anesthesia (P < 0.001, median change:-0.031). Distinct between-network connectivity patterns for both wakefulness and general anesthesia were observed irrespective of the anesthetic agent (P < 0.001), and there were fewer transitions in between-network connectivity patterns during general anesthesia (P < 0.001, median number of transitions during wakefulness: 4 and during general anesthesia: 0). Conclusions: These results suggest that (1) higher-order brain regions play a crucial role in the generation of specific between-network connectivity patterns and their dynamics, and (2) the capability to interact with external stimuli is represented by complex between-network connectivity patterns.","Changes in Whole Brain Dynamics and Connectivity Patterns during Sevoflurane- A nd Propofol-induced Unconsciousness Identified by Functional Magnetic Resonance Imaging. Editor's Perspective What We Already Know about This Topic The extent to which alterations within specific brain networks impairs communication among networks remains unknown What This Article Tells Us That Is New In a volunteer functional magnetic resonance study, general anesthesia reduced activity within and among networks Specific between-network connectivity is necessary for consciousness Background: A key feature of the human brain is its capability to adapt flexibly to changing external stimuli. This capability can be eliminated by general anesthesia, a state characterized by unresponsiveness, amnesia, and (most likely) unconsciousness. Previous studies demonstrated decreased connectivity within the thalamus, frontoparietal, and default mode networks during general anesthesia. We hypothesized that these alterations within specific brain networks lead to a change of communication between networks and their temporal dynamics. Methods: We conducted a pooled spatial independent component analysis of resting-state functional magnetic resonance imaging data obtained from 16 volunteers during propofol and 14 volunteers during sevoflurane general anesthesia that have been previously published. Similar to previous studies, mean z-scores of the resulting spatial maps served as a measure of the activity within a network. Additionally, correlations of associated time courses served as a measure of the connectivity between networks. To analyze the temporal dynamics of between-network connectivity, we computed the correlation matrices during sliding windows of 1 min and applied k-means clustering to the matrices during both general anesthesia and wakefulness. Results: Within-network activity was decreased in the default mode, attentional, and salience networks during general anesthesia (P < 0.001, range of median changes:-0.34,-0.13). Average between-network connectivity was reduced during general anesthesia (P < 0.001, median change:-0.031). Distinct between-network connectivity patterns for both wakefulness and general anesthesia were observed irrespective of the anesthetic agent (P < 0.001), and there were fewer transitions in between-network connectivity patterns during general anesthesia (P < 0.001, median number of transitions during wakefulness: 4 and during general anesthesia: 0). Conclusions: These results suggest that (1) higher-order brain regions play a crucial role in the generation of specific between-network connectivity patterns and their dynamics, and (2) the capability to interact with external stimuli is represented by complex between-network connectivity patterns."
0,Spurious interaction as a result of categorization,"BACKGROUND: It is common in applied epidemiological and clinical research to convert continuous variables into categorical variables by grouping values into categories. Such categorized variables are then often used as exposure variables in some regression model. There are numerous statistical arguments why this practice should be avoided, and in this paper we present yet another such argument. METHODS: We show that categorization may lead to spurious interaction in multiple regression models. We give precise analytical expressions for when this may happen in the linear regression model with normally distributed exposure variables, and we show by simulations that the analytical results are valid also for other distributions. Further, we give an interpretation of the results in terms of a measurement error problem. RESULTS: We show that, in the case of a linear model with two normally distributed exposure variables, both categorized at the same cut point, a spurious interaction will be induced unless the two variables are categorized at the median or they are uncorrelated. In simulations with exposure variables following other distributions, we confirm this general effect of categorization, but we also show that the effect of the choice of cut point varies over different distributions. CONCLUSION: Categorization of continuous exposure variables leads to a number of problems, among them spurious interaction effects. Hence, this practice should be avoided and other methods should be considered.","Spurious interaction as a result of categorization. BACKGROUND: It is common in applied epidemiological and clinical research to convert continuous variables into categorical variables by grouping values into categories. Such categorized variables are then often used as exposure variables in some regression model. There are numerous statistical arguments why this practice should be avoided, and in this paper we present yet another such argument. METHODS: We show that categorization may lead to spurious interaction in multiple regression models. We give precise analytical expressions for when this may happen in the linear regression model with normally distributed exposure variables, and we show by simulations that the analytical results are valid also for other distributions. Further, we give an interpretation of the results in terms of a measurement error problem. RESULTS: We show that, in the case of a linear model with two normally distributed exposure variables, both categorized at the same cut point, a spurious interaction will be induced unless the two variables are categorized at the median or they are uncorrelated. In simulations with exposure variables following other distributions, we confirm this general effect of categorization, but we also show that the effect of the choice of cut point varies over different distributions. CONCLUSION: Categorization of continuous exposure variables leads to a number of problems, among them spurious interaction effects. Hence, this practice should be avoided and other methods should be considered."
0,"Blockchain in health care: hype, trust, and digital health",,
0,Medicine in the digital age,,
0,Three-dimensional spatially resolved geometrical and functional models of human liver tissue reveal new aspects of NAFLD progression,"Early disease diagnosis is key to the effective treatment of diseases. Histopathological analysis of human biopsies is the gold standard to diagnose tissue alterations. However, this approach has low resolution and overlooks 3D (three-dimensional) structural changes resulting from functional alterations. Here, we applied multiphoton imaging, 3D digital reconstructions and computational simulations to generate spatially resolved geometrical and functional models of human liver tissue at different stages of non-alcoholic fatty liver disease (NAFLD). We identified a set of morphometric cellular and tissue parameters correlated with disease progression, and discover profound topological defects in the 3D bile canalicular (BC) network. Personalized biliary fluid dynamic simulations predicted an increased pericentral biliary pressure and micro-cholestasis, consistent with elevated cholestatic biomarkers in patientsâ€™ sera. Our spatially resolved models of human liver tissue can contribute to high-definition medicine by identifying quantitative multiparametric cellular and tissue signatures to define disease progression and provide new insights into NAFLD pathophysiology.","Three-dimensional spatially resolved geometrical and functional models of human liver tissue reveal new aspects of NAFLD progression. Early disease diagnosis is key to the effective treatment of diseases. Histopathological analysis of human biopsies is the gold standard to diagnose tissue alterations. However, this approach has low resolution and overlooks 3D (three-dimensional) structural changes resulting from functional alterations. Here, we applied multiphoton imaging, 3D digital reconstructions and computational simulations to generate spatially resolved geometrical and functional models of human liver tissue at different stages of non-alcoholic fatty liver disease (NAFLD). We identified a set of morphometric cellular and tissue parameters correlated with disease progression, and discover profound topological defects in the 3D bile canalicular (BC) network. Personalized biliary fluid dynamic simulations predicted an increased pericentral biliary pressure and micro-cholestasis, consistent with elevated cholestatic biomarkers in patientsâ€™ sera. Our spatially resolved models of human liver tissue can contribute to high-definition medicine by identifying quantitative multiparametric cellular and tissue signatures to define disease progression and provide new insights into NAFLD pathophysiology."
0,A retrospective study on clinical manifestations of neonates with FXIII-A deficiency,"We assessed clinical presentations and the rate of central nervous system (CNS) bleeding in neonates with FXIIID who exhibited bleeding diathesis in the early days of their lives. A total of 27 neonates presented bleeding or abnormal clinical symptoms, diagnosed with FXIII deficiency were evaluated. Factor XIII concentrate was initiated as the first-line of treatment, and prophylactic therapy was given to all patients. Umbilical cord bleeding, delayed detachment of umbilical stunt, seizure, hematoma, and ecchymosis were concurrent complications in 27 (100%), 5 (18.5%), 5 (18.5%), 3 (11.1%), and 1 (3.7%) of the patients, respectively. History of having CNS bleeding was detected in 13 (48.1%) patients. There was no significant association between CNS bleeding and gender, familial history of FXIIID, or other clinical presentations. Also, there was no significant difference in the mean age of the patients who had CNS bleeding (3.4 Â± 0.9 days) and without CNS bleeding (2.9 Â± 0.7 days). However, a near significant threshold difference between the patients with and without CNS bleeding was found regarding the mean number of suspicious FXIIID death in their family (1.8 Â± 0.5 and 0.7 Â± 0.1, respectively, P = 0.05). Therefore, a suggested diagnostic algorithm based on prenatal diagnosis could be useful for timely detection of FXIII deficiency in neonates.","A retrospective study on clinical manifestations of neonates with FXIII-A deficiency. We assessed clinical presentations and the rate of central nervous system (CNS) bleeding in neonates with FXIIID who exhibited bleeding diathesis in the early days of their lives. A total of 27 neonates presented bleeding or abnormal clinical symptoms, diagnosed with FXIII deficiency were evaluated. Factor XIII concentrate was initiated as the first-line of treatment, and prophylactic therapy was given to all patients. Umbilical cord bleeding, delayed detachment of umbilical stunt, seizure, hematoma, and ecchymosis were concurrent complications in 27 (100%), 5 (18.5%), 5 (18.5%), 3 (11.1%), and 1 (3.7%) of the patients, respectively. History of having CNS bleeding was detected in 13 (48.1%) patients. There was no significant association between CNS bleeding and gender, familial history of FXIIID, or other clinical presentations. Also, there was no significant difference in the mean age of the patients who had CNS bleeding (3.4 Â± 0.9 days) and without CNS bleeding (2.9 Â± 0.7 days). However, a near significant threshold difference between the patients with and without CNS bleeding was found regarding the mean number of suspicious FXIIID death in their family (1.8 Â± 0.5 and 0.7 Â± 0.1, respectively, P = 0.05). Therefore, a suggested diagnostic algorithm based on prenatal diagnosis could be useful for timely detection of FXIII deficiency in neonates."
0,Identification of the potential prognostic genes of human melanoma,"The melanoma is one of the most dangerous forms of skin diseases. It may spread to other parts of the body and cause serious illness and death. Early detection and diagnosis are crucial. However, the systemic expression analysis for the different staging of melanoma is still lacking to date. In this study, we analyzed the gene expression profiles of the different staging of melanoma by the differential expression analysis and random forest analysis. First, the results of the principal component analysis showed that the clustering of primary tumor samples, normal samples, and pigment nevus samples got closer, while the clustering of tumor metastatic samples and normal samples was far away. Moreover, the gene expression of tumor metastasis stage and the initial stage had obvious differences. Almost 426 genes identified had differential expression. The functional enrichment of differentially expressed genes was associated with the epidermal cell differentiation, epidermis development, and the keratinocyte differentiation. Taken together, our findings identified the differentially expressed signatures between primary melanoma and metastatic melanoma. Our results would provide the potential mechanisms of melanoma.","Identification of the potential prognostic genes of human melanoma. The melanoma is one of the most dangerous forms of skin diseases. It may spread to other parts of the body and cause serious illness and death. Early detection and diagnosis are crucial. However, the systemic expression analysis for the different staging of melanoma is still lacking to date. In this study, we analyzed the gene expression profiles of the different staging of melanoma by the differential expression analysis and random forest analysis. First, the results of the principal component analysis showed that the clustering of primary tumor samples, normal samples, and pigment nevus samples got closer, while the clustering of tumor metastatic samples and normal samples was far away. Moreover, the gene expression of tumor metastasis stage and the initial stage had obvious differences. Almost 426 genes identified had differential expression. The functional enrichment of differentially expressed genes was associated with the epidermal cell differentiation, epidermis development, and the keratinocyte differentiation. Taken together, our findings identified the differentially expressed signatures between primary melanoma and metastatic melanoma. Our results would provide the potential mechanisms of melanoma."
0,Cerebral organoids at the air-liquid interface generate diverse nerve tracts with functional output,,
0,The global burden of non-typhoidal salmonella invasive disease: a systematic analysis for the Global Burden of Disease Study 2017,"Background: Non-typhoidal salmonella invasive disease is a major cause of global morbidity and mortality. Malnourished children, those with recent malaria or sickle-cell anaemia, and adults with HIV infection are at particularly high risk of disease. We sought to estimate the burden of disease attributable to non-typhoidal salmonella invasive disease for the Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2017. Methods: We did a systematic review of scientific databases and grey literature, and estimated non-typhoidal salmonella invasive disease incidence and mortality for the years 1990 to 2017, by age, sex, and geographical location using DisMod-MR, a Bayesian meta-regression tool. We estimated case fatality by age, HIV status, and sociodemographic development. We also calculated the HIV-attributable fraction and estimated health gap metrics, including disability-adjusted life-years (DALYs). Findings: We estimated that 535 000 (95% uncertainty interval 409 000â€“705 000) cases of non-typhoidal salmonella invasive disease occurred in 2017, with the highest incidence in sub-Saharan Africa (34Â·5 [26Â·6â€“45Â·0] cases per 100 000 person-years) and in children younger than 5 years (34Â·3 [23Â·2â€“54Â·7] cases per 100 000 person-years). 77 500 (46 400â€“123 000) deaths were estimated in 2017, of which 18 400 (12 000â€“27 700) were attributable to HIV. The remaining 59 100 (33 300â€“98 100) deaths not attributable to HIV accounted for 4Â·26 million (2Â·38â€“7Â·38) DALYs in 2017. Mean all-age case fatality was 14Â·5% (9Â·2â€“21Â·1), with higher estimates among children younger than 5 years (13Â·5% [8Â·4â€“19Â·8]) and elderly people (51Â·2% [30Â·2â€“72Â·9] among those aged â‰¥70 years), people with HIV infection (41Â·8% [30Â·0â€“54Â·0]), and in areas of low sociodemographic development (eg, 15Â·8% [10Â·0â€“22Â·9] in sub-Saharan Africa). Interpretation: We present the first global estimates of non-typhoidal salmonella invasive disease that have been produced as part of GBD 2017. Given the high disease burden, particularly in children, elderly people, and people with HIV infection, investigating the sources and transmission pathways of non-typhoidal salmonella invasive disease is crucial to implement effective preventive and control measures. Funding: Bill & Melinda Gates Foundation.","The global burden of non-typhoidal salmonella invasive disease: a systematic analysis for the Global Burden of Disease Study 2017. Background: Non-typhoidal salmonella invasive disease is a major cause of global morbidity and mortality. Malnourished children, those with recent malaria or sickle-cell anaemia, and adults with HIV infection are at particularly high risk of disease. We sought to estimate the burden of disease attributable to non-typhoidal salmonella invasive disease for the Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2017. Methods: We did a systematic review of scientific databases and grey literature, and estimated non-typhoidal salmonella invasive disease incidence and mortality for the years 1990 to 2017, by age, sex, and geographical location using DisMod-MR, a Bayesian meta-regression tool. We estimated case fatality by age, HIV status, and sociodemographic development. We also calculated the HIV-attributable fraction and estimated health gap metrics, including disability-adjusted life-years (DALYs). Findings: We estimated that 535 000 (95% uncertainty interval 409 000â€“705 000) cases of non-typhoidal salmonella invasive disease occurred in 2017, with the highest incidence in sub-Saharan Africa (34Â·5 [26Â·6â€“45Â·0] cases per 100 000 person-years) and in children younger than 5 years (34Â·3 [23Â·2â€“54Â·7] cases per 100 000 person-years). 77 500 (46 400â€“123 000) deaths were estimated in 2017, of which 18 400 (12 000â€“27 700) were attributable to HIV. The remaining 59 100 (33 300â€“98 100) deaths not attributable to HIV accounted for 4Â·26 million (2Â·38â€“7Â·38) DALYs in 2017. Mean all-age case fatality was 14Â·5% (9Â·2â€“21Â·1), with higher estimates among children younger than 5 years (13Â·5% [8Â·4â€“19Â·8]) and elderly people (51Â·2% [30Â·2â€“72Â·9] among those aged â‰¥70 years), people with HIV infection (41Â·8% [30Â·0â€“54Â·0]), and in areas of low sociodemographic development (eg, 15Â·8% [10Â·0â€“22Â·9] in sub-Saharan Africa). Interpretation: We present the first global estimates of non-typhoidal salmonella invasive disease that have been produced as part of GBD 2017. Given the high disease burden, particularly in children, elderly people, and people with HIV infection, investigating the sources and transmission pathways of non-typhoidal salmonella invasive disease is crucial to implement effective preventive and control measures. Funding: Bill & Melinda Gates Foundation."
0,Tracing outbreaks with machine learning,,
0,"Identification of potential drugs targeting L,L-diaminopimelate aminotransferase of Chlamydia trachomatis: An integrative pharmacoinformatics approach","Chlamydia trachomatis (C.t) is a gram-negative obligate intracellular bacteria, which is a major causative of infectious blindness and sexually transmitted diseases. A surge in multidrug resistance among chlamydial species has posed a challenge to adopt alternative drug targeting strategies. Recently, in C.t, L,L-diaminopimelate aminotransferase (CtDAP-AT) is proven to be a potential drug target due its essential role in cell survival and host nonspecificity. Hence, in this study, a multilevel precision-based virtual screening of CtDAP-AT was performed to identify potential inhibitors, wherein, an integrative stringent scoring and filtration were performed by coupling, glide docking score, binding free energy, ADMET (absorption, distribution, metabolism, and excretion, toxicity) prediction, density functional theory (quantum mechanics), and molecular dynamics simulation (molecular mechanics). On cumulative analysis, NSC_5485 (1,3-bis((7-chloro-4-quinolinyl)amino)-2-propanol) was found to be the most potential lead, as it showed higher order significance in terms of binding affinity, bonded interactions, favorable ADMET, chemical reactivity, and greater stabilization during complex formation. This is the first report on prioritization of small molecules from National Cancer Institute (NCI) and Maybridge data sets (341 519 compounds) towards targeting CtDAP-AT. Thus, the proposed compound shall aid in effective combating of a broad spectrum of C.t infections as it surpassed all the levels of prioritization.","Identification of potential drugs targeting L,L-diaminopimelate aminotransferase of Chlamydia trachomatis: An integrative pharmacoinformatics approach. Chlamydia trachomatis (C.t) is a gram-negative obligate intracellular bacteria, which is a major causative of infectious blindness and sexually transmitted diseases. A surge in multidrug resistance among chlamydial species has posed a challenge to adopt alternative drug targeting strategies. Recently, in C.t, L,L-diaminopimelate aminotransferase (CtDAP-AT) is proven to be a potential drug target due its essential role in cell survival and host nonspecificity. Hence, in this study, a multilevel precision-based virtual screening of CtDAP-AT was performed to identify potential inhibitors, wherein, an integrative stringent scoring and filtration were performed by coupling, glide docking score, binding free energy, ADMET (absorption, distribution, metabolism, and excretion, toxicity) prediction, density functional theory (quantum mechanics), and molecular dynamics simulation (molecular mechanics). On cumulative analysis, NSC_5485 (1,3-bis((7-chloro-4-quinolinyl)amino)-2-propanol) was found to be the most potential lead, as it showed higher order significance in terms of binding affinity, bonded interactions, favorable ADMET, chemical reactivity, and greater stabilization during complex formation. This is the first report on prioritization of small molecules from National Cancer Institute (NCI) and Maybridge data sets (341 519 compounds) towards targeting CtDAP-AT. Thus, the proposed compound shall aid in effective combating of a broad spectrum of C.t infections as it surpassed all the levels of prioritization."
0,Virtual Noncalcium Dual-Energy CT: Detection of Lumbar Disk Herniation in Comparison with Standard Gray-Scale CT,,
0,Reduced acquisition time PET pharmacokinetic modelling using simultaneous ASLâ€“MRI: proof of concept,"Pharmacokinetic modelling on dynamic positron emission tomography (PET) data is a quantitative technique. However, the long acquisition time is prohibitive for routine clinical use. Instead, the semi-quantitative standardised uptake value ratio (SUVR) from a shorter static acquisition is used, despite its sensitivity to blood flow confounding longitudinal analysis. A method has been proposed to reduce the dynamic acquisition time for quantification by incorporating cerebral blood flow (CBF) information from arterial spin labelling (ASL) magnetic resonance imaging (MRI) into the pharmacokinetic modelling. In this work, we optimise and validate this framework for a study of ageing and preclinical Alzheimer's disease. This methodology adapts the simplified reference tissue model (SRTM) for a reduced acquisition time (RT-SRTM) and is applied to [18F]-florbetapir PET data for amyloid-Î² quantification. Evaluation shows that the optimised RT-SRTM can achieve amyloid burden estimation from a 30-min PET/MR acquisition which is comparable with the gold standard SRTM applied to 60 min of PET data. Conversely, SUVR showed a significantly higher error and bias, and a statistically significant correlation with tracer delivery due to the influence of blood flow. The optimised RT-SRTM produced amyloid burden estimates which were uncorrelated with tracer delivery indicating its suitability for longitudinal studies.","Reduced acquisition time PET pharmacokinetic modelling using simultaneous ASLâ€“MRI: proof of concept. Pharmacokinetic modelling on dynamic positron emission tomography (PET) data is a quantitative technique. However, the long acquisition time is prohibitive for routine clinical use. Instead, the semi-quantitative standardised uptake value ratio (SUVR) from a shorter static acquisition is used, despite its sensitivity to blood flow confounding longitudinal analysis. A method has been proposed to reduce the dynamic acquisition time for quantification by incorporating cerebral blood flow (CBF) information from arterial spin labelling (ASL) magnetic resonance imaging (MRI) into the pharmacokinetic modelling. In this work, we optimise and validate this framework for a study of ageing and preclinical Alzheimer's disease. This methodology adapts the simplified reference tissue model (SRTM) for a reduced acquisition time (RT-SRTM) and is applied to [18F]-florbetapir PET data for amyloid-Î² quantification. Evaluation shows that the optimised RT-SRTM can achieve amyloid burden estimation from a 30-min PET/MR acquisition which is comparable with the gold standard SRTM applied to 60 min of PET data. Conversely, SUVR showed a significantly higher error and bias, and a statistically significant correlation with tracer delivery due to the influence of blood flow. The optimised RT-SRTM produced amyloid burden estimates which were uncorrelated with tracer delivery indicating its suitability for longitudinal studies."
0,Casticin inhibits nasopharyngeal carcinoma growth by targeting phosphoinositide 3-kinase,"Background: Casticin, an isoflavone compound extracted from the herb Fructus Viticis, has demonstrated anti-inflammatory and anticancer activities and properties. The aim of this study was to investigate the effects and mechanisms of casticin in nasopharyngeal carcinoma (NPC) cells and to determine its potential for targeted use as a medicine. Methods: NPC cells were used to perform the experiments. The CCK 8 assay and colony formation assays were used to assess cell viability. Flow cytometry was used to measure the cell cycle and apoptosis analysis (annexin V/PI assay). A three-dimensional (3D) tumour sphere culture system was used to characterize the effect of casticin on NPC stem cells. In silico molecular docking prediction and high-throughput KINOME scan assays were used to evaluate the binding of casticin to phosphoinositide 3-kinase (PI3K), including wild-type and most of mutants variants. We also used the SelectScreen assay to detect the IC50 of ATP activity in the active site of the target kinase. Western blotting was used to evaluate the changes in key proteins involved cell cycle, apoptosis, stemness, and PI3K/protein kinase B (AKT) signalling. The effect of casticin treatment in vivo was determined by using a xenograft mouse model. Results: Our results indicate that casticin is a new and novel selective PI3K inhibitor that can significantly inhibit NPC proliferation and that it induces G2/GM arrest and apoptosis by upregulating Bax/BCL2 expression. Moreover, casticin was observed to affect the self-renewal ability of the nasopharyngeal carcinoma cell lines, and a combination of casticin with BYL719 was observed to induce a decrease in the level of the phosphorylation of mTORC1 downstream targets in BYL719-insensitive NPC cell lines. Conclusion: Casticin is a newly emerging selective PI3K inhibitor with potential for use as a targeted therapeutic treatment for nasopharyngeal carcinoma. Accordingly, casticin might represent a novel and effective agent against NPC and likely has high potential for combined use with pharmacological agents targeting PI3K/AKT.","Casticin inhibits nasopharyngeal carcinoma growth by targeting phosphoinositide 3-kinase. Background: Casticin, an isoflavone compound extracted from the herb Fructus Viticis, has demonstrated anti-inflammatory and anticancer activities and properties. The aim of this study was to investigate the effects and mechanisms of casticin in nasopharyngeal carcinoma (NPC) cells and to determine its potential for targeted use as a medicine. Methods: NPC cells were used to perform the experiments. The CCK 8 assay and colony formation assays were used to assess cell viability. Flow cytometry was used to measure the cell cycle and apoptosis analysis (annexin V/PI assay). A three-dimensional (3D) tumour sphere culture system was used to characterize the effect of casticin on NPC stem cells. In silico molecular docking prediction and high-throughput KINOME scan assays were used to evaluate the binding of casticin to phosphoinositide 3-kinase (PI3K), including wild-type and most of mutants variants. We also used the SelectScreen assay to detect the IC50 of ATP activity in the active site of the target kinase. Western blotting was used to evaluate the changes in key proteins involved cell cycle, apoptosis, stemness, and PI3K/protein kinase B (AKT) signalling. The effect of casticin treatment in vivo was determined by using a xenograft mouse model. Results: Our results indicate that casticin is a new and novel selective PI3K inhibitor that can significantly inhibit NPC proliferation and that it induces G2/GM arrest and apoptosis by upregulating Bax/BCL2 expression. Moreover, casticin was observed to affect the self-renewal ability of the nasopharyngeal carcinoma cell lines, and a combination of casticin with BYL719 was observed to induce a decrease in the level of the phosphorylation of mTORC1 downstream targets in BYL719-insensitive NPC cell lines. Conclusion: Casticin is a newly emerging selective PI3K inhibitor with potential for use as a targeted therapeutic treatment for nasopharyngeal carcinoma. Accordingly, casticin might represent a novel and effective agent against NPC and likely has high potential for combined use with pharmacological agents targeting PI3K/AKT."
0,Publisher Correction: Artificial intelligence aims to improve cancer screenings in Kenya,An amendment to this paper has been published and can be accessed via a link at the top of the paper.,Publisher Correction: Artificial intelligence aims to improve cancer screenings in Kenya. An amendment to this paper has been published and can be accessed via a link at the top of the paper.
0,"Cyclin D-Cdk4,6 Drives Cell-Cycle Progression via the Retinoblastoma Protein's C-Terminal Helix","The cyclin-dependent kinases Cdk4 and Cdk6 form complexes with D-type cyclins to drive cell proliferation. A well-known target of cyclin D-Cdk4,6 is the retinoblastoma protein Rb, which inhibits cell-cycle progression until its inactivation by phosphorylation. However, the role of Rb phosphorylation by cyclin D-Cdk4,6 in cell-cycle progression is unclear because Rb can be phosphorylated by other cyclin-Cdks, and cyclin D-Cdk4,6 has other targets involved in cell division. Here, we show that cyclin D-Cdk4,6 docks one side of an alpha-helix in the Rb C terminus, which is not recognized by cyclins E, A, and B. This helix-based docking mechanism is shared by the p107 and p130 Rb-family members across metazoans. Mutation of the Rb C-terminal helix prevents its phosphorylation, promotes G1 arrest, and enhances Rb's tumor suppressive function. Our work conclusively demonstrates that the cyclin D-Rb interaction drives cell division and expands the diversity of known cyclin-based protein docking mechanisms. Precise timing of cell-cycle transitions relies on regulation of the activity and specificity of cyclin-dependent kinases. Topacio et al. show that the G1 cyclin-Cdk complex cyclin D-Cdk4,6 targets its well-known substrate, the retinoblastoma protein Rb, through recognition of a C-terminal alpha-helix and demonstrate that this specific cyclin-Cdk-substrate interaction drives cell proliferation.","Cyclin D-Cdk4,6 Drives Cell-Cycle Progression via the Retinoblastoma Protein's C-Terminal Helix. The cyclin-dependent kinases Cdk4 and Cdk6 form complexes with D-type cyclins to drive cell proliferation. A well-known target of cyclin D-Cdk4,6 is the retinoblastoma protein Rb, which inhibits cell-cycle progression until its inactivation by phosphorylation. However, the role of Rb phosphorylation by cyclin D-Cdk4,6 in cell-cycle progression is unclear because Rb can be phosphorylated by other cyclin-Cdks, and cyclin D-Cdk4,6 has other targets involved in cell division. Here, we show that cyclin D-Cdk4,6 docks one side of an alpha-helix in the Rb C terminus, which is not recognized by cyclins E, A, and B. This helix-based docking mechanism is shared by the p107 and p130 Rb-family members across metazoans. Mutation of the Rb C-terminal helix prevents its phosphorylation, promotes G1 arrest, and enhances Rb's tumor suppressive function. Our work conclusively demonstrates that the cyclin D-Rb interaction drives cell division and expands the diversity of known cyclin-based protein docking mechanisms. Precise timing of cell-cycle transitions relies on regulation of the activity and specificity of cyclin-dependent kinases. Topacio et al. show that the G1 cyclin-Cdk complex cyclin D-Cdk4,6 targets its well-known substrate, the retinoblastoma protein Rb, through recognition of a C-terminal alpha-helix and demonstrate that this specific cyclin-Cdk-substrate interaction drives cell proliferation."
0,"Genetic Variations rs859, rs4646, and rs372883 in the 3â€²-Untranslated Regions of Genes Are Associated with a Risk of IgA Nephropathy","Background: Previous studies indicate that genetic factors play an important role in the pathogenesis of IgA nephropathy (IgAN). To evaluate the association between single nucleotide polymorphisms (SNPs) in the 3â€²-untranslated region (3â€²-UTR) of genes and IgAN risk, we performed a case-control study in a Chinese Han population. Materials: Twelve SNPs were selected and genotyped in 384 IgAN patients and 357 healthy controls. Odds ratio (OR) and 95% confidence intervals (CI) were calculated by logistic regression adjusted for age and gender. Multifactor dimensionality reduction (MDR) was used to analyze the interaction of SNP-SNP with IgAN risk. Results: Our study demonstrated that IL-16 rs859 (OR = 0.75, p = 0.040) and CYP19A1 rs4646 (OR = 2.58, p = 0.017) polymorphism were related to the risk of IgAN. In stratified analyses by gender, CYP19A1 rs4646 (OR = 2.96, p = 0.015) and BACH1 rs372883 (OR = 1.81, p = 0.038) polymorphisms conferred susceptibility to IgAN in males. Besides, rs372883 reduced IgAN risk in females (OR = 0.44, p = 0.042). We also found rs859 polymorphism was correlated with grade I-II (OR = 0.42, p = 0.028) in subgroup analysis of Lee's classification. Additionally, we found rs4646 polymorphism was correlated with serum creatinine (p = 0.035). Conclusion: Our results suggested that the IL-16 rs859, CYP19A1 rs4646, and BACH1 rs372883 polymorphisms have potential roles in the genetic susceptibility to IgAN in Chinese Han population.","Genetic Variations rs859, rs4646, and rs372883 in the 3â€²-Untranslated Regions of Genes Are Associated with a Risk of IgA Nephropathy. Background: Previous studies indicate that genetic factors play an important role in the pathogenesis of IgA nephropathy (IgAN). To evaluate the association between single nucleotide polymorphisms (SNPs) in the 3â€²-untranslated region (3â€²-UTR) of genes and IgAN risk, we performed a case-control study in a Chinese Han population. Materials: Twelve SNPs were selected and genotyped in 384 IgAN patients and 357 healthy controls. Odds ratio (OR) and 95% confidence intervals (CI) were calculated by logistic regression adjusted for age and gender. Multifactor dimensionality reduction (MDR) was used to analyze the interaction of SNP-SNP with IgAN risk. Results: Our study demonstrated that IL-16 rs859 (OR = 0.75, p = 0.040) and CYP19A1 rs4646 (OR = 2.58, p = 0.017) polymorphism were related to the risk of IgAN. In stratified analyses by gender, CYP19A1 rs4646 (OR = 2.96, p = 0.015) and BACH1 rs372883 (OR = 1.81, p = 0.038) polymorphisms conferred susceptibility to IgAN in males. Besides, rs372883 reduced IgAN risk in females (OR = 0.44, p = 0.042). We also found rs859 polymorphism was correlated with grade I-II (OR = 0.42, p = 0.028) in subgroup analysis of Lee's classification. Additionally, we found rs4646 polymorphism was correlated with serum creatinine (p = 0.035). Conclusion: Our results suggested that the IL-16 rs859, CYP19A1 rs4646, and BACH1 rs372883 polymorphisms have potential roles in the genetic susceptibility to IgAN in Chinese Han population."
0,Predict drug sensitivity of cancer cells with pathway activity inference,"Background: Predicting cellular responses to drugs has been a major challenge for personalized drug therapy regimen. Recent pharmacogenomic studies measured the sensitivities of heterogeneous cell lines to numerous drugs, and provided valuable data resources to develop and validate computational approaches for the prediction of drug responses. Most of current approaches predict drug sensitivity by building prediction models with individual genes, which suffer from low reproducibility due to biologic variability and difficulty to interpret biological relevance of novel gene-drug associations. As an alternative, pathway activity scores derived from gene expression could predict drug response of cancer cells. Method: In this study, pathway-based prediction models were built with four approaches inferring pathway activity in unsupervised manner, including competitive scoring approaches (DiffRank and GSVA) and self-contained scoring approaches (PLAGE and Z-score). These unsupervised pathway activity inference approaches were applied to predict drug responses of cancer cells using data from Cancer Cell Line Encyclopedia (CCLE). Results: Our analysis on all the 24 drugs from CCLE demonstrated that pathway-based models achieved better predictions for 14 out of the 24 drugs, while taking fewer features as inputs. Further investigation on indicated that pathway-based models indeed captured pathways involving drug-related genes (targets, transporters and metabolic enzymes) for majority of drugs, whereas gene-models failed to identify these drug-related genes, in most cases. Among the four approaches, competitive scoring (DiffRank and GSVA) provided more accurate predictions and captured more pathways involving drug-related genes than self-contained scoring (PLAGE and Z-Score). Detailed interpretation of top pathways from the top method (DiffRank) highlights the merit of pathway-based approaches to predict drug response by identifying pathways relevant to drug mechanisms. Conclusion: Taken together, pathway-based modeling with inferred pathway activity is a promising alternative to predict drug response, with the ability to easily interpret results and provide biological insights into the mechanisms of drug actions.","Predict drug sensitivity of cancer cells with pathway activity inference. Background: Predicting cellular responses to drugs has been a major challenge for personalized drug therapy regimen. Recent pharmacogenomic studies measured the sensitivities of heterogeneous cell lines to numerous drugs, and provided valuable data resources to develop and validate computational approaches for the prediction of drug responses. Most of current approaches predict drug sensitivity by building prediction models with individual genes, which suffer from low reproducibility due to biologic variability and difficulty to interpret biological relevance of novel gene-drug associations. As an alternative, pathway activity scores derived from gene expression could predict drug response of cancer cells. Method: In this study, pathway-based prediction models were built with four approaches inferring pathway activity in unsupervised manner, including competitive scoring approaches (DiffRank and GSVA) and self-contained scoring approaches (PLAGE and Z-score). These unsupervised pathway activity inference approaches were applied to predict drug responses of cancer cells using data from Cancer Cell Line Encyclopedia (CCLE). Results: Our analysis on all the 24 drugs from CCLE demonstrated that pathway-based models achieved better predictions for 14 out of the 24 drugs, while taking fewer features as inputs. Further investigation on indicated that pathway-based models indeed captured pathways involving drug-related genes (targets, transporters and metabolic enzymes) for majority of drugs, whereas gene-models failed to identify these drug-related genes, in most cases. Among the four approaches, competitive scoring (DiffRank and GSVA) provided more accurate predictions and captured more pathways involving drug-related genes than self-contained scoring (PLAGE and Z-Score). Detailed interpretation of top pathways from the top method (DiffRank) highlights the merit of pathway-based approaches to predict drug response by identifying pathways relevant to drug mechanisms. Conclusion: Taken together, pathway-based modeling with inferred pathway activity is a promising alternative to predict drug response, with the ability to easily interpret results and provide biological insights into the mechanisms of drug actions."
0,Identification of diterpenoid compounds that interfere with Fli-1 DNA binding to suppress leukemogenesis,"The ETS transcription factor Fli-1 controls the expression of genes involved in hematopoiesis including cell proliferation, survival, and differentiation. Dysregulation of Fli-1 induces hematopoietic and solid tumors, rendering it an important target for therapeutic intervention. Through high content screens of a library of chemicals isolated from medicinal plants in China for inhibitors of a Fli-1 transcriptional reporter cells, we hereby report the identification of diterpenoid-like compounds that strongly inhibit Fli-1 transcriptional activity. These agents suppressed the growth of erythroleukemic cells by inducing apoptosis and differentiation. They also inhibited survival and proliferation of B-cell leukemic cell lines as well as primary B-cell lymphocytic leukemia (B-CLL) isolated from 7 patients. Moreover, these inhibitors blocked leukemogenesis in a mouse model of erythroleukemia, in which Fli-1 is the driver of tumor initiation. Computational docking analysis revealed that the diterpenoid-like compounds bind with high affinity to nucleotide residues in a pocket near the major groove within the DNA-binding sites of Fli-1. Functional inhibition of Fli-1 by these compounds triggered its further downregulation through miR-145, whose promoter is normally repressed by Fli-1. These results uncover the importance of Fli-1 in leukemogenesis, a Fli-1-miR145 autoregulatory loop and new anti-Fli-1 diterpenoid agents for the treatment of diverse hematological malignancies overexpressing this transcription factor.","Identification of diterpenoid compounds that interfere with Fli-1 DNA binding to suppress leukemogenesis. The ETS transcription factor Fli-1 controls the expression of genes involved in hematopoiesis including cell proliferation, survival, and differentiation. Dysregulation of Fli-1 induces hematopoietic and solid tumors, rendering it an important target for therapeutic intervention. Through high content screens of a library of chemicals isolated from medicinal plants in China for inhibitors of a Fli-1 transcriptional reporter cells, we hereby report the identification of diterpenoid-like compounds that strongly inhibit Fli-1 transcriptional activity. These agents suppressed the growth of erythroleukemic cells by inducing apoptosis and differentiation. They also inhibited survival and proliferation of B-cell leukemic cell lines as well as primary B-cell lymphocytic leukemia (B-CLL) isolated from 7 patients. Moreover, these inhibitors blocked leukemogenesis in a mouse model of erythroleukemia, in which Fli-1 is the driver of tumor initiation. Computational docking analysis revealed that the diterpenoid-like compounds bind with high affinity to nucleotide residues in a pocket near the major groove within the DNA-binding sites of Fli-1. Functional inhibition of Fli-1 by these compounds triggered its further downregulation through miR-145, whose promoter is normally repressed by Fli-1. These results uncover the importance of Fli-1 in leukemogenesis, a Fli-1-miR145 autoregulatory loop and new anti-Fli-1 diterpenoid agents for the treatment of diverse hematological malignancies overexpressing this transcription factor."
0,Contriving multiepitope subunit vaccine by exploiting structural and nonstructural viral proteins to prevent Epsteinâ€“Barr virus-associated malignancy,"Cancer is one of the common lifestyle diseases and is considered to be the leading cause of death worldwide. Epsteinâ€“Barr virus (EBV)-infected individuals remain asymptomatic; but under certain stress conditions, EBV may lead to the development of cancers such as Burkittâ€™s and Hodgkinâ€™s lymphoma and nasopharyngeal carcinoma. EBV-associated cancers result in a large number of deaths in Asian and African population, and no effective cure has still been developed. We, therefore, tried to devise a subunit vaccine with the help of immunoinformatic approaches that can be used for the prevention of EBV-associated malignancies. The epitopes were predicted through B-cell, cytotoxic T lymphocytes (CTL), and helper T lymphocytes (HTL) from the different oncogenic proteins of EBV. A vaccine was designed by combining the B-cell and T-cell (HTL and CTL) epitopes through linkers, and for the enhancement of immunogenicity, an adjuvant was added at the N-terminal. Further, homology modeling was performed to generate the 3D structure of the designed vaccine. Moreover, molecular docking was performed between the designed vaccine and immune receptor (TLR-3) to determine the interaction between the final vaccine construct and the immune receptor complex. In addition, molecular dynamics was performed to analyze the stable interactions between the ligand final vaccine model and receptor TLR-3 molecule. Lastly, to check the expression of our vaccine construct, we performed in silico cloning. This study needed experimental validation to ensure its effectiveness and potency to control malignancy.","Contriving multiepitope subunit vaccine by exploiting structural and nonstructural viral proteins to prevent Epsteinâ€“Barr virus-associated malignancy. Cancer is one of the common lifestyle diseases and is considered to be the leading cause of death worldwide. Epsteinâ€“Barr virus (EBV)-infected individuals remain asymptomatic; but under certain stress conditions, EBV may lead to the development of cancers such as Burkittâ€™s and Hodgkinâ€™s lymphoma and nasopharyngeal carcinoma. EBV-associated cancers result in a large number of deaths in Asian and African population, and no effective cure has still been developed. We, therefore, tried to devise a subunit vaccine with the help of immunoinformatic approaches that can be used for the prevention of EBV-associated malignancies. The epitopes were predicted through B-cell, cytotoxic T lymphocytes (CTL), and helper T lymphocytes (HTL) from the different oncogenic proteins of EBV. A vaccine was designed by combining the B-cell and T-cell (HTL and CTL) epitopes through linkers, and for the enhancement of immunogenicity, an adjuvant was added at the N-terminal. Further, homology modeling was performed to generate the 3D structure of the designed vaccine. Moreover, molecular docking was performed between the designed vaccine and immune receptor (TLR-3) to determine the interaction between the final vaccine construct and the immune receptor complex. In addition, molecular dynamics was performed to analyze the stable interactions between the ligand final vaccine model and receptor TLR-3 molecule. Lastly, to check the expression of our vaccine construct, we performed in silico cloning. This study needed experimental validation to ensure its effectiveness and potency to control malignancy."
0,Artificial intelligence and computer-aided diagnosis in colonoscopy: current evidence and future directions,"Computer-aided diagnosis offers a promising solution to reduce variation in colonoscopy performance. Pooled miss rates for polyps are as high as 22%, and associated interval colorectal cancers after colonoscopy are of concern. Optical biopsy, whereby in-vivo classification of polyps based on enhanced imaging replaces histopathology, has not been incorporated into routine practice because it is limited by interobserver variability and generally only meets accepted standards in expert settings. Real-time decision-support software has been developed to detect and characterise polyps, and also to offer feedback on the technical quality of inspection. Some of the current algorithms, particularly with recent advances in artificial intelligence techniques, match human expert performance for optical biopsy. In this Review, we summarise the evidence for clinical applications of computer-aided diagnosis and artificial intelligence in colonoscopy.","Artificial intelligence and computer-aided diagnosis in colonoscopy: current evidence and future directions. Computer-aided diagnosis offers a promising solution to reduce variation in colonoscopy performance. Pooled miss rates for polyps are as high as 22%, and associated interval colorectal cancers after colonoscopy are of concern. Optical biopsy, whereby in-vivo classification of polyps based on enhanced imaging replaces histopathology, has not been incorporated into routine practice because it is limited by interobserver variability and generally only meets accepted standards in expert settings. Real-time decision-support software has been developed to detect and characterise polyps, and also to offer feedback on the technical quality of inspection. Some of the current algorithms, particularly with recent advances in artificial intelligence techniques, match human expert performance for optical biopsy. In this Review, we summarise the evidence for clinical applications of computer-aided diagnosis and artificial intelligence in colonoscopy."
0,Single-Cell Delineation of Who's on First and Second Heart Fields During Development,,
0,Generative adversarial network in medical imaging: A review,,
0,Neuronal Development and Onset of Electrical Activity in the Human Enteric Nervous System,,
0,Detection of Brain Activation in Unresponsive Patients with Acute Brain Injury,"BACKGROUND: Brain activation in response to spoken motor commands can be detected by electroencephalography (EEG) in clinically unresponsive patients. The prevalence and prognostic importance of a dissociation between commanded motor behavior and brain activation in the first few days after brain injury are not well understood. METHODS: We studied a prospective, consecutive series of patients in a single intensive care unit who had acute brain injury from a variety of causes and who were unresponsive to spoken commands, including some patients with the ability to localize painful stimuli or to fixate on or track visual stimuli. Machine learning was applied to EEG recordings to detect brain activation in response to commands that patients move their hands. The functional outcome at 12 months was determined with the Glasgow Outcome Scale-Extended (GOS-E; levels range from 1 to 8, with higher levels indicating better outcomes). RESULTS: A total of 16 of 104 unresponsive patients (15%) had brain activation detected by EEG at a median of 4 days after injury. The condition in 8 of these 16 patients (50%) and in 23 of 88 patients (26%) without brain activation improved such that they were able to follow commands before discharge. At 12 months, 7 of 16 patients (44%) with brain activation and 12 of 84 patients (14%) without brain activation had a GOS-E level of 4 or higher, denoting the ability to function independently for 8 hours (odds ratio, 4.6; 95% confidence interval, 1.2 to 17.1). CONCLUSIONS: A dissociation between the absence of behavioral responses to motor commands and the evidence of brain activation in response to these commands in EEG recordings was found in 15% of patients in a consecutive series of patients with acute brain injury. (Supported by the Dana Foundation and the James S. McDonnell Foundation.).","Detection of Brain Activation in Unresponsive Patients with Acute Brain Injury. BACKGROUND: Brain activation in response to spoken motor commands can be detected by electroencephalography (EEG) in clinically unresponsive patients. The prevalence and prognostic importance of a dissociation between commanded motor behavior and brain activation in the first few days after brain injury are not well understood. METHODS: We studied a prospective, consecutive series of patients in a single intensive care unit who had acute brain injury from a variety of causes and who were unresponsive to spoken commands, including some patients with the ability to localize painful stimuli or to fixate on or track visual stimuli. Machine learning was applied to EEG recordings to detect brain activation in response to commands that patients move their hands. The functional outcome at 12 months was determined with the Glasgow Outcome Scale-Extended (GOS-E; levels range from 1 to 8, with higher levels indicating better outcomes). RESULTS: A total of 16 of 104 unresponsive patients (15%) had brain activation detected by EEG at a median of 4 days after injury. The condition in 8 of these 16 patients (50%) and in 23 of 88 patients (26%) without brain activation improved such that they were able to follow commands before discharge. At 12 months, 7 of 16 patients (44%) with brain activation and 12 of 84 patients (14%) without brain activation had a GOS-E level of 4 or higher, denoting the ability to function independently for 8 hours (odds ratio, 4.6; 95% confidence interval, 1.2 to 17.1). CONCLUSIONS: A dissociation between the absence of behavioral responses to motor commands and the evidence of brain activation in response to these commands in EEG recordings was found in 15% of patients in a consecutive series of patients with acute brain injury. (Supported by the Dana Foundation and the James S. McDonnell Foundation.)."
0,Machine Learning in Medicine,,
0,Does Hepatocellular Carcinoma Screening with US Work? Using the US LI-RADS Algorithm,,
0,Development and Performance of a Checklist for Initial Triage After an Anthrax Mass Exposure Event,"Background: Population exposure to Bacillus anthracis spores could cause mass casualties requiring complex medical care. Rapid identification of patients needing anthrax-specific therapies will improve patient outcomes and resource use. Objective: To develop a checklist that rapidly distinguishes most anthrax from nonanthrax illnesses on the basis of clinical presentation and identifies patients requiring diagnostic testing after a population exposure. Design: Comparison of published anthrax case reports from 1880 through 2013 that included patients seeking anthrax-related care at 2 epicenters of the 2001 U.S. anthrax attacks. Setting: Outpatient and inpatient. Patients: 408 case patients with inhalation, ingestion, and cutaneous anthrax and primary anthrax meningitis, and 657 control patients. Measurements: Diagnostic test characteristics, including positive and negative likelihood ratios (LRs) and patient triage assignation. Results: Checklist-directed triage without diagnostic testing correctly classified 95% (95% CI, 93% to 97%) of 353 adult anthrax case patients and 76% (CI, 73% to 79%) of 647 control patients (positive LR, 3.96 [CI, 3.45 to 4.55]; negative LR, 0.07 [CI, 0.04 to 0.11]; false-negative rate, 5%; false-positive rate, 24%). Diagnostic testing was needed for triage in up to 5% of case patients and 15% of control patients and improved overall test characteristics (positive LR, 8.90 [CI, 7.05 to 11.24]; negative LR, 0.06 [CI, 0.04 to 0.09]; false-negative rate, 5%; false-positive rate, 11%). Checklist sensitivity and specificity were minimally affected by inclusion of pediatric patients. Sensitivity increased to 97% (CI, 94% to 100%) and 98% (CI, 96% to 100%), respectively, when only inhalation anthrax cases or higher-quality case reports were investigated. Limitations: Data on case patients were limited to nonstandardized, published observational reports, many of which lacked complete data on symptoms and signs of interest. Reporting bias favoring more severe cases and lack of intercurrent outbreaks (such as influenza) in the control populations may have improved test characteristics. Conclusion: A brief checklist covering symptoms and signs can distinguish anthrax from other conditions with minimal need for diagnostic testing after known or suspected population exposure. Primary Funding Source: U.S. Department of Health and Human Services.","Development and Performance of a Checklist for Initial Triage After an Anthrax Mass Exposure Event. Background: Population exposure to Bacillus anthracis spores could cause mass casualties requiring complex medical care. Rapid identification of patients needing anthrax-specific therapies will improve patient outcomes and resource use. Objective: To develop a checklist that rapidly distinguishes most anthrax from nonanthrax illnesses on the basis of clinical presentation and identifies patients requiring diagnostic testing after a population exposure. Design: Comparison of published anthrax case reports from 1880 through 2013 that included patients seeking anthrax-related care at 2 epicenters of the 2001 U.S. anthrax attacks. Setting: Outpatient and inpatient. Patients: 408 case patients with inhalation, ingestion, and cutaneous anthrax and primary anthrax meningitis, and 657 control patients. Measurements: Diagnostic test characteristics, including positive and negative likelihood ratios (LRs) and patient triage assignation. Results: Checklist-directed triage without diagnostic testing correctly classified 95% (95% CI, 93% to 97%) of 353 adult anthrax case patients and 76% (CI, 73% to 79%) of 647 control patients (positive LR, 3.96 [CI, 3.45 to 4.55]; negative LR, 0.07 [CI, 0.04 to 0.11]; false-negative rate, 5%; false-positive rate, 24%). Diagnostic testing was needed for triage in up to 5% of case patients and 15% of control patients and improved overall test characteristics (positive LR, 8.90 [CI, 7.05 to 11.24]; negative LR, 0.06 [CI, 0.04 to 0.09]; false-negative rate, 5%; false-positive rate, 11%). Checklist sensitivity and specificity were minimally affected by inclusion of pediatric patients. Sensitivity increased to 97% (CI, 94% to 100%) and 98% (CI, 96% to 100%), respectively, when only inhalation anthrax cases or higher-quality case reports were investigated. Limitations: Data on case patients were limited to nonstandardized, published observational reports, many of which lacked complete data on symptoms and signs of interest. Reporting bias favoring more severe cases and lack of intercurrent outbreaks (such as influenza) in the control populations may have improved test characteristics. Conclusion: A brief checklist covering symptoms and signs can distinguish anthrax from other conditions with minimal need for diagnostic testing after known or suspected population exposure. Primary Funding Source: U.S. Department of Health and Human Services."
0,AI can now identify atrial fibrillation through sinus rhythm,,
0,The road map of cancer precision medicine with the innovation of advanced cancer detection technology and personalized immunotherapy,"The advancement of cancer genomics research due to the development of next generation sequencing technologies is going to bring the promise of cancer precision medicine, in turn revolutionizing cancer detection and treatment. In this review, we will discuss the possible road map for implementation of cancer precision medicine into the clinical practice by mainly focusing on the role of liquid biopsy, particularly circulating tumor DNA, as a potential tool for cancer screening, selection of an appropriate drug(s), surveillance of minimal residual diseases, and early detection of recurrence. We will also review the current status of genome-driven oncology and emerging field of immunotherapies that could be provided to patients to improve their clinical outcome and quality of life. Lastly, we will discuss the usefulness of artificial intelligence that facilitate complex data integration in our health care/medical care system.","The road map of cancer precision medicine with the innovation of advanced cancer detection technology and personalized immunotherapy. The advancement of cancer genomics research due to the development of next generation sequencing technologies is going to bring the promise of cancer precision medicine, in turn revolutionizing cancer detection and treatment. In this review, we will discuss the possible road map for implementation of cancer precision medicine into the clinical practice by mainly focusing on the role of liquid biopsy, particularly circulating tumor DNA, as a potential tool for cancer screening, selection of an appropriate drug(s), surveillance of minimal residual diseases, and early detection of recurrence. We will also review the current status of genome-driven oncology and emerging field of immunotherapies that could be provided to patients to improve their clinical outcome and quality of life. Lastly, we will discuss the usefulness of artificial intelligence that facilitate complex data integration in our health care/medical care system."
0,Activation of the PP2A catalytic subunit by ivabradine attenuates the development of diabetic cardiomyopathy,"Hyperglycemia-induced apoptosis plays a critical role in the pathogenesis of diabetic cardiomyopathy (DCM). Our previous study demonstrated that ivabradine, a selective If current antagonist, significantly attenuated myocardial apoptosis in diabetic mice, but the underlying mechanisms remained unknown. This study investigated the underlying mechanisms by which ivabradine exerts anti-apoptotic effects in experimental DCM. Pretreatment with ivabradine, but not ZD7288 (an established If current blocker), profoundly inhibited high glucose-induced apoptosis via inactivation of nuclear factor (NF)-ÎºB signaling in neonatal rat cardiomyocytes. The effect was abolished by transfection of an siRNA targeting protein phosphatase 2A catalytic subunit (PP2Ac). In streptozotocin-induced diabetic mice, ivabradine treatment significantly inhibited left ventricular hyperpolarization-activated cyclic nucleotide-gated channel 2 (HCN2) and HCN4 (major components of the If current), activated PP2Ac, and attenuated NF-ÎºB signaling activation and apoptosis, in line with improved histological abnormalities, fibrosis, and cardiac dysfunction without affecting hyperglycemia. These effects were not observed in diabetic mice with virus-mediated knockdown of HCN2 or HCN4 after myocardial injection, but were alleviated by knockdown of PP2AcÎ±. Molecular docking and phosphatase activity assay confirmed direct binding of ivabradine to, and activation of, PP2Ac. In conclusion, ivabradine may directly activate PP2Ac, leading to inhibition of NF-ÎºB signaling activation, myocardial apoptosis, and fibrosis, and eventually improving cardiac function in experimental DCM. Taken together, the present findings suggest that ivabradine may be a promising drug for treatment of DCM.","Activation of the PP2A catalytic subunit by ivabradine attenuates the development of diabetic cardiomyopathy. Hyperglycemia-induced apoptosis plays a critical role in the pathogenesis of diabetic cardiomyopathy (DCM). Our previous study demonstrated that ivabradine, a selective If current antagonist, significantly attenuated myocardial apoptosis in diabetic mice, but the underlying mechanisms remained unknown. This study investigated the underlying mechanisms by which ivabradine exerts anti-apoptotic effects in experimental DCM. Pretreatment with ivabradine, but not ZD7288 (an established If current blocker), profoundly inhibited high glucose-induced apoptosis via inactivation of nuclear factor (NF)-ÎºB signaling in neonatal rat cardiomyocytes. The effect was abolished by transfection of an siRNA targeting protein phosphatase 2A catalytic subunit (PP2Ac). In streptozotocin-induced diabetic mice, ivabradine treatment significantly inhibited left ventricular hyperpolarization-activated cyclic nucleotide-gated channel 2 (HCN2) and HCN4 (major components of the If current), activated PP2Ac, and attenuated NF-ÎºB signaling activation and apoptosis, in line with improved histological abnormalities, fibrosis, and cardiac dysfunction without affecting hyperglycemia. These effects were not observed in diabetic mice with virus-mediated knockdown of HCN2 or HCN4 after myocardial injection, but were alleviated by knockdown of PP2AcÎ±. Molecular docking and phosphatase activity assay confirmed direct binding of ivabradine to, and activation of, PP2Ac. In conclusion, ivabradine may directly activate PP2Ac, leading to inhibition of NF-ÎºB signaling activation, myocardial apoptosis, and fibrosis, and eventually improving cardiac function in experimental DCM. Taken together, the present findings suggest that ivabradine may be a promising drug for treatment of DCM."
0,Comparison and validation of accelerometer wear time and non-wear time algorithms for assessing physical activity levels in children and adolescents,"BACKGROUND: Accelerometers are widely used to measure sedentary time and daily physical activity (PA). However, data collection and processing criteria, such as non-wear time rules might affect the assessment of total PA and sedentary time and the associations with health variables. The study aimed to investigate whether the choice of different non-wear time definitions would affect the outcomes of PA levels in youth. METHODS: Seventy-seven healthy youngsters (44 boys), aged 10-17â€‰years, wore an accelerometer and kept a non-wear log diary during 4 consecutives days. We compared 7 published algorithms (10, 15, 20, 30, 60â€‰min of continuous zeros, Choi, and Troiano algorithms). Agreements of each algorithm with the log diary method were assessed using Bland-Altmans plots and by calculating the concordance correlation coefficient for repeated measures. RESULTS: Variations in time spent in sedentary and moderate to vigorous PA (MVPA) were 30 and 3.7%. Compared with the log diary method, greater discrepancies were found for the algorithmÂ 10â€‰min (pâ€‰<â€‰0.001). For the time assessed in sedentary, the agreement with diary was excellent for the 4 algorithms (Choi, râ€‰=â€‰0.79; Troiano, râ€‰=â€‰0.81; 30â€‰min, râ€‰=â€‰0.79; 60â€‰min, râ€‰=â€‰0.81). Concordance for each method was excellent for the assessment of time spent in MVPA (>â€‰0.86). The agreement for the wear time assessment was excellent for 5 algorithms (Choi râ€‰=â€‰0.79; Troiano râ€‰=â€‰0.79; 20â€‰min râ€‰=â€‰0.77; 30â€‰min râ€‰=â€‰0.80; 60â€‰min râ€‰=â€‰0.80). CONCLUSIONS: The choice of non-wear time rules may considerably affect the sedentary time assessment in youth. Using of appropriate data reduction decision in youth is needed to limit differences in associations between health outcomes and sedentary behaviors and may improve comparability for future studies. Based on our results, we recommend the use of the algorithm of 30â€‰min of continuous zeros for defining non-wear time to improve the accuracy in assessing PA levels in youth. TRIAL REGISTRATION: NCT02844101 (retrospectively registered at July 13th 2016).","Comparison and validation of accelerometer wear time and non-wear time algorithms for assessing physical activity levels in children and adolescents. BACKGROUND: Accelerometers are widely used to measure sedentary time and daily physical activity (PA). However, data collection and processing criteria, such as non-wear time rules might affect the assessment of total PA and sedentary time and the associations with health variables. The study aimed to investigate whether the choice of different non-wear time definitions would affect the outcomes of PA levels in youth. METHODS: Seventy-seven healthy youngsters (44 boys), aged 10-17â€‰years, wore an accelerometer and kept a non-wear log diary during 4 consecutives days. We compared 7 published algorithms (10, 15, 20, 30, 60â€‰min of continuous zeros, Choi, and Troiano algorithms). Agreements of each algorithm with the log diary method were assessed using Bland-Altmans plots and by calculating the concordance correlation coefficient for repeated measures. RESULTS: Variations in time spent in sedentary and moderate to vigorous PA (MVPA) were 30 and 3.7%. Compared with the log diary method, greater discrepancies were found for the algorithmÂ 10â€‰min (pâ€‰<â€‰0.001). For the time assessed in sedentary, the agreement with diary was excellent for the 4 algorithms (Choi, râ€‰=â€‰0.79; Troiano, râ€‰=â€‰0.81; 30â€‰min, râ€‰=â€‰0.79; 60â€‰min, râ€‰=â€‰0.81). Concordance for each method was excellent for the assessment of time spent in MVPA (>â€‰0.86). The agreement for the wear time assessment was excellent for 5 algorithms (Choi râ€‰=â€‰0.79; Troiano râ€‰=â€‰0.79; 20â€‰min râ€‰=â€‰0.77; 30â€‰min râ€‰=â€‰0.80; 60â€‰min râ€‰=â€‰0.80). CONCLUSIONS: The choice of non-wear time rules may considerably affect the sedentary time assessment in youth. Using of appropriate data reduction decision in youth is needed to limit differences in associations between health outcomes and sedentary behaviors and may improve comparability for future studies. Based on our results, we recommend the use of the algorithm of 30â€‰min of continuous zeros for defining non-wear time to improve the accuracy in assessing PA levels in youth. TRIAL REGISTRATION: NCT02844101 (retrospectively registered at July 13th 2016)."
0,Structure and function of a monocarboxylate transporter homolog specific for L-lactate,Monocarboxylate transporters play important roles in certain cancers. We have reported structures of an L-lactate-transporting solute carrier family 16 homolog with bound substrate and inhibitor. The structures show the transporter in the pharmacologically relevant outward-open conformation. Structureâ€“function analysis provides insights into the molecular working mechanisms of ligand binding and L-lactate transport.,Structure and function of a monocarboxylate transporter homolog specific for L-lactate. Monocarboxylate transporters play important roles in certain cancers. We have reported structures of an L-lactate-transporting solute carrier family 16 homolog with bound substrate and inhibitor. The structures show the transporter in the pharmacologically relevant outward-open conformation. Structureâ€“function analysis provides insights into the molecular working mechanisms of ligand binding and L-lactate transport.
0,Trials and tribulations of corrected QT interval monitoring in oncology: Rationale for a practice-changing standardized approach,,
0,"Comment on ""Is Medicine Still an Art?""",,
0,Tracking More and Better Surgical Evidence,,
0,In silico identification and in vitro validation of nogalamycin N-oxide (NSC116555) as a potent anticancer compound against nonâ€“small-cell lung cancer cells,"The epidermal growth factor receptor (EGFR) was found to be overexpressed in several cancers, especially in lung cancers. Finding new effective drug against EGFR is the key to cancer treatment. In this study, the GOLD docking algorithm was used to virtually screen for novel human EGFR inhibitors from the NCI database. Thirty-four hit compounds were tested for EGFR-tyrosine kinase (TK) inhibition. Two potent compounds, 1-amino-4-(4-[4-amino-2-sulfophenyl]anilino)-9,10-dioxoanthracene-2-sulfonic acid (NSC125910), and nogalamycin N-oxide (NSC116555) were identified with IC50 values against EGFR-TK comparable to gefitinib; 16.14 and 37.71 nM, respectively. However, only NSC116555 demonstrated cytotoxic effects against nonâ€“small-cell lung cancer, A549, shown in the cell cytotoxicity assay with an IC50 of 0.19 + 0.01 ÂµM, which was more potent than gefitinib. Furthermore, NSC116555 showed cytotoxicity against A549 via apoptosis in a dose-dependent manner.","In silico identification and in vitro validation of nogalamycin N-oxide (NSC116555) as a potent anticancer compound against nonâ€“small-cell lung cancer cells. The epidermal growth factor receptor (EGFR) was found to be overexpressed in several cancers, especially in lung cancers. Finding new effective drug against EGFR is the key to cancer treatment. In this study, the GOLD docking algorithm was used to virtually screen for novel human EGFR inhibitors from the NCI database. Thirty-four hit compounds were tested for EGFR-tyrosine kinase (TK) inhibition. Two potent compounds, 1-amino-4-(4-[4-amino-2-sulfophenyl]anilino)-9,10-dioxoanthracene-2-sulfonic acid (NSC125910), and nogalamycin N-oxide (NSC116555) were identified with IC50 values against EGFR-TK comparable to gefitinib; 16.14 and 37.71 nM, respectively. However, only NSC116555 demonstrated cytotoxic effects against nonâ€“small-cell lung cancer, A549, shown in the cell cytotoxicity assay with an IC50 of 0.19 + 0.01 ÂµM, which was more potent than gefitinib. Furthermore, NSC116555 showed cytotoxicity against A549 via apoptosis in a dose-dependent manner."
0,Robot-assisted versus open cystectomy in the RAZOR trial,,
0,"Serum Metabolites as Diagnostic Biomarkers for Cholangiocarcinoma, Hepatocellular Carcinoma, and Primary Sclerosing Cholangitis",,
0,Chronic Obstructive Pulmonary Disease: Thoracic CT Texture Analysis and Machine Learnins to Predict Pulmonary Ventilation,,
0,The effect of motion correction interpolation on quantitative T1 mapping with MRI,"Quantitative magnetic resonance imaging (qMRI) is a technique for mapping the physical properties of the underlying tissue using several MR images with different contrasts. To overcome subject motion between the acquired images, it is necessary to register the images to a common reference frame. A drawback of registration is the use of interpolation and resampling techniques, which can introduce artifacts into the interpolated data. These artifacts could have unfavorable effects on the accuracy of the estimated tissue's physical properties. Here, we quantified the error of interpolation and resampling on T1-weighted images and studied its effects on the mapping of the longitudinal relaxation time (T1) using variable flip angles. We simulated T1-weighted images and calculated the transformation error resulting from interpolation and resampling. We found that the error is a function of the image contrast (i.e., flip angle) and of the translation and rotation of the image. Furthermore, we found that the error in the T1-weighted images has a substantial effect on the T1 estimation, of the order of 10% of the signal in the brain's gray and white matter. Hence, minimizing the registration error can enable more accurate in vivo modeling of brain microstructure.","The effect of motion correction interpolation on quantitative T1 mapping with MRI. Quantitative magnetic resonance imaging (qMRI) is a technique for mapping the physical properties of the underlying tissue using several MR images with different contrasts. To overcome subject motion between the acquired images, it is necessary to register the images to a common reference frame. A drawback of registration is the use of interpolation and resampling techniques, which can introduce artifacts into the interpolated data. These artifacts could have unfavorable effects on the accuracy of the estimated tissue's physical properties. Here, we quantified the error of interpolation and resampling on T1-weighted images and studied its effects on the mapping of the longitudinal relaxation time (T1) using variable flip angles. We simulated T1-weighted images and calculated the transformation error resulting from interpolation and resampling. We found that the error is a function of the image contrast (i.e., flip angle) and of the translation and rotation of the image. Furthermore, we found that the error in the T1-weighted images has a substantial effect on the T1 estimation, of the order of 10% of the signal in the brain's gray and white matter. Hence, minimizing the registration error can enable more accurate in vivo modeling of brain microstructure."
0,Validation of an algorithmic nutritional approach in children undergoing chemotherapy for cancer,"Background: Undernutrition impacts clinical outcome adversely in children with cancer. This study aimed to validate a nutritional algorithm with specific application to the low- and middle-income country (LMIC) setting. Procedure: Fifty children with a new diagnosis of cancer were enrolled in this randomized interventional study. Weight, height/length, and mid-upper-arm circumference (MUAC) were measured at baseline. The study arm was administered nutritional care as per the algorithm and the control arm received the institutional standard of care. Weight was monitored regularly and MUAC was repeated after 3 months. Children were classified based on weight for height if <2 years of age or body mass index if â‰¥2 years, as normal, wasted, and severely wasted. The algorithmic approach comprised administration of oral supplements, nasogastric feeds, and/or parenteral nutrition based on objective assessment of the nutritional status. Results: Fifty patients were analyzed (study: 25, control: 25). Four in the study arm (16%) and six in the control arm (24%) had wasting at baseline. MUAC was <5th percentile in 15 (60%) and 13 (52%) patients in the study and control arms, respectively. At the end of 3 months, the median increment in weight was 0.8Â kg (interquartile range [IQR]: â€“0.02; 2.00) and 0.0Â kg (IQR: â€“0.70; 1.25) in the study and control arms, respectively (PÂ =.153). The median increment in MUAC was 1.20Â cm (IQR: 0.10; 2.30) and 0.00Â cm (IQR: â€“0.50; 1.10) in the study and control arms, respectively (PÂ =.020). Conclusions: The application of an algorithm designed for use in LMICs resulted in significant improvement in nutritional status, as measured by MUAC.","Validation of an algorithmic nutritional approach in children undergoing chemotherapy for cancer. Background: Undernutrition impacts clinical outcome adversely in children with cancer. This study aimed to validate a nutritional algorithm with specific application to the low- and middle-income country (LMIC) setting. Procedure: Fifty children with a new diagnosis of cancer were enrolled in this randomized interventional study. Weight, height/length, and mid-upper-arm circumference (MUAC) were measured at baseline. The study arm was administered nutritional care as per the algorithm and the control arm received the institutional standard of care. Weight was monitored regularly and MUAC was repeated after 3 months. Children were classified based on weight for height if <2 years of age or body mass index if â‰¥2 years, as normal, wasted, and severely wasted. The algorithmic approach comprised administration of oral supplements, nasogastric feeds, and/or parenteral nutrition based on objective assessment of the nutritional status. Results: Fifty patients were analyzed (study: 25, control: 25). Four in the study arm (16%) and six in the control arm (24%) had wasting at baseline. MUAC was <5th percentile in 15 (60%) and 13 (52%) patients in the study and control arms, respectively. At the end of 3 months, the median increment in weight was 0.8Â kg (interquartile range [IQR]: â€“0.02; 2.00) and 0.0Â kg (IQR: â€“0.70; 1.25) in the study and control arms, respectively (PÂ =.153). The median increment in MUAC was 1.20Â cm (IQR: 0.10; 2.30) and 0.00Â cm (IQR: â€“0.50; 1.10) in the study and control arms, respectively (PÂ =.020). Conclusions: The application of an algorithm designed for use in LMICs resulted in significant improvement in nutritional status, as measured by MUAC."
0,HPOAnnotator: improving large-scale prediction of HPO annotations by low-rank approximation with HPO semantic similarities and multiple PPI networks,"Background: As a standardized vocabulary of phenotypic abnormalities associated with human diseases, the Human Phenotype Ontology (HPO) has been widely used by researchers to annotate phenotypes of genes/proteins. For saving the cost and time spent on experiments, many computational approaches have been proposed. They are able to alleviate the problem to some extent, but their performances are still far from satisfactory. Method: For inferring large-scale protein-phenotype associations, we propose HPOAnnotator that incorporates multiple Protein-Protein Interaction (PPI) information and the hierarchical structure of HPO. Specifically, we use a dual graph to regularize Non-negative Matrix Factorization (NMF) in a way that the information from different sources can be seamlessly integrated. In essence, HPOAnnotator solves the sparsity problem of a protein-phenotype association matrix by using a low-rank approximation. Results: By combining the hierarchical structure of HPO and co-annotations of proteins, our model can well capture the HPO semantic similarities. Moreover, graph Laplacian regularizations are imposed in the latent space so as to utilize multiple PPI networks. The performance of HPOAnnotator has been validated under cross-validation and independent test. Experimental results have shown that HPOAnnotator outperforms the competing methods significantly. Conclusions: Through extensive comparisons with the state-of-the-art methods, we conclude that the proposed HPOAnnotator is able to achieve the superior performance as a result of using a low-rank approximation with a graph regularization. It is promising in that our approach can be considered as a starting point to study more efficient matrix factorization-based algorithms.","HPOAnnotator: improving large-scale prediction of HPO annotations by low-rank approximation with HPO semantic similarities and multiple PPI networks. Background: As a standardized vocabulary of phenotypic abnormalities associated with human diseases, the Human Phenotype Ontology (HPO) has been widely used by researchers to annotate phenotypes of genes/proteins. For saving the cost and time spent on experiments, many computational approaches have been proposed. They are able to alleviate the problem to some extent, but their performances are still far from satisfactory. Method: For inferring large-scale protein-phenotype associations, we propose HPOAnnotator that incorporates multiple Protein-Protein Interaction (PPI) information and the hierarchical structure of HPO. Specifically, we use a dual graph to regularize Non-negative Matrix Factorization (NMF) in a way that the information from different sources can be seamlessly integrated. In essence, HPOAnnotator solves the sparsity problem of a protein-phenotype association matrix by using a low-rank approximation. Results: By combining the hierarchical structure of HPO and co-annotations of proteins, our model can well capture the HPO semantic similarities. Moreover, graph Laplacian regularizations are imposed in the latent space so as to utilize multiple PPI networks. The performance of HPOAnnotator has been validated under cross-validation and independent test. Experimental results have shown that HPOAnnotator outperforms the competing methods significantly. Conclusions: Through extensive comparisons with the state-of-the-art methods, we conclude that the proposed HPOAnnotator is able to achieve the superior performance as a result of using a low-rank approximation with a graph regularization. It is promising in that our approach can be considered as a starting point to study more efficient matrix factorization-based algorithms."
0,SOD1 in amyotrophic lateral sclerosis development â€“ in silico analysis and molecular dynamics of A4F and A4V variants,"Amyotrophic lateral sclerosis (ALS) is a neurodegenerative disease that is characterized by the selective loss of motor neurons. Approximately 5% to 10% of patients with ALS have a family history of the disease, and approximately 20% of familial amyotrophic lateral sclerosis (fALS) cases are associated with mutations in Cu/Zn superoxide dismutase (SOD1). In this study, we evaluated the structural and functional effects of human A4F and A4V SOD1 protein mutations. We performed an in silico analysis using prediction algorithms of nonsynonymous single-nucleotide polymorphisms (nsSNPs) associated with the fALS development. Our structural conservation results show that the mutations analyzed (A4V and A4F) were in a highly conserved region. Molecular dynamics simulations using the Linux GROMACS package revealed how these mutations affect protein structure, protein stability, and aggregation. These results suggest that there might be an effect on the SOD1 function. Understanding the molecular basis of disease provides new insights useful for rational drug design and advancing our understanding of the ALS development.","SOD1 in amyotrophic lateral sclerosis development â€“ in silico analysis and molecular dynamics of A4F and A4V variants. Amyotrophic lateral sclerosis (ALS) is a neurodegenerative disease that is characterized by the selective loss of motor neurons. Approximately 5% to 10% of patients with ALS have a family history of the disease, and approximately 20% of familial amyotrophic lateral sclerosis (fALS) cases are associated with mutations in Cu/Zn superoxide dismutase (SOD1). In this study, we evaluated the structural and functional effects of human A4F and A4V SOD1 protein mutations. We performed an in silico analysis using prediction algorithms of nonsynonymous single-nucleotide polymorphisms (nsSNPs) associated with the fALS development. Our structural conservation results show that the mutations analyzed (A4V and A4F) were in a highly conserved region. Molecular dynamics simulations using the Linux GROMACS package revealed how these mutations affect protein structure, protein stability, and aggregation. These results suggest that there might be an effect on the SOD1 function. Understanding the molecular basis of disease provides new insights useful for rational drug design and advancing our understanding of the ALS development."
0,Selective organ ischaemia/reperfusion identifies liver as the key driver of the post-injury plasma metabolome derangements,"Background. Understanding the molecular mechanisms in perturbation of the metabolome following ischaemia and reperfusion is critical in developing novel therapeutic strategies to prevent the sequelae of post-injury shock. While the metabolic substrates fueling these alterations have been defined, the relative contribution of specific organs to the systemic metabolic reprogramming secondary to ischaemic or haemorrhagic hypoxia remains unclear. Materials and methods. A porcine model of selected organ ischaemia was employed to investigate the relative contribution of liver, kidney, spleen and small bowel ischaemia/reperfusion to the plasma metabolic phenotype, as gleaned through ultra-high performance liquid chromatography-mass spectrometry-based metabolomics. Results. Liver ischaemia/reperfusion promotes glycaemia, with increases in circulating carboxylic acid anions and purine oxidation metabolites, suggesting that this organ is the dominant contributor to the accumulation of these metabolites in response to ischaemic hypoxia. Succinate, in particular, accumulates selectively in response to the hepatic ischemia, with levels 6.5 times spleen, 8.2 times small bowel, and 6 times renal levels. Similar trends, but lower fold-change increase in comparison to baseline values, were observed upon ischaemia/reperfusion of kidney, spleen and small bowel. Discussion. These observations suggest that the liver may play a critical role in mediating the accumulation of the same metabolites in response to haemorrhagic hypoxia, especially with respect to succinate, a metabolite that has been increasingly implicated in the coagulopathy and pro-inflammatory sequelae of ischaemic and haemorrhagic shock.","Selective organ ischaemia/reperfusion identifies liver as the key driver of the post-injury plasma metabolome derangements. Background. Understanding the molecular mechanisms in perturbation of the metabolome following ischaemia and reperfusion is critical in developing novel therapeutic strategies to prevent the sequelae of post-injury shock. While the metabolic substrates fueling these alterations have been defined, the relative contribution of specific organs to the systemic metabolic reprogramming secondary to ischaemic or haemorrhagic hypoxia remains unclear. Materials and methods. A porcine model of selected organ ischaemia was employed to investigate the relative contribution of liver, kidney, spleen and small bowel ischaemia/reperfusion to the plasma metabolic phenotype, as gleaned through ultra-high performance liquid chromatography-mass spectrometry-based metabolomics. Results. Liver ischaemia/reperfusion promotes glycaemia, with increases in circulating carboxylic acid anions and purine oxidation metabolites, suggesting that this organ is the dominant contributor to the accumulation of these metabolites in response to ischaemic hypoxia. Succinate, in particular, accumulates selectively in response to the hepatic ischemia, with levels 6.5 times spleen, 8.2 times small bowel, and 6 times renal levels. Similar trends, but lower fold-change increase in comparison to baseline values, were observed upon ischaemia/reperfusion of kidney, spleen and small bowel. Discussion. These observations suggest that the liver may play a critical role in mediating the accumulation of the same metabolites in response to haemorrhagic hypoxia, especially with respect to succinate, a metabolite that has been increasingly implicated in the coagulopathy and pro-inflammatory sequelae of ischaemic and haemorrhagic shock."
0,Fast predictive simple geodesic regression,,
0,"miRNAs responsive to the diabetic microenvironment in the human beta cell line EndoC-Î²H1 may target genes in the FOXO, HIPPO and Lysine degradation pathways","Altered expression of miRNAs is evident in the islets of diabetic human donors, but the effects of specific aspects of the diabetic microenvironment and identity of gene ontology pathways demonstrating target gene enrichment in response to each is understudied. We assessed changes in the miRNA milieu in response to high/low glucose, hypoxia, dyslipidaemia and inflammatory factors in a humanised EndoC-Î²H1 beta cell culture system and performed miRPath analysis for each treatment individually. The 10 miRNAs demonstrating the greatest dysregulation across treatments were then independently validated and Gene Set Enrichment Analysis to confirm targeted pathways undertaken. 171 of 392 miRNAs displayed altered expression in response to one or more cellular stressors. miRNA changes were treatment specific, but their target genes were enriched in conserved pathways. 5 miRNAs (miR-136-5p, miR299-5p, miR-454-5p, miR-152 and miR-185) were dysregulated in response to multiple stressors and survived validation in independent samples (p = 0.008, 0.002, 0.012, 0.005 and 0.024 respectively). Target genes of dysregulated miRNAs were clustered into FOXO1, HIPPO and Lysine degradation pathways (p = 0.02, p = 5.84 Ã— 10âˆ’5 and p = 3.00 Ã— 10âˆ’3 respectively). We provide evidence that the diabetic microenvironment may induce changes to the expression of miRNAs targeting genes enriched in pathways involved in cell stress response and cell survival.","miRNAs responsive to the diabetic microenvironment in the human beta cell line EndoC-Î²H1 may target genes in the FOXO, HIPPO and Lysine degradation pathways. Altered expression of miRNAs is evident in the islets of diabetic human donors, but the effects of specific aspects of the diabetic microenvironment and identity of gene ontology pathways demonstrating target gene enrichment in response to each is understudied. We assessed changes in the miRNA milieu in response to high/low glucose, hypoxia, dyslipidaemia and inflammatory factors in a humanised EndoC-Î²H1 beta cell culture system and performed miRPath analysis for each treatment individually. The 10 miRNAs demonstrating the greatest dysregulation across treatments were then independently validated and Gene Set Enrichment Analysis to confirm targeted pathways undertaken. 171 of 392 miRNAs displayed altered expression in response to one or more cellular stressors. miRNA changes were treatment specific, but their target genes were enriched in conserved pathways. 5 miRNAs (miR-136-5p, miR299-5p, miR-454-5p, miR-152 and miR-185) were dysregulated in response to multiple stressors and survived validation in independent samples (p = 0.008, 0.002, 0.012, 0.005 and 0.024 respectively). Target genes of dysregulated miRNAs were clustered into FOXO1, HIPPO and Lysine degradation pathways (p = 0.02, p = 5.84 Ã— 10âˆ’5 and p = 3.00 Ã— 10âˆ’3 respectively). We provide evidence that the diabetic microenvironment may induce changes to the expression of miRNAs targeting genes enriched in pathways involved in cell stress response and cell survival."
0,Automated Design of Pluripotent Stem Cell Self-Organization,"Human pluripotent stem cells (hPSCs) have the intrinsic ability to self-organize into complex multicellular organoids that recapitulate many aspects of tissue development. However, robustly directing morphogenesis of hPSC-derived organoids requires novel approaches to accurately control self-directed pattern formation. Here, we combined genetic engineering with computational modeling, machine learning, and mathematical pattern optimization to create a data-driven approach to control hPSC self-organization by knock down of genes previously shown to affect stem cell colony organization, CDH1 and ROCK1. Computational replication of the in vitro system in silico using an extended cellular Potts model enabled machine learning-driven optimization of parameters that yielded emergence of desired patterns. Furthermore, in vitro the predicted experimental parameters quantitatively recapitulated the in silico patterns. These results demonstrate that morphogenic dynamics can be accurately predicted through model-driven exploration of hPSC behaviors via machine learning, thereby enabling spatial control of multicellular patterning to engineer human organoids and tissues. A record of this paper's Transparent Peer Review process is included in the Supplemental Information.","Automated Design of Pluripotent Stem Cell Self-Organization. Human pluripotent stem cells (hPSCs) have the intrinsic ability to self-organize into complex multicellular organoids that recapitulate many aspects of tissue development. However, robustly directing morphogenesis of hPSC-derived organoids requires novel approaches to accurately control self-directed pattern formation. Here, we combined genetic engineering with computational modeling, machine learning, and mathematical pattern optimization to create a data-driven approach to control hPSC self-organization by knock down of genes previously shown to affect stem cell colony organization, CDH1 and ROCK1. Computational replication of the in vitro system in silico using an extended cellular Potts model enabled machine learning-driven optimization of parameters that yielded emergence of desired patterns. Furthermore, in vitro the predicted experimental parameters quantitatively recapitulated the in silico patterns. These results demonstrate that morphogenic dynamics can be accurately predicted through model-driven exploration of hPSC behaviors via machine learning, thereby enabling spatial control of multicellular patterning to engineer human organoids and tissues. A record of this paper's Transparent Peer Review process is included in the Supplemental Information."
0,Deep Learning for Cardiac MRI: The Time Has Come,,
0,Immunotherapy in myasthenia gravis in the era of biologics,,
0,"Ethics, Transparency, and Public Health at the Intersection of Innovation and Facebook's Suicide Prevention Efforts",,
0,Drug diffusion along an intact mammalian cochlea,"Intratympanic drug administration depends on the ability of drugs to pass through the round window membrane (RW) at the base of the cochlea and diffuse from this location to the apex. While the RW permeability for many different drugs can be promoted, passive diffusion along the narrowing spiral of the cochlea is limited. Earlier measurements of the distribution of marker ions, corticosteroids, and antibiotics demonstrated that the concentration of substances applied to the RW was two to three orders of magnitude higher in the base compared to the apex. The measurements, however, involved perforating the cochlear bony wall and, in some cases, sampling perilymph. These manipulations can change the flow rate of perilymph and lead to intake of perilymph through the cochlear aqueduct, thereby disguising concentration gradients of the delivered substances. In this study, the suppressive effect of salicylate on cochlear amplification via block of the outer hair cell (OHC) somatic motility was utilized to assess salicylate diffusion along an intact guinea pig cochlea in vivo. Salicylate solution was applied to the RW and threshold elevation of auditory nerve responses was measured at different times and frequencies after application. Resultant concentrations of salicylate along the cochlea were calculated by fitting the experimental data using a mathematical model of the diffusion and clearing of salicylate in a tube of variable diameter combined with a model describing salicylate action on cochlear amplification. Concentrations reach a steady-state at different times for different cochlear locations and it takes longer to reach the steady-state at more apical locations. Even at the steady-state, the predicted concentration at the apex is negligible. Model predictions for the geometry of the longer human cochlea show even higher differences in the steady-state concentrations of the drugs between cochlear base and apex. Our findings confirm conclusions that achieving therapeutic drug concentrations throughout the entire cochlear duct is hardly possible when the drugs are applied to the RW and are distributed via passive diffusion. Assisted methods of drug delivery are needed to reach a more uniform distribution of drugs along the cochlea.","Drug diffusion along an intact mammalian cochlea. Intratympanic drug administration depends on the ability of drugs to pass through the round window membrane (RW) at the base of the cochlea and diffuse from this location to the apex. While the RW permeability for many different drugs can be promoted, passive diffusion along the narrowing spiral of the cochlea is limited. Earlier measurements of the distribution of marker ions, corticosteroids, and antibiotics demonstrated that the concentration of substances applied to the RW was two to three orders of magnitude higher in the base compared to the apex. The measurements, however, involved perforating the cochlear bony wall and, in some cases, sampling perilymph. These manipulations can change the flow rate of perilymph and lead to intake of perilymph through the cochlear aqueduct, thereby disguising concentration gradients of the delivered substances. In this study, the suppressive effect of salicylate on cochlear amplification via block of the outer hair cell (OHC) somatic motility was utilized to assess salicylate diffusion along an intact guinea pig cochlea in vivo. Salicylate solution was applied to the RW and threshold elevation of auditory nerve responses was measured at different times and frequencies after application. Resultant concentrations of salicylate along the cochlea were calculated by fitting the experimental data using a mathematical model of the diffusion and clearing of salicylate in a tube of variable diameter combined with a model describing salicylate action on cochlear amplification. Concentrations reach a steady-state at different times for different cochlear locations and it takes longer to reach the steady-state at more apical locations. Even at the steady-state, the predicted concentration at the apex is negligible. Model predictions for the geometry of the longer human cochlea show even higher differences in the steady-state concentrations of the drugs between cochlear base and apex. Our findings confirm conclusions that achieving therapeutic drug concentrations throughout the entire cochlear duct is hardly possible when the drugs are applied to the RW and are distributed via passive diffusion. Assisted methods of drug delivery are needed to reach a more uniform distribution of drugs along the cochlea."
0,Activation of prolyl hydroxylase-2 for stabilization of mitochondrial stress along with simultaneous downregulation of HIF-1Î±/FASN in ERÂ +Â breast cancer subtype,"The present study was undertaken to inquest the chemical activation of prolyl hydroxylase-2 for the curtailment of hypoxia-inducible factor-1Î± and fatty acid synthase. It was well documented that hypoxia-inducible factor-1Î± and fatty acid synthase were overexpressed in mammary gland carcinomas. After screening a battery of compounds, BBAP-2 was retrieved as a potential prolyl hydroxylase-2 activator and validates its activity using ERÂ +Â MCF-7 cell line and n-methyl-n-nitrosourea-induced rat in vivo model, respectively. BBAP-2 was palpable for the morphological characteristics of apoptosis along with changes in the mitochondrial intergrity as visualized by acridine orange/ethidium bromide and JC-1 staining against ERÂ +Â MCF-7 cells. BBAP-2 also arrest the cell cycle of ERÂ +Â MCF-7 cells at G2/M phase. Afterward, BBAP-2 has scrutinized against n-methyl-n-nitrosourea-induced mammary gland carcinoma in albino Wistar rats. BBAP-2 restored the morphological architecture when screened through carmine staining, haematoxylin and eosin staining, and scanning electron microscopy. BBAP-2 also delineated the markers of oxidative stress favourably. The immunoblotting and mRNA expression analysis validated that BBAP-2 has a potentialty activate the prolyl hydroxylase-2 with sequential downregulating effect on hypoxia-inducible factor-1Î± and its downstream checkpoint. BBAP-2 also fostered apoptosis through mitochondrial-mediated death pathway. The present study elaborates the chemical activation of prolyl hydroxylase-2 by which the increased expression of HIF-1Î± and FASN can be reduced in mammary gland carcinoma.","Activation of prolyl hydroxylase-2 for stabilization of mitochondrial stress along with simultaneous downregulation of HIF-1Î±/FASN in ERÂ +Â breast cancer subtype. The present study was undertaken to inquest the chemical activation of prolyl hydroxylase-2 for the curtailment of hypoxia-inducible factor-1Î± and fatty acid synthase. It was well documented that hypoxia-inducible factor-1Î± and fatty acid synthase were overexpressed in mammary gland carcinomas. After screening a battery of compounds, BBAP-2 was retrieved as a potential prolyl hydroxylase-2 activator and validates its activity using ERÂ +Â MCF-7 cell line and n-methyl-n-nitrosourea-induced rat in vivo model, respectively. BBAP-2 was palpable for the morphological characteristics of apoptosis along with changes in the mitochondrial intergrity as visualized by acridine orange/ethidium bromide and JC-1 staining against ERÂ +Â MCF-7 cells. BBAP-2 also arrest the cell cycle of ERÂ +Â MCF-7 cells at G2/M phase. Afterward, BBAP-2 has scrutinized against n-methyl-n-nitrosourea-induced mammary gland carcinoma in albino Wistar rats. BBAP-2 restored the morphological architecture when screened through carmine staining, haematoxylin and eosin staining, and scanning electron microscopy. BBAP-2 also delineated the markers of oxidative stress favourably. The immunoblotting and mRNA expression analysis validated that BBAP-2 has a potentialty activate the prolyl hydroxylase-2 with sequential downregulating effect on hypoxia-inducible factor-1Î± and its downstream checkpoint. BBAP-2 also fostered apoptosis through mitochondrial-mediated death pathway. The present study elaborates the chemical activation of prolyl hydroxylase-2 by which the increased expression of HIF-1Î± and FASN can be reduced in mammary gland carcinoma."
0,A Novel N Staging System for Predicting Survival in Patients with Medullary Thyroid Cancer,"Introduction: Despite the crucially prognostic value of lymph node metastasis (LNM) in patients with medullary thyroid cancer (MTC), only the LNM compartment alone was reflected in the 8th edition of the American Joint Committee on Cancer (AJCC) system. Objective: This study aimed to incorporate the metastatic lymph node number and metastatic lymph node ratio to generate a more accurate and appropriate N staging system for patients with MTC based on recursive partitioning analysis. Design, Setting, and Patients: Two cohorts were included in the analysis, including 1374 MTC patients from the Surveillance, Epidemiology, and End Results database as the derivation cohort, and 164 patients from Fudan University Shanghai Cancer Center as the validation cohort. The predictive performance of the alternative proposed N staging system was compared with that of the 8th AJCC system by using the Harrell concordance index (C-index) and the area under the receiver operating characteristic curve (AUC). Results: In the derivation cohort, the C-index and the AUC at 10Â years were 0.778 and 0.789, respectively, for the novel N staging system, and 0.749 and 0.741, respectively, for the 8th AJCC N staging system. Similar trends were also observed in the validation cohort. The proposed N staging system had a better prognostic performance. Conclusion: With some improvements, the novel N staging system for MTC suggested from this research may be assessed for potential adoption in the next edition of the AJCC N staging system.","A Novel N Staging System for Predicting Survival in Patients with Medullary Thyroid Cancer. Introduction: Despite the crucially prognostic value of lymph node metastasis (LNM) in patients with medullary thyroid cancer (MTC), only the LNM compartment alone was reflected in the 8th edition of the American Joint Committee on Cancer (AJCC) system. Objective: This study aimed to incorporate the metastatic lymph node number and metastatic lymph node ratio to generate a more accurate and appropriate N staging system for patients with MTC based on recursive partitioning analysis. Design, Setting, and Patients: Two cohorts were included in the analysis, including 1374 MTC patients from the Surveillance, Epidemiology, and End Results database as the derivation cohort, and 164 patients from Fudan University Shanghai Cancer Center as the validation cohort. The predictive performance of the alternative proposed N staging system was compared with that of the 8th AJCC system by using the Harrell concordance index (C-index) and the area under the receiver operating characteristic curve (AUC). Results: In the derivation cohort, the C-index and the AUC at 10Â years were 0.778 and 0.789, respectively, for the novel N staging system, and 0.749 and 0.741, respectively, for the 8th AJCC N staging system. Similar trends were also observed in the validation cohort. The proposed N staging system had a better prognostic performance. Conclusion: With some improvements, the novel N staging system for MTC suggested from this research may be assessed for potential adoption in the next edition of the AJCC N staging system."
0,Microvesicle proteomic profiling of uterine liquid biopsy for ovarian cancer early detection,"High-grade ovarian cancer (HGOC) is the leading cause of mortality from gynecological malignancies, because of diagnosis at a metastatic stage. Current screening options fail to improve mortality because of the absence of early-stage-specific biomarkers. We postulated that a liquid biopsy, such as utero-tubal lavage (UtL), may identify localized lesions better than systemic approaches of serum/plasma analysis. Further, while mutation-based assays are challenged by the rarity of tumor DNA within nonmutated DNA, analyzing the proteomic profile, is expected to enable earlier detection, as it reveals perturbations in both the tumor as well as in its microenvironment. To attain deep proteomic coverage and overcome the high dynamic range of this body fluid, we applied our method for microvesicle proteomics to the UtL samples. Liquid biopsies from HGOC patients (n 49) and controls (n 127) were divided into a discovery and validation sets. Data-dependent analysis of the samples on the Q-Exactive mass spectrometer provided depth of 8578 UtL proteins in total, and on average 3000 proteins per sample. We used support vector machine algorithms for sample classification, and crossed three feature-selection algorithms, to construct and validate a 9-protein classifier with 70% sensitivity and 76.2% specificity. The signature correctly identified all Stage I lesions. These results demonstrate the potential power of microvesicle-based proteomic biomarkers for early cancer diagnosis.","Microvesicle proteomic profiling of uterine liquid biopsy for ovarian cancer early detection. High-grade ovarian cancer (HGOC) is the leading cause of mortality from gynecological malignancies, because of diagnosis at a metastatic stage. Current screening options fail to improve mortality because of the absence of early-stage-specific biomarkers. We postulated that a liquid biopsy, such as utero-tubal lavage (UtL), may identify localized lesions better than systemic approaches of serum/plasma analysis. Further, while mutation-based assays are challenged by the rarity of tumor DNA within nonmutated DNA, analyzing the proteomic profile, is expected to enable earlier detection, as it reveals perturbations in both the tumor as well as in its microenvironment. To attain deep proteomic coverage and overcome the high dynamic range of this body fluid, we applied our method for microvesicle proteomics to the UtL samples. Liquid biopsies from HGOC patients (n 49) and controls (n 127) were divided into a discovery and validation sets. Data-dependent analysis of the samples on the Q-Exactive mass spectrometer provided depth of 8578 UtL proteins in total, and on average 3000 proteins per sample. We used support vector machine algorithms for sample classification, and crossed three feature-selection algorithms, to construct and validate a 9-protein classifier with 70% sensitivity and 76.2% specificity. The signature correctly identified all Stage I lesions. These results demonstrate the potential power of microvesicle-based proteomic biomarkers for early cancer diagnosis."
0,A cross-disorder connectome landscape of brain dysconnectivity,,
0,Automated versus physician assignment of cause of death for verbal autopsies: randomized trial of 9374 deaths in 117 villages in India,,
0,Human cognition involves the dynamic integration of neural activity and neuromodulatory systems,,
0,Intracellular MLCK1 diversion reverses barrier loss to restore mucosal homeostasis,"Epithelial barrier loss is a driver of intestinal and systemic diseases. Myosin light chain kinase (MLCK) is a key effector of barrier dysfunction and a potential therapeutic target, but enzymatic inhibition has unacceptable toxicity. Here, we show that a unique domain within the MLCK splice variant MLCK1 directs perijunctional actomyosin ring (PAMR) recruitment. Using the domain structure and multiple screens, we identify a domain-binding small molecule (divertin) that blocks MLCK1 recruitment without inhibiting enzymatic function. Divertin blocks acute, tumor necrosis factor (TNF)-induced MLCK1 recruitment as well as downstream myosin light chain (MLC) phosphorylation, barrier loss, and diarrhea in vitro and in vivo. Divertin corrects barrier dysfunction and prevents disease development and progression in experimental inflammatory bowel disease. Beyond applications of divertin in gastrointestinal disease, this general approach to enzymatic inhibition by preventing access to specific subcellular sites provides a new paradigm for safely and precisely targeting individual properties of enzymes with multiple functions.","Intracellular MLCK1 diversion reverses barrier loss to restore mucosal homeostasis. Epithelial barrier loss is a driver of intestinal and systemic diseases. Myosin light chain kinase (MLCK) is a key effector of barrier dysfunction and a potential therapeutic target, but enzymatic inhibition has unacceptable toxicity. Here, we show that a unique domain within the MLCK splice variant MLCK1 directs perijunctional actomyosin ring (PAMR) recruitment. Using the domain structure and multiple screens, we identify a domain-binding small molecule (divertin) that blocks MLCK1 recruitment without inhibiting enzymatic function. Divertin blocks acute, tumor necrosis factor (TNF)-induced MLCK1 recruitment as well as downstream myosin light chain (MLC) phosphorylation, barrier loss, and diarrhea in vitro and in vivo. Divertin corrects barrier dysfunction and prevents disease development and progression in experimental inflammatory bowel disease. Beyond applications of divertin in gastrointestinal disease, this general approach to enzymatic inhibition by preventing access to specific subcellular sites provides a new paradigm for safely and precisely targeting individual properties of enzymes with multiple functions."
0,Management of Patients With Fever and Neutropenia Through the Arc of Time A Narrative Review,,
0,"Machine Learning for Anesthesiologists: A Primer (vol 129, pg A29, 2018)",,
0,Identification of long non-coding RNA-related and -coexpressed mRNA biomarkers for hepatocellular carcinoma,"Background: While changes in mRNA expression during tumorigenesis have been used widely as molecular biomarkers for the diagnosis of a number of cancers, the approach has limitations. For example, traditional methods do not consider the regulatory and positional relationship between mRNA and lncRNA. The latter has been largely shown to possess tumor suppressive or oncogenic properties. The combined analysis of mRNA and lncRNA is likely to facilitate the identification of biomarkers with higher confidence. Results: Therefore, we have developed an lncRNA-related method to identify traditional mRNA biomarkers. First we identified mRNAs that are differentially expressed in Hepatocellular Carcinoma (HCC) by comparing cancer and matched adjacent non-tumorous liver tissues. Then, we performed mRNA-lncRNA relationship and coexpression analysis and obtained 41 lncRNA-related and -coexpressed mRNA biomarkers. Next, we performed network analysis, gene ontology analysis and pathway analysis to unravel the functional roles and molecular mechanisms of these lncRNA-related and -coexpressed mRNA biomarkers. Finally, we validated the prediction and performance of the 41 lncRNA-related and -coexpressed mRNA biomarkers using Support Vector Machine model with five-fold cross-validation in an independent HCC dataset from RNA-seq. Conclusions: Our results suggested that mRNAs expression profiles coexpressed with positionally related lncRNAs can provide important insights into early diagnosis and specific targeted gene therapy of HCC.","Identification of long non-coding RNA-related and -coexpressed mRNA biomarkers for hepatocellular carcinoma. Background: While changes in mRNA expression during tumorigenesis have been used widely as molecular biomarkers for the diagnosis of a number of cancers, the approach has limitations. For example, traditional methods do not consider the regulatory and positional relationship between mRNA and lncRNA. The latter has been largely shown to possess tumor suppressive or oncogenic properties. The combined analysis of mRNA and lncRNA is likely to facilitate the identification of biomarkers with higher confidence. Results: Therefore, we have developed an lncRNA-related method to identify traditional mRNA biomarkers. First we identified mRNAs that are differentially expressed in Hepatocellular Carcinoma (HCC) by comparing cancer and matched adjacent non-tumorous liver tissues. Then, we performed mRNA-lncRNA relationship and coexpression analysis and obtained 41 lncRNA-related and -coexpressed mRNA biomarkers. Next, we performed network analysis, gene ontology analysis and pathway analysis to unravel the functional roles and molecular mechanisms of these lncRNA-related and -coexpressed mRNA biomarkers. Finally, we validated the prediction and performance of the 41 lncRNA-related and -coexpressed mRNA biomarkers using Support Vector Machine model with five-fold cross-validation in an independent HCC dataset from RNA-seq. Conclusions: Our results suggested that mRNAs expression profiles coexpressed with positionally related lncRNAs can provide important insights into early diagnosis and specific targeted gene therapy of HCC."
0,Humanizing Artificial Intelligence,,
0,An ordinal model to predict the risk of symptomatic liver failure in patients with cirrhosis undergoing hepatectomy,,
0,Virulence of Pseudomonas aeruginosa exposed to carvacrol: alterations of the Quorum sensing at enzymatic and gene levels,"The main goal of this study was to evaluate the inhibition of Pseudomonas aeruginosa virulence factors and Quorum Sensing during exposure to carvacrol. P. aeruginosa (ATCC 10154) was exposed to carvacrol determining changes in biofilm development, motility, acyl-homoserine lactones (AHL) synthesis and relative expression of lasI/lasR. Docking analysis was used to determinate interactions between carvacrol with LasI and LasR proteins. P. aeruginosa produced 60% lower AHLs when exposed to carvacrol (1.9Â mM) compared to control, without affecting cellular viability, indicating a reduction on the LasI synthase activity. AHL-C12, C6, and C4 were detected and related to biofilm development, motility, and pyocyanin production, respectively. The presence of carvacrol reduced the expression of lasR, without affecting lasI gen. Moreover, computational docking showed interactions of carvacrol with amino acids in the active site pocket of LasI (âˆ’5.6Â kcalÂ molâˆ’1) and within the binding pocket of LasR (âˆ’6.7Â kcalÂ molâˆ’1) of P. aeruginosa. These results demonstrated that virulence of P. aeruginosa was reduced by carvacrol, by inhibiting LasI activity with the concomitant reduction on the expression of lasR, biofilm and swarming motility. This study provides relevant information about the effect of carvacrol against quorum sensing to inhibit virulence factors of P. aeruginosa at enzymatic and gene levels. These findings can contribute to the development of natural anti-QS products, which can affect pathogenesis.","Virulence of Pseudomonas aeruginosa exposed to carvacrol: alterations of the Quorum sensing at enzymatic and gene levels. The main goal of this study was to evaluate the inhibition of Pseudomonas aeruginosa virulence factors and Quorum Sensing during exposure to carvacrol. P. aeruginosa (ATCC 10154) was exposed to carvacrol determining changes in biofilm development, motility, acyl-homoserine lactones (AHL) synthesis and relative expression of lasI/lasR. Docking analysis was used to determinate interactions between carvacrol with LasI and LasR proteins. P. aeruginosa produced 60% lower AHLs when exposed to carvacrol (1.9Â mM) compared to control, without affecting cellular viability, indicating a reduction on the LasI synthase activity. AHL-C12, C6, and C4 were detected and related to biofilm development, motility, and pyocyanin production, respectively. The presence of carvacrol reduced the expression of lasR, without affecting lasI gen. Moreover, computational docking showed interactions of carvacrol with amino acids in the active site pocket of LasI (âˆ’5.6Â kcalÂ molâˆ’1) and within the binding pocket of LasR (âˆ’6.7Â kcalÂ molâˆ’1) of P. aeruginosa. These results demonstrated that virulence of P. aeruginosa was reduced by carvacrol, by inhibiting LasI activity with the concomitant reduction on the expression of lasR, biofilm and swarming motility. This study provides relevant information about the effect of carvacrol against quorum sensing to inhibit virulence factors of P. aeruginosa at enzymatic and gene levels. These findings can contribute to the development of natural anti-QS products, which can affect pathogenesis."
0,Geometric Sketching Compactly Summarizes the Single-Cell Transcriptomic Landscape,"Large-scale single-cell RNA sequencing (scRNA-seq) studies that profile hundreds of thousands of cells are becoming increasingly common, overwhelming existing analysis pipelines. Here, we describe how to enhance and accelerate single-cell data analysis by summarizing the transcriptomic heterogeneity within a dataset using a small subset of cells, which we refer to as a geometric sketch. Our sketches provide more comprehensive visualization of transcriptional diversity, capture rare cell types with high sensitivity, and reveal biological cell types via clustering. Our sketch of umbilical cord blood cells uncovers a rare subpopulation of inflammatory macrophages, which we experimentally validated. The construction of our sketches is extremely fast, which enabled us to accelerate other crucial resource-intensive tasks, such as scRNA-seq data integration, while maintaining accuracy. We anticipate our algorithm will become an increasingly essential step when sharing and analyzing the rapidly growing volume of scRNA-seq data and help enable the democratization of single-cell omics.","Geometric Sketching Compactly Summarizes the Single-Cell Transcriptomic Landscape. Large-scale single-cell RNA sequencing (scRNA-seq) studies that profile hundreds of thousands of cells are becoming increasingly common, overwhelming existing analysis pipelines. Here, we describe how to enhance and accelerate single-cell data analysis by summarizing the transcriptomic heterogeneity within a dataset using a small subset of cells, which we refer to as a geometric sketch. Our sketches provide more comprehensive visualization of transcriptional diversity, capture rare cell types with high sensitivity, and reveal biological cell types via clustering. Our sketch of umbilical cord blood cells uncovers a rare subpopulation of inflammatory macrophages, which we experimentally validated. The construction of our sketches is extremely fast, which enabled us to accelerate other crucial resource-intensive tasks, such as scRNA-seq data integration, while maintaining accuracy. We anticipate our algorithm will become an increasingly essential step when sharing and analyzing the rapidly growing volume of scRNA-seq data and help enable the democratization of single-cell omics."
0,NMR structure determination of Ixolaris and factor X(a) interaction reveals a noncanonical mechanism of Kunitz inhibition,"Ixolaris is a potent tick salivary anticoagulant that binds coagulation factor Xa (FXa) and zymogen FX, with formation of a quaternary tissue factor (TF)/FVIIa/ FX(a)/Ixolaris inhibitory complex. Ixolaris blocks TF-induced coagulation and PAR2 signaling and prevents thrombosis, tumor growth, and immune activation. We present a high-resolution structure and dynamics of Ixolaris and describe the structural basis for recognition of FX. Ixolaris consists of 2 Kunitz domains (K1 and K2) in which K2 is strikingly dynamic and encompasses several residues involved in FX binding. This indicates that the backbone plasticity of K2 is critical for Ixolaris biological activity. Notably, a nuclear magnetic resonanceâ€“derived model reveals a mechanism for an electrostatically guided, high-affinity interaction between Ixolaris and FX heparin-binding (pro)exosite, resulting in an allosteric switch in the catalytic site. This is the first report revealing the structure-function relationship of an anticoagulant targeting a zymogen serving as a scaffold for TF inhibition.","NMR structure determination of Ixolaris and factor X(a) interaction reveals a noncanonical mechanism of Kunitz inhibition. Ixolaris is a potent tick salivary anticoagulant that binds coagulation factor Xa (FXa) and zymogen FX, with formation of a quaternary tissue factor (TF)/FVIIa/ FX(a)/Ixolaris inhibitory complex. Ixolaris blocks TF-induced coagulation and PAR2 signaling and prevents thrombosis, tumor growth, and immune activation. We present a high-resolution structure and dynamics of Ixolaris and describe the structural basis for recognition of FX. Ixolaris consists of 2 Kunitz domains (K1 and K2) in which K2 is strikingly dynamic and encompasses several residues involved in FX binding. This indicates that the backbone plasticity of K2 is critical for Ixolaris biological activity. Notably, a nuclear magnetic resonanceâ€“derived model reveals a mechanism for an electrostatically guided, high-affinity interaction between Ixolaris and FX heparin-binding (pro)exosite, resulting in an allosteric switch in the catalytic site. This is the first report revealing the structure-function relationship of an anticoagulant targeting a zymogen serving as a scaffold for TF inhibition."
0,High-performance medicine: the convergence of human and artificial intelligence,"The use of artificial intelligence, and the deep-learning subtype in particular, has been enabled by the use of labeled big data, along with markedly enhanced computing power and cloud storage, across all sectors. In medicine, this is beginning to have an impact at three levels: for clinicians, predominantly via rapid, accurate image interpretation; for health systems, by improving workflow and the potential for reducing medical errors; and for patients, by enabling them to process their own data to promote health. The current limitations, including bias, privacy and security, and lack of transparency, along with the future directions of these applications will be discussed in this article. Over time, marked improvements in accuracy, productivity, and workflow will likely be actualized, but whether that will be used to improve the patient-doctor relationship or facilitate its erosion remains to be seen.","High-performance medicine: the convergence of human and artificial intelligence. The use of artificial intelligence, and the deep-learning subtype in particular, has been enabled by the use of labeled big data, along with markedly enhanced computing power and cloud storage, across all sectors. In medicine, this is beginning to have an impact at three levels: for clinicians, predominantly via rapid, accurate image interpretation; for health systems, by improving workflow and the potential for reducing medical errors; and for patients, by enabling them to process their own data to promote health. The current limitations, including bias, privacy and security, and lack of transparency, along with the future directions of these applications will be discussed in this article. Over time, marked improvements in accuracy, productivity, and workflow will likely be actualized, but whether that will be used to improve the patient-doctor relationship or facilitate its erosion remains to be seen."
0,"Reply to Siebren Dijkstra and Carl J. Wijburg's Letter to the Editor re: Bernard H. Bochner, Guido Dalbagni, Karim H. Marzouk, et al. Randomized Trial Comparing Open Radical Cystectomy and Robot-assisted Laparoscopic Radical Cystectomy: Oncologic Outcomes. Eur Urol 2018;74:465-71. Can the Pattern of Cancer Recurrence Truly be Assigned to the Surgical Modality?",,
0,Improved homology modeling of the human & rat EP4 prostanoid receptors,"Background: The EP4 prostanoid receptor is one of four GPCRs that mediate the diverse actions of prostaglandin E2 (PGE2). Novel selective EP4 receptor agonists would assist to further elucidate receptor sub-type function and promote development of therapeutics for bone healing, heart failure, and other receptor associated conditions. The rat EP4 (rEP4) receptor has been used as a surrogate for the human EP4 (hEP4) receptor in multiple SAR studies. To better understand the validity of this traditional approach, homology models were generated by threading for both receptors using the RaptorX server. These models were fit to an implicit membrane using the PPM server and OPM database with refinement of intra and extracellular loops by Prime (SchrÃ¶dinger). To understand the interaction between the receptors and known agonists, induced-fit docking experiments were performed using Glide and Prime (SchrÃ¶dinger), with both endogenous agonists and receptor sub-type selective, small-molecule agonists. The docking scores and observed interactions were compared with radioligand displacement experiments and receptor (rat & human) activation assays monitoring cAMP. Results: Rank-ordering of in silico compound docking scores aligned well with in vitro activity assay EC50 and radioligand binding Ki. We observed variations between rat and human EP4 binding pockets that have implications in future small-molecule receptor-modulator design and SAR, specifically a S103G mutation within the rEP4 receptor. Additionally, these models helped identify key interactions between the EP4 receptor and ligands including PGE2 and several known sub-type selective agonists while serving as a marked improvement over the previously reported models. Conclusions: This work has generated a set of novel homology models of the rEP4 and hEP4 receptors. The homology models provide an improvement upon the previously reported model, largely due to improved solvation. The hEP4 docking scores correlates best with the cAMP activation data, where both data sets rank order Rivenprost>CAY10684 > PGE1 â‰ˆ PGE2 > 11-deoxy-PGE1 â‰ˆ 11-dexoy-PGE2 > 8-aza-11-deoxy-PGE1. This rank-ordering matches closely with the rEP4 receptor as well. Species-specific differences were noted for the weak agonists Sulprostone and Misoprostol, which appear to dock more readily within human receptor versus rat receptor.","Improved homology modeling of the human & rat EP4 prostanoid receptors. Background: The EP4 prostanoid receptor is one of four GPCRs that mediate the diverse actions of prostaglandin E2 (PGE2). Novel selective EP4 receptor agonists would assist to further elucidate receptor sub-type function and promote development of therapeutics for bone healing, heart failure, and other receptor associated conditions. The rat EP4 (rEP4) receptor has been used as a surrogate for the human EP4 (hEP4) receptor in multiple SAR studies. To better understand the validity of this traditional approach, homology models were generated by threading for both receptors using the RaptorX server. These models were fit to an implicit membrane using the PPM server and OPM database with refinement of intra and extracellular loops by Prime (SchrÃ¶dinger). To understand the interaction between the receptors and known agonists, induced-fit docking experiments were performed using Glide and Prime (SchrÃ¶dinger), with both endogenous agonists and receptor sub-type selective, small-molecule agonists. The docking scores and observed interactions were compared with radioligand displacement experiments and receptor (rat & human) activation assays monitoring cAMP. Results: Rank-ordering of in silico compound docking scores aligned well with in vitro activity assay EC50 and radioligand binding Ki. We observed variations between rat and human EP4 binding pockets that have implications in future small-molecule receptor-modulator design and SAR, specifically a S103G mutation within the rEP4 receptor. Additionally, these models helped identify key interactions between the EP4 receptor and ligands including PGE2 and several known sub-type selective agonists while serving as a marked improvement over the previously reported models. Conclusions: This work has generated a set of novel homology models of the rEP4 and hEP4 receptors. The homology models provide an improvement upon the previously reported model, largely due to improved solvation. The hEP4 docking scores correlates best with the cAMP activation data, where both data sets rank order Rivenprost>CAY10684 > PGE1 â‰ˆ PGE2 > 11-deoxy-PGE1 â‰ˆ 11-dexoy-PGE2 > 8-aza-11-deoxy-PGE1. This rank-ordering matches closely with the rEP4 receptor as well. Species-specific differences were noted for the weak agonists Sulprostone and Misoprostol, which appear to dock more readily within human receptor versus rat receptor."
0,Critical Decision Points for Augmenting Interpersonal Psychotherapy for Depressed Adolescents: A Pilot Sequential Multiple Assignment Randomized Trial,,
0,A new method for mining information of co-expression network based on multi-cancers integrated data,"Background: Gene co-expression network is a favorable method to reveal the nature of disease. With the development of cancer, the way to build gene co-expression networks based on cancer data has been become a hot spot. However, there are still a limited number of current node measurement methods and node mining strategies for multi-cancers network construction. Methods: In this paper, we introduce a new method for mining information of co-expression network based on multi-cancers integrated data, named PMN. We construct the network by combining the different types of relevant measures (linear and nonlinear rules) for different nodes based on integrated gene expression data of multi-cancers from The Cancer Genome Atlas (TCGA). For mining genes, we combine different properties (local and global characteristics) of the nodes. Results: We uncover more suspicious abnormally expressed genes and shared pathways of different cancers. And we have also found some proven genes and pathways; of course, there are some suspicious factors and molecules that need clinical validation. Conclusions: The results demonstrate that our method is very effective in excavating gene co-expression genes of multi-cancers.","A new method for mining information of co-expression network based on multi-cancers integrated data. Background: Gene co-expression network is a favorable method to reveal the nature of disease. With the development of cancer, the way to build gene co-expression networks based on cancer data has been become a hot spot. However, there are still a limited number of current node measurement methods and node mining strategies for multi-cancers network construction. Methods: In this paper, we introduce a new method for mining information of co-expression network based on multi-cancers integrated data, named PMN. We construct the network by combining the different types of relevant measures (linear and nonlinear rules) for different nodes based on integrated gene expression data of multi-cancers from The Cancer Genome Atlas (TCGA). For mining genes, we combine different properties (local and global characteristics) of the nodes. Results: We uncover more suspicious abnormally expressed genes and shared pathways of different cancers. And we have also found some proven genes and pathways; of course, there are some suspicious factors and molecules that need clinical validation. Conclusions: The results demonstrate that our method is very effective in excavating gene co-expression genes of multi-cancers."
0,Better Cholera Counts Through Machine Learning Models,,
0,Grid cell co-activity patterns during sleep reflect spatial overlap of grid fields during active behaviors,,
0,Prior dengue virus infection and risk of Zika: A pediatric cohort in Nicaragua,"Background: Zika virus (ZIKV) emerged in northeast Brazil in 2015 and spread rapidly across the Americas, in populations that have been largely exposed to dengue virus (DENV). The impact of prior DENV infection on ZIKV infection outcome remains unclear. To study this potential impact, we analyzed the large 2016 Zika epidemic in Managua, Nicaragua, in a pediatric cohort with well-characterized DENV infection histories. Methods and findings: Symptomatic ZIKV infections (Zika cases) were identified by real-time reverse transcription PCR and serology in a community-based cohort study that follows approximately 3,700 children aged 2-14 years old. Annual blood samples were used to identify clinically inapparent ZIKV infections using a novel, well-characterized serological assay. Multivariable Poisson regression was used to examine the relation between prior DENV infection and incidence of symptomatic and inapparent ZIKV infection. The generalized-growth method was used to estimate the effective reproduction number. From January 1, 2016, to February 28, 2017, 560 symptomatic ZIKV infections and 1,356 total ZIKV infections (symptomatic and inapparent) were identified, for an overall incidence of 14.0 symptomatic infections (95% CI: 12.9, 15.2) and 36.5 total infections (95% CI: 34.7, 38.6) per 100 person-years. Effective reproduction number estimates ranged from 3.3 to 3.4, depending on the ascending wave period. Incidence of symptomatic and total ZIKV infections was higher in females and older children. Analysis of the effect of prior DENV infection was performed on 3,027 participants with documented DENV infection histories, of which 743 (24.5%) had experienced at least 1 prior DENV infection during cohort follow-up. Prior DENV infection was inversely associated with risk of symptomatic ZIKV infection in the total cohort population (incidence rate ratio [IRR]: 0.63; 95% CI: 0.48, 0.81; p < 0.005) and with risk of symptomatic presentation given ZIKV infection (IRR: 0.62; 95% CI: 0.44, 0.86) when adjusted for age, sex, and recent DENV infection (1-2 years before ZIKV infection). Recent DENV infection was significantly associated with decreased risk of symptomatic ZIKV infection when adjusted for age and sex, but not when adjusted for prior DENV infection. Prior or recent DENV infection did not affect the rate of total ZIKV infections. Our findings are limited to a pediatric population and constrained by the epidemiology of the site. Conclusions: These findings support that prior DENV infection may protect individuals from symptomatic Zika. More research is needed to address the possible immunological mechanism(s) of cross-protection between ZIKV and DENV and whether DENV immunity also modulates other ZIKV infection outcomes such as neurological or congenital syndromes.","Prior dengue virus infection and risk of Zika: A pediatric cohort in Nicaragua. Background: Zika virus (ZIKV) emerged in northeast Brazil in 2015 and spread rapidly across the Americas, in populations that have been largely exposed to dengue virus (DENV). The impact of prior DENV infection on ZIKV infection outcome remains unclear. To study this potential impact, we analyzed the large 2016 Zika epidemic in Managua, Nicaragua, in a pediatric cohort with well-characterized DENV infection histories. Methods and findings: Symptomatic ZIKV infections (Zika cases) were identified by real-time reverse transcription PCR and serology in a community-based cohort study that follows approximately 3,700 children aged 2-14 years old. Annual blood samples were used to identify clinically inapparent ZIKV infections using a novel, well-characterized serological assay. Multivariable Poisson regression was used to examine the relation between prior DENV infection and incidence of symptomatic and inapparent ZIKV infection. The generalized-growth method was used to estimate the effective reproduction number. From January 1, 2016, to February 28, 2017, 560 symptomatic ZIKV infections and 1,356 total ZIKV infections (symptomatic and inapparent) were identified, for an overall incidence of 14.0 symptomatic infections (95% CI: 12.9, 15.2) and 36.5 total infections (95% CI: 34.7, 38.6) per 100 person-years. Effective reproduction number estimates ranged from 3.3 to 3.4, depending on the ascending wave period. Incidence of symptomatic and total ZIKV infections was higher in females and older children. Analysis of the effect of prior DENV infection was performed on 3,027 participants with documented DENV infection histories, of which 743 (24.5%) had experienced at least 1 prior DENV infection during cohort follow-up. Prior DENV infection was inversely associated with risk of symptomatic ZIKV infection in the total cohort population (incidence rate ratio [IRR]: 0.63; 95% CI: 0.48, 0.81; p < 0.005) and with risk of symptomatic presentation given ZIKV infection (IRR: 0.62; 95% CI: 0.44, 0.86) when adjusted for age, sex, and recent DENV infection (1-2 years before ZIKV infection). Recent DENV infection was significantly associated with decreased risk of symptomatic ZIKV infection when adjusted for age and sex, but not when adjusted for prior DENV infection. Prior or recent DENV infection did not affect the rate of total ZIKV infections. Our findings are limited to a pediatric population and constrained by the epidemiology of the site. Conclusions: These findings support that prior DENV infection may protect individuals from symptomatic Zika. More research is needed to address the possible immunological mechanism(s) of cross-protection between ZIKV and DENV and whether DENV immunity also modulates other ZIKV infection outcomes such as neurological or congenital syndromes."
0,Calibration: The Achilles heel of predictive analytics,"Background: The assessment of calibration performance of risk prediction models based on regression or more flexible machine learning algorithms receives little attention. Main text: Herein, we argue that this needs to change immediately because poorly calibrated algorithms can be misleading and potentially harmful for clinical decision-making. We summarize how to avoid poor calibration at algorithm development and how to assess calibration at algorithm validation, emphasizing balance between model complexity and the available sample size. At external validation, calibration curves require sufficiently large samples. Algorithm updating should be considered for appropriate support of clinical practice. Conclusion: Efforts are required to avoid poor calibration when developing prediction models, to evaluate calibration when validating models, and to update models when indicated. The ultimate aim is to optimize the utility of predictive analytics for shared decision-making and patient counseling.","Calibration: The Achilles heel of predictive analytics. Background: The assessment of calibration performance of risk prediction models based on regression or more flexible machine learning algorithms receives little attention. Main text: Herein, we argue that this needs to change immediately because poorly calibrated algorithms can be misleading and potentially harmful for clinical decision-making. We summarize how to avoid poor calibration at algorithm development and how to assess calibration at algorithm validation, emphasizing balance between model complexity and the available sample size. At external validation, calibration curves require sufficiently large samples. Algorithm updating should be considered for appropriate support of clinical practice. Conclusion: Efforts are required to avoid poor calibration when developing prediction models, to evaluate calibration when validating models, and to update models when indicated. The ultimate aim is to optimize the utility of predictive analytics for shared decision-making and patient counseling."
0,Artificial intelligence to predict AKI: is it a breakthrough?,,
0,"Reporting guidelines for clinical trials evaluating artificial intelligence interventions are needed (vol 25, pg 1467, 2019)",,
0,Metabolic pathways associated with right ventricular adaptation to pulmonary hypertension: 3D analysis of cardiac magnetic resonance imaging,"AIMS: We sought to identify metabolic pathways associated with right ventricular (RV) adaptation to pulmonary hypertension (PH). We evaluated candidate metabolites, previously associated with survival in pulmonary arterial hypertension, and used automated image segmentation and parametric mapping to model their relationship to adverse patterns of remodelling and wall stress. METHODS AND RESULTS: In 312 PH subjects (47.1% female, mean age 60.8â€‰Â±â€‰15.9â€‰years), of which 182 (50.5% female, mean age 58.6â€‰Â±â€‰16.8â€‰years) had metabolomics, we modelled the relationship between the RV phenotype, haemodynamic state, and metabolite levels. Atlas-based segmentation and co-registration of cardiac magnetic resonance imaging was used to create a quantitative 3D model of RV geometry and function-including maps of regional wall stress. Increasing mean pulmonary artery pressure was associated with hypertrophy of the basal free wall (Î²â€‰=â€‰0.29) and reduced relative wall thickness (Î² = -0.38), indicative of eccentric remodelling. Wall stress was an independent predictor of all-cause mortality (hazard ratio = 1.27, Pâ€‰=â€‰0.04). Six metabolites were significantly associated with elevated wall stress (Î²â€‰=â€‰0.28-0.34) including increased levels of tRNA-specific modified nucleosides and fatty acid acylcarnitines, and decreased levels (Î² = -0.40) of sulfated androgen. CONCLUSION: Using computational image phenotyping, we identify metabolic profiles, reporting on energy metabolism and cellular stress-response, which are associated with adaptive RV mechanisms to PH.","Metabolic pathways associated with right ventricular adaptation to pulmonary hypertension: 3D analysis of cardiac magnetic resonance imaging. AIMS: We sought to identify metabolic pathways associated with right ventricular (RV) adaptation to pulmonary hypertension (PH). We evaluated candidate metabolites, previously associated with survival in pulmonary arterial hypertension, and used automated image segmentation and parametric mapping to model their relationship to adverse patterns of remodelling and wall stress. METHODS AND RESULTS: In 312 PH subjects (47.1% female, mean age 60.8â€‰Â±â€‰15.9â€‰years), of which 182 (50.5% female, mean age 58.6â€‰Â±â€‰16.8â€‰years) had metabolomics, we modelled the relationship between the RV phenotype, haemodynamic state, and metabolite levels. Atlas-based segmentation and co-registration of cardiac magnetic resonance imaging was used to create a quantitative 3D model of RV geometry and function-including maps of regional wall stress. Increasing mean pulmonary artery pressure was associated with hypertrophy of the basal free wall (Î²â€‰=â€‰0.29) and reduced relative wall thickness (Î² = -0.38), indicative of eccentric remodelling. Wall stress was an independent predictor of all-cause mortality (hazard ratio = 1.27, Pâ€‰=â€‰0.04). Six metabolites were significantly associated with elevated wall stress (Î²â€‰=â€‰0.28-0.34) including increased levels of tRNA-specific modified nucleosides and fatty acid acylcarnitines, and decreased levels (Î² = -0.40) of sulfated androgen. CONCLUSION: Using computational image phenotyping, we identify metabolic profiles, reporting on energy metabolism and cellular stress-response, which are associated with adaptive RV mechanisms to PH."
0,DIAlignR provides precise retention time alignment across distant runs in DIA and targeted proteomics,"Sequential Windowed Acquisition of All Theoretical Fragment Ion Mass Spectra (SWATH-MS) is widely used for proteomics analysis given its high throughput and reproducibility, but ensuring consistent quantification of analytes across large-scale studies of heterogeneous samples such as human plasma remains challenging. Heterogeneity in large-scale studies can be caused by large time intervals between data acquisition, acquisition by different operators or instruments, and intermittent repair or replacement of parts, such as the liquid chromatography column, all of which affect retention time (RT) reproducibility and, successively, performance of SWATH-MS data analysis. Here, we present a novel algorithm for RT alignment of SWATH-MS data based on direct alignment of raw MS2 chromatograms using a hybrid dynamic programming approach. The algorithm does not impose a chronological order of elution and allows for alignment of elution-order-swapped peaks. Furthermore, allowing RT mapping in a certain window around a coarse global fit makes it robust against noise. On a manually validated dataset, this strategy outperformed the current state-ofthe-art approaches. In addition, on real-world clinical data, our approach outperformed global alignment methods by mapping 98% of peaks compared with 67% cumulatively. DIAlignR reduced alignment error up to 30-fold for extremely distant runs. The robustness of technical parameters used in this pairwise alignment strategy is also demonstrated. The source code is released under the BSD license at https://github.com/Roestlab/DIAlignR.","DIAlignR provides precise retention time alignment across distant runs in DIA and targeted proteomics. Sequential Windowed Acquisition of All Theoretical Fragment Ion Mass Spectra (SWATH-MS) is widely used for proteomics analysis given its high throughput and reproducibility, but ensuring consistent quantification of analytes across large-scale studies of heterogeneous samples such as human plasma remains challenging. Heterogeneity in large-scale studies can be caused by large time intervals between data acquisition, acquisition by different operators or instruments, and intermittent repair or replacement of parts, such as the liquid chromatography column, all of which affect retention time (RT) reproducibility and, successively, performance of SWATH-MS data analysis. Here, we present a novel algorithm for RT alignment of SWATH-MS data based on direct alignment of raw MS2 chromatograms using a hybrid dynamic programming approach. The algorithm does not impose a chronological order of elution and allows for alignment of elution-order-swapped peaks. Furthermore, allowing RT mapping in a certain window around a coarse global fit makes it robust against noise. On a manually validated dataset, this strategy outperformed the current state-ofthe-art approaches. In addition, on real-world clinical data, our approach outperformed global alignment methods by mapping 98% of peaks compared with 67% cumulatively. DIAlignR reduced alignment error up to 30-fold for extremely distant runs. The robustness of technical parameters used in this pairwise alignment strategy is also demonstrated. The source code is released under the BSD license at https://github.com/Roestlab/DIAlignR."
0,10-year performance of four models of breast cancer risk:a validation study,,
0,White Matter Microstructure in Pediatric Bipolar Disorder and Disruptive Mood Dysregulation Disorder,,
0,Differential expression of the TwHMGS gene and its effect on triptolide biosynthesis in Tripterygium wilfordii,"3-Hydroxy-3-methylglutaryl-CoA synthase (HMGS) is the first committed enzyme in the MVA pathway and involved in the biosynthesis of terpenes in Tripterygium wilfordii. The full-length cDNA and a 515 bp RNAi target fragment of TwHMGS were ligated into the pH7WG2D and pK7GWIWG2D vectors to respectively overexpress and silence, TwHMGS was overexpressed and silenced in T. wilfordii suspension cells using biolistic-gun mediated transformation, which resulted in 2-fold increase and a drop to 70% in the expression level compared to cells with empty vector controls. During TwHMGS overexpression, the expression of TwHMGR, TwDXR and TwTPS7v2 was significantly upregulated to the control. In the RNAi group, the expression of TwHMGR, TwDXS, TwDXR and TwMCT visibly displayed downregulation to the control. The cells with TwHMGS overexpressed produced twice higher than the control value. These results proved that differential expression of TwHMGS determined the production of triptolide in T. wilfordii and laterally caused different trends of relative gene expression in the terpene biosynthetic pathway. Finally, the substrate acetyl-CoA was docked into the active site of TwHMGS, suggesting the key residues including His247, Lys256 and Arg296 undergo electrostatic or H-bond interactions with acetyl-CoA.","Differential expression of the TwHMGS gene and its effect on triptolide biosynthesis in Tripterygium wilfordii. 3-Hydroxy-3-methylglutaryl-CoA synthase (HMGS) is the first committed enzyme in the MVA pathway and involved in the biosynthesis of terpenes in Tripterygium wilfordii. The full-length cDNA and a 515 bp RNAi target fragment of TwHMGS were ligated into the pH7WG2D and pK7GWIWG2D vectors to respectively overexpress and silence, TwHMGS was overexpressed and silenced in T. wilfordii suspension cells using biolistic-gun mediated transformation, which resulted in 2-fold increase and a drop to 70% in the expression level compared to cells with empty vector controls. During TwHMGS overexpression, the expression of TwHMGR, TwDXR and TwTPS7v2 was significantly upregulated to the control. In the RNAi group, the expression of TwHMGR, TwDXS, TwDXR and TwMCT visibly displayed downregulation to the control. The cells with TwHMGS overexpressed produced twice higher than the control value. These results proved that differential expression of TwHMGS determined the production of triptolide in T. wilfordii and laterally caused different trends of relative gene expression in the terpene biosynthetic pathway. Finally, the substrate acetyl-CoA was docked into the active site of TwHMGS, suggesting the key residues including His247, Lys256 and Arg296 undergo electrostatic or H-bond interactions with acetyl-CoA."
0,Predicting Splicing from Primary Sequence with Deep Learning,"The splicing of pre-mRNAs into mature transcripts is remarkable for its precision, but the mechanisms by which the cellular machinery achieves such specificity are incompletely understood. Here, we describe a deep neural network that accurately predicts splice junctions from an arbitrary pre-mRNA transcript sequence, enabling precise prediction of noncoding genetic variants that cause cryptic splicing. Synonymous and intronic mutations with predicted splice-altering consequence validate at a high rate on RNA-seq and are strongly deleterious in the human population. De novo mutations with predicted splice-altering consequence are significantly enriched in patients with autism and intellectual disability compared to healthy controls and validate against RNA-seq in 21 out of 28 of these patients. We estimate that 9%-11% of pathogenic mutations in patients with rare genetic disorders are caused by this previously underappreciated class of disease variation.","Predicting Splicing from Primary Sequence with Deep Learning. The splicing of pre-mRNAs into mature transcripts is remarkable for its precision, but the mechanisms by which the cellular machinery achieves such specificity are incompletely understood. Here, we describe a deep neural network that accurately predicts splice junctions from an arbitrary pre-mRNA transcript sequence, enabling precise prediction of noncoding genetic variants that cause cryptic splicing. Synonymous and intronic mutations with predicted splice-altering consequence validate at a high rate on RNA-seq and are strongly deleterious in the human population. De novo mutations with predicted splice-altering consequence are significantly enriched in patients with autism and intellectual disability compared to healthy controls and validate against RNA-seq in 21 out of 28 of these patients. We estimate that 9%-11% of pathogenic mutations in patients with rare genetic disorders are caused by this previously underappreciated class of disease variation."
0,Dual Sympathetic Input into Developing Salivary Glands,,
0,Deep learning in ophthalmology: The technical and clinical considerations,"The advent of computer graphic processing units, improvement in mathematical models and availability of big data has allowed artificial intelligence (AI) using machine learning (ML) and deep learning (DL) techniques to achieve robust performance for broad applications in social-media, the internet of things, the automotive industry and healthcare. DL systems in particular provide improved capability in image, speech and motion recognition as well as in natural language processing. In medicine, significant progress of AI and DL systems has been demonstrated in image-centric specialties such as radiology, dermatology, pathology and ophthalmology. New studies, including pre-registered prospective clinical trials, have shown DL systems are accurate and effective in detecting diabetic retinopathy (DR), glaucoma, age-related macular degeneration (AMD), retinopathy of prematurity, refractive error and in identifying cardiovascular risk factors and diseases, from digital fundus photographs. There is also increasing attention on the use of AI and DL systems in identifying disease features, progression and treatment response for retinal diseases such as neovascular AMD and diabetic macular edema using optical coherence tomography (OCT). Additionally, the application of ML to visual fields may be useful in detecting glaucoma progression. There are limited studies that incorporate clinical data including electronic health records, in AL and DL algorithms, and no prospective studies to demonstrate that AI and DL algorithms can predict the development of clinical eye disease. This article describes global eye disease burden, unmet needs and common conditions of public health importance for which AI and DL systems may be applicable. Technical and clinical aspects to build a DL system to address those needs, and the potential challenges for clinical adoption are discussed. AI, ML and DL will likely play a crucial role in clinical ophthalmology practice, with implications for screening, diagnosis and follow up of the major causes of vision impairment in the setting of ageing populations globally.","Deep learning in ophthalmology: The technical and clinical considerations. The advent of computer graphic processing units, improvement in mathematical models and availability of big data has allowed artificial intelligence (AI) using machine learning (ML) and deep learning (DL) techniques to achieve robust performance for broad applications in social-media, the internet of things, the automotive industry and healthcare. DL systems in particular provide improved capability in image, speech and motion recognition as well as in natural language processing. In medicine, significant progress of AI and DL systems has been demonstrated in image-centric specialties such as radiology, dermatology, pathology and ophthalmology. New studies, including pre-registered prospective clinical trials, have shown DL systems are accurate and effective in detecting diabetic retinopathy (DR), glaucoma, age-related macular degeneration (AMD), retinopathy of prematurity, refractive error and in identifying cardiovascular risk factors and diseases, from digital fundus photographs. There is also increasing attention on the use of AI and DL systems in identifying disease features, progression and treatment response for retinal diseases such as neovascular AMD and diabetic macular edema using optical coherence tomography (OCT). Additionally, the application of ML to visual fields may be useful in detecting glaucoma progression. There are limited studies that incorporate clinical data including electronic health records, in AL and DL algorithms, and no prospective studies to demonstrate that AI and DL algorithms can predict the development of clinical eye disease. This article describes global eye disease burden, unmet needs and common conditions of public health importance for which AI and DL systems may be applicable. Technical and clinical aspects to build a DL system to address those needs, and the potential challenges for clinical adoption are discussed. AI, ML and DL will likely play a crucial role in clinical ophthalmology practice, with implications for screening, diagnosis and follow up of the major causes of vision impairment in the setting of ageing populations globally."
0,Novel putative drugs and key initiating genes for neurodegenerative disease determined using network-based genetic integrative analysis,"Understanding the genetic causes of neurodegenerative disease (ND) can be useful for their prevention and treatment. Among the genetic variations responsible for ND, heritable germline variants have been discovered in genome-wide association studies (GWAS), and nonheritable somatic mutations have been discovered in sequencing projects. Distinguishing the important initiating genes in ND and comparing the importance of heritable and nonheritable genetic variants forÂ treating ND are important challenges. In this study, we analysed GWAS results, somatic mutations and drug targets of ND from large databanks by performing directed network-based analysis considering a randomised network hypothesis testing procedure. A disease-associated biological network was created in the context of the functional interactome, and the nonrandom topological characteristics of directed-edge classes were interpreted. Hierarchical network analysis indicated that drug targets tend to lie upstream of somatic mutations and germline variants. Furthermore, using directed path length information and biological explanations, we provide information on the most important genes in these created node classes and their associated drugs. Finally, we identified nine germline variants overlapping with drug targets for ND, seven somatic mutations close to drug targets from the hierarchical network analysis and six crucial genes in controlling other genes from the network analysis. Based on these findings, some drugs have been proposed for treating ND via drug repurposing. Our results provide new insights into the therapeutic actionability of GWAS results and somatic mutations for ND. The interesting properties of each node class and the existing relationships between them can broaden our knowledge of ND.","Novel putative drugs and key initiating genes for neurodegenerative disease determined using network-based genetic integrative analysis. Understanding the genetic causes of neurodegenerative disease (ND) can be useful for their prevention and treatment. Among the genetic variations responsible for ND, heritable germline variants have been discovered in genome-wide association studies (GWAS), and nonheritable somatic mutations have been discovered in sequencing projects. Distinguishing the important initiating genes in ND and comparing the importance of heritable and nonheritable genetic variants forÂ treating ND are important challenges. In this study, we analysed GWAS results, somatic mutations and drug targets of ND from large databanks by performing directed network-based analysis considering a randomised network hypothesis testing procedure. A disease-associated biological network was created in the context of the functional interactome, and the nonrandom topological characteristics of directed-edge classes were interpreted. Hierarchical network analysis indicated that drug targets tend to lie upstream of somatic mutations and germline variants. Furthermore, using directed path length information and biological explanations, we provide information on the most important genes in these created node classes and their associated drugs. Finally, we identified nine germline variants overlapping with drug targets for ND, seven somatic mutations close to drug targets from the hierarchical network analysis and six crucial genes in controlling other genes from the network analysis. Based on these findings, some drugs have been proposed for treating ND via drug repurposing. Our results provide new insights into the therapeutic actionability of GWAS results and somatic mutations for ND. The interesting properties of each node class and the existing relationships between them can broaden our knowledge of ND."
0,Predicting progression to AD using a deep-learning model,,
0,Portraits of communication in neuronal networks,,
0,AI-Assisted Forward Modeling of Biological Structures,"The rise of machine learning and deep learning technologies have allowed researchers to automate image classification. We describe a method that incorporates automated image classification and principal component analysis to evaluate computational models of biological structures. We use a computational model of the kinetochore to demonstrate our artificial-intelligence (AI)-assisted modeling method. The kinetochore is a large protein complex that connects chromosomes to the mitotic spindle to facilitate proper cell division. The kinetochore can be divided into two regions: the inner kinetochore, including proteins that interact with DNA; and the outer kinetochore, comprised of microtubule-binding proteins. These two kinetochore regions have been shown to have different distributions during metaphase in live budding yeast and therefore act as a test case for our forward modeling technique. We find that a simple convolutional neural net (CNN) can correctly classify fluorescent images of inner and outer kinetochore proteins and show a CNN trained on simulated, fluorescent images can detect difference in experimental images. A polymer model of the ribosomal DNA locus serves as a second test for the method. The nucleolus surrounds the ribosomal DNA locus and appears amorphous in live-cell, fluorescent microscopy experiments in budding yeast, making detection of morphological changes challenging. We show a simple CNN can detect subtle differences in simulated images of the ribosomal DNA locus, demonstrating our CNN-based classification technique can be used on a variety of biological structures.","AI-Assisted Forward Modeling of Biological Structures. The rise of machine learning and deep learning technologies have allowed researchers to automate image classification. We describe a method that incorporates automated image classification and principal component analysis to evaluate computational models of biological structures. We use a computational model of the kinetochore to demonstrate our artificial-intelligence (AI)-assisted modeling method. The kinetochore is a large protein complex that connects chromosomes to the mitotic spindle to facilitate proper cell division. The kinetochore can be divided into two regions: the inner kinetochore, including proteins that interact with DNA; and the outer kinetochore, comprised of microtubule-binding proteins. These two kinetochore regions have been shown to have different distributions during metaphase in live budding yeast and therefore act as a test case for our forward modeling technique. We find that a simple convolutional neural net (CNN) can correctly classify fluorescent images of inner and outer kinetochore proteins and show a CNN trained on simulated, fluorescent images can detect difference in experimental images. A polymer model of the ribosomal DNA locus serves as a second test for the method. The nucleolus surrounds the ribosomal DNA locus and appears amorphous in live-cell, fluorescent microscopy experiments in budding yeast, making detection of morphological changes challenging. We show a simple CNN can detect subtle differences in simulated images of the ribosomal DNA locus, demonstrating our CNN-based classification technique can be used on a variety of biological structures."
0,Applications of machine learning in drug discovery and development,"Drug discovery and development pipelines are long, complex and depend on numerous factors. Machine learning (ML) approaches provide a set of tools that can improve discovery and decision making for well-specified questions with abundant, high-quality data. Opportunities to apply ML occur in all stages of drug discovery. Examples include target validation, identification of prognostic biomarkers and analysis of digital pathology data in clinical trials. Applications have ranged in context and methodology, with some approaches yielding accurate predictions and insights. The challenges of applying ML lie primarily with the lack of interpretability and repeatability of ML-generated results, which may limit their application. In all areas, systematic and comprehensive high-dimensional data still need to be generated. With ongoing efforts to tackle these issues, as well as increasing awareness of the factors needed to validate ML approaches, the application of ML can promote data-driven decision making and has the potential to speed up the process and reduce failure rates in drug discovery and development.","Applications of machine learning in drug discovery and development. Drug discovery and development pipelines are long, complex and depend on numerous factors. Machine learning (ML) approaches provide a set of tools that can improve discovery and decision making for well-specified questions with abundant, high-quality data. Opportunities to apply ML occur in all stages of drug discovery. Examples include target validation, identification of prognostic biomarkers and analysis of digital pathology data in clinical trials. Applications have ranged in context and methodology, with some approaches yielding accurate predictions and insights. The challenges of applying ML lie primarily with the lack of interpretability and repeatability of ML-generated results, which may limit their application. In all areas, systematic and comprehensive high-dimensional data still need to be generated. With ongoing efforts to tackle these issues, as well as increasing awareness of the factors needed to validate ML approaches, the application of ML can promote data-driven decision making and has the potential to speed up the process and reduce failure rates in drug discovery and development."
0,Quercetin protects rats from catheter-related Staphylococcus aureus infections by inhibiting coagulase activity,"Coagulase (Coa) activity is essential for the virulence of Staphylococcus aureus (SÂ aureus), one of the most important pathogenic bacteria leading to catheter-related bloodstream infections (CRBSI). We have demonstrated that the mutation of coagulase improved outcomes in disease models of SÂ aureus CRBSI, suggesting that targeting Coa may represent a novel antiinfective strategy for CRBSI. Here, we found that quercetin, a natural compound that does not affect SÂ aureus viability, could inhibit Coa activity. Chemical biological analysis revealed that the direct engagement of quercetin with the active site (residues Tyr187, Leu221 and His228) of Coa inhibited its activity. Furthermore, treatment with quercetin reduced the retention of bacteria on catheter surfaces, decreased the bacterial load in the kidneys and alleviated kidney abscesses in vivo. These data suggest that antiinfective therapy targeting Coa with quercetin may represent a novel strategy and provide a new leading compound with which to combat bacterial infections.","Quercetin protects rats from catheter-related Staphylococcus aureus infections by inhibiting coagulase activity. Coagulase (Coa) activity is essential for the virulence of Staphylococcus aureus (SÂ aureus), one of the most important pathogenic bacteria leading to catheter-related bloodstream infections (CRBSI). We have demonstrated that the mutation of coagulase improved outcomes in disease models of SÂ aureus CRBSI, suggesting that targeting Coa may represent a novel antiinfective strategy for CRBSI. Here, we found that quercetin, a natural compound that does not affect SÂ aureus viability, could inhibit Coa activity. Chemical biological analysis revealed that the direct engagement of quercetin with the active site (residues Tyr187, Leu221 and His228) of Coa inhibited its activity. Furthermore, treatment with quercetin reduced the retention of bacteria on catheter surfaces, decreased the bacterial load in the kidneys and alleviated kidney abscesses in vivo. These data suggest that antiinfective therapy targeting Coa with quercetin may represent a novel strategy and provide a new leading compound with which to combat bacterial infections."
0,Hypoxia-induced regulation of mTOR signaling by miR-7 targeting REDD1,"Oxygen is an important factor mediating cell growth and survival under physiological and pathological conditions. Therefore, cells have well-regulated response mechanisms in the face of changes in oxygen levels in their environment. A subset of microRNAs (miRNAs) termed the hypoxamir has been suggested to be a critical mediator of the cellular response to hypoxia. Regulated in development and DNA damage response 1 (REDD1) is a negative regulator of mammalian target of rapamycin (mTOR) signaling in the response to cellular stress, and is elevated in many cell types under hypoxia, with consequent inhibition of mTOR signaling. However, the underlying posttranscriptional regulatory mechanism by miRNAs that contribute to this hypoxia-induced reduction in REDD1 expression remain unknown. Therefore, the aim of the current study was to identify the miRNAs participating in the hypoxic cellular response by scanning the 3â€²-untranslated region (3â€²-UTR) of REDD1 for potential miRNA-binding sites using a computer algorithm, TargetScan. miR-7 emerged as a novel hypoxamir that regulates REDD1 expression and is involved in mTOR signaling. miR-7 could repress REDD1 expression posttranscriptionally by directly binding with the 3â€²-UTR. Upon hypoxia, miR-7 expression was downregulated in HeLa cells to consequently derepress REDD1, resulting in inhibition of mTOR signaling. Moreover, overexpression of miR-7 was sufficient to reverse the hypoxia-induced inhibition of mTOR signaling. Therefore, our findings suggest miR-7 as a key regulator of hypoxia-mediated mTOR signaling through modulation of REDD1 expression. These findings contribute new insight into the miRNA-mediated molecular mechanism of the hypoxic response through mTOR signaling, highlighting potential targets for tumor suppression.","Hypoxia-induced regulation of mTOR signaling by miR-7 targeting REDD1. Oxygen is an important factor mediating cell growth and survival under physiological and pathological conditions. Therefore, cells have well-regulated response mechanisms in the face of changes in oxygen levels in their environment. A subset of microRNAs (miRNAs) termed the hypoxamir has been suggested to be a critical mediator of the cellular response to hypoxia. Regulated in development and DNA damage response 1 (REDD1) is a negative regulator of mammalian target of rapamycin (mTOR) signaling in the response to cellular stress, and is elevated in many cell types under hypoxia, with consequent inhibition of mTOR signaling. However, the underlying posttranscriptional regulatory mechanism by miRNAs that contribute to this hypoxia-induced reduction in REDD1 expression remain unknown. Therefore, the aim of the current study was to identify the miRNAs participating in the hypoxic cellular response by scanning the 3â€²-untranslated region (3â€²-UTR) of REDD1 for potential miRNA-binding sites using a computer algorithm, TargetScan. miR-7 emerged as a novel hypoxamir that regulates REDD1 expression and is involved in mTOR signaling. miR-7 could repress REDD1 expression posttranscriptionally by directly binding with the 3â€²-UTR. Upon hypoxia, miR-7 expression was downregulated in HeLa cells to consequently derepress REDD1, resulting in inhibition of mTOR signaling. Moreover, overexpression of miR-7 was sufficient to reverse the hypoxia-induced inhibition of mTOR signaling. Therefore, our findings suggest miR-7 as a key regulator of hypoxia-mediated mTOR signaling through modulation of REDD1 expression. These findings contribute new insight into the miRNA-mediated molecular mechanism of the hypoxic response through mTOR signaling, highlighting potential targets for tumor suppression."
0,Deep learning identifies DDR1 kinase inhibitors,,
0,Emerging imaging technologies in dermatology Part II: Applications and limitations,,
0,An allosteric inhibitory site conserved in the ectodomain of P2X receptor channels,"P2X receptors constitute a gene family of cation channels gated by extracellular ATP. They mediate fast ionotropic purinergic signaling in neurons and non-excitable cell types in vertebrates. The highly calcium-permeable P2X4 subtype has been shown to play a significant role in cardiovascular physiology, inflammatory responses and neuro-immune communication. We previously reported the discovery of a P2X4-selective antagonist, the small organic compound BX430, with submicromolar potency for human P2X4 receptors and marked species-dependence (Ase et al., 2015). The present study investigates the molecular basis of P2X4 inhibition by the non-competitive blocker BX430 using a structural and functional approach relying on mutagenesis and electrophysiology. We provide evidence for the critical contribution of a single hydrophobic residue located in the ectodomain of P2X4 channel subunits, Ile312 in human P2X4, which determines blockade by BX430. We also show that the nature of this extracellular residue in various vertebrate P2X4 orthologs underlies their specific sensitivity or resistance to the inhibitory effects of BX430. Taking advantage of high-resolution crystallographic data available on zebrafish P2X4, we used molecular dynamics simulation to model the docking of BX430 on an allosteric binding site around Ile315 (zebrafish numbering) in the ectodomain of P2X4. We also observed that the only substitution I312D (human numbering) that renders P2X4 silent by itself has also a profound silencing effect on all other P2X subtypes tested when introduced at homologous positions. The generic impact of this aspartate mutation on P2X function indicates that the pre-TM2 subregion involved is conserved functionally and defines a novel allosteric inhibitory site present in all P2X receptor channels. This conserved structure-channel activity relationship might be exploited for the rational design of potent P2X subtype-selective antagonists of therapeutic value.","An allosteric inhibitory site conserved in the ectodomain of P2X receptor channels. P2X receptors constitute a gene family of cation channels gated by extracellular ATP. They mediate fast ionotropic purinergic signaling in neurons and non-excitable cell types in vertebrates. The highly calcium-permeable P2X4 subtype has been shown to play a significant role in cardiovascular physiology, inflammatory responses and neuro-immune communication. We previously reported the discovery of a P2X4-selective antagonist, the small organic compound BX430, with submicromolar potency for human P2X4 receptors and marked species-dependence (Ase et al., 2015). The present study investigates the molecular basis of P2X4 inhibition by the non-competitive blocker BX430 using a structural and functional approach relying on mutagenesis and electrophysiology. We provide evidence for the critical contribution of a single hydrophobic residue located in the ectodomain of P2X4 channel subunits, Ile312 in human P2X4, which determines blockade by BX430. We also show that the nature of this extracellular residue in various vertebrate P2X4 orthologs underlies their specific sensitivity or resistance to the inhibitory effects of BX430. Taking advantage of high-resolution crystallographic data available on zebrafish P2X4, we used molecular dynamics simulation to model the docking of BX430 on an allosteric binding site around Ile315 (zebrafish numbering) in the ectodomain of P2X4. We also observed that the only substitution I312D (human numbering) that renders P2X4 silent by itself has also a profound silencing effect on all other P2X subtypes tested when introduced at homologous positions. The generic impact of this aspartate mutation on P2X function indicates that the pre-TM2 subregion involved is conserved functionally and defines a novel allosteric inhibitory site present in all P2X receptor channels. This conserved structure-channel activity relationship might be exploited for the rational design of potent P2X subtype-selective antagonists of therapeutic value."
0,Transport of reactive oxygen and nitrogen species across aquaporin: A molecular level picture,"Aquaporins (AQPs) are transmembrane proteins that conduct not only water molecules across the cell membrane but also other solutes, such as reactive oxygen and nitrogen species (RONS), produced (among others) by cold atmospheric plasma (CAP). These RONS may induce oxidative stress in the cell interior, which plays a role in cancer treatment. The underlying mechanisms of the transport of RONS across AQPs, however, still remain obscure. We apply molecular dynamics simulations to investigate the permeation of both hydrophilic (H2O2 and OH) and hydrophobic (NO2 and NO) RONS through AQP1. Our simulations show that these RONS can all penetrate across the pores of AQP1. The permeation free energy barrier of OH and NO is lower than that of H2O2 and NO2, indicating that these radicals may have easier access to the pore interior and interact with the amino acid residues of AQP1. We also study the effect of RONS-induced oxidation of both the phospholipids and AQP1 (i.e., sulfenylation of Cys191) on the transport of the above-mentioned RONS across AQP1. Both lipid and protein oxidation seem to slightly increase the free energy barrier for H2O2 and NO2 permeation, while for OH and NO, we do not observe a strong effect of oxidation. The simulation results help to gain insight in the underlying mechanisms of the noticeable rise of CAP-induced RONS in cancer cells, thereby improving our understanding on the role of AQPs in the selective anticancer capacity of CAP.","Transport of reactive oxygen and nitrogen species across aquaporin: A molecular level picture. Aquaporins (AQPs) are transmembrane proteins that conduct not only water molecules across the cell membrane but also other solutes, such as reactive oxygen and nitrogen species (RONS), produced (among others) by cold atmospheric plasma (CAP). These RONS may induce oxidative stress in the cell interior, which plays a role in cancer treatment. The underlying mechanisms of the transport of RONS across AQPs, however, still remain obscure. We apply molecular dynamics simulations to investigate the permeation of both hydrophilic (H2O2 and OH) and hydrophobic (NO2 and NO) RONS through AQP1. Our simulations show that these RONS can all penetrate across the pores of AQP1. The permeation free energy barrier of OH and NO is lower than that of H2O2 and NO2, indicating that these radicals may have easier access to the pore interior and interact with the amino acid residues of AQP1. We also study the effect of RONS-induced oxidation of both the phospholipids and AQP1 (i.e., sulfenylation of Cys191) on the transport of the above-mentioned RONS across AQP1. Both lipid and protein oxidation seem to slightly increase the free energy barrier for H2O2 and NO2 permeation, while for OH and NO, we do not observe a strong effect of oxidation. The simulation results help to gain insight in the underlying mechanisms of the noticeable rise of CAP-induced RONS in cancer cells, thereby improving our understanding on the role of AQPs in the selective anticancer capacity of CAP."
0,"Stereotactic ablative radiotherapy versus standard of care palliative treatment in patients with oligometastatic cancers (SABR-COMET): a randomised, phase 2, open-label trial",,
0,Single-Cell Multi-omic Integration Compares and Contrasts Features of Brain Cell Identity,,
0,MCLPMDA: A novel method for miRNA-disease association prediction based on matrix completion and label propagation,"MiRNAs are a class of small non-coding RNAs that are involved in the development and progression of various complex diseases. Great efforts have been made to discover potential associations between miRNAs and diseases recently. As experimental methods are in general expensive and time-consuming, a large number of computational models have been developed to effectively predict reliable disease-related miRNAs. However, the inherent noise and incompleteness in the existing biological datasets have inevitably limited the prediction accuracy of current computational models. To solve this issue, in this paper, we propose a novel method for miRNA-disease association prediction based on matrix completion and label propagation. Specifically, our method first reconstructs a new miRNA/disease similarity matrix by matrix completion algorithm based on known experimentally verified miRNA-disease associations and then utilizes the label propagation algorithm to reliably predict disease-related miRNAs. As a result, MCLPMDA achieved comparable performance under different evaluation metrics and was capable of discovering greater number of true miRNA-disease associations. Moreover, case study conducted on Breast Neoplasms further confirmed the prediction reliability of the proposed method. Taken together, the experimental results clearly demonstrated that MCLPMDA can serve as an effective and reliable tool for miRNA-disease association prediction.","MCLPMDA: A novel method for miRNA-disease association prediction based on matrix completion and label propagation. MiRNAs are a class of small non-coding RNAs that are involved in the development and progression of various complex diseases. Great efforts have been made to discover potential associations between miRNAs and diseases recently. As experimental methods are in general expensive and time-consuming, a large number of computational models have been developed to effectively predict reliable disease-related miRNAs. However, the inherent noise and incompleteness in the existing biological datasets have inevitably limited the prediction accuracy of current computational models. To solve this issue, in this paper, we propose a novel method for miRNA-disease association prediction based on matrix completion and label propagation. Specifically, our method first reconstructs a new miRNA/disease similarity matrix by matrix completion algorithm based on known experimentally verified miRNA-disease associations and then utilizes the label propagation algorithm to reliably predict disease-related miRNAs. As a result, MCLPMDA achieved comparable performance under different evaluation metrics and was capable of discovering greater number of true miRNA-disease associations. Moreover, case study conducted on Breast Neoplasms further confirmed the prediction reliability of the proposed method. Taken together, the experimental results clearly demonstrated that MCLPMDA can serve as an effective and reliable tool for miRNA-disease association prediction."
0,Implementation of genotype-guided dosing of warfarin with point-of-care genetic testing in three UK clinics: a matched cohort study,"BACKGROUND: Warfarin is a widely used oral anticoagulant. Determining the correct dose required to maintain the international normalised ratio (INR) within a therapeutic range can be challenging. In a previous trial, we showed that a dosing algorithm incorporating point-of-care genotyping information ('POCT-GGD' approach) led to improved anticoagulation control. To determine whether this approach could translate into clinical practice, we undertook an implementation project using a matched cohort design. METHODS: At three clinics (implementation group; n = 119), initial doses were calculated using the POCT-GGD approach; at another three matched clinics (control group; n = 93), patients were dosed according to the clinic's routine practice. We also utilised data on 640 patients obtained from routinely collected data at comparable clinics. Primary outcome was percentage time in target INR range. Patients and staff from the implementation group also provided questionnaire feedback on POCT-GGD. RESULTS: Mean percentage time in INR target range was 55.25% in the control group and 62.74% in the implementation group; therefore, 7.49% (95% CI 3.41-11.57%) higher in the implementation group (p = 0.0004). Overall, patients and staff viewed POCT-GGD positively, suggesting minor adjustments to allow smooth implementation into practice. CONCLUSIONS: In the first demonstration of the implementation of genotype-guided dosing, we show that warfarin dosing determined using an algorithm incorporating genetic and clinical factors can be implemented smoothly into clinic, to ensure target INR range is reached sooner and maintained. The findings are like our previous randomised controlled trial, providing an alternative method for improving the risk-benefit of warfarin use in daily practice.","Implementation of genotype-guided dosing of warfarin with point-of-care genetic testing in three UK clinics: a matched cohort study. BACKGROUND: Warfarin is a widely used oral anticoagulant. Determining the correct dose required to maintain the international normalised ratio (INR) within a therapeutic range can be challenging. In a previous trial, we showed that a dosing algorithm incorporating point-of-care genotyping information ('POCT-GGD' approach) led to improved anticoagulation control. To determine whether this approach could translate into clinical practice, we undertook an implementation project using a matched cohort design. METHODS: At three clinics (implementation group; n = 119), initial doses were calculated using the POCT-GGD approach; at another three matched clinics (control group; n = 93), patients were dosed according to the clinic's routine practice. We also utilised data on 640 patients obtained from routinely collected data at comparable clinics. Primary outcome was percentage time in target INR range. Patients and staff from the implementation group also provided questionnaire feedback on POCT-GGD. RESULTS: Mean percentage time in INR target range was 55.25% in the control group and 62.74% in the implementation group; therefore, 7.49% (95% CI 3.41-11.57%) higher in the implementation group (p = 0.0004). Overall, patients and staff viewed POCT-GGD positively, suggesting minor adjustments to allow smooth implementation into practice. CONCLUSIONS: In the first demonstration of the implementation of genotype-guided dosing, we show that warfarin dosing determined using an algorithm incorporating genetic and clinical factors can be implemented smoothly into clinic, to ensure target INR range is reached sooner and maintained. The findings are like our previous randomised controlled trial, providing an alternative method for improving the risk-benefit of warfarin use in daily practice."
0,"A perturbed gene network containing PI3K-AKT, RAS-ERK and WNT-beta-catenin pathways in leukocytes is linked to ASD genetics and symptom severity",,
0,"Motor primitives in space and time via targeted gain modulation in cortical networks (vol 21, pg 1774, 2018)",,
0,Multi-task exclusive relationship learning for alzheimer's disease progression prediction with longitudinal data,"Alzheimer's disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Currently, many multi-task learning approaches have been proposed to predict the disease progression at the early stage using longitudinal data, with each task corresponding to a particular time point. However, the underlying association among different time points in disease progression is still under-explored in previous studies. To this end, we propose a multi-task exclusive relationship learning model to automatically capture the intrinsic relationship among tasks at different time points for estimating clinical measures based on longitudinal imaging data. The proposed method can select the most discriminative features for different tasks and also model the intrinsic relatedness among different time points, by utilizing an exclusive lasso regularization and a relationship induced regularization. Specifically, the exclusive lasso regularization enables partial group structure feature selection among the longitudinal data, while the relationship induced regularization efficiently introduces the relationship information from data to guide knowledge transfer. We further develop an efficient optimization algorithm to solve the proposed objective function. Extensive experiments on both synthetic and real datasets demonstrate the effectiveness of our proposed method. In comparison with several state-of-the-art methods, our proposed method can achieve promising performance for cognitive status prediction and also can help discover disease-related biomarkers.","Multi-task exclusive relationship learning for alzheimer's disease progression prediction with longitudinal data. Alzheimer's disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Currently, many multi-task learning approaches have been proposed to predict the disease progression at the early stage using longitudinal data, with each task corresponding to a particular time point. However, the underlying association among different time points in disease progression is still under-explored in previous studies. To this end, we propose a multi-task exclusive relationship learning model to automatically capture the intrinsic relationship among tasks at different time points for estimating clinical measures based on longitudinal imaging data. The proposed method can select the most discriminative features for different tasks and also model the intrinsic relatedness among different time points, by utilizing an exclusive lasso regularization and a relationship induced regularization. Specifically, the exclusive lasso regularization enables partial group structure feature selection among the longitudinal data, while the relationship induced regularization efficiently introduces the relationship information from data to guide knowledge transfer. We further develop an efficient optimization algorithm to solve the proposed objective function. Extensive experiments on both synthetic and real datasets demonstrate the effectiveness of our proposed method. In comparison with several state-of-the-art methods, our proposed method can achieve promising performance for cognitive status prediction and also can help discover disease-related biomarkers."
0,Gene-Environment Interaction in the Era of Precision Medicine,,
0,An exoskeleton controlled by an epidural wireless brain-machine interface in a tetraplegic patient: a proof-of-concept demonstration,,
0,Bisecting GlcNAc is a general suppressor of terminal modification of N-glycan,"Glycoproteins are decorated with complex glycans for protein functions. However, regulation mechanisms of complex glycan biosynthesis are largely unclear. Here we found that bisecting GlcNAc, a branching sugar residue in N-glycan, suppresses the biosynthesis of various types of terminal epitopes in N-glycans, including fucose, sialic acid and human natural killer-1. Expression of these epitopes in N-glycan was elevated in mice lacking the biosynthetic enzyme of bisecting GlcNAc, GnT-III, and was conversely suppressed by GnT-III overexpression in cells. Many glycosyltransferases for N-glycan terminals were revealed to prefer a nonbisected N-glycan as a substrate to its bisected counterpart, whereas no up-regulation of their mRNAs was found. This indicates that the elevated expression of the terminal N-glycan epitopes in GnT-III-deficient mice is attributed to the substrate specificity of the biosynthetic enzymes. Molecular dynamics simulations further confirmed that nonbisected glycans were preferentially accepted by those glycosyltransferases. These findings unveil a new regulation mechanism of protein N-glycosylation.","Bisecting GlcNAc is a general suppressor of terminal modification of N-glycan. Glycoproteins are decorated with complex glycans for protein functions. However, regulation mechanisms of complex glycan biosynthesis are largely unclear. Here we found that bisecting GlcNAc, a branching sugar residue in N-glycan, suppresses the biosynthesis of various types of terminal epitopes in N-glycans, including fucose, sialic acid and human natural killer-1. Expression of these epitopes in N-glycan was elevated in mice lacking the biosynthetic enzyme of bisecting GlcNAc, GnT-III, and was conversely suppressed by GnT-III overexpression in cells. Many glycosyltransferases for N-glycan terminals were revealed to prefer a nonbisected N-glycan as a substrate to its bisected counterpart, whereas no up-regulation of their mRNAs was found. This indicates that the elevated expression of the terminal N-glycan epitopes in GnT-III-deficient mice is attributed to the substrate specificity of the biosynthetic enzymes. Molecular dynamics simulations further confirmed that nonbisected glycans were preferentially accepted by those glycosyltransferases. These findings unveil a new regulation mechanism of protein N-glycosylation."
0,Discovery of stroke-related blood biomarkers from gene expression network models,"Background: Identifying molecular biomarkers characteristic of ischemic stroke has the potential to aid in distinguishing stroke cases from stroke mimicking symptoms, as well as advancing the understanding of the physiological changes that underlie the body's response to stroke. This study uses machine learning-based analysis of gene co-expression to identify transcription patterns characteristic of patients with acute ischemic stroke. Methods: Mutual information values for the expression levels among 13,243 quantified transcripts were computed for blood samples from 82 stroke patients and 68 controls to construct a co-expression network of genes (separately) for stroke and control samples. Page rank centrality scores were computed for every gene; a gene's significance in the network was assessed according to the differences in their network's pagerank centrality between stroke and control expression patterns. A hybrid genetic algorithm - support vector machine learning tool was used to classify samples based on gene centrality in order to identify an optimal set of predictor genes for stroke while minimizing the number of genes in the model. Results: A predictive model with 89.6% accuracy was identified using 6 network-central and differentially expressed genes (ID3, MBTPS1, NOG, SFXN2, BMX, SLC22A1), characterized by large differences in association network connectivity between stroke and control samples. In contrast, classification models based solely on individual genes identified by significant fold-changes in expression level provided lower predictive accuracies: < 71% for any single gene, and even models with larger (10-25) numbers of gene transcript biomarkers gave lower predictive accuracies (â‰¤ 82%) than the 6 network-based gene signature classification. miRNA:mRNA target prediction computational analysis revealed 8 differentially expressed micro-RNAs (miRNAs) that are significantly associated with at least 2 of the 6 network-central genes. Conclusions: Network-based models have the potential to identify a more statistically robust pattern of gene expression typical of acute ischemic stroke and to generate hypotheses about possible interactions among functionally relevant genes, leading to the identification of more informative biomarkers.","Discovery of stroke-related blood biomarkers from gene expression network models. Background: Identifying molecular biomarkers characteristic of ischemic stroke has the potential to aid in distinguishing stroke cases from stroke mimicking symptoms, as well as advancing the understanding of the physiological changes that underlie the body's response to stroke. This study uses machine learning-based analysis of gene co-expression to identify transcription patterns characteristic of patients with acute ischemic stroke. Methods: Mutual information values for the expression levels among 13,243 quantified transcripts were computed for blood samples from 82 stroke patients and 68 controls to construct a co-expression network of genes (separately) for stroke and control samples. Page rank centrality scores were computed for every gene; a gene's significance in the network was assessed according to the differences in their network's pagerank centrality between stroke and control expression patterns. A hybrid genetic algorithm - support vector machine learning tool was used to classify samples based on gene centrality in order to identify an optimal set of predictor genes for stroke while minimizing the number of genes in the model. Results: A predictive model with 89.6% accuracy was identified using 6 network-central and differentially expressed genes (ID3, MBTPS1, NOG, SFXN2, BMX, SLC22A1), characterized by large differences in association network connectivity between stroke and control samples. In contrast, classification models based solely on individual genes identified by significant fold-changes in expression level provided lower predictive accuracies: < 71% for any single gene, and even models with larger (10-25) numbers of gene transcript biomarkers gave lower predictive accuracies (â‰¤ 82%) than the 6 network-based gene signature classification. miRNA:mRNA target prediction computational analysis revealed 8 differentially expressed micro-RNAs (miRNAs) that are significantly associated with at least 2 of the 6 network-central genes. Conclusions: Network-based models have the potential to identify a more statistically robust pattern of gene expression typical of acute ischemic stroke and to generate hypotheses about possible interactions among functionally relevant genes, leading to the identification of more informative biomarkers."
0,Evaluation of Cardiac Rhythm Abnormalities From Wearable Devices,,
0,Tissue-Specific Macrophage Responses to Remote Injury Impact the Outcome of Subsequent Local Immune Challenge,,
0,"Laparoscopic supracervical hysterectomy versus endometrial ablation for women with heavy menstrual bleeding (HEALTH): a parallel-group, open-label, randomised controlled trial",,
0,Predicting outcomes in cardiogenic shock: Are we at risk of having too many scores but too little information?,,
0,Equalization of four cardiovascular risk algorithms after systematic recalibration: individual-participant meta-analysis of 86 prospective studies,"AIMS: There is debate about the optimum algorithm for cardiovascular disease (CVD) risk estimation. We conducted head-to-head comparisons of four algorithms recommended by primary prevention guidelines, before and after 'recalibration', a method that adapts risk algorithms to take account of differences in the risk characteristics of the populations being studied. METHODS AND RESULTS: Using individual-participant data on 360 737 participants without CVD at baseline in 86 prospective studies from 22 countries, we compared the Framingham risk score (FRS), Systematic COronary Risk Evaluation (SCORE), pooled cohort equations (PCE), and Reynolds risk score (RRS). We calculated measures of risk discrimination and calibration, and modelled clinical implications of initiating statin therapy in people judged to be at 'high' 10 year CVD risk. Original risk algorithms were recalibrated using the risk factor profile and CVD incidence of target populations. The four algorithms had similar risk discrimination. Before recalibration, FRS, SCORE, and PCE over-predicted CVD risk on average by 10%, 52%, and 41%, respectively, whereas RRS under-predicted by 10%. Original versions of algorithms classified 29-39% of individuals aged >/=40 years as high risk. By contrast, recalibration reduced this proportion to 22-24% for every algorithm. We estimated that to prevent one CVD event, it would be necessary to initiate statin therapy in 44-51 such individuals using original algorithms, in contrast to 37-39 individuals with recalibrated algorithms. CONCLUSION: Before recalibration, the clinical performance of four widely used CVD risk algorithms varied substantially. By contrast, simple recalibration nearly equalized their performance and improved modelled targeting of preventive action to clinical need.","Equalization of four cardiovascular risk algorithms after systematic recalibration: individual-participant meta-analysis of 86 prospective studies. AIMS: There is debate about the optimum algorithm for cardiovascular disease (CVD) risk estimation. We conducted head-to-head comparisons of four algorithms recommended by primary prevention guidelines, before and after 'recalibration', a method that adapts risk algorithms to take account of differences in the risk characteristics of the populations being studied. METHODS AND RESULTS: Using individual-participant data on 360 737 participants without CVD at baseline in 86 prospective studies from 22 countries, we compared the Framingham risk score (FRS), Systematic COronary Risk Evaluation (SCORE), pooled cohort equations (PCE), and Reynolds risk score (RRS). We calculated measures of risk discrimination and calibration, and modelled clinical implications of initiating statin therapy in people judged to be at 'high' 10 year CVD risk. Original risk algorithms were recalibrated using the risk factor profile and CVD incidence of target populations. The four algorithms had similar risk discrimination. Before recalibration, FRS, SCORE, and PCE over-predicted CVD risk on average by 10%, 52%, and 41%, respectively, whereas RRS under-predicted by 10%. Original versions of algorithms classified 29-39% of individuals aged >/=40 years as high risk. By contrast, recalibration reduced this proportion to 22-24% for every algorithm. We estimated that to prevent one CVD event, it would be necessary to initiate statin therapy in 44-51 such individuals using original algorithms, in contrast to 37-39 individuals with recalibrated algorithms. CONCLUSION: Before recalibration, the clinical performance of four widely used CVD risk algorithms varied substantially. By contrast, simple recalibration nearly equalized their performance and improved modelled targeting of preventive action to clinical need."
0,Lifecycle Regulation of Artificial Intelligence- and Machine Learning-Based Software Devices in Medicine,,
0,Key challenges for delivering clinical impact with artificial intelligence,"BACKGROUND: Artificial intelligence (AI) research in healthcare is accelerating rapidly, with potential applications being demonstrated across various domains of medicine. However, there are currently limited examples of such techniques being successfully deployed into clinical practice. This article explores the main challenges and limitations of AI in healthcare, and considers the steps required to translate these potentially transformative technologies from research to clinical practice. MAIN BODY: Key challenges for the translation of AI systems in healthcare include those intrinsic to the science of machine learning, logistical difficulties in implementation, and consideration of the barriers to adoption as well as of the necessary sociocultural or pathway changes. Robust peer-reviewed clinical evaluation as part of randomised controlled trials should be viewed as the gold standard for evidence generation, but conducting these in practice may not always be appropriate or feasible. Performance metrics should aim to capture real clinical applicability and be understandable to intended users. Regulation that balances the pace of innovation with the potential for harm, alongside thoughtful post-market surveillance, is required to ensure that patients are not exposed to dangerous interventions nor deprived of access to beneficial innovations. Mechanisms to enable direct comparisons of AI systems must be developed, including the use of independent, local and representative test sets. Developers of AI algorithms must be vigilant to potential dangers, including dataset shift, accidental fitting of confounders, unintended discriminatory bias, the challenges of generalisation to new populations, and the unintended negative consequences of new algorithms on health outcomes. CONCLUSION: The safe and timely translation of AI research into clinically validated and appropriately regulated systems that can benefit everyone is challenging. Robust clinical evaluation, using metrics that are intuitive to clinicians and ideally go beyond measures of technical accuracy to include quality of care and patient outcomes, is essential. Further work is required (1) to identify themes of algorithmic bias and unfairness while developing mitigations to address these, (2) to reduce brittleness and improve generalisability, and (3) to develop methods for improved interpretability of machine learning predictions. If these goals can be achieved, the benefits for patients are likely to be transformational.","Key challenges for delivering clinical impact with artificial intelligence. BACKGROUND: Artificial intelligence (AI) research in healthcare is accelerating rapidly, with potential applications being demonstrated across various domains of medicine. However, there are currently limited examples of such techniques being successfully deployed into clinical practice. This article explores the main challenges and limitations of AI in healthcare, and considers the steps required to translate these potentially transformative technologies from research to clinical practice. MAIN BODY: Key challenges for the translation of AI systems in healthcare include those intrinsic to the science of machine learning, logistical difficulties in implementation, and consideration of the barriers to adoption as well as of the necessary sociocultural or pathway changes. Robust peer-reviewed clinical evaluation as part of randomised controlled trials should be viewed as the gold standard for evidence generation, but conducting these in practice may not always be appropriate or feasible. Performance metrics should aim to capture real clinical applicability and be understandable to intended users. Regulation that balances the pace of innovation with the potential for harm, alongside thoughtful post-market surveillance, is required to ensure that patients are not exposed to dangerous interventions nor deprived of access to beneficial innovations. Mechanisms to enable direct comparisons of AI systems must be developed, including the use of independent, local and representative test sets. Developers of AI algorithms must be vigilant to potential dangers, including dataset shift, accidental fitting of confounders, unintended discriminatory bias, the challenges of generalisation to new populations, and the unintended negative consequences of new algorithms on health outcomes. CONCLUSION: The safe and timely translation of AI research into clinically validated and appropriately regulated systems that can benefit everyone is challenging. Robust clinical evaluation, using metrics that are intuitive to clinicians and ideally go beyond measures of technical accuracy to include quality of care and patient outcomes, is essential. Further work is required (1) to identify themes of algorithmic bias and unfairness while developing mitigations to address these, (2) to reduce brittleness and improve generalisability, and (3) to develop methods for improved interpretability of machine learning predictions. If these goals can be achieved, the benefits for patients are likely to be transformational."
0,Clinical algorithm to screen for cardiopulmonary disease in low-income settings,,
0,miR-9 Upregulation Integrates Post-ischemic Neuronal Survival and Regeneration In Vitro,"The irrefutable change in the expression of brain-enriched microRNAs (miRNAs) following ischemic stroke has promoted the development of radical miRNA-based therapeutics encompassing neuroprotection and neuronal restoration. Our previous report on the systems-level prediction of miR-9 in post-stroke-induced neurogenesis served as a premise to experimentally uncover the functional role of miR-9 in post-ischemic neuronal survival and regeneration. The oxygen-glucose deprivation (OGD) in SH-SY5Y cells significantly reduced miR-9 expression, while miR-9 mimic transfection enhanced post-ischemic neuronal cell viability. The next major objective involved the execution of a drug repositioning strategy to augment miR-9 expression via structure-based screening of Food and Drug Administration (FDA)-approved drugs that bind to Histone Deacetylase 4 (HDAC4), a known miR-9 target. Glucosamine emerged as the top hit and its binding potential to HDAC4 was verified by Molecular Dynamics (MD) Simulation, Drug Affinity Responsive Target Stability (DARTS) assay, and MALDI-TOF MS. It was intriguing that the glucosamine treatment 1-h post-OGD was associated with the increased miR-9 level as well as enhanced neuronal viability. miR-9 mimic or post-OGD glucosamine treatment significantly increased the cellular proliferation (BrdU assay), while the neurite outgrowth assay displayed elongated neurites. The enhanced BCL2 and VEGF parallel with the reduced NFÎºB1, TNF-Î±, IL-1Î², and iNOS mRNA levels in miR-9 mimic or glucosamine-treated cells further substantiated their post-ischemic neuroprotective and regenerative efficacy. Hence, this study unleashes a potential therapeutic approach that integrates neuronal survival and regeneration via small-molecule-based regulation of miR-9 favoring long-term recovery against ischemic stroke.","miR-9 Upregulation Integrates Post-ischemic Neuronal Survival and Regeneration In Vitro. The irrefutable change in the expression of brain-enriched microRNAs (miRNAs) following ischemic stroke has promoted the development of radical miRNA-based therapeutics encompassing neuroprotection and neuronal restoration. Our previous report on the systems-level prediction of miR-9 in post-stroke-induced neurogenesis served as a premise to experimentally uncover the functional role of miR-9 in post-ischemic neuronal survival and regeneration. The oxygen-glucose deprivation (OGD) in SH-SY5Y cells significantly reduced miR-9 expression, while miR-9 mimic transfection enhanced post-ischemic neuronal cell viability. The next major objective involved the execution of a drug repositioning strategy to augment miR-9 expression via structure-based screening of Food and Drug Administration (FDA)-approved drugs that bind to Histone Deacetylase 4 (HDAC4), a known miR-9 target. Glucosamine emerged as the top hit and its binding potential to HDAC4 was verified by Molecular Dynamics (MD) Simulation, Drug Affinity Responsive Target Stability (DARTS) assay, and MALDI-TOF MS. It was intriguing that the glucosamine treatment 1-h post-OGD was associated with the increased miR-9 level as well as enhanced neuronal viability. miR-9 mimic or post-OGD glucosamine treatment significantly increased the cellular proliferation (BrdU assay), while the neurite outgrowth assay displayed elongated neurites. The enhanced BCL2 and VEGF parallel with the reduced NFÎºB1, TNF-Î±, IL-1Î², and iNOS mRNA levels in miR-9 mimic or glucosamine-treated cells further substantiated their post-ischemic neuroprotective and regenerative efficacy. Hence, this study unleashes a potential therapeutic approach that integrates neuronal survival and regeneration via small-molecule-based regulation of miR-9 favoring long-term recovery against ischemic stroke."
0,Continual Learning in a Multi-Layer Network of an Electric Fish,,
0,WDR72 Mutations Associated with Amelogenesis Imperfecta and Acidosis,,
0,Prediction of forelimb reach results from motor cortex activities based on calcium imaging and deep learning,"Brain-wide activities revealed by neuroimaging and recording techniques have been used to predict motor and cognitive functions in both human and animal models. However, although studies have shown the existence of micrometer-scale spatial organization of neurons in the motor cortex relevant to motor control, two-photon microscopy (TPM) calcium imaging at cellular resolution has not been fully exploited for the same purpose. Here, we ask if calcium imaging data recorded by TPM in rodent brain can provide enough information to predict features of upcoming movement. We collected calcium imaging signal from rostral forelimb area in layer 2/3 of the motor cortex while mice performed a two-dimensional lever reaching task. Images of average calcium activity collected during motion preparation period and inter-trial interval (ITI) were used to predict the forelimb reach results. The evaluation was based on a deep learning model that had been applied for object recognition. We found that the prediction accuracy for both maximum reaching location and trial outcome based on motion preparation period but not ITI were higher than the probabilities governed by chance. Our study demonstrated that imaging data encompassing information on the spatial organization of functional neuronal clusters in the motor cortex is useful in predicting motor acts even in the absence of detailed dynamics of neural activities.","Prediction of forelimb reach results from motor cortex activities based on calcium imaging and deep learning. Brain-wide activities revealed by neuroimaging and recording techniques have been used to predict motor and cognitive functions in both human and animal models. However, although studies have shown the existence of micrometer-scale spatial organization of neurons in the motor cortex relevant to motor control, two-photon microscopy (TPM) calcium imaging at cellular resolution has not been fully exploited for the same purpose. Here, we ask if calcium imaging data recorded by TPM in rodent brain can provide enough information to predict features of upcoming movement. We collected calcium imaging signal from rostral forelimb area in layer 2/3 of the motor cortex while mice performed a two-dimensional lever reaching task. Images of average calcium activity collected during motion preparation period and inter-trial interval (ITI) were used to predict the forelimb reach results. The evaluation was based on a deep learning model that had been applied for object recognition. We found that the prediction accuracy for both maximum reaching location and trial outcome based on motion preparation period but not ITI were higher than the probabilities governed by chance. Our study demonstrated that imaging data encompassing information on the spatial organization of functional neuronal clusters in the motor cortex is useful in predicting motor acts even in the absence of detailed dynamics of neural activities."
0,A classification framework for exploiting sparse multi-variate temporal features with application to adverse drug event detection in medical records,"BACKGROUND: Adverse drug events (ADEs) as well as other preventable adverse events in the hospital setting incur a yearly monetary cost of approximately $3.5 billion, in the United States alone. Therefore, it is of paramount importance to reduce the impact and prevalence of ADEs within the healthcare sector, not only since it will result in reducing human suffering, but also as a means to substantially reduce economical strains on the healthcare system. One approach to mitigate this problem is to employ predictive models. While existing methods have been focusing on the exploitation of static features, limited attention has been given to temporal features. METHODS: In this paper, we present a novel classification framework for detecting ADEs in complex Electronic health records (EHRs) by exploiting the temporality and sparsity of the underlying features. The proposed framework consists of three phases for transforming sparse and multi-variate time series features into a single-valued feature representation, which can then be used by any classifier. Moreover, we propose and evaluate three different strategies for leveraging feature sparsity by incorporating it into the new representation. RESULTS: A large-scale evaluation on 15 ADE datasets extracted from a real-world EHR system shows that the proposed framework achieves significantly improved predictive performance compared to state-of-the-art. Moreover, our framework can reveal features that are clinically consistent with medical findings on ADE detection. CONCLUSIONS: Our study and experimental findings demonstrate that temporal multi-variate features of variable length and with high sparsity can be effectively utilized to predict ADEs from EHRs. Two key advantages of our framework are that it is method agnostic, i.e., versatile, and of low computational cost, i.e., fast; hence providing an important building block for future exploitation within the domain of machine learning from EHRs.","A classification framework for exploiting sparse multi-variate temporal features with application to adverse drug event detection in medical records. BACKGROUND: Adverse drug events (ADEs) as well as other preventable adverse events in the hospital setting incur a yearly monetary cost of approximately $3.5 billion, in the United States alone. Therefore, it is of paramount importance to reduce the impact and prevalence of ADEs within the healthcare sector, not only since it will result in reducing human suffering, but also as a means to substantially reduce economical strains on the healthcare system. One approach to mitigate this problem is to employ predictive models. While existing methods have been focusing on the exploitation of static features, limited attention has been given to temporal features. METHODS: In this paper, we present a novel classification framework for detecting ADEs in complex Electronic health records (EHRs) by exploiting the temporality and sparsity of the underlying features. The proposed framework consists of three phases for transforming sparse and multi-variate time series features into a single-valued feature representation, which can then be used by any classifier. Moreover, we propose and evaluate three different strategies for leveraging feature sparsity by incorporating it into the new representation. RESULTS: A large-scale evaluation on 15 ADE datasets extracted from a real-world EHR system shows that the proposed framework achieves significantly improved predictive performance compared to state-of-the-art. Moreover, our framework can reveal features that are clinically consistent with medical findings on ADE detection. CONCLUSIONS: Our study and experimental findings demonstrate that temporal multi-variate features of variable length and with high sparsity can be effectively utilized to predict ADEs from EHRs. Two key advantages of our framework are that it is method agnostic, i.e., versatile, and of low computational cost, i.e., fast; hence providing an important building block for future exploitation within the domain of machine learning from EHRs."
0,TFAP2C- and p63-Dependent Networks Sequentially Rearrange Chromatin Landscapes to Drive Human Epidermal Lineage Commitment,"Tissue development results from lineage-specific transcription factors (TFs) programming a dynamic chromatin landscape through progressive cell fate transitions. Here, we define epigenomic landscape during epidermal differentiation of human pluripotent stem cells (PSCs) and create inference networks that integrate gene expression, chromatin accessibility, and TF binding to define regulatory mechanisms during keratinocyte specification. We found two critical chromatin networks during surface ectoderm initiation and keratinocyte maturation, which are driven by TFAP2C and p63, respectively. Consistently, TFAP2C, but not p63, is sufficient to initiate surface ectoderm differentiation, and TFAP2C-initiated progenitor cells are capable of maturing into functional keratinocytes. Mechanistically, TFAP2C primes the surface ectoderm chromatin landscape and induces p63 expression and binding sites, thus allowing maturation factor p63 to positively autoregulate its own expression and close a subset of the TFAP2C-initiated surface ectoderm program. Our work provides a general framework to infer TF networks controlling chromatin transitions that will facilitate future regenerative medicine advances.","TFAP2C- and p63-Dependent Networks Sequentially Rearrange Chromatin Landscapes to Drive Human Epidermal Lineage Commitment. Tissue development results from lineage-specific transcription factors (TFs) programming a dynamic chromatin landscape through progressive cell fate transitions. Here, we define epigenomic landscape during epidermal differentiation of human pluripotent stem cells (PSCs) and create inference networks that integrate gene expression, chromatin accessibility, and TF binding to define regulatory mechanisms during keratinocyte specification. We found two critical chromatin networks during surface ectoderm initiation and keratinocyte maturation, which are driven by TFAP2C and p63, respectively. Consistently, TFAP2C, but not p63, is sufficient to initiate surface ectoderm differentiation, and TFAP2C-initiated progenitor cells are capable of maturing into functional keratinocytes. Mechanistically, TFAP2C primes the surface ectoderm chromatin landscape and induces p63 expression and binding sites, thus allowing maturation factor p63 to positively autoregulate its own expression and close a subset of the TFAP2C-initiated surface ectoderm program. Our work provides a general framework to infer TF networks controlling chromatin transitions that will facilitate future regenerative medicine advances."
0,Atrial fibrillation ablation in practice: assessing CABANA generalizability,,
0,Better Sensation and Dexterity for Artificial Hands,,
0,The Metastable XBP1u Transmembrane Domain Defines Determinants for Intramembrane Proteolysis by Signal Peptide Peptidase,"Unspliced XBP1 mRNA encodes XBP1u, the transcriptionally inert variant of the unfolded protein response (UPR) transcription factor XBP1s. XBP1u targets its mRNA-ribosome-nascent-chain-complex to the endoplasmic reticulum (ER) to facilitate UPR activation and prevents overactivation. Yet, its membrane association is controversial. Here, we use cell-free translocation and cellular assays to define a moderately hydrophobic stretch in XBP1u that is sufficient to mediate insertion into the ER membrane. Mutagenesis of this transmembrane (TM) region reveals residues that facilitate XBP1u turnover by an ER-associated degradation route that is dependent on signal peptide peptidase (SPP). Furthermore, the impact of these mutations on TM helix dynamics was assessed by residue-specific amide exchange kinetics, evaluated by a semi-automated algorithm. Based on our results, we suggest that SPP-catalyzed intramembrane proteolysis of TM helices is not only determined by their conformational flexibility, but also by side-chain interactions near the scissile peptide bond with the enzyme's active site.","The Metastable XBP1u Transmembrane Domain Defines Determinants for Intramembrane Proteolysis by Signal Peptide Peptidase. Unspliced XBP1 mRNA encodes XBP1u, the transcriptionally inert variant of the unfolded protein response (UPR) transcription factor XBP1s. XBP1u targets its mRNA-ribosome-nascent-chain-complex to the endoplasmic reticulum (ER) to facilitate UPR activation and prevents overactivation. Yet, its membrane association is controversial. Here, we use cell-free translocation and cellular assays to define a moderately hydrophobic stretch in XBP1u that is sufficient to mediate insertion into the ER membrane. Mutagenesis of this transmembrane (TM) region reveals residues that facilitate XBP1u turnover by an ER-associated degradation route that is dependent on signal peptide peptidase (SPP). Furthermore, the impact of these mutations on TM helix dynamics was assessed by residue-specific amide exchange kinetics, evaluated by a semi-automated algorithm. Based on our results, we suggest that SPP-catalyzed intramembrane proteolysis of TM helices is not only determined by their conformational flexibility, but also by side-chain interactions near the scissile peptide bond with the enzyme's active site."
0,Quantitative Interferon Gamma Release Assay and Tuberculin Skin Test Results to Predict Incident Tuberculosis: A Prospective Cohort Study,,
0,High-Frequency Ultrasound Imaging for Examination of Early Dental Caries,"The extent of dental tissue destruction during the treatment of white spot lesions (WSLs) increases with the severity of the lesion. If the depth and shape of WSLs can be predicted with a noninvasive diagnostic method before dental caries treatment, more conservative interventions can be planned. Given the superiority of high-frequency ultrasound (HFUS) imaging in observing the internal structures of the body, the present study aimed to verify the possibility of HFUS imaging to examine the depth and shape of WSLs. We prepared tooth samples and developed a biomicroscopic system with a HFUS transducer to obtain images of normal and WSL regions. HFUS images were compared with conventional ultrasound images and micro-computed tomography images. HFUS distinctly differentiated demineralization within WSL and normal regions. WSL depth calculated in the micro-computed tomography image was similar to that in HFUS. This study revealed that HFUS imaging has the potential to detect early dental caries and offer information on the invasion depth of early dental caries quantitatively.","High-Frequency Ultrasound Imaging for Examination of Early Dental Caries. The extent of dental tissue destruction during the treatment of white spot lesions (WSLs) increases with the severity of the lesion. If the depth and shape of WSLs can be predicted with a noninvasive diagnostic method before dental caries treatment, more conservative interventions can be planned. Given the superiority of high-frequency ultrasound (HFUS) imaging in observing the internal structures of the body, the present study aimed to verify the possibility of HFUS imaging to examine the depth and shape of WSLs. We prepared tooth samples and developed a biomicroscopic system with a HFUS transducer to obtain images of normal and WSL regions. HFUS images were compared with conventional ultrasound images and micro-computed tomography images. HFUS distinctly differentiated demineralization within WSL and normal regions. WSL depth calculated in the micro-computed tomography image was similar to that in HFUS. This study revealed that HFUS imaging has the potential to detect early dental caries and offer information on the invasion depth of early dental caries quantitatively."
0,"Smartphone-Based, Artificial Intelligence-Enabled Diabetic Retinopathy Screening",,
0,Anticancer effect of deuterium depleted water - Redox disbalance leads to oxidative stress,"Despite the convincing empirical evidence that deuterium depleted water (DDW, 25-125 ppm deuterium) has anticancer effect, the molecular mechanism remains unclear. Here, redox proteomics investigation of the DDW action in A549 cells revealed an increased level of oxidative stress, whereas expression proteomics in combination with thermal profiling uncovered crucial role of mitochondrial proteins. In the proposed scenario, reversal of the normally positive deuterium gradient across the inner membrane leads to an increased export of protons from the matrix to intermembrane space and an increase in the mitochondrial membrane potential, enhancing the production of reactive oxygen species (ROS). The resulting oxidative stress leads to slower growth and can induce apoptosis. However, further deuterium depletion in ambient water triggers a feedback mechanism, which leads to restoration of the redox equilibrium and resumed growth. The DDW-induced oxidative stress, verified by traditional biochemical assays, may be helpful as an adjuvant to ROS-inducing anticancer therapy.","Anticancer effect of deuterium depleted water - Redox disbalance leads to oxidative stress. Despite the convincing empirical evidence that deuterium depleted water (DDW, 25-125 ppm deuterium) has anticancer effect, the molecular mechanism remains unclear. Here, redox proteomics investigation of the DDW action in A549 cells revealed an increased level of oxidative stress, whereas expression proteomics in combination with thermal profiling uncovered crucial role of mitochondrial proteins. In the proposed scenario, reversal of the normally positive deuterium gradient across the inner membrane leads to an increased export of protons from the matrix to intermembrane space and an increase in the mitochondrial membrane potential, enhancing the production of reactive oxygen species (ROS). The resulting oxidative stress leads to slower growth and can induce apoptosis. However, further deuterium depletion in ambient water triggers a feedback mechanism, which leads to restoration of the redox equilibrium and resumed growth. The DDW-induced oxidative stress, verified by traditional biochemical assays, may be helpful as an adjuvant to ROS-inducing anticancer therapy."
0,Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network,"Computerized electrocardiogram (ECG) interpretation plays a critical role in the clinical ECG workflow(1). Widely available digital ECG data and the algorithmic paradigm of deep learning(2) present an opportunity to substantially improve the accuracy and scalability of automated ECG analysis. However, a comprehensive evaluation of an end-to-end deep learning approach for ECG analysis across a wide variety of diagnostic classes has not been previously reported. Here, we develop a deep neural network (DNN) to classify 12 rhythm classes using 91,232 single-lead ECGs from 53,549 patients who used a single-lead ambulatory ECG monitoring device. When validated against an independent test dataset annotated by a consensus committee of board-certified practicing cardiologists, the DNN achieved an average area under the receiver operating characteristic curve (ROC) of 0.97. The average F1 score, which is the harmonic mean of the positive predictive value and sensitivity, for the DNN (0.837) exceeded that of average cardiologists (0.780). With specificity fixed at the average specificity achieved by cardiologists, the sensitivity of the DNN exceeded the average cardiologist sensitivity for all rhythm classes. These findings demonstrate that an end-to-end deep learning approach can classify a broad range of distinct arrhythmias from single-lead ECGs with high diagnostic performance similar to that of cardiologists. If confirmed in clinical settings, this approach could reduce the rate of misdiagnosed computerized ECG interpretations and improve the efficiency of expert human ECG interpretation by accurately triaging or prioritizing the most urgent conditions.","Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network. Computerized electrocardiogram (ECG) interpretation plays a critical role in the clinical ECG workflow(1). Widely available digital ECG data and the algorithmic paradigm of deep learning(2) present an opportunity to substantially improve the accuracy and scalability of automated ECG analysis. However, a comprehensive evaluation of an end-to-end deep learning approach for ECG analysis across a wide variety of diagnostic classes has not been previously reported. Here, we develop a deep neural network (DNN) to classify 12 rhythm classes using 91,232 single-lead ECGs from 53,549 patients who used a single-lead ambulatory ECG monitoring device. When validated against an independent test dataset annotated by a consensus committee of board-certified practicing cardiologists, the DNN achieved an average area under the receiver operating characteristic curve (ROC) of 0.97. The average F1 score, which is the harmonic mean of the positive predictive value and sensitivity, for the DNN (0.837) exceeded that of average cardiologists (0.780). With specificity fixed at the average specificity achieved by cardiologists, the sensitivity of the DNN exceeded the average cardiologist sensitivity for all rhythm classes. These findings demonstrate that an end-to-end deep learning approach can classify a broad range of distinct arrhythmias from single-lead ECGs with high diagnostic performance similar to that of cardiologists. If confirmed in clinical settings, this approach could reduce the rate of misdiagnosed computerized ECG interpretations and improve the efficiency of expert human ECG interpretation by accurately triaging or prioritizing the most urgent conditions."
0,"Protein tyrosine phosphatase 1B inhibitory activities of ursane-type triterpenes from Chinese raspberry, fruits of Rubus chingii","Protein tyrosine phosphatase 1B (PTP1B) has led to an intense interest in developing its inhibitors as anti-diabetes, anti-obesity and anti-cancer agents. The fruits of Rubus chingii (Chinese raspberry) were used as a kind of dietary traditional Chinese medicine. The methanolic extract of R. chingii fruits exhibited significant PTP1B inhibitory activity. Further bioactivity-guided fractionation resulted in the isolation of three PTP1B inhibitory ursane-type triterpenes: ursolic acid (1), 2-oxopomolic acid (2), and 2Î± 19Î±-dihydroxy-3-oxo-urs-12-en-28-oic acid (3). Kinetics analyses revealed that 1 was a non-competitive PTP1B inhibitor, and 2 and 3 were mixed type PTP1B inhibitors. Compounds 1â€“3 and structurally related triterpenes (4â€“8) were further analyzed the structure-activity relationship, and were evaluated the inhibitory selectivity against four homologous protein tyrosine phosphatases (TCPTP, VHR, SHP-1 and SHP-2). Molecular docking simulations were also carried out, and the result indicated that 1, 3-acetoxy-urs-12-ene-28-oic acid (5), and pomolic acid-3Î²-acetate (6) bound at the allosteric site including Î±3, Î±6, and Î±7 helix of PTP1B.","Protein tyrosine phosphatase 1B inhibitory activities of ursane-type triterpenes from Chinese raspberry, fruits of Rubus chingii. Protein tyrosine phosphatase 1B (PTP1B) has led to an intense interest in developing its inhibitors as anti-diabetes, anti-obesity and anti-cancer agents. The fruits of Rubus chingii (Chinese raspberry) were used as a kind of dietary traditional Chinese medicine. The methanolic extract of R. chingii fruits exhibited significant PTP1B inhibitory activity. Further bioactivity-guided fractionation resulted in the isolation of three PTP1B inhibitory ursane-type triterpenes: ursolic acid (1), 2-oxopomolic acid (2), and 2Î± 19Î±-dihydroxy-3-oxo-urs-12-en-28-oic acid (3). Kinetics analyses revealed that 1 was a non-competitive PTP1B inhibitor, and 2 and 3 were mixed type PTP1B inhibitors. Compounds 1â€“3 and structurally related triterpenes (4â€“8) were further analyzed the structure-activity relationship, and were evaluated the inhibitory selectivity against four homologous protein tyrosine phosphatases (TCPTP, VHR, SHP-1 and SHP-2). Molecular docking simulations were also carried out, and the result indicated that 1, 3-acetoxy-urs-12-ene-28-oic acid (5), and pomolic acid-3Î²-acetate (6) bound at the allosteric site including Î±3, Î±6, and Î±7 helix of PTP1B."
0,Retinal capillary perfusion: Spatial and temporal heterogeneity,,
0,"The ai-2/luxsquorum sensing system affects the growth characteristics, biofilm formation, and virulence of haemophilus parasuis","Haemophilus parasuis (H. parasuis) is a kind of opportunistic pathogen of the upper respiratory tract of piglets. Under certain circumstances, virulent strains can breach the mucosal barrier and enter the bloodstream, causing severe GlÃ¤sser's disease. Many virulence factors are found to be related to the pathogenicity of H. parasuis strain, but the pathogenic mechanism remains unclear. LuxS/AI-2, as a kind of very important quorum sensing system, affects the growth characteristics, biofilm formation, antibiotic production, virulence, and metabolism of different strains. In order to investigate the effect of luxS/AI-2 quorum sensing system on the virulence of H. parasuis, a deletion mutant strain (luxS) and complemented strain (C-luxS) were constructed and characterized. The results showed that the luxS gene participated in regulating and controlling stress resistance, biofilm formation and virulence. Compared with wild-Type strain, luxS strain decreased the production of AI-2 molecules and the tolerance toward oxidative stress and heat shock, and it reduced the abilities of autoagglutination, hemagglutination, and adherence, whereas it increased the abilities to form biofilm in vitro. In vivo experiments showed that luxS strain attenuated its virulence about 10-folds and significantly decreased its tissue burden of bacteria in mice, compared with the wild-Type strain. Taken together, the luxS/AI-2 quorum sensing system in H. parasuis not only plays an important role in growth and biofilm formation, but also affects the pathogenicity of H. parasuis.","The ai-2/luxsquorum sensing system affects the growth characteristics, biofilm formation, and virulence of haemophilus parasuis. Haemophilus parasuis (H. parasuis) is a kind of opportunistic pathogen of the upper respiratory tract of piglets. Under certain circumstances, virulent strains can breach the mucosal barrier and enter the bloodstream, causing severe GlÃ¤sser's disease. Many virulence factors are found to be related to the pathogenicity of H. parasuis strain, but the pathogenic mechanism remains unclear. LuxS/AI-2, as a kind of very important quorum sensing system, affects the growth characteristics, biofilm formation, antibiotic production, virulence, and metabolism of different strains. In order to investigate the effect of luxS/AI-2 quorum sensing system on the virulence of H. parasuis, a deletion mutant strain (luxS) and complemented strain (C-luxS) were constructed and characterized. The results showed that the luxS gene participated in regulating and controlling stress resistance, biofilm formation and virulence. Compared with wild-Type strain, luxS strain decreased the production of AI-2 molecules and the tolerance toward oxidative stress and heat shock, and it reduced the abilities of autoagglutination, hemagglutination, and adherence, whereas it increased the abilities to form biofilm in vitro. In vivo experiments showed that luxS strain attenuated its virulence about 10-folds and significantly decreased its tissue burden of bacteria in mice, compared with the wild-Type strain. Taken together, the luxS/AI-2 quorum sensing system in H. parasuis not only plays an important role in growth and biofilm formation, but also affects the pathogenicity of H. parasuis."
0,Editorial: The second international workshop on health natural language processing (HealthNLP 2019),,
0,The neural mechanisms and consequences of paternal caregiving,,
0,"Deep Learning in Medicine-Promise, Progress, and Challenges",,
0,YEARS Algorithm in Pregnant Patients With Suspected Pulmonary Embolism: October 2019 Annals of Emergency Medicine Journal Club,,
0,Randomized comparison of a reduced-visit prenatal care model enhanced with remote monitoring,,
0,A novel gene selection algorithm for cancer classification using microarray datasets,"Background: Microarray datasets are an important medical diagnostic tool as they represent the states of a cell at the molecular level. Available microarray datasets for classifying cancer types generally have a fairly small sample size compared to the large number of genes involved. This fact is known as a curse of dimensionality, which is a challenging problem. Gene selection is a promising approach that addresses this problem and plays an important role in the development of efficient cancer classification due to the fact that only a small number of genes are related to the classification problem. Gene selection addresses many problems in microarray datasets such as reducing the number of irrelevant and noisy genes, and selecting the most related genes to improve the classification results. Methods: An innovative Gene Selection Programming (GSP) method is proposed to select relevant genes for effective and efficient cancer classification. GSP is based on Gene Expression Programming (GEP) method with a new defined population initialization algorithm, a new fitness function definition, and improved mutation and recombination operators. Support Vector Machine (SVM) with a linear kernel serves as a classifier of the GSP. Results: Experimental results on ten microarray cancer datasets demonstrate that Gene Selection Programming (GSP) is effective and efficient in eliminating irrelevant and redundant genes/features from microarray datasets. The comprehensive evaluations and comparisons with other methods show that GSP gives a better compromise in terms of all three evaluation criteria, i.e., classification accuracy, number of selected genes, and computational cost. The gene set selected by GSP has shown its superior performances in cancer classification compared to those selected by the up-to-date representative gene selection methods. Conclusion: Gene subset selected by GSP can achieve a higher classification accuracy with less processing time.","A novel gene selection algorithm for cancer classification using microarray datasets. Background: Microarray datasets are an important medical diagnostic tool as they represent the states of a cell at the molecular level. Available microarray datasets for classifying cancer types generally have a fairly small sample size compared to the large number of genes involved. This fact is known as a curse of dimensionality, which is a challenging problem. Gene selection is a promising approach that addresses this problem and plays an important role in the development of efficient cancer classification due to the fact that only a small number of genes are related to the classification problem. Gene selection addresses many problems in microarray datasets such as reducing the number of irrelevant and noisy genes, and selecting the most related genes to improve the classification results. Methods: An innovative Gene Selection Programming (GSP) method is proposed to select relevant genes for effective and efficient cancer classification. GSP is based on Gene Expression Programming (GEP) method with a new defined population initialization algorithm, a new fitness function definition, and improved mutation and recombination operators. Support Vector Machine (SVM) with a linear kernel serves as a classifier of the GSP. Results: Experimental results on ten microarray cancer datasets demonstrate that Gene Selection Programming (GSP) is effective and efficient in eliminating irrelevant and redundant genes/features from microarray datasets. The comprehensive evaluations and comparisons with other methods show that GSP gives a better compromise in terms of all three evaluation criteria, i.e., classification accuracy, number of selected genes, and computational cost. The gene set selected by GSP has shown its superior performances in cancer classification compared to those selected by the up-to-date representative gene selection methods. Conclusion: Gene subset selected by GSP can achieve a higher classification accuracy with less processing time."
0,"Biochemical, machine learning and molecular approaches for the differential diagnosis of Mucopolysaccharidoses","This study was aimed to construct classification and regression tree (CART) model of glycosaminoglycans (GAGs) for the differential diagnosis of Mucopolysaccharidoses (MPS). Two-dimensional electrophoresis and liquid chromatographyâ€“tandem mass spectrometry (LCâ€“MS/MS) were used for the qualitative and quantitative analysis of GAGs. Specific enzyme assays and targeted gene sequencing were performed to confirm the diagnosis. Machine learning tools were used to develop CART model based on GAG profile. Qualitative and quantitative CART models showed 96.3% and 98.3% accuracy, respectively, in the differential diagnosis of MPS. The thresholds of different GAGs diagnostic of specific MPS types were established. In 60 MPS positive cases, 46 different mutations were identified in six specific genes. Among 31 different mutations identified in IDUA, nine were nonsense mutations and two were gross deletions while the remaining were missense mutations. In IDS gene, four missense, two frameshift, and one deletion were identified. In NAGLU gene, c.1693C > T and c.1914_1914insT were the most common mutations. Two ARSB, one case each of SGSH and GALNS mutations were observed. LCâ€“MS/MS-based GAG pattern showed higher accuracy in the differential diagnosis of MPS. The mutation spectrum of MPS, specifically in IDUA and IDS genes, is highly heterogeneous among the cases studied.","Biochemical, machine learning and molecular approaches for the differential diagnosis of Mucopolysaccharidoses. This study was aimed to construct classification and regression tree (CART) model of glycosaminoglycans (GAGs) for the differential diagnosis of Mucopolysaccharidoses (MPS). Two-dimensional electrophoresis and liquid chromatographyâ€“tandem mass spectrometry (LCâ€“MS/MS) were used for the qualitative and quantitative analysis of GAGs. Specific enzyme assays and targeted gene sequencing were performed to confirm the diagnosis. Machine learning tools were used to develop CART model based on GAG profile. Qualitative and quantitative CART models showed 96.3% and 98.3% accuracy, respectively, in the differential diagnosis of MPS. The thresholds of different GAGs diagnostic of specific MPS types were established. In 60 MPS positive cases, 46 different mutations were identified in six specific genes. Among 31 different mutations identified in IDUA, nine were nonsense mutations and two were gross deletions while the remaining were missense mutations. In IDS gene, four missense, two frameshift, and one deletion were identified. In NAGLU gene, c.1693C > T and c.1914_1914insT were the most common mutations. Two ARSB, one case each of SGSH and GALNS mutations were observed. LCâ€“MS/MS-based GAG pattern showed higher accuracy in the differential diagnosis of MPS. The mutation spectrum of MPS, specifically in IDUA and IDS genes, is highly heterogeneous among the cases studied."
0,CONFIGURE: A pipeline for identifying context specific regulatory modules from gene expression data and its application to breast cancer,"Background: Gene expression data is widely used for identifying subtypes of diseases such as cancer. Differentially expressed gene analysis and gene set enrichment analysis are widely used for identifying biological mechanisms at the gene level and gene set level, respectively. However, the results of differentially expressed gene analysis are difficult to interpret and gene set enrichment analysis does not consider the interactions among genes in a gene set. Results: We present CONFIGURE, a pipeline that identifies context specific regulatory modules from gene expression data. First, CONFIGURE takes gene expression data and context label information as inputs and constructs regulatory modules. Then, CONFIGURE makes a regulatory module enrichment score (RMES) matrix of enrichment scores of the regulatory modules on samples using the single-sample GSEA method. CONFIGURE calculates the importance scores of the regulatory modules on each context to rank the regulatory modules. We evaluated CONFIGURE on the Cancer Genome Atlas (TCGA) breast cancer RNA-seq dataset to determine whether it can produce biologically meaningful regulatory modules for breast cancer subtypes. We first evaluated whether RMESs are useful for differentiating breast cancer subtypes using a multi-class classifier and one-vs-rest binary SVM classifiers. The multi-class and one-vs-rest binary classifiers were trained using the RMESs as features and outperformed baseline classifiers. Furthermore, we conducted literature surveys on the basal-like type specific regulatory modules obtained by CONFIGURE and showed that highly ranked modules were associated with the phenotypes of basal-like type breast cancers. Conclusions: We showed that enrichment scores of regulatory modules are useful for differentiating breast cancer subtypes and validated the basal-like type specific regulatory modules by literature surveys. In doing so, we found regulatory module candidates that have not been reported in previous literature. This demonstrates that CONFIGURE can be used to predict novel regulatory markers which can be validated by downstream wet lab experiments. We validated CONFIGURE on the breast cancer RNA-seq dataset in this work but CONFIGURE can be applied to any gene expression dataset containing context information.","CONFIGURE: A pipeline for identifying context specific regulatory modules from gene expression data and its application to breast cancer. Background: Gene expression data is widely used for identifying subtypes of diseases such as cancer. Differentially expressed gene analysis and gene set enrichment analysis are widely used for identifying biological mechanisms at the gene level and gene set level, respectively. However, the results of differentially expressed gene analysis are difficult to interpret and gene set enrichment analysis does not consider the interactions among genes in a gene set. Results: We present CONFIGURE, a pipeline that identifies context specific regulatory modules from gene expression data. First, CONFIGURE takes gene expression data and context label information as inputs and constructs regulatory modules. Then, CONFIGURE makes a regulatory module enrichment score (RMES) matrix of enrichment scores of the regulatory modules on samples using the single-sample GSEA method. CONFIGURE calculates the importance scores of the regulatory modules on each context to rank the regulatory modules. We evaluated CONFIGURE on the Cancer Genome Atlas (TCGA) breast cancer RNA-seq dataset to determine whether it can produce biologically meaningful regulatory modules for breast cancer subtypes. We first evaluated whether RMESs are useful for differentiating breast cancer subtypes using a multi-class classifier and one-vs-rest binary SVM classifiers. The multi-class and one-vs-rest binary classifiers were trained using the RMESs as features and outperformed baseline classifiers. Furthermore, we conducted literature surveys on the basal-like type specific regulatory modules obtained by CONFIGURE and showed that highly ranked modules were associated with the phenotypes of basal-like type breast cancers. Conclusions: We showed that enrichment scores of regulatory modules are useful for differentiating breast cancer subtypes and validated the basal-like type specific regulatory modules by literature surveys. In doing so, we found regulatory module candidates that have not been reported in previous literature. This demonstrates that CONFIGURE can be used to predict novel regulatory markers which can be validated by downstream wet lab experiments. We validated CONFIGURE on the breast cancer RNA-seq dataset in this work but CONFIGURE can be applied to any gene expression dataset containing context information."
0,Proteasome-associated cysteine deubiquitinases are molecular targets of environmental optical brightener compounds,"The levels of organic pollutants, such as optical brightener (OB) compounds, in the global environment have been increasing in recent years. The toxicological effects and signal transduction systems associated with OB toxicity have not been thoroughly studied. The ubiquitin-proteasome system (UPS) plays a crucial role in regulating multiple essential cellular processes, and proteasome-associated cysteine deubiquitinases (DUBs), ubiquitin C-terminal hydrolase L5 (UCHL5) and USP14, are two major regulators for (de)ubiquitination and stability of many important target proteins. Therefore, potential inhibition of UCHL5 and USP14 activities by some environmental chemicals might cause in vivo toxicity. In the current study we hypothesize that electrophilic OB compounds, such as 4,4â€²-diamino-2,2â€²-stilbenedisulfonic acid(DAST), fluorescent brightener 28 (FB-28) and FB-71, can interact with the catalytic triads (CYS, HIS, and ASP) of UCHL5 and USP14 and inhibit their enzymatic activities, leading to cell growth suppression. This hypothesis is supported by our findings presented in this study. Results from in silico computational docking and ubiquitin vinyl sulfone assay confirmed the UCHL5/USP14-inhibitory activities of these OB compounds that have potencies in an order of: FB-71 > FB-28 > DAST. Furthermore, inhibition of these two proteasomal DUBs by OBs resulted in cell growth inhibition and apoptosis induction in two human breast cancer cell models. In addition, we found that OB-mediated DUB inhibition triggers a feedback reaction in which expression of UCHL5 and USP14 proteins is increased to compromise the suppressed activities. Our study suggests that these commonly used OB compounds may target and inhibit proteasomal cysteine DUBs, which should contribute to their toxicological effects in vivo.","Proteasome-associated cysteine deubiquitinases are molecular targets of environmental optical brightener compounds. The levels of organic pollutants, such as optical brightener (OB) compounds, in the global environment have been increasing in recent years. The toxicological effects and signal transduction systems associated with OB toxicity have not been thoroughly studied. The ubiquitin-proteasome system (UPS) plays a crucial role in regulating multiple essential cellular processes, and proteasome-associated cysteine deubiquitinases (DUBs), ubiquitin C-terminal hydrolase L5 (UCHL5) and USP14, are two major regulators for (de)ubiquitination and stability of many important target proteins. Therefore, potential inhibition of UCHL5 and USP14 activities by some environmental chemicals might cause in vivo toxicity. In the current study we hypothesize that electrophilic OB compounds, such as 4,4â€²-diamino-2,2â€²-stilbenedisulfonic acid(DAST), fluorescent brightener 28 (FB-28) and FB-71, can interact with the catalytic triads (CYS, HIS, and ASP) of UCHL5 and USP14 and inhibit their enzymatic activities, leading to cell growth suppression. This hypothesis is supported by our findings presented in this study. Results from in silico computational docking and ubiquitin vinyl sulfone assay confirmed the UCHL5/USP14-inhibitory activities of these OB compounds that have potencies in an order of: FB-71 > FB-28 > DAST. Furthermore, inhibition of these two proteasomal DUBs by OBs resulted in cell growth inhibition and apoptosis induction in two human breast cancer cell models. In addition, we found that OB-mediated DUB inhibition triggers a feedback reaction in which expression of UCHL5 and USP14 proteins is increased to compromise the suppressed activities. Our study suggests that these commonly used OB compounds may target and inhibit proteasomal cysteine DUBs, which should contribute to their toxicological effects in vivo."
0,Pan-cancer Convergence to a Small-Cell Neuroendocrine Phenotype that Shares Susceptibilities with Hematological Malignancies,"Small-cell neuroendocrine cancers (SCNCs) are an aggressive cancer subtype. Transdifferentiation toward an SCN phenotype has been reported as a resistance route in response to targeted therapies. Here, we identified a convergence to an SCN state that is widespread across epithelial cancers and is associated with poor prognosis. More broadly, non-SCN metastases have higher expression of SCN-associated transcription factors than non-SCN primary tumors. Drug sensitivity and gene dependency screens demonstrate that these convergent SCNCs have shared vulnerabilities. These common vulnerabilities are found across unannotated SCN-like epithelial cases, small-round-blue cell tumors, and unexpectedly in hematological malignancies. The SCN convergent phenotype and common sensitivity profiles with hematological cancers can guide treatment options beyond tissue-specific targeted therapies. Balanis, Sheu, et al. identify a small-cell neuroendocrine (SCN) state across various epithelial cancer types that shares genome-wide expression, methylation, and copy-number alteration patterns and associates with poor prognosis. SCN cancers have common vulnerabilities that, unexpectedly, are shared with blood cancers.","Pan-cancer Convergence to a Small-Cell Neuroendocrine Phenotype that Shares Susceptibilities with Hematological Malignancies. Small-cell neuroendocrine cancers (SCNCs) are an aggressive cancer subtype. Transdifferentiation toward an SCN phenotype has been reported as a resistance route in response to targeted therapies. Here, we identified a convergence to an SCN state that is widespread across epithelial cancers and is associated with poor prognosis. More broadly, non-SCN metastases have higher expression of SCN-associated transcription factors than non-SCN primary tumors. Drug sensitivity and gene dependency screens demonstrate that these convergent SCNCs have shared vulnerabilities. These common vulnerabilities are found across unannotated SCN-like epithelial cases, small-round-blue cell tumors, and unexpectedly in hematological malignancies. The SCN convergent phenotype and common sensitivity profiles with hematological cancers can guide treatment options beyond tissue-specific targeted therapies. Balanis, Sheu, et al. identify a small-cell neuroendocrine (SCN) state across various epithelial cancer types that shares genome-wide expression, methylation, and copy-number alteration patterns and associates with poor prognosis. SCN cancers have common vulnerabilities that, unexpectedly, are shared with blood cancers."
0,CARD9(+) microglia promote antifungal immunity via IL-1 beta- and CXCL1-mediated neutrophil recruitment,,
0,Nuclear stress bodies: Interaction of its components in oncogenic regulation,"Oncogenesis involves continuous genetic alterations that lead to compromised cellular integrity and immortal cell fate. The cells remain under excessive stress due to endo- and exogenous influences. Human Satellite III long noncoding RNA (SatIII lncRNA) is a key regulator of the global cellular stress response, although its function is poorly explained in cancers. The principal regulator of cancer meshwork is tumor protein p53, which if altered may result in chemoresistance. The heat shock factor 1 (HSF1) being a common molecule between the oncogenic control and global cellular stress acts as an oncogene as well as transcribes SatIII upon heat shock. This prompted us to determine the structure of SatIII RNA and establish the association between SatIII-HSF1-p53. We determined the most stable structure of SatIII RNA with the least energy of âˆ’ 115.7 kcal/mol. Also, we observed a possible interaction of p53 with SatIII and HSF1 using support vector machine (SVM) algorithm for predicting RNA-protein interaction (RPI). Further, we employ the STRING database to understand if p53 is an interacting component of the nuclear stress bodies (nSBs). A precise inference was drawn from molecular docking which confirmed the interaction of SatIII-HSF1-p53, where a mutated p53 resulted in an altered DNA-binding property with the SatIII molecule. This study being first of its kind infers p53 to be a possible integral component of the nSBs, which may regulate cellular stress response during cancer progression in the presence of HSF1 and SatIII. An extended research on the regulations of SatIII and p53 may open new avenues in the field of apoptosis in cancer and the early approach of molecular targeting.","Nuclear stress bodies: Interaction of its components in oncogenic regulation. Oncogenesis involves continuous genetic alterations that lead to compromised cellular integrity and immortal cell fate. The cells remain under excessive stress due to endo- and exogenous influences. Human Satellite III long noncoding RNA (SatIII lncRNA) is a key regulator of the global cellular stress response, although its function is poorly explained in cancers. The principal regulator of cancer meshwork is tumor protein p53, which if altered may result in chemoresistance. The heat shock factor 1 (HSF1) being a common molecule between the oncogenic control and global cellular stress acts as an oncogene as well as transcribes SatIII upon heat shock. This prompted us to determine the structure of SatIII RNA and establish the association between SatIII-HSF1-p53. We determined the most stable structure of SatIII RNA with the least energy of âˆ’ 115.7 kcal/mol. Also, we observed a possible interaction of p53 with SatIII and HSF1 using support vector machine (SVM) algorithm for predicting RNA-protein interaction (RPI). Further, we employ the STRING database to understand if p53 is an interacting component of the nuclear stress bodies (nSBs). A precise inference was drawn from molecular docking which confirmed the interaction of SatIII-HSF1-p53, where a mutated p53 resulted in an altered DNA-binding property with the SatIII molecule. This study being first of its kind infers p53 to be a possible integral component of the nSBs, which may regulate cellular stress response during cancer progression in the presence of HSF1 and SatIII. An extended research on the regulations of SatIII and p53 may open new avenues in the field of apoptosis in cancer and the early approach of molecular targeting."
0,Taxifolin prevents postprandial hyperglycemia by regulating the activity of Î±-amylase: Evidence from an in vivo and in silico studies,"There has been a dramatic increase in the prevalence of diabetes mellitus (DM) and its associated complications globally. The postprandial stage of DM involves prompt elevation in the levels of blood glucose and Î±-amylase, a carbohydrate-metabolizing enzyme is mainly involved in the regulation of postprandial hyperglycemia. This study was designed to assess the ability of a well-known flavonoid, taxifolin (TFN), against postprandial hyperglycemia and its inhibitory effects on Î±-amylase activity through the assessment of therapeutic potentials of TFN in an alloxan-induced diabetic animal model. The binding potential TFN with an Î±-amylase receptor was also investigated through molecular dynamics (MD) simulation and docking of to compare the binding affinities and energies of TFN and standard drug acarbose (ACB) with target enzyme. TFN significantly improved the postprandial hyperglycemia, lipid profile, and serum levels of Î±-amylase, lipase, and C-reactive protein in a dose-dependent manner when compared with that of either DM-induced and ACB-treated alloxan-induced diabetic rats. Moreover, TFN also enhanced the anti-oxidant status and normal functioning of the liver in alloxan-induced diabetic rats more efficiently as compared to that of ACB-treated alloxan-induced diabetic rats. Therapeutic potentials of TFN were also verified by MD simulation and docking results, which exhibited that the binding energy and affinity of TFN to bind with receptor was significantly higher as compared to that of ACB. Hence, the results of this study signify that TFN might be a potent inhibitor of Î±-amylase that has the potential to regulate the postprandial hyperglycemia along with its anti-inflammatory and anti-oxidant properties during the treatment of DM.","Taxifolin prevents postprandial hyperglycemia by regulating the activity of Î±-amylase: Evidence from an in vivo and in silico studies. There has been a dramatic increase in the prevalence of diabetes mellitus (DM) and its associated complications globally. The postprandial stage of DM involves prompt elevation in the levels of blood glucose and Î±-amylase, a carbohydrate-metabolizing enzyme is mainly involved in the regulation of postprandial hyperglycemia. This study was designed to assess the ability of a well-known flavonoid, taxifolin (TFN), against postprandial hyperglycemia and its inhibitory effects on Î±-amylase activity through the assessment of therapeutic potentials of TFN in an alloxan-induced diabetic animal model. The binding potential TFN with an Î±-amylase receptor was also investigated through molecular dynamics (MD) simulation and docking of to compare the binding affinities and energies of TFN and standard drug acarbose (ACB) with target enzyme. TFN significantly improved the postprandial hyperglycemia, lipid profile, and serum levels of Î±-amylase, lipase, and C-reactive protein in a dose-dependent manner when compared with that of either DM-induced and ACB-treated alloxan-induced diabetic rats. Moreover, TFN also enhanced the anti-oxidant status and normal functioning of the liver in alloxan-induced diabetic rats more efficiently as compared to that of ACB-treated alloxan-induced diabetic rats. Therapeutic potentials of TFN were also verified by MD simulation and docking results, which exhibited that the binding energy and affinity of TFN to bind with receptor was significantly higher as compared to that of ACB. Hence, the results of this study signify that TFN might be a potent inhibitor of Î±-amylase that has the potential to regulate the postprandial hyperglycemia along with its anti-inflammatory and anti-oxidant properties during the treatment of DM."
0,Molecular inhibitory mechanism study on the potent inhibitor brigatinib against four crizotinib-resistant ALK mutations,"As a potent and selective drug, brigatinib exhibits high efficacy against wild-type and mutant anaplastic lymphoma kinase (ALK) proteins to treat nonâ€“small cell lung cancer. In this work, the mechanisms of brigatinib binding to wild type and four mutant ALKs were investigated to gain insight into the dynamic energetic and structural information with respect to the design of novel inhibitors. Comparison between ALK-brigatinib and ALK-crizotinib suggests that the scaffold of brigatinib is well anchored to the residue Met1199 of hinge region by two hydrogen bonds, and the residue Lys1150 has the strong electrostatic interaction with the dimethylphosphine oxide moiety in brigatinib. These ALK mutations have significant influences on the flexibility of P-loop region and DFG sequences, but do not impair the hydrogen bonds between brigatinib and the residue Met1199 of hinge region. And mutations (L1196M, G1269A, F1174L, and R1275Q) induce diverse conformational changes of brigatinib and the obvious energy variation of residues Glu1167, Arg1209, Asp1270, and Asp1203. Together, the detailed explanation of mechanisms of those mutations with brigatinib further provide several guidelines for the development of more effective ALK inhibitors.","Molecular inhibitory mechanism study on the potent inhibitor brigatinib against four crizotinib-resistant ALK mutations. As a potent and selective drug, brigatinib exhibits high efficacy against wild-type and mutant anaplastic lymphoma kinase (ALK) proteins to treat nonâ€“small cell lung cancer. In this work, the mechanisms of brigatinib binding to wild type and four mutant ALKs were investigated to gain insight into the dynamic energetic and structural information with respect to the design of novel inhibitors. Comparison between ALK-brigatinib and ALK-crizotinib suggests that the scaffold of brigatinib is well anchored to the residue Met1199 of hinge region by two hydrogen bonds, and the residue Lys1150 has the strong electrostatic interaction with the dimethylphosphine oxide moiety in brigatinib. These ALK mutations have significant influences on the flexibility of P-loop region and DFG sequences, but do not impair the hydrogen bonds between brigatinib and the residue Met1199 of hinge region. And mutations (L1196M, G1269A, F1174L, and R1275Q) induce diverse conformational changes of brigatinib and the obvious energy variation of residues Glu1167, Arg1209, Asp1270, and Asp1203. Together, the detailed explanation of mechanisms of those mutations with brigatinib further provide several guidelines for the development of more effective ALK inhibitors."
0,Recovery of 3D rib motion from dynamic chest radiography and CT data using local contrast normalization and articular motion model,"Dynamic chest radiography (2D x-ray video) is a low-dose and cost-effective functional imaging method with high temporal resolution. While the analysis of rib-cage motion has been shown to be effective for evaluating respiratory function, it has been limited to 2D. We aim at 3D rib-motion analysis for high temporal resolution while keeping the radiation dose at a level comparable to conventional examination. To achieve this, we developed a method for automatically recovering 3D rib motion based on 2D-3D registration of x-ray video and single-time-phase computed tomography. We introduce the following two novel components into the conventional intensity-based 2D-3D registration pipeline: (1) a rib-motion model based on a uniaxial joint to constrain the search space and (2) local contrast normalization (LCN) as a pre-process of x-ray video to improve the cost function of the optimization parameters, which is often called the landscape. The effects of each component on the registration results were quantitatively evaluated through experiments using simulated images and real patients' x-ray videos obtained in a clinical setting. The rotation-angle error of the rib and the mean projection contour distance (mPCD) were used as the error metrics. The simulation experiments indicate that the proposed uniaxial joint model improved registration accuracy. By searching the rotation axis along with the rotation angle of the ribs, the rotation-angle error and mPCD significantly decreased from 2.246+/-1.839 degrees and 1.148+/-0.743 mm to 1.495+/-0.993 degrees and 0.742+/-0.281 mm, compared to simply applying De Troyer's model. The real-image experiments with eight patients demonstrated that LCN improved the cost function space; thus, robustness in optimization resulting in an average mPCD of 1.255+/-0.615 mm. We demonstrated that an anatomical-knowledge based constraint and an intensity normalization, LCN, significantly improved robustness and accuracy in rib-motion reconstruction using chest x-ray video.","Recovery of 3D rib motion from dynamic chest radiography and CT data using local contrast normalization and articular motion model. Dynamic chest radiography (2D x-ray video) is a low-dose and cost-effective functional imaging method with high temporal resolution. While the analysis of rib-cage motion has been shown to be effective for evaluating respiratory function, it has been limited to 2D. We aim at 3D rib-motion analysis for high temporal resolution while keeping the radiation dose at a level comparable to conventional examination. To achieve this, we developed a method for automatically recovering 3D rib motion based on 2D-3D registration of x-ray video and single-time-phase computed tomography. We introduce the following two novel components into the conventional intensity-based 2D-3D registration pipeline: (1) a rib-motion model based on a uniaxial joint to constrain the search space and (2) local contrast normalization (LCN) as a pre-process of x-ray video to improve the cost function of the optimization parameters, which is often called the landscape. The effects of each component on the registration results were quantitatively evaluated through experiments using simulated images and real patients' x-ray videos obtained in a clinical setting. The rotation-angle error of the rib and the mean projection contour distance (mPCD) were used as the error metrics. The simulation experiments indicate that the proposed uniaxial joint model improved registration accuracy. By searching the rotation axis along with the rotation angle of the ribs, the rotation-angle error and mPCD significantly decreased from 2.246+/-1.839 degrees and 1.148+/-0.743 mm to 1.495+/-0.993 degrees and 0.742+/-0.281 mm, compared to simply applying De Troyer's model. The real-image experiments with eight patients demonstrated that LCN improved the cost function space; thus, robustness in optimization resulting in an average mPCD of 1.255+/-0.615 mm. We demonstrated that an anatomical-knowledge based constraint and an intensity normalization, LCN, significantly improved robustness and accuracy in rib-motion reconstruction using chest x-ray video."
0,How to Approach a Patient With Refractory or Recurrent Benign Esophageal Stricture,,
0,Bioprospection of anti-inflammatory phytochemicals suggests rutaecarpine and quinine as promising 15-lipoxygenase inhibitors,"15-Lipoxygenase (15-LOX) belongs to the family of nonheme iron containing enzymes that catalyzes the peroxidation of polyunsaturated fatty acids (PUFAs) to generate eicosanoids that play an important role in signaling pathways. The role of 15-LOX has been demonstrated in atherosclerosis as well as other inflammatory diseases. In the present study, drug-like compounds were first screened from a set of anti-inflammatory phytochemicals based on Lipinski's rule of five (ROF) and in silico toxicity filters. Two lead compounds-quinine (QUIN) and rutaecarpine (RUT) were shortlisted by analyzing molecular interactions and binding energies of the filtered compounds with the target using molecular docking. Molecular dynamics simulation studies indicate stable trajectories of apo_15-LOX and docked complexes (15-LOX_QUIN and 15-LOX_RUT). In vitro 15-LOX inhibition studies shows that both QUIN and RUT have lower inhibitory concentration (IC50) value than the control (quercetin). Both QUIN and RUT exhibit moderate antioxidant activities. The cell viability study of these compounds suggests no significant toxicity in HEK-293 cell lines. Further, QUIN and RUT both did not show any inhibition against selected Gram-positive and Gram-negative bacterial species. Thus, based on our present findings, rutaecarpine and quinine may be suggested as promising 15-LOX inhibitor for the prevention of the atherosclerosis development.","Bioprospection of anti-inflammatory phytochemicals suggests rutaecarpine and quinine as promising 15-lipoxygenase inhibitors. 15-Lipoxygenase (15-LOX) belongs to the family of nonheme iron containing enzymes that catalyzes the peroxidation of polyunsaturated fatty acids (PUFAs) to generate eicosanoids that play an important role in signaling pathways. The role of 15-LOX has been demonstrated in atherosclerosis as well as other inflammatory diseases. In the present study, drug-like compounds were first screened from a set of anti-inflammatory phytochemicals based on Lipinski's rule of five (ROF) and in silico toxicity filters. Two lead compounds-quinine (QUIN) and rutaecarpine (RUT) were shortlisted by analyzing molecular interactions and binding energies of the filtered compounds with the target using molecular docking. Molecular dynamics simulation studies indicate stable trajectories of apo_15-LOX and docked complexes (15-LOX_QUIN and 15-LOX_RUT). In vitro 15-LOX inhibition studies shows that both QUIN and RUT have lower inhibitory concentration (IC50) value than the control (quercetin). Both QUIN and RUT exhibit moderate antioxidant activities. The cell viability study of these compounds suggests no significant toxicity in HEK-293 cell lines. Further, QUIN and RUT both did not show any inhibition against selected Gram-positive and Gram-negative bacterial species. Thus, based on our present findings, rutaecarpine and quinine may be suggested as promising 15-LOX inhibitor for the prevention of the atherosclerosis development."
0,Tumor-associated Macrophage-derived Interleukin-23 Interlinks Kidney Cancer Glutamine Addiction with Immune Evasion,"BACKGROUND: Glutamine addiction is a hallmark of clear cell renal cell carcinoma (ccRCC); yet whether glutamine metabolism impacts local immune surveillance is unclear. This knowledge may yield novel immunotherapeutic opportunities. OBJECTIVE: To seek a potential therapeutic target in glutamine-addicted ccRCC. DESIGN, SETTING, AND PARTICIPANTS: Tumors from ccRCC patients from a Shanghai cohort and ccRCC tumor data from The Cancer Genome Atlas (TCGA) cohort were analyzed. In vivo and in vitro studies were conducted with fresh human ccRCC tumors and murine tumor cells. OUTCOME MEASUREMENTS AND STATISTICAL ANALYSIS: Immune cell numbers and functions were analyzed by flow cytometry. Glutamine and cytokine concentrations were determined. Survival was compared between different subpopulations of patients using Kaplan-Meier and Cox regression analyses. RESULTS AND LIMITATIONS: We found that in ccRCC, high interleukin (IL)-23 expression was significantly associated with poor survival in both TCGA (overall survival [OS] hazard ratio [HR]=2.04, cancer-specific survival [CSS] HR=2.95; all p<0.001) and Shanghai (OS HR=2.07, CSS HR=3.92; all p<0.001) cohorts. IL-23 blockade prolongs the survival of tumor-bearing mice, promotes T-cell cytotoxicity in in vitro cultures of human ccRCC tumors, and augments the therapeutic benefits of anti-PD-1 antibodies. Mechanistically, glutamine consumption by ccRCC tumor cells results in the local deprivation of extracellular glutamine, which induces IL-23 secretion by tumor-infiltrating macrophages via the activation of hypoxia-inducible factor 1alpha (HIF1alpha). IL-23 activates regulatory T-cell proliferation and promotes IL-10 and transforming growth factor beta expression, thereby suppressing tumor cell killing by cytotoxic lymphocytes. The positive correlations between glutamine metabolism, IL-23 levels, and Treg responses are confirmed in both TCGA cohort and tumors from Shanghai ccRCC patients. Study limitations include the unclear impacts of glutamine deprivation and IL-23 on other immune cells. CONCLUSIONS: Macrophage-secreted IL-23 enhanced Treg functions in glutamine-addicted tumors; thus, IL-23 is a promising target for immunotherapy in ccRCC. PATIENT SUMMARY: In this study, we analyzed the immune components in glutamine-addicted clear cell renal cell carcinoma (ccRCC) tumors from two patient cohorts and conducted both in vitro and in vivo studies. We found that ccRCC tumor cell-intrinsic glutamine metabolism orchestrates immune evasion via interleukin (IL)-23, and IL-23-high patients had significantly poorer survival than IL-23-low patients. IL-23 should thus be considered a therapeutic target in ccRCC, either alone or in combination with immune checkpoint inhibitors.","Tumor-associated Macrophage-derived Interleukin-23 Interlinks Kidney Cancer Glutamine Addiction with Immune Evasion. BACKGROUND: Glutamine addiction is a hallmark of clear cell renal cell carcinoma (ccRCC); yet whether glutamine metabolism impacts local immune surveillance is unclear. This knowledge may yield novel immunotherapeutic opportunities. OBJECTIVE: To seek a potential therapeutic target in glutamine-addicted ccRCC. DESIGN, SETTING, AND PARTICIPANTS: Tumors from ccRCC patients from a Shanghai cohort and ccRCC tumor data from The Cancer Genome Atlas (TCGA) cohort were analyzed. In vivo and in vitro studies were conducted with fresh human ccRCC tumors and murine tumor cells. OUTCOME MEASUREMENTS AND STATISTICAL ANALYSIS: Immune cell numbers and functions were analyzed by flow cytometry. Glutamine and cytokine concentrations were determined. Survival was compared between different subpopulations of patients using Kaplan-Meier and Cox regression analyses. RESULTS AND LIMITATIONS: We found that in ccRCC, high interleukin (IL)-23 expression was significantly associated with poor survival in both TCGA (overall survival [OS] hazard ratio [HR]=2.04, cancer-specific survival [CSS] HR=2.95; all p<0.001) and Shanghai (OS HR=2.07, CSS HR=3.92; all p<0.001) cohorts. IL-23 blockade prolongs the survival of tumor-bearing mice, promotes T-cell cytotoxicity in in vitro cultures of human ccRCC tumors, and augments the therapeutic benefits of anti-PD-1 antibodies. Mechanistically, glutamine consumption by ccRCC tumor cells results in the local deprivation of extracellular glutamine, which induces IL-23 secretion by tumor-infiltrating macrophages via the activation of hypoxia-inducible factor 1alpha (HIF1alpha). IL-23 activates regulatory T-cell proliferation and promotes IL-10 and transforming growth factor beta expression, thereby suppressing tumor cell killing by cytotoxic lymphocytes. The positive correlations between glutamine metabolism, IL-23 levels, and Treg responses are confirmed in both TCGA cohort and tumors from Shanghai ccRCC patients. Study limitations include the unclear impacts of glutamine deprivation and IL-23 on other immune cells. CONCLUSIONS: Macrophage-secreted IL-23 enhanced Treg functions in glutamine-addicted tumors; thus, IL-23 is a promising target for immunotherapy in ccRCC. PATIENT SUMMARY: In this study, we analyzed the immune components in glutamine-addicted clear cell renal cell carcinoma (ccRCC) tumors from two patient cohorts and conducted both in vitro and in vivo studies. We found that ccRCC tumor cell-intrinsic glutamine metabolism orchestrates immune evasion via interleukin (IL)-23, and IL-23-high patients had significantly poorer survival than IL-23-low patients. IL-23 should thus be considered a therapeutic target in ccRCC, either alone or in combination with immune checkpoint inhibitors."
0,Association of Mental Health Disorders with Health Care Utilization and Costs among Adults with Chronic Disease,"Importance: A population-based study using validated algorithms to estimate the costs of treating people with chronic disease with and without mental health disorders is needed. Objective: To determine the association of mental health disorders with health care costs among people with chronic diseases. Design, Setting, and Participants: This population-based cohort study in the Canadian province of Alberta collected data from April 1, 2012, to March 31, 2015, among 991445 adults 18 years and older with a chronic disease (ie, asthma, congestive heart failure, myocardial infarction, diabetes, epilepsy, hypertension, chronic pulmonary disease, or chronic kidney disease). Data analysis was conducted from October 2017 to August 2018. Exposures: Mental health disorder (ie, depression, schizophrenia, alcohol use disorder, or drug use disorder). Main Outcomes and Measures: Resource use, mean total unadjusted and adjusted 3-year health care costs, and mean total unadjusted 3-year costs for hospitalization and emergency department visits for ambulatory care-sensitive conditions. Results: Among 991445 participants, 156296 (15.8%) had a mental health disorder. Those with no mental health disorder were older (mean [SD] age, 58.1 [17.6] years vs 55.4 [17.0] years; P <.001) and less likely to be women (50.4% [95% CI, 50.3%-50.5%] vs 57.7% [95% CI, 57.4%-58.0%]; P <.001) than those with mental health disorders. For those with a mental health disorder, mean total 3-year adjusted costs were $38250 (95% CI, $36476-$39935), and for those without a mental health disorder, mean total 3-year adjusted costs were $22280 (95% CI, $21780-$22760). Having a mental health disorder was associated with significantly higher resource use, including hospitalization and emergency department visit rates, length of stay, and hospitalization for ambulatory care-sensitive conditions. Higher resource use by patients with mental health disorders was not associated with health care presentations owing to chronic diseases compared with patients without a mental health disorder (chronic disease hospitalization rate per 1000 patient days, 0.11 [95% CI, 0.11-0.12] vs 0.06 [95% CI, 0.06-0.06]; P <.001; overall hospitalization rate per 1000 patient days, 0.88 [95% CI, 0.87-0.88] vs 0.43 [95% CI, 0.43-0.43]; P <.001). Conclusions and Relevance: This study suggests that mental health disorders are associated with substantially higher resource utilization and health care costs among patients with chronic diseases. These findings have clinical and health policy implications..","Association of Mental Health Disorders with Health Care Utilization and Costs among Adults with Chronic Disease. Importance: A population-based study using validated algorithms to estimate the costs of treating people with chronic disease with and without mental health disorders is needed. Objective: To determine the association of mental health disorders with health care costs among people with chronic diseases. Design, Setting, and Participants: This population-based cohort study in the Canadian province of Alberta collected data from April 1, 2012, to March 31, 2015, among 991445 adults 18 years and older with a chronic disease (ie, asthma, congestive heart failure, myocardial infarction, diabetes, epilepsy, hypertension, chronic pulmonary disease, or chronic kidney disease). Data analysis was conducted from October 2017 to August 2018. Exposures: Mental health disorder (ie, depression, schizophrenia, alcohol use disorder, or drug use disorder). Main Outcomes and Measures: Resource use, mean total unadjusted and adjusted 3-year health care costs, and mean total unadjusted 3-year costs for hospitalization and emergency department visits for ambulatory care-sensitive conditions. Results: Among 991445 participants, 156296 (15.8%) had a mental health disorder. Those with no mental health disorder were older (mean [SD] age, 58.1 [17.6] years vs 55.4 [17.0] years; P <.001) and less likely to be women (50.4% [95% CI, 50.3%-50.5%] vs 57.7% [95% CI, 57.4%-58.0%]; P <.001) than those with mental health disorders. For those with a mental health disorder, mean total 3-year adjusted costs were $38250 (95% CI, $36476-$39935), and for those without a mental health disorder, mean total 3-year adjusted costs were $22280 (95% CI, $21780-$22760). Having a mental health disorder was associated with significantly higher resource use, including hospitalization and emergency department visit rates, length of stay, and hospitalization for ambulatory care-sensitive conditions. Higher resource use by patients with mental health disorders was not associated with health care presentations owing to chronic diseases compared with patients without a mental health disorder (chronic disease hospitalization rate per 1000 patient days, 0.11 [95% CI, 0.11-0.12] vs 0.06 [95% CI, 0.06-0.06]; P <.001; overall hospitalization rate per 1000 patient days, 0.88 [95% CI, 0.87-0.88] vs 0.43 [95% CI, 0.43-0.43]; P <.001). Conclusions and Relevance: This study suggests that mental health disorders are associated with substantially higher resource utilization and health care costs among patients with chronic diseases. These findings have clinical and health policy implications.."
0,Author Correction: Reporting guidelines for clinical trials evaluating artificial intelligence interventions are needed,An amendment to this paper has been published and can be accessed via a link at the top of the paper.,Author Correction: Reporting guidelines for clinical trials evaluating artificial intelligence interventions are needed. An amendment to this paper has been published and can be accessed via a link at the top of the paper.
0,Proof-of-concept study: Homomorphically encrypted data can support real-time learning in personalized cancer medicine,"BACKGROUND: The successful introduction of homomorphic encryption (HE) in clinical research holds promise for improving acceptance of data-sharing protocols, increasing sample sizes, and accelerating learning from real-world data (RWD). A well-scoped use case for HE would pave the way for more widespread adoption in healthcare applications. Determining the efficacy of targeted cancer treatments used off-label for a variety of genetically defined conditions is an excellent candidate for introduction of HE-based learning systems because of a significant unmet need to share and combine confidential data, the use of relatively simple algorithms, and an opportunity to reach large numbers of willing study participants. METHODS: We used published literature to estimate the numbers of patients who might be eligible to receive treatments approved for other indications based on molecular profiles. We then estimated the sample size and number of variables that would be required for a successful system to detect exceptional responses with sufficient power. We generated an appropriately sized, simulated dataset (nÂ =â€‰5000) and used an established HE algorithm to detect exceptional responses and calculate total drug exposure, while the data remained encrypted. RESULTS: Our results demonstrated the feasibility of using an HE-based system to identify exceptional responders and perform calculations on patient data during a hypothetical 3-year study. Although homomorphically encrypted computations are time consuming, the required basic computations (i.e., addition) do not pose a critical bottleneck to the analysis. CONCLUSION: In this proof-of-concept study, based on simulated data, we demonstrate that identifying exceptional responders to targeted cancer treatments represents a valuable and feasible use case. Past solutions to either completely anonymize data or restrict access through stringent data use agreements have limited the utility of abundant and valuable data. Because of its privacy protections, we believe that an HE-based learning system for real-world cancer treatment would entice thousands more patients to voluntarily contribute data through participation in research studies beyond the currently available secondary data populated from hospital electronic health records and administrative claims. Forming collaborations between technical experts, physicians, patient advocates, payers, and researchers, and testing the system on existing RWD are critical next steps to making HE-based learning a reality in healthcare.","Proof-of-concept study: Homomorphically encrypted data can support real-time learning in personalized cancer medicine. BACKGROUND: The successful introduction of homomorphic encryption (HE) in clinical research holds promise for improving acceptance of data-sharing protocols, increasing sample sizes, and accelerating learning from real-world data (RWD). A well-scoped use case for HE would pave the way for more widespread adoption in healthcare applications. Determining the efficacy of targeted cancer treatments used off-label for a variety of genetically defined conditions is an excellent candidate for introduction of HE-based learning systems because of a significant unmet need to share and combine confidential data, the use of relatively simple algorithms, and an opportunity to reach large numbers of willing study participants. METHODS: We used published literature to estimate the numbers of patients who might be eligible to receive treatments approved for other indications based on molecular profiles. We then estimated the sample size and number of variables that would be required for a successful system to detect exceptional responses with sufficient power. We generated an appropriately sized, simulated dataset (nÂ =â€‰5000) and used an established HE algorithm to detect exceptional responses and calculate total drug exposure, while the data remained encrypted. RESULTS: Our results demonstrated the feasibility of using an HE-based system to identify exceptional responders and perform calculations on patient data during a hypothetical 3-year study. Although homomorphically encrypted computations are time consuming, the required basic computations (i.e., addition) do not pose a critical bottleneck to the analysis. CONCLUSION: In this proof-of-concept study, based on simulated data, we demonstrate that identifying exceptional responders to targeted cancer treatments represents a valuable and feasible use case. Past solutions to either completely anonymize data or restrict access through stringent data use agreements have limited the utility of abundant and valuable data. Because of its privacy protections, we believe that an HE-based learning system for real-world cancer treatment would entice thousands more patients to voluntarily contribute data through participation in research studies beyond the currently available secondary data populated from hospital electronic health records and administrative claims. Forming collaborations between technical experts, physicians, patient advocates, payers, and researchers, and testing the system on existing RWD are critical next steps to making HE-based learning a reality in healthcare."
0,Identification of synthetic lethality based on a functional network by using machine learning algorithms,"Synthetic lethality is the synthesis of mutations leading to cell death. Tumor-specific synthetic lethality has been targeted in research to improve cancer therapy. With the advances of techniques in molecular biology, such as RNAi and CRISPR/Cas9 gene editing, efforts have been made to systematically identify synthetic lethal interactions, especially for frequently mutated genes in cancers. However, elucidating the mechanism of synthetic lethality remains a challenge because of the complexity of its influencing conditions. In this study, we proposed a new computational method to identify critical functional features that can accurately predict synthetic lethal interactions. This method incorporates several machine learning algorithms and encodes protein-coding genes by an enrichment system derived from gene ontology terms and Kyoto Encyclopedia of Genes and Genomes pathways to represent their functional features. We built a random forest-based prediction engine by using 2120 selected features and obtained a Matthews correlation coefficient of 0.532. We examined the top 15 features and found that most of them have potential roles in synthetic lethality according to previous studies. These results demonstrate the ability of our proposed method to predict synthetic lethal interactions and provide a basis for further characterization of these particular genetic combinations.","Identification of synthetic lethality based on a functional network by using machine learning algorithms. Synthetic lethality is the synthesis of mutations leading to cell death. Tumor-specific synthetic lethality has been targeted in research to improve cancer therapy. With the advances of techniques in molecular biology, such as RNAi and CRISPR/Cas9 gene editing, efforts have been made to systematically identify synthetic lethal interactions, especially for frequently mutated genes in cancers. However, elucidating the mechanism of synthetic lethality remains a challenge because of the complexity of its influencing conditions. In this study, we proposed a new computational method to identify critical functional features that can accurately predict synthetic lethal interactions. This method incorporates several machine learning algorithms and encodes protein-coding genes by an enrichment system derived from gene ontology terms and Kyoto Encyclopedia of Genes and Genomes pathways to represent their functional features. We built a random forest-based prediction engine by using 2120 selected features and obtained a Matthews correlation coefficient of 0.532. We examined the top 15 features and found that most of them have potential roles in synthetic lethality according to previous studies. These results demonstrate the ability of our proposed method to predict synthetic lethal interactions and provide a basis for further characterization of these particular genetic combinations."
0,Discovery of Small Molecules that Activate RNA Methylation through Cooperative Binding to the METTL3-14-WTAP Complex Active Site,"Chemical modifications of RNA provide an additional, epitranscriptomic, level of control over cellular functions. N-6-methylated adenosines (m6As) are found in several types of RNA, and their amounts are regulated by methyltransferases and demethylases. One of the most important enzymes catalyzing generation of m6A on mRNA is the trimer N-6-methyltransferase METTL3-14-WTAP complex. Its activity has been linked to such critical biological processes as cell differentiation, proliferation, and death. We used in silico-based discovery to identify small-molecule ligands that bind to METTL3-14-WTAP and determined experimentally their binding affinity and kinetics, as well as their effect on enzymatic function. We show that these ligands serve as activators of the METTL3-14-WTAP complex. The methyltransferase complex METTL3-14-WTAP catalyzes generation of m6A on mRNA. Selberg et al. report the in silico discovery and experimental characterization of small-molecule compounds with exceptionally high binding efficiencies to METTL3-14-WTAP. Remarkably, these compounds act as enzyme activators and lead to increased m6A levels in RNA.","Discovery of Small Molecules that Activate RNA Methylation through Cooperative Binding to the METTL3-14-WTAP Complex Active Site. Chemical modifications of RNA provide an additional, epitranscriptomic, level of control over cellular functions. N-6-methylated adenosines (m6As) are found in several types of RNA, and their amounts are regulated by methyltransferases and demethylases. One of the most important enzymes catalyzing generation of m6A on mRNA is the trimer N-6-methyltransferase METTL3-14-WTAP complex. Its activity has been linked to such critical biological processes as cell differentiation, proliferation, and death. We used in silico-based discovery to identify small-molecule ligands that bind to METTL3-14-WTAP and determined experimentally their binding affinity and kinetics, as well as their effect on enzymatic function. We show that these ligands serve as activators of the METTL3-14-WTAP complex. The methyltransferase complex METTL3-14-WTAP catalyzes generation of m6A on mRNA. Selberg et al. report the in silico discovery and experimental characterization of small-molecule compounds with exceptionally high binding efficiencies to METTL3-14-WTAP. Remarkably, these compounds act as enzyme activators and lead to increased m6A levels in RNA."
0,"Longitudinal Changes in Peripapillary Retinal Nerve Fiber Layer Thickness in High Myopia A Prospective, Observational Study",,
0,Combined treatment with emodin and a telomerase inhibitor induces significant telomere damage/dysfunction and cell death,"G-quadruplex telomeric secondary structures represent natural replication fork barriers and must be resolved to permit efficient replication. Stabilization of telomeric G4 leads to telomere dysfunctions demonstrated by telomere shortening or damage, resulting in genome instability and apoptosis. Chemical compounds targeting G4 structures have been reported to induce telomere disturbance and tumor suppression. Here, virtual screening was performed in a natural compound library using PyRx to identify novel G4 ligands. Emodin was identified as one of the best candidates, showing a great G4-binding potential. Subsequently, we confirmed that emodin could stabilize G4 structures in vitro and trigger telomere dysfunctions including fragile telomeres, telomere loss, and telomeric DNA damage. However, this telomere disturbance could be rescued by subsequent elevation of telomerase activity; in contrast, when we treated the cells with the telomerase inhibitor BIBR1532 upon emodin treatment, permanent telomere disturbance and obvious growth inhibition of 4T1-cell xenograft tumors were observed in mice. Taken together, our results show for the first time that emodin-induced telomeric DNA damage can upregulate telomerase activity, which may weaken its anticancer effect. The combined use of emodin and the telomerase inhibitor synergistically induced telomere dysfunction and inhibited tumor generation.","Combined treatment with emodin and a telomerase inhibitor induces significant telomere damage/dysfunction and cell death. G-quadruplex telomeric secondary structures represent natural replication fork barriers and must be resolved to permit efficient replication. Stabilization of telomeric G4 leads to telomere dysfunctions demonstrated by telomere shortening or damage, resulting in genome instability and apoptosis. Chemical compounds targeting G4 structures have been reported to induce telomere disturbance and tumor suppression. Here, virtual screening was performed in a natural compound library using PyRx to identify novel G4 ligands. Emodin was identified as one of the best candidates, showing a great G4-binding potential. Subsequently, we confirmed that emodin could stabilize G4 structures in vitro and trigger telomere dysfunctions including fragile telomeres, telomere loss, and telomeric DNA damage. However, this telomere disturbance could be rescued by subsequent elevation of telomerase activity; in contrast, when we treated the cells with the telomerase inhibitor BIBR1532 upon emodin treatment, permanent telomere disturbance and obvious growth inhibition of 4T1-cell xenograft tumors were observed in mice. Taken together, our results show for the first time that emodin-induced telomeric DNA damage can upregulate telomerase activity, which may weaken its anticancer effect. The combined use of emodin and the telomerase inhibitor synergistically induced telomere dysfunction and inhibited tumor generation."
0,High-spatial-resolution diffusion MRI in Parkinson disease: Lateral asymmetry of the substantia nigra,"Background: Motor symptoms in Parkinson disease (PD) have exhibited lateral asymmetry, suggesting asymmetric neuronal loss in the substantia nigra (SN). Diffusion MRI may be able to help confirm tissue microstructural alterations in the substantia nigra to probe for the presence of asymmetry. Purpose: To investigate lateral asymmetry in the SN of patients with PD by using diffusion MRI with both Gaussian and non-Gaussian models. Materials and Methods: In this cross-sectional study conducted from March 2015 to March 2017, 27 participants with PD and 27 age-matched healthy control (HC) participants, all right handed, underwent MRI at 3.0 T. High-spatial-resolution diffusion images were acquired with a reduced field of view by using seven b values up to 3000 sec/mm2. A continuous-time random-walk (CTRW) non-Gaussian diffusion model was used to produce anomalous diffusion coefficient (Dm) and temporal (a) and spatial (b) diffusion heterogeneity indexes followed by a Gaussian diffusion model to yield an apparent diffusion coefficient (ADC). Individual or linear combinations of diffusion parameters in the SN were unilaterally and bilaterally compared between the PD and HC groups. Results: In the bilateral comparison between the PD and HC groups, differences were observed in b (0.67 6 0.06 [standard deviation] vs 0.64 6 0.04, respectively; P = .016), ADC (0.48 mm2/msec 6 0.08 vs 0.53 mm2/msec 6 0.06, respectively; P = .03), and the combination of CTRW parameters (P = .02). In the unilateral comparison, differences were observed in all diffusion parameters on the left SN (P , .03), but not on the right (P . .20). In a receiver operating characteristic (ROC) analysis to delineate left SN abnormality in PD, the combination of Dm, a, and b produced the best sensitivity (sensitivity, 0.78); the combination of Dm and b produced the best specificity (specificity, 0.85); and the combination of a and b produced the largest area under the ROC curve (area under the ROC curve, 0.73). Conclusion: These results suggest that quantitative diffusion MRI is sensitive to brain tissue changes in participants with Parkinson disease and provide evidence of substantia nigra lateral asymmetry in this disease.","High-spatial-resolution diffusion MRI in Parkinson disease: Lateral asymmetry of the substantia nigra. Background: Motor symptoms in Parkinson disease (PD) have exhibited lateral asymmetry, suggesting asymmetric neuronal loss in the substantia nigra (SN). Diffusion MRI may be able to help confirm tissue microstructural alterations in the substantia nigra to probe for the presence of asymmetry. Purpose: To investigate lateral asymmetry in the SN of patients with PD by using diffusion MRI with both Gaussian and non-Gaussian models. Materials and Methods: In this cross-sectional study conducted from March 2015 to March 2017, 27 participants with PD and 27 age-matched healthy control (HC) participants, all right handed, underwent MRI at 3.0 T. High-spatial-resolution diffusion images were acquired with a reduced field of view by using seven b values up to 3000 sec/mm2. A continuous-time random-walk (CTRW) non-Gaussian diffusion model was used to produce anomalous diffusion coefficient (Dm) and temporal (a) and spatial (b) diffusion heterogeneity indexes followed by a Gaussian diffusion model to yield an apparent diffusion coefficient (ADC). Individual or linear combinations of diffusion parameters in the SN were unilaterally and bilaterally compared between the PD and HC groups. Results: In the bilateral comparison between the PD and HC groups, differences were observed in b (0.67 6 0.06 [standard deviation] vs 0.64 6 0.04, respectively; P = .016), ADC (0.48 mm2/msec 6 0.08 vs 0.53 mm2/msec 6 0.06, respectively; P = .03), and the combination of CTRW parameters (P = .02). In the unilateral comparison, differences were observed in all diffusion parameters on the left SN (P , .03), but not on the right (P . .20). In a receiver operating characteristic (ROC) analysis to delineate left SN abnormality in PD, the combination of Dm, a, and b produced the best sensitivity (sensitivity, 0.78); the combination of Dm and b produced the best specificity (specificity, 0.85); and the combination of a and b produced the largest area under the ROC curve (area under the ROC curve, 0.73). Conclusion: These results suggest that quantitative diffusion MRI is sensitive to brain tissue changes in participants with Parkinson disease and provide evidence of substantia nigra lateral asymmetry in this disease."
0,Surveillance for Hepatocellular Carcinoma: Current Best Practice and Future Direction,,
0,Deep Conservation of cis-Element Variants Regulating Plant Hormonal Responses,"Phytohormones regulate many aspects of plant life by activating transcription factors (TFs) that bind sequence-specific response elements (REs) in regulatory regions of target genes. Despite their short length, REs are degenerate, with a core of just 3 to 4 bp. This degeneracy is paradoxical, as it reduces specificity and REs are extremely common in the genome. To study whether RE degeneracy might serve a biological function, we developed an algorithm for the detection of regulatory sequence conservation and applied it to phytohormone REs in 45 angiosperms. Surprisingly, we found that specific RE variants are highly conserved in core hormone response genes. Experimental evidence showed that specific variants act to regulate the magnitude and spatial profile of hormonal response in Arabidopsis (Arabidopsis thaliana) and tomato (Solanum lycopersicum). Our results suggest that hormone-regulated TFs bind a spectrum of REs, each coding for a distinct transcriptional response profile. Our approach has implications for precise genome editing and for rational promoter design.","Deep Conservation of cis-Element Variants Regulating Plant Hormonal Responses. Phytohormones regulate many aspects of plant life by activating transcription factors (TFs) that bind sequence-specific response elements (REs) in regulatory regions of target genes. Despite their short length, REs are degenerate, with a core of just 3 to 4 bp. This degeneracy is paradoxical, as it reduces specificity and REs are extremely common in the genome. To study whether RE degeneracy might serve a biological function, we developed an algorithm for the detection of regulatory sequence conservation and applied it to phytohormone REs in 45 angiosperms. Surprisingly, we found that specific RE variants are highly conserved in core hormone response genes. Experimental evidence showed that specific variants act to regulate the magnitude and spatial profile of hormonal response in Arabidopsis (Arabidopsis thaliana) and tomato (Solanum lycopersicum). Our results suggest that hormone-regulated TFs bind a spectrum of REs, each coding for a distinct transcriptional response profile. Our approach has implications for precise genome editing and for rational promoter design."
0,Can machine learning predict responses to TNF inhibitors?,,
0,Virtual screening identifies a PIN1 inhibitor with possible antiovarian cancer effects,"Peptidyl-prolyl cisâ€“trans isomerase, NIMA-interacting 1 (PIN1) is a peptidyl-prolyl isomerase that binds phospho-Ser/Thr-Pro motifs in proteins and catalyzes the cisâ€“trans isomerization of proline peptide bonds. PIN1 is overexpressed in several cancers including high-grade serous ovarian cancer. Since few therapies are effective against this cancer, PIN1 could be a therapeutic target but effective PIN1 inhibitors are lacking. To identify molecules with in vivo inhibitory effects on PIN1, we used consensus docking to model existing PIN1-ligand X-ray structures and to screen a chemical database for candidate inhibitors. Ten molecules were selected and tested in cellular assays, leading to the identification of VS10 that bound and inhibited PIN1. VS10 treatment reduced the viability of ovarian cancer cell lines by inducing proteasomal PIN1 degradation, without effects on PIN1 transcription, and also reduced the levels of downstream targets Î²-catenin, cyclin D1, and pSer473-Akt. VS10 is a selective PIN1 inhibitor that may offer new opportunities for treating PIN1-overexpressing tumors.","Virtual screening identifies a PIN1 inhibitor with possible antiovarian cancer effects. Peptidyl-prolyl cisâ€“trans isomerase, NIMA-interacting 1 (PIN1) is a peptidyl-prolyl isomerase that binds phospho-Ser/Thr-Pro motifs in proteins and catalyzes the cisâ€“trans isomerization of proline peptide bonds. PIN1 is overexpressed in several cancers including high-grade serous ovarian cancer. Since few therapies are effective against this cancer, PIN1 could be a therapeutic target but effective PIN1 inhibitors are lacking. To identify molecules with in vivo inhibitory effects on PIN1, we used consensus docking to model existing PIN1-ligand X-ray structures and to screen a chemical database for candidate inhibitors. Ten molecules were selected and tested in cellular assays, leading to the identification of VS10 that bound and inhibited PIN1. VS10 treatment reduced the viability of ovarian cancer cell lines by inducing proteasomal PIN1 degradation, without effects on PIN1 transcription, and also reduced the levels of downstream targets Î²-catenin, cyclin D1, and pSer473-Akt. VS10 is a selective PIN1 inhibitor that may offer new opportunities for treating PIN1-overexpressing tumors."
0,The modulation of gamma oscillations by methamphetamine in rat hippocampal slices,"Gamma frequency oscillations (Î³, 30â€“100 Hz) have been suggested to underlie various cognitive and motor functions. The psychotomimetic drug methamphetamine (MA) enhances brain Î³ oscillations associated with changes in psychomotor state. Little is known about the cellular mechanisms of MA modulation on Î³ oscillations. We explored the effects of multiple intracellular kinases on MA modulation of Î³ induced by kainate in area CA3 of rat ventral hippocampal slices. We found that dopamine receptor type 1 and 2 (DR1 and DR2) antagonists, the serine/threonine kinase PKB/Akt inhibitor and N-methyl-D-aspartate receptor (NMDAR) antagonists prevented the enhancing effect of MA on Î³ oscillations, whereas none of them affected baseline Î³ strength. Protein kinase A, phosphoinositide 3-kinase and extracellular signal-related kinases inhibitors had no effect on MA. We propose that the DR1/DR2-Akt-NMDAR pathway plays a critical role for the MA enhancement of Î³ oscillations. Our study provides an new insight into the mechanisms of acute MA on MA-induced psychosis.","The modulation of gamma oscillations by methamphetamine in rat hippocampal slices. Gamma frequency oscillations (Î³, 30â€“100 Hz) have been suggested to underlie various cognitive and motor functions. The psychotomimetic drug methamphetamine (MA) enhances brain Î³ oscillations associated with changes in psychomotor state. Little is known about the cellular mechanisms of MA modulation on Î³ oscillations. We explored the effects of multiple intracellular kinases on MA modulation of Î³ induced by kainate in area CA3 of rat ventral hippocampal slices. We found that dopamine receptor type 1 and 2 (DR1 and DR2) antagonists, the serine/threonine kinase PKB/Akt inhibitor and N-methyl-D-aspartate receptor (NMDAR) antagonists prevented the enhancing effect of MA on Î³ oscillations, whereas none of them affected baseline Î³ strength. Protein kinase A, phosphoinositide 3-kinase and extracellular signal-related kinases inhibitors had no effect on MA. We propose that the DR1/DR2-Akt-NMDAR pathway plays a critical role for the MA enhancement of Î³ oscillations. Our study provides an new insight into the mechanisms of acute MA on MA-induced psychosis."
0,Modeling RNA-Binding Protein Specificity In Vivo by Precisely Registering Protein-RNA Crosslink Sites,"Feng et al. described an algorithm called mCross to accurately define RNA-binding protein specificity by precisely registering protein-RNA crosslink sites using CLIP data. This method was applied to >100 RBPs and identified a noncanoncial binding motif of SRSF1, which implicates the protein in modulating phase separation.","Modeling RNA-Binding Protein Specificity In Vivo by Precisely Registering Protein-RNA Crosslink Sites. Feng et al. described an algorithm called mCross to accurately define RNA-binding protein specificity by precisely registering protein-RNA crosslink sites using CLIP data. This method was applied to >100 RBPs and identified a noncanoncial binding motif of SRSF1, which implicates the protein in modulating phase separation."
0,Segmentation and analysis of surface characteristics of oral tissues obtained by scanning electron microscopy to differentiate normal and oral precancerous condition,"Abnormal epithelial stratification is a sign of oral dysplasia and hence evaluation of surface characteristics of oral epithelial region can help in detection of cancerous progression. Surface characteristics can be better visualised by Scanning Electron Microscopy (SEM) in comparison to light microscopy. In our study we have developed automated image processing algorithms i.e. Gaussian with median filtering and Gradient filtering, using MATLAB 2016b, to segment the surface characteristics i.e. the ridges and pits in the SEM images of oral tissue of normal (13 samples) and Oral Submucous Fibrosis (OSF) (36 samples) subjects. After segmentation, quantitative measurement of the parameters like area, thickness and textural features like entropy, contrast and range filter of ridges as well as area of pit and the ratio of area of ridge vs. area of pit was done. Statistical significant differences were obtained in between normal and OSF study groups for thickness (p=0.0107), entropy (p<0.00001) and contrast of ridge (p<0.00001) for Gaussian with median filtering and for all the parameters except thickness of the ridge(p=1.386), for Gradient filtering. Thus, computer aided image processing by Gradient filter followed by quantitative measurement of the surface characteristics provided precise differentiation between normal and precancerous oral condition.","Segmentation and analysis of surface characteristics of oral tissues obtained by scanning electron microscopy to differentiate normal and oral precancerous condition. Abnormal epithelial stratification is a sign of oral dysplasia and hence evaluation of surface characteristics of oral epithelial region can help in detection of cancerous progression. Surface characteristics can be better visualised by Scanning Electron Microscopy (SEM) in comparison to light microscopy. In our study we have developed automated image processing algorithms i.e. Gaussian with median filtering and Gradient filtering, using MATLAB 2016b, to segment the surface characteristics i.e. the ridges and pits in the SEM images of oral tissue of normal (13 samples) and Oral Submucous Fibrosis (OSF) (36 samples) subjects. After segmentation, quantitative measurement of the parameters like area, thickness and textural features like entropy, contrast and range filter of ridges as well as area of pit and the ratio of area of ridge vs. area of pit was done. Statistical significant differences were obtained in between normal and OSF study groups for thickness (p=0.0107), entropy (p<0.00001) and contrast of ridge (p<0.00001) for Gaussian with median filtering and for all the parameters except thickness of the ridge(p=1.386), for Gradient filtering. Thus, computer aided image processing by Gradient filter followed by quantitative measurement of the surface characteristics provided precise differentiation between normal and precancerous oral condition."
0,Paradigm of Sudden Death Prevention in Hypertrophic Cardiomyopathy,,
0,Structural determinants of the APOBEC3G N-terminal domain for HIV-1 RNA association,"APOBEC3G (A3G) is a cellular protein that inhibits HIV-1 infection through virion incorporation. The interaction of the A3G N-terminal domain (NTD) with RNA is essential for A3G incorporation in the HIV-1 virion. The interaction between A3G-NTD and RNA is not completely understood. The A3G-NTD is also recognized by HIV-1 Viral infectivity factor (Vif) and A3G-Vif binding leads to A3G degradation. Therefore, the A3G-Vif interaction is a target for the development of antiviral therapies that block HIV-1 replication. However, targeting the A3G-Vif interactions could disrupt the A3G-RNA interactions that are required for A3G's antiviral activity. To better understand A3G-RNA binding, we generated in silico docking models to simulate the RNA-binding propensity of A3G-NTD. We simulated the A3G-NTD residues with high RNA-binding propensity, experimentally validated our prediction by testing A3G-NTD mutations, and identified structural determinants of A3G-RNA binding. In addition, we found a novel amino acid residue, I26 responsible for RNA interaction. The new structural insights provided here will facilitate the design of pharmaceuticals that inhibit A3G-Vif interactions without negatively impacting A3G-RNA interactions.","Structural determinants of the APOBEC3G N-terminal domain for HIV-1 RNA association. APOBEC3G (A3G) is a cellular protein that inhibits HIV-1 infection through virion incorporation. The interaction of the A3G N-terminal domain (NTD) with RNA is essential for A3G incorporation in the HIV-1 virion. The interaction between A3G-NTD and RNA is not completely understood. The A3G-NTD is also recognized by HIV-1 Viral infectivity factor (Vif) and A3G-Vif binding leads to A3G degradation. Therefore, the A3G-Vif interaction is a target for the development of antiviral therapies that block HIV-1 replication. However, targeting the A3G-Vif interactions could disrupt the A3G-RNA interactions that are required for A3G's antiviral activity. To better understand A3G-RNA binding, we generated in silico docking models to simulate the RNA-binding propensity of A3G-NTD. We simulated the A3G-NTD residues with high RNA-binding propensity, experimentally validated our prediction by testing A3G-NTD mutations, and identified structural determinants of A3G-RNA binding. In addition, we found a novel amino acid residue, I26 responsible for RNA interaction. The new structural insights provided here will facilitate the design of pharmaceuticals that inhibit A3G-Vif interactions without negatively impacting A3G-RNA interactions."
0,High-sensitivity cardiac troponin assays: finally ready for prime time?,"High-sensitivity cardiac troponin (hs-cTn) assays facilitate the ruling-out of myocardial infarction (MI) but identify a high number of patients with elevated troponin levels but without MI. Consequently, the term myocardial injury was included in the latest universal definition of MI. In the High-STEACS trial, use of a hs-cTnI assay was safe but had no prognostic benefit.","High-sensitivity cardiac troponin assays: finally ready for prime time?. High-sensitivity cardiac troponin (hs-cTn) assays facilitate the ruling-out of myocardial infarction (MI) but identify a high number of patients with elevated troponin levels but without MI. Consequently, the term myocardial injury was included in the latest universal definition of MI. In the High-STEACS trial, use of a hs-cTnI assay was safe but had no prognostic benefit."
0,Digital pathology and artificial intelligence,"In modern clinical practice, digital pathology has a crucial role and is increasingly a technological requirement in the scientific laboratory environment. The advent of whole-slide imaging, availability of faster networks, and cheaper storage solutions has made it easier for pathologists to manage digital slide images and share them for clinical use. In parallel, unprecedented advances in machine learning have enabled the synergy of artificial intelligence and digital pathology, which offers image-based diagnosis possibilities that were once limited only to radiology and cardiology. Integration of digital slides into the pathology workflow, advanced algorithms, and computer-aided diagnostic techniques extend the frontiers of the pathologist's view beyond a microscopic slide and enable true utilisation and integration of knowledge that is beyond human limits and boundaries, and we believe there is clear potential for artificial intelligence breakthroughs in the pathology setting. In this Review, we discuss advancements in digital slide-based image diagnosis for cancer along with some challenges and opportunities for artificial intelligence in digital pathology.","Digital pathology and artificial intelligence. In modern clinical practice, digital pathology has a crucial role and is increasingly a technological requirement in the scientific laboratory environment. The advent of whole-slide imaging, availability of faster networks, and cheaper storage solutions has made it easier for pathologists to manage digital slide images and share them for clinical use. In parallel, unprecedented advances in machine learning have enabled the synergy of artificial intelligence and digital pathology, which offers image-based diagnosis possibilities that were once limited only to radiology and cardiology. Integration of digital slides into the pathology workflow, advanced algorithms, and computer-aided diagnostic techniques extend the frontiers of the pathologist's view beyond a microscopic slide and enable true utilisation and integration of knowledge that is beyond human limits and boundaries, and we believe there is clear potential for artificial intelligence breakthroughs in the pathology setting. In this Review, we discuss advancements in digital slide-based image diagnosis for cancer along with some challenges and opportunities for artificial intelligence in digital pathology."
0,Validation of the Welch Allyn Home blood pressure monitor with professional SureBP algorithm with a special feature of accuracy during involuntary (tremor) patient movement,"Background Current blood pressure (BP) measurement guidelines recommend certain patient requirements, especially keeping still for 5 min. Some patients cannot comply. My colleagues and I have reported accurate performance of the Welch Allyn SureBP algorithm for BP estimates during voluntary patient motion. No validation studies for involuntary patient movement (tremor) BP readings have been reported. This paper reports the validation of the Welch Allyn Home BP monitor, the 1700 Series, which contains that same SureBP algorithm, and the results of tremor testing as well. This device has multiple clinical advantages. Patients and methods Eighty-five patients (49 females) were studied using the ANSI/AAMI/ISO 81060-2, 2013 requirements. Three sizes of cuffs were included. The tremor experiments used a simulator programmed to frequency and amplitude of oscillometric impulses typically seen in patients with diseases causing tremors. This is the first protocol developed for this clinical scenario. The device uses an inflation-based algorithm, reducing discomfort and cycle times. Results The meanÂ±SD for the device minus manual readings per ISO Criterion 1 were-2.93Â±6.64 mmHg for systolic BP and-2.453Â±5.48 mmHg for diastolic BP. The tremor testing was performed at low, normal, and high BP simulations. The device recorded a BP value for every cycle tested. The errors (device minus manual BP estimates) were quite low. Conclusion The Welch Allyn Home BP monitor is accurate in the presence of involuntary patient motion (tremor). Clinicians can have a high level of confidence in the use of a self-measurement device, which operates using the same algorithm as contained in the 'professional grade' family of devices.","Validation of the Welch Allyn Home blood pressure monitor with professional SureBP algorithm with a special feature of accuracy during involuntary (tremor) patient movement. Background Current blood pressure (BP) measurement guidelines recommend certain patient requirements, especially keeping still for 5 min. Some patients cannot comply. My colleagues and I have reported accurate performance of the Welch Allyn SureBP algorithm for BP estimates during voluntary patient motion. No validation studies for involuntary patient movement (tremor) BP readings have been reported. This paper reports the validation of the Welch Allyn Home BP monitor, the 1700 Series, which contains that same SureBP algorithm, and the results of tremor testing as well. This device has multiple clinical advantages. Patients and methods Eighty-five patients (49 females) were studied using the ANSI/AAMI/ISO 81060-2, 2013 requirements. Three sizes of cuffs were included. The tremor experiments used a simulator programmed to frequency and amplitude of oscillometric impulses typically seen in patients with diseases causing tremors. This is the first protocol developed for this clinical scenario. The device uses an inflation-based algorithm, reducing discomfort and cycle times. Results The meanÂ±SD for the device minus manual readings per ISO Criterion 1 were-2.93Â±6.64 mmHg for systolic BP and-2.453Â±5.48 mmHg for diastolic BP. The tremor testing was performed at low, normal, and high BP simulations. The device recorded a BP value for every cycle tested. The errors (device minus manual BP estimates) were quite low. Conclusion The Welch Allyn Home BP monitor is accurate in the presence of involuntary patient motion (tremor). Clinicians can have a high level of confidence in the use of a self-measurement device, which operates using the same algorithm as contained in the 'professional grade' family of devices."
0,"Hierarchical-Clustering, Scaffold-Mining Exercises and Dynamics Simulations for Effectual Inhibitors Against Lipid-A Biosynthesis of Helicobacter pylori","Introduction: Treatment failures of standard regimens and new strains egression are due to the augmented drug resistance conundrum. These confounding factors now became the drug designers spotlight to implement therapeutics against Helicobacter pylori strains and to safeguard infected victims with devoid of adverse drug reactions. Thereby, to navigate the chemical space for medicine, paramount vital drug target opting considerations should be imperative. The study is therefore aimed to develop potent therapeutic variants against an insightful extrapolative, common target LpxC as a follow-up to previous studies. Methods: We explored the relationships between existing inhibitors and novel leads at the scaffold level in an appropriate conformational plasticity for lead-optimization campaign. Hierarchical-clustering and shape-based screening against an in-house library of > 21 million compounds resulted in panel of 11,000 compounds. Rigid-receptor docking through virtual screening cascade, quantum-polarized-ligand, induced-fit dockings, post-docking processes and system stability assessments were performed. Results: After docking experiments, an enrichment performance unveiled seven ranked actives better binding efficiencies with Zinc-binding potency than substrate and in-actives (decoy-set) with ROC (1.0) and area under accumulation curve (0.90) metrics. Physics-based membrane permeability accompanied ADME/T predictions and long-range dynamic simulations of 250 ns chemical time have depicted good passive diffusion with no toxicity of leads and sustained consistency of lead1-LpxC in the physiological milieu respectively. Conclusions: In the study, as these static outcomes obtained from this approach competed with the substrate and existing ligands in binding affinity estimations as well as positively correlated from different aspects of predictions, which could facilitate promiscuous new chemical entities against H. pylori.","Hierarchical-Clustering, Scaffold-Mining Exercises and Dynamics Simulations for Effectual Inhibitors Against Lipid-A Biosynthesis of Helicobacter pylori. Introduction: Treatment failures of standard regimens and new strains egression are due to the augmented drug resistance conundrum. These confounding factors now became the drug designers spotlight to implement therapeutics against Helicobacter pylori strains and to safeguard infected victims with devoid of adverse drug reactions. Thereby, to navigate the chemical space for medicine, paramount vital drug target opting considerations should be imperative. The study is therefore aimed to develop potent therapeutic variants against an insightful extrapolative, common target LpxC as a follow-up to previous studies. Methods: We explored the relationships between existing inhibitors and novel leads at the scaffold level in an appropriate conformational plasticity for lead-optimization campaign. Hierarchical-clustering and shape-based screening against an in-house library of > 21 million compounds resulted in panel of 11,000 compounds. Rigid-receptor docking through virtual screening cascade, quantum-polarized-ligand, induced-fit dockings, post-docking processes and system stability assessments were performed. Results: After docking experiments, an enrichment performance unveiled seven ranked actives better binding efficiencies with Zinc-binding potency than substrate and in-actives (decoy-set) with ROC (1.0) and area under accumulation curve (0.90) metrics. Physics-based membrane permeability accompanied ADME/T predictions and long-range dynamic simulations of 250 ns chemical time have depicted good passive diffusion with no toxicity of leads and sustained consistency of lead1-LpxC in the physiological milieu respectively. Conclusions: In the study, as these static outcomes obtained from this approach competed with the substrate and existing ligands in binding affinity estimations as well as positively correlated from different aspects of predictions, which could facilitate promiscuous new chemical entities against H. pylori."
0,Pediatric patients with acute lymphoblastic leukemia generate abundant and functional neoantigen-specific CD8(+) T cell responses,,
0,Paraneoplastic neurological syndromes in the era of immune-checkpoint inhibitors,,
0,Privacy in the age of medical big data,,
0,Extension of the CONSORT and SPIRIT statements,,
0,Protein modeling to assess the pathogenicity of rare variants of SERPINA1 in patients suspected of having Alpha 1 Antitrypsin Deficiency,"Background: Alpha 1 Antitrypsin (AAT) is a key serum proteinase inhibitor encoded by SERPINA1. Sequence variants of the gene can cause Alpha 1 Antitrypsin Deficiency (AATD), a condition associated with lung and liver disease. The majority of AATD cases are caused by the 'Z' and 'S' variants - single-nucleotide variations (SNVs) that result in amino acid substitutions of E342K and E264V. However, SERPINA1 is highly polymorphic, with numerous potentially clinically relevant variants reported. Novel variants continue to be discovered, and without reports of pathogenicity, it can be difficult for clinicians to determine the best course of treatment. Methods: We assessed the utility of next-generation sequencing (NGS) and predictive computational analysis to guide the diagnosis of patients suspected of having AATD. Blood samples on serum separator cards were submitted to the DNA1 Advanced Screening Program (Biocerna LLC, Fulton, Maryland, USA) by physicians whose patients were suspected of having AATD. Laboratory analyses included quantification of serum AAT levels, qualitative analysis by isoelectric focusing, and targeted genotyping and NGS of the SERPINA1 gene. Molecular modeling software UCSF Chimera (University College of San Francisco, CA) was used to visualize the positions of amino acid changes as a result of rare/novel SNVs. Predictive software was used to assess the potential pathogenicity of these variants; methods included a support vector machine (SVM) program, PolyPhen-2 (Harvard University, Cambridge, MA), and FoldX (Centre for Genomic Regulation, Barcelona, Spain). Results: Samples from 23 patients were analyzed; 21 rare/novel sequence variants were identified by NGS, including splice variants (n = 2), base pair deletions (n = 1), stop codon insertions (n = 2), and SNVs (n = 16). Computational modeling of protein structures caused by the novel SNVs showed that 8 were probably deleterious, and two were possibly deleterious. For the majority of probably/possibly deleterious SNVs (I50N, P289S, M385T, M221T, D341V, V210E, P369H, V333M and A142D), the mechanism is probably via disruption of the packed hydrophobic core of AAT. Several deleterious variants occurred in combination with more common deficiency alleles, resulting in very low AAT levels. Conclusions: NGS and computational modeling are useful tools that can facilitate earlier, more precise diagnosis, and consideration for AAT therapy in AATD.","Protein modeling to assess the pathogenicity of rare variants of SERPINA1 in patients suspected of having Alpha 1 Antitrypsin Deficiency. Background: Alpha 1 Antitrypsin (AAT) is a key serum proteinase inhibitor encoded by SERPINA1. Sequence variants of the gene can cause Alpha 1 Antitrypsin Deficiency (AATD), a condition associated with lung and liver disease. The majority of AATD cases are caused by the 'Z' and 'S' variants - single-nucleotide variations (SNVs) that result in amino acid substitutions of E342K and E264V. However, SERPINA1 is highly polymorphic, with numerous potentially clinically relevant variants reported. Novel variants continue to be discovered, and without reports of pathogenicity, it can be difficult for clinicians to determine the best course of treatment. Methods: We assessed the utility of next-generation sequencing (NGS) and predictive computational analysis to guide the diagnosis of patients suspected of having AATD. Blood samples on serum separator cards were submitted to the DNA1 Advanced Screening Program (Biocerna LLC, Fulton, Maryland, USA) by physicians whose patients were suspected of having AATD. Laboratory analyses included quantification of serum AAT levels, qualitative analysis by isoelectric focusing, and targeted genotyping and NGS of the SERPINA1 gene. Molecular modeling software UCSF Chimera (University College of San Francisco, CA) was used to visualize the positions of amino acid changes as a result of rare/novel SNVs. Predictive software was used to assess the potential pathogenicity of these variants; methods included a support vector machine (SVM) program, PolyPhen-2 (Harvard University, Cambridge, MA), and FoldX (Centre for Genomic Regulation, Barcelona, Spain). Results: Samples from 23 patients were analyzed; 21 rare/novel sequence variants were identified by NGS, including splice variants (n = 2), base pair deletions (n = 1), stop codon insertions (n = 2), and SNVs (n = 16). Computational modeling of protein structures caused by the novel SNVs showed that 8 were probably deleterious, and two were possibly deleterious. For the majority of probably/possibly deleterious SNVs (I50N, P289S, M385T, M221T, D341V, V210E, P369H, V333M and A142D), the mechanism is probably via disruption of the packed hydrophobic core of AAT. Several deleterious variants occurred in combination with more common deficiency alleles, resulting in very low AAT levels. Conclusions: NGS and computational modeling are useful tools that can facilitate earlier, more precise diagnosis, and consideration for AAT therapy in AATD."
0,Predicting Responders to Reslizumab after 16 Weeks of Treatment Using an Algorithm Derived from Clinical Studies of Patients with Severe Eosinophilic Asthma,"RATIONALE: Reslizumab is a humanized anti-IL-5 monoclonal antibody used as add-on maintenance treatment for patients with uncontrolled eosinophilic asthma. OBJECTIVES: To predict response and nonresponse to intravenous reslizumab at 52 weeks with an algorithm we developed based on clinical indicators from pivotal clinical trials. METHODS: Patients aged 18 years and older who met Global Initiative for Asthma 4 or 5 criteria and received intravenous reslizumab (n = 321) in two trials ( www.clinicaltrials.gov identifiers, NCT01287039 and NCT01285323) were selected as the data source. A mathematical model was constructed that was based on change from baseline to 16 weeks in Asthma Control Questionnaire and Asthma Quality of Life Questionnaire scores and FEV1, and number of clinical asthma exacerbations during the year before enrollment and in the first 16 weeks of treatment, and these measures were evaluated for their ability to predict the outcome at 52 weeks: responder, nonresponder, or indeterminate. MEASUREMENTS AND MAIN RESULTS: The algorithm predicted that 276 patients would be classified as responders; in 248 (89.9%), the prediction was correct. In comparison, 26 patients were predicted to be nonresponders; 50.0% of these predictions were correct. Nineteen patients were classified as indeterminate. The algorithm had 95.4-95.5% sensitivity and 40.6-54.1% specificity. Jackknife and cross-study validation confirmed the robustness of the algorithm. CONCLUSIONS: Our algorithm enabled prediction at 16 weeks of treatment of the response to intravenous reslizumab treatment at 52 weeks, but it was not suitable for predicting nonresponse. A positive score at 16 weeks should encourage continued treatment, and a negative score should prompt close monitoring to determine whether discontinuation is warranted.","Predicting Responders to Reslizumab after 16 Weeks of Treatment Using an Algorithm Derived from Clinical Studies of Patients with Severe Eosinophilic Asthma. RATIONALE: Reslizumab is a humanized anti-IL-5 monoclonal antibody used as add-on maintenance treatment for patients with uncontrolled eosinophilic asthma. OBJECTIVES: To predict response and nonresponse to intravenous reslizumab at 52 weeks with an algorithm we developed based on clinical indicators from pivotal clinical trials. METHODS: Patients aged 18 years and older who met Global Initiative for Asthma 4 or 5 criteria and received intravenous reslizumab (n = 321) in two trials ( www.clinicaltrials.gov identifiers, NCT01287039 and NCT01285323) were selected as the data source. A mathematical model was constructed that was based on change from baseline to 16 weeks in Asthma Control Questionnaire and Asthma Quality of Life Questionnaire scores and FEV1, and number of clinical asthma exacerbations during the year before enrollment and in the first 16 weeks of treatment, and these measures were evaluated for their ability to predict the outcome at 52 weeks: responder, nonresponder, or indeterminate. MEASUREMENTS AND MAIN RESULTS: The algorithm predicted that 276 patients would be classified as responders; in 248 (89.9%), the prediction was correct. In comparison, 26 patients were predicted to be nonresponders; 50.0% of these predictions were correct. Nineteen patients were classified as indeterminate. The algorithm had 95.4-95.5% sensitivity and 40.6-54.1% specificity. Jackknife and cross-study validation confirmed the robustness of the algorithm. CONCLUSIONS: Our algorithm enabled prediction at 16 weeks of treatment of the response to intravenous reslizumab treatment at 52 weeks, but it was not suitable for predicting nonresponse. A positive score at 16 weeks should encourage continued treatment, and a negative score should prompt close monitoring to determine whether discontinuation is warranted."
0,Prediction of novel mouse TLR9 agonists using a random forest approach,"Background: Toll-like receptor 9 is a key innate immune receptor involved in detecting infectious diseases and cancer. TLR9 activates the innate immune system following the recognition of single-stranded DNA oligonucleotides (ODN) containing unmethylated cytosine-guanine (CpG) motifs. Due to the considerable number of rotatable bonds in ODNs, high-throughput in silico screening for potential TLR9 activity via traditional structure-based virtual screening approaches of CpG ODNs is challenging. In the current study, we present a machine learning based method for predicting novel mouse TLR9 (mTLR9) agonists based on features including count and position of motifs, the distance between the motifs and graphically derived features such as the radius of gyration and moment of Inertia. We employed an in-house experimentally validated dataset of 396 single-stranded synthetic ODNs, to compare the results of five machine learning algorithms. Since the dataset was highly imbalanced, we used an ensemble learning approach based on repeated random down-sampling. Results: Using in-house experimental TLR9 activity data we found that random forest algorithm outperformed other algorithms for our dataset for TLR9 activity prediction. Therefore, we developed a cross-validated ensemble classifier of 20 random forest models. The average Matthews correlation coefficient and balanced accuracy of our ensemble classifier in test samples was 0.61 and 80.0%, respectively, with the maximum balanced accuracy and Matthews correlation coefficient of 87.0% and 0.75, respectively. We confirmed common sequence motifs including 'CC', 'GG','AG', 'CCCG' and 'CGGC' were overrepresented in mTLR9 agonists. Predictions on 6000 randomly generated ODNs were ranked and the top 100 ODNs were synthesized and experimentally tested for activity in a mTLR9 reporter cell assay, with 91 of the 100 selected ODNs showing high activity, confirming the accuracy of the model in predicting mTLR9 activity. Conclusion: We combined repeated random down-sampling with random forest to overcome the class imbalance problem and achieved promising results. Overall, we showed that the random forest algorithm outperformed other machine learning algorithms including support vector machines, shrinkage discriminant analysis, gradient boosting machine and neural networks. Due to its predictive performance and simplicity, the random forest technique is a useful method for prediction of mTLR9 ODN agonists.","Prediction of novel mouse TLR9 agonists using a random forest approach. Background: Toll-like receptor 9 is a key innate immune receptor involved in detecting infectious diseases and cancer. TLR9 activates the innate immune system following the recognition of single-stranded DNA oligonucleotides (ODN) containing unmethylated cytosine-guanine (CpG) motifs. Due to the considerable number of rotatable bonds in ODNs, high-throughput in silico screening for potential TLR9 activity via traditional structure-based virtual screening approaches of CpG ODNs is challenging. In the current study, we present a machine learning based method for predicting novel mouse TLR9 (mTLR9) agonists based on features including count and position of motifs, the distance between the motifs and graphically derived features such as the radius of gyration and moment of Inertia. We employed an in-house experimentally validated dataset of 396 single-stranded synthetic ODNs, to compare the results of five machine learning algorithms. Since the dataset was highly imbalanced, we used an ensemble learning approach based on repeated random down-sampling. Results: Using in-house experimental TLR9 activity data we found that random forest algorithm outperformed other algorithms for our dataset for TLR9 activity prediction. Therefore, we developed a cross-validated ensemble classifier of 20 random forest models. The average Matthews correlation coefficient and balanced accuracy of our ensemble classifier in test samples was 0.61 and 80.0%, respectively, with the maximum balanced accuracy and Matthews correlation coefficient of 87.0% and 0.75, respectively. We confirmed common sequence motifs including 'CC', 'GG','AG', 'CCCG' and 'CGGC' were overrepresented in mTLR9 agonists. Predictions on 6000 randomly generated ODNs were ranked and the top 100 ODNs were synthesized and experimentally tested for activity in a mTLR9 reporter cell assay, with 91 of the 100 selected ODNs showing high activity, confirming the accuracy of the model in predicting mTLR9 activity. Conclusion: We combined repeated random down-sampling with random forest to overcome the class imbalance problem and achieved promising results. Overall, we showed that the random forest algorithm outperformed other machine learning algorithms including support vector machines, shrinkage discriminant analysis, gradient boosting machine and neural networks. Due to its predictive performance and simplicity, the random forest technique is a useful method for prediction of mTLR9 ODN agonists."
0,Plasmacytoid Dendritic Cells and Infected Cells Form an Interferogenic Synapse Required for Antiviral Responses,,
0,Single-Cell RNA-Seq of the Developing Cardiac Outflow Tract Reveals Convergent Development of the Vascular Smooth Muscle Cells,"Cardiac outflow tract (OFT) is a major hotspot for congenital heart diseases. A thorough understanding of the cellular diversity, transitions, and regulatory networks of normal OFT development is essential to decipher the etiology of OFT malformations. We performed single-cell transcriptomic sequencing of 55,611 mouse OFT cells from three developmental stages that generally correspond to the early, middle, and late stages of OFT remodeling and septation. Known cellular transitions, such as endothelial-to-mesenchymal transition, have been recapitulated. In particular, we identified convergent development of the vascular smooth muscle cell (VSMC) lineage where intermediate cell subpopulations were found to be involved in either myocardial-to-VSMC trans-differentiation or mesenchymal-to-VSMC transition. Finally, we uncovered transcriptional regulators potentially governing cellular transitions. Our study provides a single-cell reference map of cell states for normal OFT development and paves the way for further studies of the etiology of OFT malformations at the single-cell level. Liu et al. present single-cell transcriptomes of over 50,000 cells from the developing cardiac outflow tract in mice. They identify convergent development of the vascular smooth muscle cell (VSMC) lineage, with these cells arising either by a myocardial-to-VSMC trans-differentiation or mesenchymal-to-VSMC transition.","Single-Cell RNA-Seq of the Developing Cardiac Outflow Tract Reveals Convergent Development of the Vascular Smooth Muscle Cells. Cardiac outflow tract (OFT) is a major hotspot for congenital heart diseases. A thorough understanding of the cellular diversity, transitions, and regulatory networks of normal OFT development is essential to decipher the etiology of OFT malformations. We performed single-cell transcriptomic sequencing of 55,611 mouse OFT cells from three developmental stages that generally correspond to the early, middle, and late stages of OFT remodeling and septation. Known cellular transitions, such as endothelial-to-mesenchymal transition, have been recapitulated. In particular, we identified convergent development of the vascular smooth muscle cell (VSMC) lineage where intermediate cell subpopulations were found to be involved in either myocardial-to-VSMC trans-differentiation or mesenchymal-to-VSMC transition. Finally, we uncovered transcriptional regulators potentially governing cellular transitions. Our study provides a single-cell reference map of cell states for normal OFT development and paves the way for further studies of the etiology of OFT malformations at the single-cell level. Liu et al. present single-cell transcriptomes of over 50,000 cells from the developing cardiac outflow tract in mice. They identify convergent development of the vascular smooth muscle cell (VSMC) lineage, with these cells arising either by a myocardial-to-VSMC trans-differentiation or mesenchymal-to-VSMC transition."
0,Postoperative Delirium Is Associated with Long-term Decline in Activities of Daily Living,,
0,Utilizing Machine Learning Methods for Preoperative Prediction of Postsurgical Mortality and Intensive Care Unit Admission,"MINI: We compared the performance of machine learning models against the traditionally derived Combined Assessment of Risk Encountered in Surgery (CARES) model and the American Society of Anaesthesiologists-Physical Status (ASA-PS) in the prediction of 30-day postsurgical mortality and need for intensive care unit (ICU) stay >24 hours. Machine learning can be used to improve surgical risk prediction compared to traditional risk calculators. AUPRC should be used to evaluate model predictive performance instead of AUROC when the dataset is imbalanced.This is an open access article distributed under the terms of the Creative Commons Attribution-Non Commercial-No Derivatives License 4.0 (CCBY-NC-ND), where it is permissible to download and share the work provided it is properly cited. The work cannot be changed in any way or used commercially without permission from the journal. http://creativecommons.org/licenses/by-nc-nd/4.0 OBJECTIVE:: To compare the performance of machine learning models against the traditionally derived Combined Assessment of Risk Encountered in Surgery (CARES) model and the American Society of Anaesthesiologists-Physical Status (ASA-PS) in the prediction of 30-day postsurgical mortality and need for intensive care unit (ICU) stay >24 hours. BACKGROUND: Prediction of surgical risk preoperatively is important for clinical shared decision-making and planning of health resources such as ICU beds. The current growth of electronic medical records coupled with machine learning presents an opportunity to improve the performance of established risk models. METHODS: All patients aged 18 years and above who underwent noncardiac and nonneurological surgery at Singapore General Hospital (SGH) between 1 January 2012 and 31 October 2016 were included. Patient demographics, comorbidities, preoperative laboratory results, and surgery details were obtained from their electronic medical records. Seventy percent of the observations were randomly selected for training, leaving 30% for testing. Baseline models were CARES and ASA-PS. Candidate models were trained using random forest, adaptive boosting, gradient boosting, and support vector machine. Models were evaluated on area under the receiver operating characteristic curve (AUROC) and area under the precision-recall curve (AUPRC). RESULTS: A total of 90,785 patients were included, of whom 539 (0.6%) died within 30 days and 1264 (1.4%) required ICU admission >24 hours postoperatively. Baseline models achieved high AUROCs despite poor sensitivities by predicting all negative in a predominantly negative dataset. Gradient boosting was the best performing model with AUPRCs of 0.23 and 0.38 for mortality and ICU admission outcomes respectively. CONCLUSIONS: Machine learning can be used to improve surgical risk prediction compared to traditional risk calculators. AUPRC should be used to evaluate model predictive performance instead of AUROC when the dataset is imbalanced.This is an open access article distributed under the terms of the Creative Commons Attribution-Non Commercial-No Derivatives License 4.0 (CCBY-NC-ND), where it is permissible to download and share the work provided it is properly cited. The work cannot be changed in any way or used commercially without permission from the journal. http://creativecommons.org/licenses/by-nc-nd/4.0.","Utilizing Machine Learning Methods for Preoperative Prediction of Postsurgical Mortality and Intensive Care Unit Admission. MINI: We compared the performance of machine learning models against the traditionally derived Combined Assessment of Risk Encountered in Surgery (CARES) model and the American Society of Anaesthesiologists-Physical Status (ASA-PS) in the prediction of 30-day postsurgical mortality and need for intensive care unit (ICU) stay >24 hours. Machine learning can be used to improve surgical risk prediction compared to traditional risk calculators. AUPRC should be used to evaluate model predictive performance instead of AUROC when the dataset is imbalanced.This is an open access article distributed under the terms of the Creative Commons Attribution-Non Commercial-No Derivatives License 4.0 (CCBY-NC-ND), where it is permissible to download and share the work provided it is properly cited. The work cannot be changed in any way or used commercially without permission from the journal. http://creativecommons.org/licenses/by-nc-nd/4.0 OBJECTIVE:: To compare the performance of machine learning models against the traditionally derived Combined Assessment of Risk Encountered in Surgery (CARES) model and the American Society of Anaesthesiologists-Physical Status (ASA-PS) in the prediction of 30-day postsurgical mortality and need for intensive care unit (ICU) stay >24 hours. BACKGROUND: Prediction of surgical risk preoperatively is important for clinical shared decision-making and planning of health resources such as ICU beds. The current growth of electronic medical records coupled with machine learning presents an opportunity to improve the performance of established risk models. METHODS: All patients aged 18 years and above who underwent noncardiac and nonneurological surgery at Singapore General Hospital (SGH) between 1 January 2012 and 31 October 2016 were included. Patient demographics, comorbidities, preoperative laboratory results, and surgery details were obtained from their electronic medical records. Seventy percent of the observations were randomly selected for training, leaving 30% for testing. Baseline models were CARES and ASA-PS. Candidate models were trained using random forest, adaptive boosting, gradient boosting, and support vector machine. Models were evaluated on area under the receiver operating characteristic curve (AUROC) and area under the precision-recall curve (AUPRC). RESULTS: A total of 90,785 patients were included, of whom 539 (0.6%) died within 30 days and 1264 (1.4%) required ICU admission >24 hours postoperatively. Baseline models achieved high AUROCs despite poor sensitivities by predicting all negative in a predominantly negative dataset. Gradient boosting was the best performing model with AUPRCs of 0.23 and 0.38 for mortality and ICU admission outcomes respectively. CONCLUSIONS: Machine learning can be used to improve surgical risk prediction compared to traditional risk calculators. AUPRC should be used to evaluate model predictive performance instead of AUROC when the dataset is imbalanced.This is an open access article distributed under the terms of the Creative Commons Attribution-Non Commercial-No Derivatives License 4.0 (CCBY-NC-ND), where it is permissible to download and share the work provided it is properly cited. The work cannot be changed in any way or used commercially without permission from the journal. http://creativecommons.org/licenses/by-nc-nd/4.0."
0,Current progress in CRISPR-based diagnostic platforms,"The CRISPR-Cas system is a key technology for genome editing and regulation in a wide range of organisms and cell types. Recently, CRISPR-Casâ€“based diagnostic platform has shown idealistic properties for pathogen detection. Integrating the CRISPR-Cas platform along with lateral flow system allows rapid, sensitive, specific, cheap, and reliable diagnostic. It has the potential to be in frontline for not only pathogen detection during the epidemic outbreak, but also cancer, and genetic diseases.","Current progress in CRISPR-based diagnostic platforms. The CRISPR-Cas system is a key technology for genome editing and regulation in a wide range of organisms and cell types. Recently, CRISPR-Casâ€“based diagnostic platform has shown idealistic properties for pathogen detection. Integrating the CRISPR-Cas platform along with lateral flow system allows rapid, sensitive, specific, cheap, and reliable diagnostic. It has the potential to be in frontline for not only pathogen detection during the epidemic outbreak, but also cancer, and genetic diseases."
0,A White-Box Machine Learning Approach for Revealing Antibiotic Mechanisms of Action,"Current machine learning techniques enable robust association of biological signals with measured phenotypes, but these approaches are incapable of identifying causal relationships. Here, we develop an integrated ""white-box"" biochemical screening, network modeling, and machine learning approach for revealing causal mechanisms and apply this approach to understanding antibiotic efficacy. We counter-screen diverse metabolites against bactericidal antibiotics in Escherichia coli and simulate their corresponding metabolic states using a genome-scale metabolic network model. Regression of the measured screening data on model simulations reveals that purine biosynthesis participates in antibiotic lethality, which we validate experimentally. We show that antibiotic-induced adenine limitation increases ATP demand, which elevates central carbon metabolism activity and oxygen consumption, enhancing the killing effects of antibiotics. This work demonstrates how prospective network modeling can couple with machine learning to identify complex causal mechanisms underlying drug efficacy.","A White-Box Machine Learning Approach for Revealing Antibiotic Mechanisms of Action. Current machine learning techniques enable robust association of biological signals with measured phenotypes, but these approaches are incapable of identifying causal relationships. Here, we develop an integrated ""white-box"" biochemical screening, network modeling, and machine learning approach for revealing causal mechanisms and apply this approach to understanding antibiotic efficacy. We counter-screen diverse metabolites against bactericidal antibiotics in Escherichia coli and simulate their corresponding metabolic states using a genome-scale metabolic network model. Regression of the measured screening data on model simulations reveals that purine biosynthesis participates in antibiotic lethality, which we validate experimentally. We show that antibiotic-induced adenine limitation increases ATP demand, which elevates central carbon metabolism activity and oxygen consumption, enhancing the killing effects of antibiotics. This work demonstrates how prospective network modeling can couple with machine learning to identify complex causal mechanisms underlying drug efficacy."
0,Pseudogene DUXAP10 can be used as a diagnostic and prognostic biomarker in human cancers,"The pseudogene DUXAP10 is overexpressed in numerous types of human cancers. However, the diagnostic and prognostic value of DUXAP10 in cancers has yet to be characterized. PubMed, EMBASE, Web of Science, the Cancer Genome Atlas (TCGA), and Gene Expression Omnibus databases were comprehensively searched in this study. A total of 50 studies comprising 11,292 patients were collected in this integrated analysis. DUXAP10 was confirmed to be significantly overexpressed in various human cancers (p <.05). Summary receiver operating characteristic (SROC) curve analysis was implemented, which indicated that DUXAP10 was a potential diagnostic biomarker for human cancers (area under the curve [AUC] of SROC curve = 0.81 [0.77â€“0.84]; pooled sensitivity = 0.69 [0.62â€“0.75]; pooled specificity = 0.81 [0.73â€“0.87]). In addition, hazard ratios (HRs) with 95% confidence intervals (CIs) were obtained to evaluate the association of DUXAP10 expression with overall survival (OS) time of cancer patients. Outcomes of meta-analysis suggested that upregulation of DUXAP10 was closely associated with poor OS (pooled HR = 1.11 [1.03â€“1.18]). Our study revealed that the pseudogene DUXAP10 was upregulated in multiple types of cancers and could be a potential biomarker with good diagnostic and prognostic value for human cancers.","Pseudogene DUXAP10 can be used as a diagnostic and prognostic biomarker in human cancers. The pseudogene DUXAP10 is overexpressed in numerous types of human cancers. However, the diagnostic and prognostic value of DUXAP10 in cancers has yet to be characterized. PubMed, EMBASE, Web of Science, the Cancer Genome Atlas (TCGA), and Gene Expression Omnibus databases were comprehensively searched in this study. A total of 50 studies comprising 11,292 patients were collected in this integrated analysis. DUXAP10 was confirmed to be significantly overexpressed in various human cancers (p <.05). Summary receiver operating characteristic (SROC) curve analysis was implemented, which indicated that DUXAP10 was a potential diagnostic biomarker for human cancers (area under the curve [AUC] of SROC curve = 0.81 [0.77â€“0.84]; pooled sensitivity = 0.69 [0.62â€“0.75]; pooled specificity = 0.81 [0.73â€“0.87]). In addition, hazard ratios (HRs) with 95% confidence intervals (CIs) were obtained to evaluate the association of DUXAP10 expression with overall survival (OS) time of cancer patients. Outcomes of meta-analysis suggested that upregulation of DUXAP10 was closely associated with poor OS (pooled HR = 1.11 [1.03â€“1.18]). Our study revealed that the pseudogene DUXAP10 was upregulated in multiple types of cancers and could be a potential biomarker with good diagnostic and prognostic value for human cancers."
0,Robust CTCF-Based Chromatin Architecture Underpins Epigenetic Changes in the Heart Failure Stress-Gene Response,"BACKGROUND: The human genome folds in 3 dimensions to form thousands of chromatin loops inside the nucleus, encasing genes and cis-regulatory elements for accurate gene expression control. Physical tethers of loops are anchored by the DNA-binding protein CTCF and the cohesin ring complex. Because heart failure is characterized by hallmark gene expression changes, it was recently reported that substantial CTCF-related chromatin reorganization underpins the myocardial stress-gene response, paralleled by chromatin domain boundary changes observed in CTCF knockout. METHODS: We undertook an independent and orthogonal analysis of chromatin organization with mouse pressure-overload model of myocardial stress (transverse aortic constriction) and cardiomyocyte-specific knockout of Ctcf. We also downloaded published data sets of similar cardiac mouse models and subjected them to independent reanalysis. RESULTS: We found that the cardiomyocyte chromatin architecture remains broadly stable in transverse aortic constriction hearts, whereas Ctcf knockout resulted in approximately 99% abolition of global chromatin loops. Disease gene expression changes correlated instead with differential histone H3K27-acetylation enrichment at their respective proximal and distal interacting genomic enhancers confined within these static chromatin structures. Moreover, coregulated genes were mapped out as interconnected gene sets on the basis of their multigene 3D interactions. CONCLUSIONS: This work reveals a more stable genome-wide chromatin framework than previously described. Myocardial stress-gene transcription responds instead through H3K27-acetylation enhancer enrichment dynamics and gene networks of coregulation. Robust and intact CTCF looping is required for the induction of a rapid and accurate stress response.","Robust CTCF-Based Chromatin Architecture Underpins Epigenetic Changes in the Heart Failure Stress-Gene Response. BACKGROUND: The human genome folds in 3 dimensions to form thousands of chromatin loops inside the nucleus, encasing genes and cis-regulatory elements for accurate gene expression control. Physical tethers of loops are anchored by the DNA-binding protein CTCF and the cohesin ring complex. Because heart failure is characterized by hallmark gene expression changes, it was recently reported that substantial CTCF-related chromatin reorganization underpins the myocardial stress-gene response, paralleled by chromatin domain boundary changes observed in CTCF knockout. METHODS: We undertook an independent and orthogonal analysis of chromatin organization with mouse pressure-overload model of myocardial stress (transverse aortic constriction) and cardiomyocyte-specific knockout of Ctcf. We also downloaded published data sets of similar cardiac mouse models and subjected them to independent reanalysis. RESULTS: We found that the cardiomyocyte chromatin architecture remains broadly stable in transverse aortic constriction hearts, whereas Ctcf knockout resulted in approximately 99% abolition of global chromatin loops. Disease gene expression changes correlated instead with differential histone H3K27-acetylation enrichment at their respective proximal and distal interacting genomic enhancers confined within these static chromatin structures. Moreover, coregulated genes were mapped out as interconnected gene sets on the basis of their multigene 3D interactions. CONCLUSIONS: This work reveals a more stable genome-wide chromatin framework than previously described. Myocardial stress-gene transcription responds instead through H3K27-acetylation enhancer enrichment dynamics and gene networks of coregulation. Robust and intact CTCF looping is required for the induction of a rapid and accurate stress response."
0,"Guadecitabine (SGI-110) in patients with intermediate or high-risk myelodysplastic syndromes: phase 2 results from a multicentre, open-label, randomised, phase 1/2 trial",,
0,Network Topologies That Can Achieve Dual Function of Adaptation and Noise Attenuation,"Many signaling systems execute adaptation under circumstances that require noise attenuation. Here, we identify an intrinsic trade-off existing between sensitivity and noise attenuation in the three-node networks. We demonstrate that although fine-tuning timescales in three-node adaptive networks can partially mediate this trade-off in this context, it prolongs adaptation time and imposes unrealistic parameter constraints. By contrast, four-node networks can effectively decouple adaptation and noise attenuation to achieve dual function without a trade-off, provided that these functions are executed sequentially. We illustrate ideas in seven biological examples, including Dictyostelium discoideum chemotaxis and the p53 signaling network and find that adaptive networks are often associated with a noise attenuation module. Our approach may be applicable to finding network design principles for other dual and multiple functions.","Network Topologies That Can Achieve Dual Function of Adaptation and Noise Attenuation. Many signaling systems execute adaptation under circumstances that require noise attenuation. Here, we identify an intrinsic trade-off existing between sensitivity and noise attenuation in the three-node networks. We demonstrate that although fine-tuning timescales in three-node adaptive networks can partially mediate this trade-off in this context, it prolongs adaptation time and imposes unrealistic parameter constraints. By contrast, four-node networks can effectively decouple adaptation and noise attenuation to achieve dual function without a trade-off, provided that these functions are executed sequentially. We illustrate ideas in seven biological examples, including Dictyostelium discoideum chemotaxis and the p53 signaling network and find that adaptive networks are often associated with a noise attenuation module. Our approach may be applicable to finding network design principles for other dual and multiple functions."
0,Prediction of acid radical ion binding residues by K-nearest neighbors classifier,"Background: Proteins perform their functions by interacting with acid radical ions. Recently, it was a challenging work to precisely predict the binding residues of acid radical ion ligands in the research field of molecular drug design. Results: In this study, we proposed an improved method to predict the acid radical ion binding residues by using K-nearest Neighbors classifier. Meanwhile, we constructed datasets of four acid radical ion ligand (NO2-, CO32-, SO42-, PO43-) binding residues from BioLip database. Then, based on the optimal window length for each acid radical ion ligand, we refined composition information and position conservative information and extracted them as feature parameters for K-nearest Neighbors classifier. In the results of 5-fold cross-validation, the Matthew's correlation coefficient was higher than 0.45, the values of accuracy, sensitivity and specificity were all higher than 69.2%, and the false positive rate was lower than 30.8%. Further, we also performed an independent test to test the practicability of the proposed method. In the obtained results, the sensitivity was higher than 40.9%, the values of accuracy and specificity were higher than 84.2%, the Matthew's correlation coefficient was higher than 0.116, and the false positive rate was lower than 15.4%. Finally, we identified binding residues of the six metal ion ligands. In the predicted results, the values of accuracy, sensitivity and specificity were all higher than 77.6%, the Matthew's correlation coefficient was higher than 0.6, and the false positive rate was lower than 19.6%. Conclusions: Taken together, the good results of our prediction method added new insights in the prediction of the binding residues of acid radical ion ligands.","Prediction of acid radical ion binding residues by K-nearest neighbors classifier. Background: Proteins perform their functions by interacting with acid radical ions. Recently, it was a challenging work to precisely predict the binding residues of acid radical ion ligands in the research field of molecular drug design. Results: In this study, we proposed an improved method to predict the acid radical ion binding residues by using K-nearest Neighbors classifier. Meanwhile, we constructed datasets of four acid radical ion ligand (NO2-, CO32-, SO42-, PO43-) binding residues from BioLip database. Then, based on the optimal window length for each acid radical ion ligand, we refined composition information and position conservative information and extracted them as feature parameters for K-nearest Neighbors classifier. In the results of 5-fold cross-validation, the Matthew's correlation coefficient was higher than 0.45, the values of accuracy, sensitivity and specificity were all higher than 69.2%, and the false positive rate was lower than 30.8%. Further, we also performed an independent test to test the practicability of the proposed method. In the obtained results, the sensitivity was higher than 40.9%, the values of accuracy and specificity were higher than 84.2%, the Matthew's correlation coefficient was higher than 0.116, and the false positive rate was lower than 15.4%. Finally, we identified binding residues of the six metal ion ligands. In the predicted results, the values of accuracy, sensitivity and specificity were all higher than 77.6%, the Matthew's correlation coefficient was higher than 0.6, and the false positive rate was lower than 19.6%. Conclusions: Taken together, the good results of our prediction method added new insights in the prediction of the binding residues of acid radical ion ligands."
0,Evaluating Modeling and Validation Strategies for Tooth Loss,,
0,The alexipharmic mechanisms of five licorice ingredients involved in CYP450 and Nrf2 pathways in paraquat-induced mice acute lung injury,"Oxidative stress is an important mechanism in acute lung injury (ALI) induced by paraquat (PQ), one of the most widely used herbicides in developing countries. In clinical prophylaxis and treatment, licorice is a widely used herbal medicine in China due to its strong alexipharmic characteristics. However, the corresponding biochemical mechanism of antioxidation and detoxification enzymes induced by licorice's ingredients is still not fully demonstrated. In this study, the detoxification effect of licorice was evaluated in vivo and in vitro. The detoxification and antioxidation effect of its active ingredients involved in the treatment was screened systematically according to Absorption, Distribution, Metabolism, and Excretion (ADME): Predictions and evidence-based literature mining methods in silico approach. Data shows that licorice alleviate pulmonary edema and fibrosis, decrease Malondialdehyde (MDA) contents and increase Superoxide Dismutase (SOD) activity in PQ-induced ALI mice, protect the morphologic appearance of lung tissues, induce cytochrome 3A4 (CYA3A4) and Nuclear factor erythroid 2-related factor 2 (Nrf2) expression to active detoxification pathways, reduce the accumulation of PQ in vivo, protect or improve the liver and renal function of mice, and increase the survival rate. The 104 genes of PPI network contained all targets of licorice ingredients and PQ, which displayed the two redox regulatory enzymatic group modules cytochrome P450 (CYP450) and Nrf2 via a score-related graphic theoretic clustering algorithm in silico. According to ADME properties, glycyrol, isolicoflavonol, licochalcone A, 18beta-glycyrrhetinic acid, and licoisoflavone A were employed due to their oral bioavailability OB â‰¤ 30%, drug-likeness DL â‰¤ 0 1, and being highly associated with CYP450 and Nrf2 pathways, as potential activators to halt PQ-induced cells death in vitro. Both 3A4 inhibitor and silenced Nrf2 gene decreased the alexipharmic effects of those ingredients significantly. All these disclosed the detoxification and antioxidation effects of licorice on acute lung injury induced by PQ, and glycyrol, isolicoflavonol, licochalcone A, 18beta-glycyrrhetinic acid, and licoisoflavone A upregulated CYP450 and Nrf2 pathways underlying the alexipharmic mechanisms of licorice.","The alexipharmic mechanisms of five licorice ingredients involved in CYP450 and Nrf2 pathways in paraquat-induced mice acute lung injury. Oxidative stress is an important mechanism in acute lung injury (ALI) induced by paraquat (PQ), one of the most widely used herbicides in developing countries. In clinical prophylaxis and treatment, licorice is a widely used herbal medicine in China due to its strong alexipharmic characteristics. However, the corresponding biochemical mechanism of antioxidation and detoxification enzymes induced by licorice's ingredients is still not fully demonstrated. In this study, the detoxification effect of licorice was evaluated in vivo and in vitro. The detoxification and antioxidation effect of its active ingredients involved in the treatment was screened systematically according to Absorption, Distribution, Metabolism, and Excretion (ADME): Predictions and evidence-based literature mining methods in silico approach. Data shows that licorice alleviate pulmonary edema and fibrosis, decrease Malondialdehyde (MDA) contents and increase Superoxide Dismutase (SOD) activity in PQ-induced ALI mice, protect the morphologic appearance of lung tissues, induce cytochrome 3A4 (CYA3A4) and Nuclear factor erythroid 2-related factor 2 (Nrf2) expression to active detoxification pathways, reduce the accumulation of PQ in vivo, protect or improve the liver and renal function of mice, and increase the survival rate. The 104 genes of PPI network contained all targets of licorice ingredients and PQ, which displayed the two redox regulatory enzymatic group modules cytochrome P450 (CYP450) and Nrf2 via a score-related graphic theoretic clustering algorithm in silico. According to ADME properties, glycyrol, isolicoflavonol, licochalcone A, 18beta-glycyrrhetinic acid, and licoisoflavone A were employed due to their oral bioavailability OB â‰¤ 30%, drug-likeness DL â‰¤ 0 1, and being highly associated with CYP450 and Nrf2 pathways, as potential activators to halt PQ-induced cells death in vitro. Both 3A4 inhibitor and silenced Nrf2 gene decreased the alexipharmic effects of those ingredients significantly. All these disclosed the detoxification and antioxidation effects of licorice on acute lung injury induced by PQ, and glycyrol, isolicoflavonol, licochalcone A, 18beta-glycyrrhetinic acid, and licoisoflavone A upregulated CYP450 and Nrf2 pathways underlying the alexipharmic mechanisms of licorice."
0,Tetrahydroxy stilbene glucoside alleviates palmitic acid-induced inflammation and apoptosis in cardiomyocytes by regulating miR-129-3p/Smad3 signaling,"Objective: Tetrahydroxy stilbene glucoside (TSG) has been reported to exert a cytoprotective effect against various toxicants. However, the function and mechanism of TSG in palmitic acid (PA)-induced inflammation and apoptosis in cardiomyocytes are still unknown. The present study was designed to investigate the post-transcriptional mechanism in TSG-treated cardiomyocytesâ€™ inflammation and apoptosis induced by PA. Methods: The mRNA and protein levels were assayed by reverse transcription-quantitative polymerase chain reaction (RT-qPCR) and western blotting, respectively. The targeted genes were predicted by a bioinformatics algorithm and confirmed by a dual luciferase reporter assay. Cell proliferation was analyzed by CCK-8 assay. Annexin V-fluorescein isothiocyanate/polyimide (annexin V-FITC/PI) staining was used to evaluate apoptosis using flow cytometry. Results: TSG restricted the detrimental effects, including the activated inflammatory response and apoptosis, of PA in cardiomyocytes, as well as the up-regulation of miR-129-3p and down-regulation of p-Smad3 expression. In addition, bioinformatics and experimental analysis suggested that Smad3 was a direct target of miR-129-3p, which could inhibit or enhance the expression of p-Smad by transfection with miR-129-3p mimics or inhibitors, respectively. Furthermore, our results demonstrated that overexpression of Smad3 reversed the inhibition of inflammation and apoptosis by overexpression of miR-129-3p in PA-stimulated cardiomyocytes. Conclusion: TSG targeted to miR-129-3p/Smad3 signaling inhibited PA-induced inflammation and apoptosis in cardiomyocytes.","Tetrahydroxy stilbene glucoside alleviates palmitic acid-induced inflammation and apoptosis in cardiomyocytes by regulating miR-129-3p/Smad3 signaling. Objective: Tetrahydroxy stilbene glucoside (TSG) has been reported to exert a cytoprotective effect against various toxicants. However, the function and mechanism of TSG in palmitic acid (PA)-induced inflammation and apoptosis in cardiomyocytes are still unknown. The present study was designed to investigate the post-transcriptional mechanism in TSG-treated cardiomyocytesâ€™ inflammation and apoptosis induced by PA. Methods: The mRNA and protein levels were assayed by reverse transcription-quantitative polymerase chain reaction (RT-qPCR) and western blotting, respectively. The targeted genes were predicted by a bioinformatics algorithm and confirmed by a dual luciferase reporter assay. Cell proliferation was analyzed by CCK-8 assay. Annexin V-fluorescein isothiocyanate/polyimide (annexin V-FITC/PI) staining was used to evaluate apoptosis using flow cytometry. Results: TSG restricted the detrimental effects, including the activated inflammatory response and apoptosis, of PA in cardiomyocytes, as well as the up-regulation of miR-129-3p and down-regulation of p-Smad3 expression. In addition, bioinformatics and experimental analysis suggested that Smad3 was a direct target of miR-129-3p, which could inhibit or enhance the expression of p-Smad by transfection with miR-129-3p mimics or inhibitors, respectively. Furthermore, our results demonstrated that overexpression of Smad3 reversed the inhibition of inflammation and apoptosis by overexpression of miR-129-3p in PA-stimulated cardiomyocytes. Conclusion: TSG targeted to miR-129-3p/Smad3 signaling inhibited PA-induced inflammation and apoptosis in cardiomyocytes."
0,Fostering a healthy ai ecosystem for radiology: Conclusions of the 2018 rsna summit on ai in radiology,"The 2018 RSNA Summit on AI in Radiology brought together a diverse group of stakeholders to identify and prioritize areas of need related to artificial intelligence in radiology. This article presents the proceedings of the summit with emphasis on RSNAâ€™s role in leading, organizing, and catalyzing change during this important time in radiology.","Fostering a healthy ai ecosystem for radiology: Conclusions of the 2018 rsna summit on ai in radiology. The 2018 RSNA Summit on AI in Radiology brought together a diverse group of stakeholders to identify and prioritize areas of need related to artificial intelligence in radiology. This article presents the proceedings of the summit with emphasis on RSNAâ€™s role in leading, organizing, and catalyzing change during this important time in radiology."
0,Viral Infections Exacerbate FUS-ALS Phenotypes in iPSC-Derived Spinal Neurons in a Virus Species-Specific Manner,"Amyotrophic lateral sclerosis (ALS) arises from an interplay of genetic mutations and environmental factors. ssRNA viruses are possible ALS risk factors, but testing their interaction with mutations such as in FUS, which encodes an RNA-binding protein, has been difficult due to the lack of a human disease model. Here, we use isogenic induced pluripotent stem cell (iPSC)-derived spinal neurons (SNs) to investigate the interaction between ssRNA viruses and mutant FUS. We find that rabies virus (RABV) spreads ALS phenotypes, including the formation of stress granules (SGs) with aberrant composition due to increased levels of FUS protein, as well as neurodegeneration and reduced restriction activity by FUS mutations. Consistent with this, iPSC-derived SNs harboring mutant FUS are more sensitive to human immunodeficiency virus (HIV-1) and Zika viruses (ZIKV). We demonstrate that RABV and HIV-1 exacerbate cytoplasmic mislocalization of FUS. Our results demonstrate that viral infections worsen ALS pathology in SNs with genetic risk factors, suggesting a novel role for viruses in modulating patient phenotypes.","Viral Infections Exacerbate FUS-ALS Phenotypes in iPSC-Derived Spinal Neurons in a Virus Species-Specific Manner. Amyotrophic lateral sclerosis (ALS) arises from an interplay of genetic mutations and environmental factors. ssRNA viruses are possible ALS risk factors, but testing their interaction with mutations such as in FUS, which encodes an RNA-binding protein, has been difficult due to the lack of a human disease model. Here, we use isogenic induced pluripotent stem cell (iPSC)-derived spinal neurons (SNs) to investigate the interaction between ssRNA viruses and mutant FUS. We find that rabies virus (RABV) spreads ALS phenotypes, including the formation of stress granules (SGs) with aberrant composition due to increased levels of FUS protein, as well as neurodegeneration and reduced restriction activity by FUS mutations. Consistent with this, iPSC-derived SNs harboring mutant FUS are more sensitive to human immunodeficiency virus (HIV-1) and Zika viruses (ZIKV). We demonstrate that RABV and HIV-1 exacerbate cytoplasmic mislocalization of FUS. Our results demonstrate that viral infections worsen ALS pathology in SNs with genetic risk factors, suggesting a novel role for viruses in modulating patient phenotypes."
0,S-9. PEV-induced hp1a propagation does not correlate with the expression of the genes located near the euheterochromatin breakpoint,"Position effect variegation (PEV) is a disturbance of the expression of euchromatic genes transferred into the heterochromatin vicinity caused by the changes in its chromatin organization (heterochromatinization). Little is known about the molecular mechanisms of interactions between gene transcription machinery and the large-scale chromatin structures like heterochromatin, and the chromosomal rearrangement In(2)A4 provide a convenient model to study PEV. The aim of our work was to track the changes in chromatin organization of euchromatin in the vicinity of In(2)A4 new eu-heterochromatin borders and analyze the possible correlations between chromatin changes and the functional organization of the affected regions. Methods: Weâ€™ve performed analysis of genome-wide HP1a distribution in In(2)A4/ In(2)A4 homozygous flies and in the control wild type flies by ChIP-Seq with qPCR verification and bioinformatic analysis of the received data. Results: In(2)A4 rearrangement is an inversion in the left arm of chromosome 2 with a breakpoint in the satellite block in the 2L pericentromeric heterochromatin. This results in two new eu-heterochromatin boundaries â€“ one near the main block of 2L heterochromatin and another one near the separated small heterochromatin block. ChIP-Seq data on HP1a distribution shows an enrichment for HP1a in the euchromatin regions near the new eu-heterochromatin borders. HP1a spreads up to 200 kb from the main pericentromeric block and up to 50 kb from the small block. No apparent correlation between HP1a enrichment and genes expression levels (studied in [1]) or gene amenability to PEV were detected. The unusual enrichment in HP1a immediately near the small separated heterochromatin block was observed. Conclusions: In In(2)A4, HP1a propagates at a distance of up to 200 kb from the breakpoints and there is no apparent correlation between HP1a enrichment and expression levels of genes in the affected region as well as no correlation between HP1a binding and sensitivity of any particular gene to heterochroma-tin repression. It seems that HP1a propagation occurs independently of local chromatin organization defined by regulatory elements.","S-9. PEV-induced hp1a propagation does not correlate with the expression of the genes located near the euheterochromatin breakpoint. Position effect variegation (PEV) is a disturbance of the expression of euchromatic genes transferred into the heterochromatin vicinity caused by the changes in its chromatin organization (heterochromatinization). Little is known about the molecular mechanisms of interactions between gene transcription machinery and the large-scale chromatin structures like heterochromatin, and the chromosomal rearrangement In(2)A4 provide a convenient model to study PEV. The aim of our work was to track the changes in chromatin organization of euchromatin in the vicinity of In(2)A4 new eu-heterochromatin borders and analyze the possible correlations between chromatin changes and the functional organization of the affected regions. Methods: Weâ€™ve performed analysis of genome-wide HP1a distribution in In(2)A4/ In(2)A4 homozygous flies and in the control wild type flies by ChIP-Seq with qPCR verification and bioinformatic analysis of the received data. Results: In(2)A4 rearrangement is an inversion in the left arm of chromosome 2 with a breakpoint in the satellite block in the 2L pericentromeric heterochromatin. This results in two new eu-heterochromatin boundaries â€“ one near the main block of 2L heterochromatin and another one near the separated small heterochromatin block. ChIP-Seq data on HP1a distribution shows an enrichment for HP1a in the euchromatin regions near the new eu-heterochromatin borders. HP1a spreads up to 200 kb from the main pericentromeric block and up to 50 kb from the small block. No apparent correlation between HP1a enrichment and genes expression levels (studied in [1]) or gene amenability to PEV were detected. The unusual enrichment in HP1a immediately near the small separated heterochromatin block was observed. Conclusions: In In(2)A4, HP1a propagates at a distance of up to 200 kb from the breakpoints and there is no apparent correlation between HP1a enrichment and expression levels of genes in the affected region as well as no correlation between HP1a binding and sensitivity of any particular gene to heterochroma-tin repression. It seems that HP1a propagation occurs independently of local chromatin organization defined by regulatory elements."
0,"Role of astrocytes, microglia, and tanycytes in brain control of systemic metabolism",,
0,Mitral stenosis found after eye problem,,
0,Graph Convolutions on Spectral Embeddings for Cortical Surface Parcellation,,
0,Simulation-based training of junior doctors in handling critically ill patients facilitates the transition to clinical practice: an interview study,"BACKGROUND: Junior doctors lack confidence and competence in handling the critically ill patient including diagnostic skills, decision-making and team working with other health care professionals. Simulation-based training on managing emergency situations can have substantial effects on satisfaction and learning. However, there are indications of problems when applying learned skills to practice. Our aim was to identify first-year doctors' perceptions, reflections and experiences on transfer of skills to a clinical setting after simulation-based training in handling critically ill patients. METHODS: We used a qualitative approach and conducted semi-structured telephone interviews with a sample of twenty first-year doctors six months after a 4-day simulation-based training course in handling critically ill patients. Interviews were transcribed verbatim. A content-analysis approach was used to analyse the data. RESULTS: The following main themes were identified from the interviews: preparedness for clinical practice, organisational readiness, use of algorithms, communication, teamwork, situational awareness and decision making. The doctors gave several examples of simulation-based training increasing their preparedness for clinical practice and handling the critically ill patient. The usefulness of algorithms and the appreciation of non-technical skills were highlighted and found to be helpful in managing clinical difficulties. Concern was expressed related to staff willingness and preparedness in using these tools. CONCLUSIONS: Overall, the simulation-based training seemed to facilitate the transition from being a medical student to become a junior doctor. The doctors experienced an ability to transfer the use of algorithms and non-technical skills trained in the simulated environment to the clinical environment. However, the application of these skills was more difficult if these skills were unfamiliar to the surrounding clinical staff. TRIAL REGISTRATION: Not applicable.","Simulation-based training of junior doctors in handling critically ill patients facilitates the transition to clinical practice: an interview study. BACKGROUND: Junior doctors lack confidence and competence in handling the critically ill patient including diagnostic skills, decision-making and team working with other health care professionals. Simulation-based training on managing emergency situations can have substantial effects on satisfaction and learning. However, there are indications of problems when applying learned skills to practice. Our aim was to identify first-year doctors' perceptions, reflections and experiences on transfer of skills to a clinical setting after simulation-based training in handling critically ill patients. METHODS: We used a qualitative approach and conducted semi-structured telephone interviews with a sample of twenty first-year doctors six months after a 4-day simulation-based training course in handling critically ill patients. Interviews were transcribed verbatim. A content-analysis approach was used to analyse the data. RESULTS: The following main themes were identified from the interviews: preparedness for clinical practice, organisational readiness, use of algorithms, communication, teamwork, situational awareness and decision making. The doctors gave several examples of simulation-based training increasing their preparedness for clinical practice and handling the critically ill patient. The usefulness of algorithms and the appreciation of non-technical skills were highlighted and found to be helpful in managing clinical difficulties. Concern was expressed related to staff willingness and preparedness in using these tools. CONCLUSIONS: Overall, the simulation-based training seemed to facilitate the transition from being a medical student to become a junior doctor. The doctors experienced an ability to transfer the use of algorithms and non-technical skills trained in the simulated environment to the clinical environment. However, the application of these skills was more difficult if these skills were unfamiliar to the surrounding clinical staff. TRIAL REGISTRATION: Not applicable."
0,Yellow vests protests: facial injuries from rubber bullets,,
0,Immunity to Severe Malaria: PfEMP1 Tags Tell a Tale,,
0,Platelet protein disulfide isomerase promotes glycoprotein ib&-mediated platelet-neutrophil interactions under thromboinflammatory conditions,"Background: Platelet-neutrophil interactions contribute to vascular occlusion and tissue damage in thromboinflammatory disease. Platelet glycoprotein Ib& (GPIb&), a key receptor for the cell-cell interaction, is believed to be constitutively active for ligand binding. Here, we established the role of platelet-derived protein disulfide isomerase (PDI) in reducing the allosteric disulfide bonds in GPIb& and enhancing the ligand-binding activity under thromboinflammatory conditions. Methods: Bioinformatic analysis identified 2 potential allosteric disulfide bonds in GPIb&. Agglutination assays, flow cytometry, surface plasmon resonance analysis, a protein-protein docking model, proximity ligation assays, and mass spectrometry were used to demonstrate a direct interaction between PDI and GPIb& and to determine a role for PDI in regulating GPIb& function and platelet-neutrophil interactions. Also, real-Time microscopy and animal disease models were used to study the pathophysiological role of PDI-GPIb& signaling under thromboinflammatory conditions. Results: Deletion or inhibition of platelet PDI significantly reduced GPIb&-mediated platelet agglutination. Studies using PDI-null platelets and recombinant PDI or Anfibatide, a clinical-stage GPIb& inhibitor, revealed that the oxidoreductase activity of platelet surface-bound PDI was required for the ligand-binding function of GPIb&. PDI directly bound to the extracellular domain of GPIb& on the platelet surface and reduced the Cys4-Cys17 and Cys209-Cys248 disulfide bonds. Real-Time microscopy with platelet-specific PDI conditional knockout and sickle cell disease mice demonstrated that PDI-regulated GPIb& function was essential for platelet-neutrophil interactions and vascular occlusion under thromboinflammatory conditions. Studies using a mouse model of ischemia/reperfusion-induced stroke indicated that PDI-GPIb& signaling played a crucial role in tissue damage. Conclusions: Our results demonstrate that PDI-facilitated cleavage of the allosteric disulfide bonds tightly regulates GPIb& function, promoting platelet-neutrophil interactions, vascular occlusion, and tissue damage under thromboinflammatory conditions.","Platelet protein disulfide isomerase promotes glycoprotein ib&-mediated platelet-neutrophil interactions under thromboinflammatory conditions. Background: Platelet-neutrophil interactions contribute to vascular occlusion and tissue damage in thromboinflammatory disease. Platelet glycoprotein Ib& (GPIb&), a key receptor for the cell-cell interaction, is believed to be constitutively active for ligand binding. Here, we established the role of platelet-derived protein disulfide isomerase (PDI) in reducing the allosteric disulfide bonds in GPIb& and enhancing the ligand-binding activity under thromboinflammatory conditions. Methods: Bioinformatic analysis identified 2 potential allosteric disulfide bonds in GPIb&. Agglutination assays, flow cytometry, surface plasmon resonance analysis, a protein-protein docking model, proximity ligation assays, and mass spectrometry were used to demonstrate a direct interaction between PDI and GPIb& and to determine a role for PDI in regulating GPIb& function and platelet-neutrophil interactions. Also, real-Time microscopy and animal disease models were used to study the pathophysiological role of PDI-GPIb& signaling under thromboinflammatory conditions. Results: Deletion or inhibition of platelet PDI significantly reduced GPIb&-mediated platelet agglutination. Studies using PDI-null platelets and recombinant PDI or Anfibatide, a clinical-stage GPIb& inhibitor, revealed that the oxidoreductase activity of platelet surface-bound PDI was required for the ligand-binding function of GPIb&. PDI directly bound to the extracellular domain of GPIb& on the platelet surface and reduced the Cys4-Cys17 and Cys209-Cys248 disulfide bonds. Real-Time microscopy with platelet-specific PDI conditional knockout and sickle cell disease mice demonstrated that PDI-regulated GPIb& function was essential for platelet-neutrophil interactions and vascular occlusion under thromboinflammatory conditions. Studies using a mouse model of ischemia/reperfusion-induced stroke indicated that PDI-GPIb& signaling played a crucial role in tissue damage. Conclusions: Our results demonstrate that PDI-facilitated cleavage of the allosteric disulfide bonds tightly regulates GPIb& function, promoting platelet-neutrophil interactions, vascular occlusion, and tissue damage under thromboinflammatory conditions."
0,"Comment on ""Robot-assisted Versus Laparoscopic Surgery for Rectal Cancer: A Phase II Open Label Prospective Randomized Controlled Trial""",,
0,Multiplexed peroxidase-based electron microscopy labeling enables simultaneous visualization of multiple cell types,,
0,Identification of diagnostic biomarker in patients with gestational diabetes mellitus based on transcriptome-wide gene expression and pattern recognition,"Gestational diabetes mellitus (GDM) is becoming a growing threat for all pregnancies. In this study, we set up an automatic screening method combining both transcriptomic databases and support vector machine (SVM)-based pattern recognition to select biomarkers that can be used in predicting and preventing GDM for gravidas. We screened 63 samples (32 GDM samples and 31 normal controls) in GEO database for the GDM-specific biomarkers. Differentially expressed genes between patients with GDM and normal controls were picked out using edgeR package. Enrichment analysis was performed using database for annotation, visualization, and integrated discovery. The regulatory gene network was constructed based on the KEGG pathway database. Genes in the hub of the network were selected as specific biomarkers of GDM and further validated through document investigation. Finally, the GDM prediction model was verified using the SVMs. In total, 189 probes corresponding to 69 genes that differentially expressed between GDM and controls were screened out by edgeR package. Nineteen pathways were clustered by KEGG enrichment analysis and were integrated into a regulatory network containing 572 nodes and 1874 edges. The intersection of 50 hub genes extracted from the network and 69 differential genes picked out by edgeR was a collection of six genes, including members of HLA superfamily. In the SVM model, the six genes had a good capacity of predicting GDM in both the training data set (area under curve [AUC] is 0.781) and the testing data set (AUC is 0.710) and had been reported to be associated with GDM. We found that the collection of six genes can be potentially applied as a biomarker for GDM diagnosis.","Identification of diagnostic biomarker in patients with gestational diabetes mellitus based on transcriptome-wide gene expression and pattern recognition. Gestational diabetes mellitus (GDM) is becoming a growing threat for all pregnancies. In this study, we set up an automatic screening method combining both transcriptomic databases and support vector machine (SVM)-based pattern recognition to select biomarkers that can be used in predicting and preventing GDM for gravidas. We screened 63 samples (32 GDM samples and 31 normal controls) in GEO database for the GDM-specific biomarkers. Differentially expressed genes between patients with GDM and normal controls were picked out using edgeR package. Enrichment analysis was performed using database for annotation, visualization, and integrated discovery. The regulatory gene network was constructed based on the KEGG pathway database. Genes in the hub of the network were selected as specific biomarkers of GDM and further validated through document investigation. Finally, the GDM prediction model was verified using the SVMs. In total, 189 probes corresponding to 69 genes that differentially expressed between GDM and controls were screened out by edgeR package. Nineteen pathways were clustered by KEGG enrichment analysis and were integrated into a regulatory network containing 572 nodes and 1874 edges. The intersection of 50 hub genes extracted from the network and 69 differential genes picked out by edgeR was a collection of six genes, including members of HLA superfamily. In the SVM model, the six genes had a good capacity of predicting GDM in both the training data set (area under curve [AUC] is 0.781) and the testing data set (AUC is 0.710) and had been reported to be associated with GDM. We found that the collection of six genes can be potentially applied as a biomarker for GDM diagnosis."
0,GRUU-Net: Integrated convolutional and gated recurrent neural network for cell segmentation,"Cell segmentation in microscopy images is a common and challenging task. In recent years, deep neural networks achieved remarkable improvements in the field of computer vision. The dominant paradigm in segmentation is using convolutional neural networks, less common are recurrent neural networks. In this work, we propose a new deep learning method for cell segmentation, which integrates convolutional neural networks and gated recurrent neural networks over multiple image scales to exploit the strength of both types of networks. To increase the robustness of the training and improve segmentation, we introduce a novel focal loss function. We also present a distributed scheme for optimized training of the integrated neural network. We applied our proposed method to challenging data of glioblastoma cell nuclei and performed a quantitative comparison with state-of-the-art methods. Insights on how our extensions affect training and inference are also provided. Moreover, we benchmarked our method using a wide spectrum of all 22 real microscopy datasets of the Cell Tracking Challenge.","GRUU-Net: Integrated convolutional and gated recurrent neural network for cell segmentation. Cell segmentation in microscopy images is a common and challenging task. In recent years, deep neural networks achieved remarkable improvements in the field of computer vision. The dominant paradigm in segmentation is using convolutional neural networks, less common are recurrent neural networks. In this work, we propose a new deep learning method for cell segmentation, which integrates convolutional neural networks and gated recurrent neural networks over multiple image scales to exploit the strength of both types of networks. To increase the robustness of the training and improve segmentation, we introduce a novel focal loss function. We also present a distributed scheme for optimized training of the integrated neural network. We applied our proposed method to challenging data of glioblastoma cell nuclei and performed a quantitative comparison with state-of-the-art methods. Insights on how our extensions affect training and inference are also provided. Moreover, we benchmarked our method using a wide spectrum of all 22 real microscopy datasets of the Cell Tracking Challenge."
0,Dysregulation of RNA Splicing in Tauopathies,"Pathological aggregation of RNA binding proteins (RBPs) is associated with dysregulation of RNA splicing in PS19 P301S tau transgenic mice and in Alzheimer's disease brain tissues. The dysregulated splicing particularly affects genes involved in synaptic transmission. The effects of neuroprotective TIA1 reduction on PS19 mice are also examined. TIA1 reduction reduces disease-linked alternative splicing events for the major synaptic mRNA transcripts examined, suggesting that normalization of RBP functions is associated with the neuroprotection. Use of the NetDecoder informatics algorithm identifies key upstream biological targets, including MYC and EGFR, underlying the transcriptional and splicing changes in the protected compared to tauopathy mice. Pharmacological inhibition of MYC and EGFR activity in neuronal cultures tau recapitulates the neuroprotective effects of TIA1 reduction. These results demonstrate that dysfunction of RBPs and RNA splicing processes are major elements of the pathophysiology of tauopathies, as well as potential therapeutic targets for tauopathies.","Dysregulation of RNA Splicing in Tauopathies. Pathological aggregation of RNA binding proteins (RBPs) is associated with dysregulation of RNA splicing in PS19 P301S tau transgenic mice and in Alzheimer's disease brain tissues. The dysregulated splicing particularly affects genes involved in synaptic transmission. The effects of neuroprotective TIA1 reduction on PS19 mice are also examined. TIA1 reduction reduces disease-linked alternative splicing events for the major synaptic mRNA transcripts examined, suggesting that normalization of RBP functions is associated with the neuroprotection. Use of the NetDecoder informatics algorithm identifies key upstream biological targets, including MYC and EGFR, underlying the transcriptional and splicing changes in the protected compared to tauopathy mice. Pharmacological inhibition of MYC and EGFR activity in neuronal cultures tau recapitulates the neuroprotective effects of TIA1 reduction. These results demonstrate that dysfunction of RBPs and RNA splicing processes are major elements of the pathophysiology of tauopathies, as well as potential therapeutic targets for tauopathies."
0,Chemogenomic Analysis of the Druggable Kinome and Its Application to Repositioning and Lead Identification Studies,"Owing to the intrinsic polypharmacological nature of most small-molecule kinase inhibitors, there is a need for computational models that enable systematic exploration of the chemogenomic landscape underlying druggable kinome toward more efficient kinome-profiling strategies. We implemented VirtualKinomeProfiler, an efficient computational platform that captures distinct representations of chemical similarity space of the druggable kinome for various drug discovery endeavors. By using the computational platform, we profiled approximately 37 million compound-kinase pairs and made predictions for 151,708 compounds in terms of their repositioning and lead molecule potential, against 248 kinases simultaneously. Experimental testing with biochemical assays validated 51 of the predicted interactions, identifying 19 small-molecule inhibitors of EGFR, HCK, FLT1, and MSK1 protein kinases. The prediction model led to a 1.5-fold increase in precision and 2.8-fold decrease in false-discovery rate, when compared with traditional single-dose biochemical screening, which demonstrates its potential to drastically expedite the kinome-specific drug discovery process. The virtual kinome profiling (VKP) platform uses compound-kinase interaction information to prioritize potent activities for further pre-clinical evaluation. The platform uses the chemogenomic relationships of kinases to expedite the kinase inhibitor screening process, as demonstrated by several case examples. The platform and the accompanying datasets are implemented as a one-click web tool.","Chemogenomic Analysis of the Druggable Kinome and Its Application to Repositioning and Lead Identification Studies. Owing to the intrinsic polypharmacological nature of most small-molecule kinase inhibitors, there is a need for computational models that enable systematic exploration of the chemogenomic landscape underlying druggable kinome toward more efficient kinome-profiling strategies. We implemented VirtualKinomeProfiler, an efficient computational platform that captures distinct representations of chemical similarity space of the druggable kinome for various drug discovery endeavors. By using the computational platform, we profiled approximately 37 million compound-kinase pairs and made predictions for 151,708 compounds in terms of their repositioning and lead molecule potential, against 248 kinases simultaneously. Experimental testing with biochemical assays validated 51 of the predicted interactions, identifying 19 small-molecule inhibitors of EGFR, HCK, FLT1, and MSK1 protein kinases. The prediction model led to a 1.5-fold increase in precision and 2.8-fold decrease in false-discovery rate, when compared with traditional single-dose biochemical screening, which demonstrates its potential to drastically expedite the kinome-specific drug discovery process. The virtual kinome profiling (VKP) platform uses compound-kinase interaction information to prioritize potent activities for further pre-clinical evaluation. The platform uses the chemogenomic relationships of kinases to expedite the kinase inhibitor screening process, as demonstrated by several case examples. The platform and the accompanying datasets are implemented as a one-click web tool."
0,Multi-omics biomarker pipeline reveals elevated levels of protein-glutamine gammaglutamyltransferase 4 in seminal plasma of prostate cancer patients,"Seminal plasma, because of its proximity to prostate, is a promising fluid for biomarker discovery and noninvasive diagnostics. In this study, we investigated if seminal plasma proteins could increase diagnostic specificity of detecting primary prostate cancer and discriminate between high- and low-grade cancers. To select 147 most promising biomarker candidates, we combined proteins identified through five independent experimental or data mining approaches: tissue transcriptomics, seminal plasma proteomics, cell line secretomics, tissue specificity, and androgen regulation. A rigorous biomarker development pipeline based on selected reaction monitoring assays was designed to evaluate the most promising candidates. As a result, we qualified 76, and verified 19 proteins in seminal plasma of 67 negative biopsy and 152 prostate cancer patients. Verification revealed a prostate-specific, secreted and androgen-regulated protein-glutamine gamma-glutamyltransferase 4 (TGM4), which predicted prostate cancer on biopsy and outperformed age and serum Prostate-Specific Antigen (PSA). A machine-learning approach for data analysis provided improved multi-marker combinations for diagnosis and prognosis. In the independent verification set measured by an in-house immunoassay, TGM4 protein was upregulated 3.7-fold (p = 0.006) and revealed AUC = 0.66 for detecting prostate cancer on biopsy for patients with serum PSA >4 ng/ml and age >50. Very low levels of TGM4 (120 pg/ml) were detected in blood serum. Collectively, our study demonstrated rigorous evaluation of one of the remaining and not well-explored prostate-specific proteins within the medium-abundance proteome of seminal plasma. Performance of TGM4 warrants its further investigation within the distinct genomic subtypes and evaluation for the inclusion into emerging multi-biomarker panels.","Multi-omics biomarker pipeline reveals elevated levels of protein-glutamine gammaglutamyltransferase 4 in seminal plasma of prostate cancer patients. Seminal plasma, because of its proximity to prostate, is a promising fluid for biomarker discovery and noninvasive diagnostics. In this study, we investigated if seminal plasma proteins could increase diagnostic specificity of detecting primary prostate cancer and discriminate between high- and low-grade cancers. To select 147 most promising biomarker candidates, we combined proteins identified through five independent experimental or data mining approaches: tissue transcriptomics, seminal plasma proteomics, cell line secretomics, tissue specificity, and androgen regulation. A rigorous biomarker development pipeline based on selected reaction monitoring assays was designed to evaluate the most promising candidates. As a result, we qualified 76, and verified 19 proteins in seminal plasma of 67 negative biopsy and 152 prostate cancer patients. Verification revealed a prostate-specific, secreted and androgen-regulated protein-glutamine gamma-glutamyltransferase 4 (TGM4), which predicted prostate cancer on biopsy and outperformed age and serum Prostate-Specific Antigen (PSA). A machine-learning approach for data analysis provided improved multi-marker combinations for diagnosis and prognosis. In the independent verification set measured by an in-house immunoassay, TGM4 protein was upregulated 3.7-fold (p = 0.006) and revealed AUC = 0.66 for detecting prostate cancer on biopsy for patients with serum PSA >4 ng/ml and age >50. Very low levels of TGM4 (120 pg/ml) were detected in blood serum. Collectively, our study demonstrated rigorous evaluation of one of the remaining and not well-explored prostate-specific proteins within the medium-abundance proteome of seminal plasma. Performance of TGM4 warrants its further investigation within the distinct genomic subtypes and evaluation for the inclusion into emerging multi-biomarker panels."
0,Early temporal characteristics of elderly patient cognitive impairment in electronic health records,"BACKGROUND: The aging population has led to an increase in cognitive impairment (CI) resulting in significant costs to patients, their families, and society. A research endeavor on a large cohort to better understand the frequency and severity of CI is urgent to respond to the health needs of this population. However, little is known about temporal trends of patient health functions (i.e., activity of daily living [ADL]) and how these trends are associated with the onset of CI in elderly patients. Also, the use of a rich source of clinical free text in electronic health records (EHRs) to facilitate CI research has not been well explored. The aim of this study is to characterize and better understand early signals of elderly patient CI by examining temporal trends of patient ADL and analyzing topics of patient medical conditions in clinical free text using topic models. METHODS: The study cohort consists of physician-diagnosed CI patients (nâ€‰=â€‰1,435) and cognitively unimpaired (CU) patients (nâ€‰=â€‰1,435) matched by age and sex, selected from patients 65â€‰years of age or older at the time of enrollment in the Mayo Clinic Biobank. A corpus analysis was performed to examine the basic statistics of event types and practice settings where the physician first diagnosed CI. We analyzed the distribution of ADL in three different age groups over time before the development of CI. Furthermore, we applied three different topic modeling approaches on clinical free text to examine how patients' medical conditions change over time when they were close to CI diagnosis. RESULTS: The trajectories of ADL deterioration became steeper in CI patients than CU patients approximately 1 to 1.5â€‰year(s) before the actual physician diagnosis of CI. The topic modeling showed that the topic terms were mostly correlated and captured the underlying semantics relevant to CI when approaching to CI diagnosis. CONCLUSIONS: There exist notable differences in temporal trends of basic and instrumental ADL between CI and CU patients. The trajectories of certain individual ADL, such as bathing and responsibility of own medication, were closely associated with CI development. The topic terms obtained by topic modeling methods from clinical free text have a potential to show how CI patients' conditions evolve and reveal overlooked conditions when they close to CI diagnosis.","Early temporal characteristics of elderly patient cognitive impairment in electronic health records. BACKGROUND: The aging population has led to an increase in cognitive impairment (CI) resulting in significant costs to patients, their families, and society. A research endeavor on a large cohort to better understand the frequency and severity of CI is urgent to respond to the health needs of this population. However, little is known about temporal trends of patient health functions (i.e., activity of daily living [ADL]) and how these trends are associated with the onset of CI in elderly patients. Also, the use of a rich source of clinical free text in electronic health records (EHRs) to facilitate CI research has not been well explored. The aim of this study is to characterize and better understand early signals of elderly patient CI by examining temporal trends of patient ADL and analyzing topics of patient medical conditions in clinical free text using topic models. METHODS: The study cohort consists of physician-diagnosed CI patients (nâ€‰=â€‰1,435) and cognitively unimpaired (CU) patients (nâ€‰=â€‰1,435) matched by age and sex, selected from patients 65â€‰years of age or older at the time of enrollment in the Mayo Clinic Biobank. A corpus analysis was performed to examine the basic statistics of event types and practice settings where the physician first diagnosed CI. We analyzed the distribution of ADL in three different age groups over time before the development of CI. Furthermore, we applied three different topic modeling approaches on clinical free text to examine how patients' medical conditions change over time when they were close to CI diagnosis. RESULTS: The trajectories of ADL deterioration became steeper in CI patients than CU patients approximately 1 to 1.5â€‰year(s) before the actual physician diagnosis of CI. The topic modeling showed that the topic terms were mostly correlated and captured the underlying semantics relevant to CI when approaching to CI diagnosis. CONCLUSIONS: There exist notable differences in temporal trends of basic and instrumental ADL between CI and CU patients. The trajectories of certain individual ADL, such as bathing and responsibility of own medication, were closely associated with CI development. The topic terms obtained by topic modeling methods from clinical free text have a potential to show how CI patients' conditions evolve and reveal overlooked conditions when they close to CI diagnosis."
0,Extracting pathway-level signatures from proteogenomic data in breast cancer using independent component analysis,"Recent advances in the multi-omics characterization necessitate knowledge integration across different data types that go beyond individual biomarker discovery. In this study, we apply independent component analysis (ICA) to human breast cancer proteogenomics data to retrieve mechanistic information. We show that as an unsupervised feature extraction method, ICA was able to construct signatures with known biological relevance on both transcriptome and proteome levels. Moreover, proteome and transcriptome signatures can be associated by their respective correlation with patient clinical features, providing an integrated description of phenotype-related biological processes. Our results demonstrate that the application of ICA to proteogenomics data could lead to pathway-level knowledge discovery. Potential extension of this approach to other data and cancer types may contribute to pan-cancer integration of multi-omics information.","Extracting pathway-level signatures from proteogenomic data in breast cancer using independent component analysis. Recent advances in the multi-omics characterization necessitate knowledge integration across different data types that go beyond individual biomarker discovery. In this study, we apply independent component analysis (ICA) to human breast cancer proteogenomics data to retrieve mechanistic information. We show that as an unsupervised feature extraction method, ICA was able to construct signatures with known biological relevance on both transcriptome and proteome levels. Moreover, proteome and transcriptome signatures can be associated by their respective correlation with patient clinical features, providing an integrated description of phenotype-related biological processes. Our results demonstrate that the application of ICA to proteogenomics data could lead to pathway-level knowledge discovery. Potential extension of this approach to other data and cancer types may contribute to pan-cancer integration of multi-omics information."
0,A Molecular Signature in Blood Reveals a Role for p53 in Regulating Malaria-Induced Inflammation,"The mechanisms that protect from febrile malaria remain unclear. Tran et al. applied a systems-based approach to a longitudinal pediatric study to identify immune signatures that associate with control of malaria fever and parasitemia, revealing that p53 upregulation in monocytes attenuates malaria-induced inflammation and predicts protection from fever.","A Molecular Signature in Blood Reveals a Role for p53 in Regulating Malaria-Induced Inflammation. The mechanisms that protect from febrile malaria remain unclear. Tran et al. applied a systems-based approach to a longitudinal pediatric study to identify immune signatures that associate with control of malaria fever and parasitemia, revealing that p53 upregulation in monocytes attenuates malaria-induced inflammation and predicts protection from fever."
0,Detection of Colorectal Hepatic Metastases Is Superior at Standard Radiation Dose CT versus Reduced Dose CT,,
0,"Adaptive platform trials: definition, design, conduct and reporting considerations The Adaptive Platform Trials Coalition",,
0,Studies of the Anti-amnesic Effects and Mechanisms of Single and Combined Use of Donepezil and Ginkgo Ketoester Tablet on Scopolamine-Induced Memory Impairment in Mice,"Ginkgo ketoester tablets (GT) and donepezil were a clinically used combination for the treatment of Alzheimer's disease (AD). The aim of the study was undertaken to investigate the antiamnesic effects of the two drugs alone and in combination through in vivo models of the Morris water maze along with in vitro antioxidants, acetylcholinesterase (AChE) and butyrylcholinesterase (BuChE). The potential mechanisms were speculated by the activities of acetylcholine (ACh), AChE, superoxide dismutase (SOD), and malondialdehyde (MDA) and the protein expression of brain-derived neurotrophic factor (BDNF) and tyrosine protein kinase B (TrkB). The combination group showed a concentration-dependent inhibition of cholinesterase and antioxidation. As far as its mechanism was concerned, the combination of two drugs exerted excellent effects on oxidative stress, cholinergic pathway damage, and inactivation of the BDNF-TrkB signaling pathway. Additionally, to elucidate the binding mechanism of GT active ingredients into the structure of AChE, the results of molecular docking studies indicated that hydrogen and/or hydrophobic bonds might play an important role in their binding process. Thus, the combination of drugs could treat AD perfectly and further verify the scientific rationality of clinical medication.","Studies of the Anti-amnesic Effects and Mechanisms of Single and Combined Use of Donepezil and Ginkgo Ketoester Tablet on Scopolamine-Induced Memory Impairment in Mice. Ginkgo ketoester tablets (GT) and donepezil were a clinically used combination for the treatment of Alzheimer's disease (AD). The aim of the study was undertaken to investigate the antiamnesic effects of the two drugs alone and in combination through in vivo models of the Morris water maze along with in vitro antioxidants, acetylcholinesterase (AChE) and butyrylcholinesterase (BuChE). The potential mechanisms were speculated by the activities of acetylcholine (ACh), AChE, superoxide dismutase (SOD), and malondialdehyde (MDA) and the protein expression of brain-derived neurotrophic factor (BDNF) and tyrosine protein kinase B (TrkB). The combination group showed a concentration-dependent inhibition of cholinesterase and antioxidation. As far as its mechanism was concerned, the combination of two drugs exerted excellent effects on oxidative stress, cholinergic pathway damage, and inactivation of the BDNF-TrkB signaling pathway. Additionally, to elucidate the binding mechanism of GT active ingredients into the structure of AChE, the results of molecular docking studies indicated that hydrogen and/or hydrophobic bonds might play an important role in their binding process. Thus, the combination of drugs could treat AD perfectly and further verify the scientific rationality of clinical medication."
0,Detecting Different Cell Populations Using Multispectral (19)F MRI,,
0,Drug-induced hypersensitivity syndrome/drug reaction with eosinophilia and systemic symptoms severity score: A useful tool for assessing disease severity and predicting fatal cytomegalovirus disease,"BACKGROUND: The prognosis of drug-induced hypersensitivity syndrome (DiHS)/drug reaction with eosinophilia and systemic symptoms (DRESS) is highly unpredictable. Severe complications, either related or unrelated to cytomegalovirus (CMV) reactivation, are a highly probable cause of death. OBJECTIVES: The aim was to establish a scoring system for DiHS/DRESS that can be used to monitor severity, predict prognosis, and stratify the risk of developing CMV disease and complications. METHODS: A retrospective analysis of 55 patients with DiHS/DRESS was performed. A composite score was created using clinical data. DiHS/DRESS patients were also stratified into 3 groups based on the scores to predict the risk of CMV reactivation and complications. RESULTS: This scoring system made it possible to predict CMV disease and complications. Scores >/=4 were associated with the later development of CMV disease and complications, while no patients with scores <4 developed complications. LIMITATIONS: This was a single-institution study with a relatively small patient cohort that lacked a validation cohort. CONCLUSIONS: Our scoring system may be useful for predicting CMV-related complications, and early intervention with anti-CMV agents should be considered in patients with scores >/=4 or with evidence of CMV reactivation.","Drug-induced hypersensitivity syndrome/drug reaction with eosinophilia and systemic symptoms severity score: A useful tool for assessing disease severity and predicting fatal cytomegalovirus disease. BACKGROUND: The prognosis of drug-induced hypersensitivity syndrome (DiHS)/drug reaction with eosinophilia and systemic symptoms (DRESS) is highly unpredictable. Severe complications, either related or unrelated to cytomegalovirus (CMV) reactivation, are a highly probable cause of death. OBJECTIVES: The aim was to establish a scoring system for DiHS/DRESS that can be used to monitor severity, predict prognosis, and stratify the risk of developing CMV disease and complications. METHODS: A retrospective analysis of 55 patients with DiHS/DRESS was performed. A composite score was created using clinical data. DiHS/DRESS patients were also stratified into 3 groups based on the scores to predict the risk of CMV reactivation and complications. RESULTS: This scoring system made it possible to predict CMV disease and complications. Scores >/=4 were associated with the later development of CMV disease and complications, while no patients with scores <4 developed complications. LIMITATIONS: This was a single-institution study with a relatively small patient cohort that lacked a validation cohort. CONCLUSIONS: Our scoring system may be useful for predicting CMV-related complications, and early intervention with anti-CMV agents should be considered in patients with scores >/=4 or with evidence of CMV reactivation."
0,Comparing drug safety of hepatitis C therapies using post-market data,"BACKGROUND: Hepatitis C affects about 3 % of the world's population. In the United States, about 3.5 million have chronic hepatitis C, and it is the leading cause of liver cancer and the most common indication for liver transplantation. In the last decades, new advances in therapy have substantially increased the cure rate of hepatitis C to more than 95% with the use of antiviral agents. However, drug safety of the new treatments remains one of the major concerns. Data from the US Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS) and the Electronic Health Record (EHR) systems provide crucial post-market information to evaluate drug safety. Currently, quantitative evidence of drug safety of hepatitis C treatments based on post-market data are still limited, and there is also a lack of a standard statistical procedure to systematically compare drug safety across multiple drugs using FAERS and EHR. METHOD: In this study, we presented a statistical procedure to compare the difference in adverse events (AE) across multiple hepatitis C drugs using data from FAERS and EHR, and to assess the consistency of results from two data bases. Through three major steps, including descriptive comparison, testing for difference among groups, and quantification of association, the proposed method can provide a quantitative comparison on safety of multiple drugs. Specifically, we compared drugs that were approved by FDA to treat hepatitis C before 2011versus those approved after 2013. We used spontaneous AE reports submitted between 2004 to 2015 from FAERS data base and medical records between 1999 to 2015 from the Cerner health facts data base to estimate and compare the rate of AE after drug use. RESULT: We studied 30 most frequently reported AEs after treatment of hepatitis C, comparing the difference between drugs approved before 2011versus those approved after 2013. Our results showed that there was difference in rate of AE between the two groups of treatment. We reported the AEs that have significant statistical difference, and estimate the difference attributable to variation of age and gender between the two groups of drug users. Our findings are consistent with results in existing literature. Moreover, we compared the results obtained from FAERS data and EHR data, and evaluated the consistency of evidence. CONCLUSION: The proposed procedure is a general and standardized pipeline that can be used to compare and visualize drug safety among multiple drugs to support regulatory decision-makings using post-market data. We showed that there was statistically significant difference in AE rates between the new and old therapies for hepatitis C. We showed that both FAERS and EHR contained large information for research of post-market drug safety, but each has its own strength and limitations. Cautions should be taken when combining evidence from the two data resources and there is a need of more sophisticated informatics and statistical tools for evidence synthesis.","Comparing drug safety of hepatitis C therapies using post-market data. BACKGROUND: Hepatitis C affects about 3 % of the world's population. In the United States, about 3.5 million have chronic hepatitis C, and it is the leading cause of liver cancer and the most common indication for liver transplantation. In the last decades, new advances in therapy have substantially increased the cure rate of hepatitis C to more than 95% with the use of antiviral agents. However, drug safety of the new treatments remains one of the major concerns. Data from the US Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS) and the Electronic Health Record (EHR) systems provide crucial post-market information to evaluate drug safety. Currently, quantitative evidence of drug safety of hepatitis C treatments based on post-market data are still limited, and there is also a lack of a standard statistical procedure to systematically compare drug safety across multiple drugs using FAERS and EHR. METHOD: In this study, we presented a statistical procedure to compare the difference in adverse events (AE) across multiple hepatitis C drugs using data from FAERS and EHR, and to assess the consistency of results from two data bases. Through three major steps, including descriptive comparison, testing for difference among groups, and quantification of association, the proposed method can provide a quantitative comparison on safety of multiple drugs. Specifically, we compared drugs that were approved by FDA to treat hepatitis C before 2011versus those approved after 2013. We used spontaneous AE reports submitted between 2004 to 2015 from FAERS data base and medical records between 1999 to 2015 from the Cerner health facts data base to estimate and compare the rate of AE after drug use. RESULT: We studied 30 most frequently reported AEs after treatment of hepatitis C, comparing the difference between drugs approved before 2011versus those approved after 2013. Our results showed that there was difference in rate of AE between the two groups of treatment. We reported the AEs that have significant statistical difference, and estimate the difference attributable to variation of age and gender between the two groups of drug users. Our findings are consistent with results in existing literature. Moreover, we compared the results obtained from FAERS data and EHR data, and evaluated the consistency of evidence. CONCLUSION: The proposed procedure is a general and standardized pipeline that can be used to compare and visualize drug safety among multiple drugs to support regulatory decision-makings using post-market data. We showed that there was statistically significant difference in AE rates between the new and old therapies for hepatitis C. We showed that both FAERS and EHR contained large information for research of post-market drug safety, but each has its own strength and limitations. Cautions should be taken when combining evidence from the two data resources and there is a need of more sophisticated informatics and statistical tools for evidence synthesis."
0,Questions for Artificial Intelligence in Health Care,,
0,A Subset of Type I Conventional Dendritic Cells Controls Cutaneous Bacterial Infections through VEGFalpha-Mediated Recruitment of Neutrophils,"Skin conventional dendritic cells (cDCs) exist as two distinct subsets, cDC1s and cDC2s, which maintain the balance of immunity to pathogens and tolerance to self and microbiota. Here, we examined the roles of dermal cDC1s and cDC2s during bacterial infection, notably Propionibacterium acnes (P. acnes). cDC1s, but not cDC2s, regulated the magnitude of the immune response to P. acnes in the murine dermis by controlling neutrophil recruitment to the inflamed site and survival and function therein. Single-cell mRNA sequencing revealed that this regulation relied on secretion of the cytokine vascular endothelial growth factor alpha (VEGF-alpha) by a minor subset of activated EpCAM(+)CD59(+)Ly-6D(+) cDC1s. Neutrophil recruitment by dermal cDC1s was also observed during S. aureus, bacillus Calmette-Guerin (BCG), or E. coli infection, as well as in a model of bacterial insult in human skin. Thus, skin cDC1s are essential regulators of the innate response in cutaneous immunity and have roles beyond classical antigen presentation.","A Subset of Type I Conventional Dendritic Cells Controls Cutaneous Bacterial Infections through VEGFalpha-Mediated Recruitment of Neutrophils. Skin conventional dendritic cells (cDCs) exist as two distinct subsets, cDC1s and cDC2s, which maintain the balance of immunity to pathogens and tolerance to self and microbiota. Here, we examined the roles of dermal cDC1s and cDC2s during bacterial infection, notably Propionibacterium acnes (P. acnes). cDC1s, but not cDC2s, regulated the magnitude of the immune response to P. acnes in the murine dermis by controlling neutrophil recruitment to the inflamed site and survival and function therein. Single-cell mRNA sequencing revealed that this regulation relied on secretion of the cytokine vascular endothelial growth factor alpha (VEGF-alpha) by a minor subset of activated EpCAM(+)CD59(+)Ly-6D(+) cDC1s. Neutrophil recruitment by dermal cDC1s was also observed during S. aureus, bacillus Calmette-Guerin (BCG), or E. coli infection, as well as in a model of bacterial insult in human skin. Thus, skin cDC1s are essential regulators of the innate response in cutaneous immunity and have roles beyond classical antigen presentation."
0,Hacking the 'Cellular Immunology Agency': T cells caught in the act,,
0,Mapping biologically active chemical space to accelerate drug discovery,,
0,Enhanced phosphorylation of AMPK by lutein and oxidised lutein that lead to mitochondrial biogenesis in hyperglycemic HepG2 cells,"The stimulation of adenosine monophosphate-activated protein kinase (AMPK) is a prime target to decrease the hyperglycemic condition, hence it is a lutein (L) and oxidised lutein (OXL) is a target molecule for the treatment of type II diabetes. In the current study, a plausible interaction of L and OXL with AMPK was investigated by molecular docking. In addition, the effect of L and OXL for the activation of AMPK that triggers the downstream regulator peroxisome proliferator-activated receptor Î³ coactivator 1Î± (PGC-1Î±), TFAM expression, mitochondrial DNA (mtDNA), mitochondrial biogenesis and superoxide dismutase 2 (SOD2) in high glucose treated HepG2 cells were investigated by quantitativeÂ polymerase chain reaction and Western blot analysis. Molecular docking reveals higher binding affinity of L (Î”G = âˆ’6.3 kcal/mol) and OXL (Î”G = âˆ’15.5 kcal/mol) with AMPK, compared with metformin (Î”G = âˆ’5.0 kcal/mol). The phosphorylation of AMPK increased by 1.3- and 1.5-fold with L and OXL treatment, respectively, in high glucose induced HepG2 cells. The activation of PGC-1Î± is significant (P < 0.05) in OXL group than L. Similarly, TFAM expression is increased with L and OXL compared with the high glucose group. Further increase in SOD2 and mtDNA, confirms the efficacy of L and OXL in restoring the mitochondrial biogenesis in high glucose induced cells through AMPK, PGC-1Î±, and TFAM.","Enhanced phosphorylation of AMPK by lutein and oxidised lutein that lead to mitochondrial biogenesis in hyperglycemic HepG2 cells. The stimulation of adenosine monophosphate-activated protein kinase (AMPK) is a prime target to decrease the hyperglycemic condition, hence it is a lutein (L) and oxidised lutein (OXL) is a target molecule for the treatment of type II diabetes. In the current study, a plausible interaction of L and OXL with AMPK was investigated by molecular docking. In addition, the effect of L and OXL for the activation of AMPK that triggers the downstream regulator peroxisome proliferator-activated receptor Î³ coactivator 1Î± (PGC-1Î±), TFAM expression, mitochondrial DNA (mtDNA), mitochondrial biogenesis and superoxide dismutase 2 (SOD2) in high glucose treated HepG2 cells were investigated by quantitativeÂ polymerase chain reaction and Western blot analysis. Molecular docking reveals higher binding affinity of L (Î”G = âˆ’6.3 kcal/mol) and OXL (Î”G = âˆ’15.5 kcal/mol) with AMPK, compared with metformin (Î”G = âˆ’5.0 kcal/mol). The phosphorylation of AMPK increased by 1.3- and 1.5-fold with L and OXL treatment, respectively, in high glucose induced HepG2 cells. The activation of PGC-1Î± is significant (P < 0.05) in OXL group than L. Similarly, TFAM expression is increased with L and OXL compared with the high glucose group. Further increase in SOD2 and mtDNA, confirms the efficacy of L and OXL in restoring the mitochondrial biogenesis in high glucose induced cells through AMPK, PGC-1Î±, and TFAM."
0,A novel protease-activated receptor 1 inhibitor from the leech Whitmania pigra,"Whitmania pigra has been used as a traditional Chinese medicine (TCM) for promoting blood circulation, alleviating blood coagulation, activating meridians and relieving stasis for several hundred years. However, the therapeutic components of this species, especially proteins and peptides were poorly exploited. Until now only a few of them were obtained by using chromatographic isolation and purification. In recent decade, transcriptome techniques were rapidly developed, and have been used to fully reveal the functional components of many animal venoms. In the present study, the cDNA of the salivary gland of Whitmania pigra was sequenced by illumina and the transcriptome was assembled by using Trinity. The proteome were analysed by LC-MS/MS. Based on the data of the transcriptome and the proteome, a potential antiplatelet protein named pigrin was found. Pigrin was cloned and expressed using P. pastoris GS115. The antiplatelet andantithrombotic bioactivities of pigrin were tested by using aggregometer and the rat arterio-venous shunt thrombosis model, respectively. Thebleeding time of pigrin was measured by a mice tail cutting method. The docking of pigrin and protease-activated receptor 1 (PAR1) or collagen were conducted using the ZDOCK Server. Pigrin was able to selectively inhibit platelet aggregation stimulated by PAR1 agonist and collagen. Pigrin attenuated thrombotic formation in vivo in rat, while did not prolong bleeding time at its effective dosage. There are significant differences in the key residues participating in binding of Pigrin-Collagen complex from Pigrin-PAR1 complex. In conclusion,a novel PAR1 inhibitor pigrin was found from the leech Whitmania pigra. This study helped to elucidate the mechanism of the leech for the treatment of cardiovascular disorder.","A novel protease-activated receptor 1 inhibitor from the leech Whitmania pigra. Whitmania pigra has been used as a traditional Chinese medicine (TCM) for promoting blood circulation, alleviating blood coagulation, activating meridians and relieving stasis for several hundred years. However, the therapeutic components of this species, especially proteins and peptides were poorly exploited. Until now only a few of them were obtained by using chromatographic isolation and purification. In recent decade, transcriptome techniques were rapidly developed, and have been used to fully reveal the functional components of many animal venoms. In the present study, the cDNA of the salivary gland of Whitmania pigra was sequenced by illumina and the transcriptome was assembled by using Trinity. The proteome were analysed by LC-MS/MS. Based on the data of the transcriptome and the proteome, a potential antiplatelet protein named pigrin was found. Pigrin was cloned and expressed using P. pastoris GS115. The antiplatelet andantithrombotic bioactivities of pigrin were tested by using aggregometer and the rat arterio-venous shunt thrombosis model, respectively. Thebleeding time of pigrin was measured by a mice tail cutting method. The docking of pigrin and protease-activated receptor 1 (PAR1) or collagen were conducted using the ZDOCK Server. Pigrin was able to selectively inhibit platelet aggregation stimulated by PAR1 agonist and collagen. Pigrin attenuated thrombotic formation in vivo in rat, while did not prolong bleeding time at its effective dosage. There are significant differences in the key residues participating in binding of Pigrin-Collagen complex from Pigrin-PAR1 complex. In conclusion,a novel PAR1 inhibitor pigrin was found from the leech Whitmania pigra. This study helped to elucidate the mechanism of the leech for the treatment of cardiovascular disorder."
0,Computational design and evaluation of Î²-sheet breaker peptides for destabilizing Alzheimer's amyloid-Î²42 protofibrils,"The Î²-sheet breaker (BSB) peptides interfere with amyloid fibril assembly and used as therapeutic agents in the treatment of Alzheimer's disease (AD). In this regard, a simple yet effective in silico screening methodology was applied in the present study to evaluate a potential 867 pentapeptide library based on known BSB peptide, LPFFD, for destabilizing AÎ²42 protofibrils. The molecular docking based virtual screening was used to filter out pentapeptides having binding affinities stronger than LPFFD. In the next step, binding free energies of the top 10 pentapeptides were evaluated using the MM-PBSA method. The residue-wise binding free energy analysis reveals that two pentapeptides, PVFFE, and PPFYE, bind to the surface of AÎ²42 protofibril and another pentapeptide, PPFFE, bind in the core region of AÎ²42 protofibril. By employing molecular dynamics simulation as a post filter for the top-hit peptides from MM-PBSA, the pentapeptides, PPFFE, PVFFE, and PPFYE, have been identified as potential BSB peptides for destabilizing AÎ²42 protofibril structure. The conformational microstate analysis, a significant decrease in the Î²-sheet content of AÎ²42 protofibril, a loss in the total number of hydrogen bonds in AÎ²42 protofibril, Asp23-Lys28 salt bridge destabilization and analysis of the free energy surfaces highlight AÎ²42 protofibril structure destabilization in presence of pentapeptides. Among three top-hit pentapeptides, PPFFE displayed the most potent AÎ²42 protofibril destabilization effect that shifted the energy minima toward lowest value of Î²-sheet content as well as lowest number of hydrogen bonds in AÎ²42 protofibril. The in silico screening workflow presented in the study highlight an alternative tool for designing novel peptides with enhanced BSB ability as potential therapeutic agents for AD.","Computational design and evaluation of Î²-sheet breaker peptides for destabilizing Alzheimer's amyloid-Î²42 protofibrils. The Î²-sheet breaker (BSB) peptides interfere with amyloid fibril assembly and used as therapeutic agents in the treatment of Alzheimer's disease (AD). In this regard, a simple yet effective in silico screening methodology was applied in the present study to evaluate a potential 867 pentapeptide library based on known BSB peptide, LPFFD, for destabilizing AÎ²42 protofibrils. The molecular docking based virtual screening was used to filter out pentapeptides having binding affinities stronger than LPFFD. In the next step, binding free energies of the top 10 pentapeptides were evaluated using the MM-PBSA method. The residue-wise binding free energy analysis reveals that two pentapeptides, PVFFE, and PPFYE, bind to the surface of AÎ²42 protofibril and another pentapeptide, PPFFE, bind in the core region of AÎ²42 protofibril. By employing molecular dynamics simulation as a post filter for the top-hit peptides from MM-PBSA, the pentapeptides, PPFFE, PVFFE, and PPFYE, have been identified as potential BSB peptides for destabilizing AÎ²42 protofibril structure. The conformational microstate analysis, a significant decrease in the Î²-sheet content of AÎ²42 protofibril, a loss in the total number of hydrogen bonds in AÎ²42 protofibril, Asp23-Lys28 salt bridge destabilization and analysis of the free energy surfaces highlight AÎ²42 protofibril structure destabilization in presence of pentapeptides. Among three top-hit pentapeptides, PPFFE displayed the most potent AÎ²42 protofibril destabilization effect that shifted the energy minima toward lowest value of Î²-sheet content as well as lowest number of hydrogen bonds in AÎ²42 protofibril. The in silico screening workflow presented in the study highlight an alternative tool for designing novel peptides with enhanced BSB ability as potential therapeutic agents for AD."
0,AI-augmented multidisciplinary teams: hype or hope?,,
0,New Phenotypes for Sepsis: The Promise and Problem of Applying Machine Learning and Artificial Intelligence in Clinical Research,,
0,Recurrent inference machines for reconstructing heterogeneous MRI data,"Deep learning allows for accelerated magnetic resonance image (MRI) reconstruction, thereby shortening measurement times. Rather than using sparsifying transforms, a prerequisite in Compressed Sensing (CS), suitable MRI prior distributions are learned from data. In clinical practice, both the underlying anatomy as well as image acquisition settings vary. For this reason, deep neural networks must be able to reapply what they learn across different measurement conditions. We propose to use Recurrent Inference Machines (RIM) as a framework for accelerated MRI reconstruction. RIMs solve inverse problems in an iterative and recurrent inference procedure by repeatedly reassessing the state of their reconstruction, and subsequently making incremental adjustments to it in accordance with the forward model of accelerated MRI. RIMs learn the inferential process of reconstructing a given signal, which, in combination with the use of internal states as part of their recurrent architecture, makes them less dependent on learning the features pertaining to the source of the signal itself. This gives RIMs a low tendency to overfit, and a high capacity to generalize to unseen types of data. We demonstrate this ability with respect to anatomy by reconstructing brain and knee scans, as well as other MRI acquisition settings, by reconstructing scans of different contrast and resolution, at different field strength, subjected to varying acceleration levels. We show that RIMs outperform CS not only with respect to quality metrics, but also according to a rating given by an experienced neuroradiologist in a double blinded experiment. Finally, we show with qualitative results that our model can be applied to prospectively under-sampled raw data, as acquired by pre-installed acquisition protocols.","Recurrent inference machines for reconstructing heterogeneous MRI data. Deep learning allows for accelerated magnetic resonance image (MRI) reconstruction, thereby shortening measurement times. Rather than using sparsifying transforms, a prerequisite in Compressed Sensing (CS), suitable MRI prior distributions are learned from data. In clinical practice, both the underlying anatomy as well as image acquisition settings vary. For this reason, deep neural networks must be able to reapply what they learn across different measurement conditions. We propose to use Recurrent Inference Machines (RIM) as a framework for accelerated MRI reconstruction. RIMs solve inverse problems in an iterative and recurrent inference procedure by repeatedly reassessing the state of their reconstruction, and subsequently making incremental adjustments to it in accordance with the forward model of accelerated MRI. RIMs learn the inferential process of reconstructing a given signal, which, in combination with the use of internal states as part of their recurrent architecture, makes them less dependent on learning the features pertaining to the source of the signal itself. This gives RIMs a low tendency to overfit, and a high capacity to generalize to unseen types of data. We demonstrate this ability with respect to anatomy by reconstructing brain and knee scans, as well as other MRI acquisition settings, by reconstructing scans of different contrast and resolution, at different field strength, subjected to varying acceleration levels. We show that RIMs outperform CS not only with respect to quality metrics, but also according to a rating given by an experienced neuroradiologist in a double blinded experiment. Finally, we show with qualitative results that our model can be applied to prospectively under-sampled raw data, as acquired by pre-installed acquisition protocols."
0,Artificial Intelligence Algorithms for Medical Prediction Should Be Nonproprietary and Readily Available-Reply,,
0,"Reply from Authors re: Jens. J. Rassweiler, Marcel Fiedler-Hruza. The Learning Curve for Robot-assisted Partial Nephrectomy: There is Much Beyond a Trifecta. Eur Urol. In press. https://doi.org/10.1016/j.eururo.2018.10.022: The Clinical Implications of Surgical Learning Curve Analysis: Can We Optimize Patient Outcomes Using Structured Training Programs?",,
0,Working memory revived in older adults by synchronizing rhythmic brain circuits,,
0,Functions of adult-born neurons in hippocampal memory interference and indexing,,
0,Intelligent liver function testing (iLFT): A trial of automated diagnosis and staging of liver disease in primary care,,
0,Quantification of Normal Parametric Values: A Prerequisite for Routine Cardiac MRI,,
0,In-silico identification of small molecules targeting H-Ras and in-vitro cytotoxicity with caspase-mediated apoptosis in carcinoma cells,"H-Ras oncogene plays a critical role in the transformation of normal cells to a malignant phenotype through constitutive activation of the GTP bound protein leading to uncontrolled cell proliferation in several human cancers. Thus, H-Ras oncoprotein serves as an excellent target for anticancer drug discovery. To identify novel H-Ras inhibitors, we performed structure-based virtual screening of the Maybridge HitFinderâ„¢ library using Schrodinger suite. Thirty ligands from the chemical library were identified as they showed preferential in silico binding initially to H-Ras proteins with Gly12Val, Gly13Asp, and Gly12Val-Gly13Asp mutations. Absorption, distribution, metabolism, excretion, and toxicity profile confirmed drug-like properties of the compounds. Three representative molecules were tested for antiproliferative effect on T24 urinary bladder carcinoma cell line, MCF-7 breast cancer cell line and HDF-7 normal dermal fibroblast cells using 3-(4,5-dimethylthiazol-2-yl)-2,5-diphenyltetrazolium bromide assay. Two compounds (Cmpds) showed antiproliferative activity exclusively in the cancer cell lines with minimal effect on the control HDF-7 cells. The effect of compound treatment on cell cycle progression, assessed by propidium iodide (PI) staining, depicted increased arrest of T24 cell line in the sub G1 phase. Further, Annexin-V PI dual staining and pan caspase inhibitor Z-VAD-fmk indicated caspase-dependent apoptotic activity of Cmpds 1 and 3. Our findings demonstrate caspase-dependent apoptotic activity of Cmpds 1 and 3 selectively against Gly12Val mutated T24 cancer cell line implicating a potential for treatment of bladder cancer. We envisage that these molecules may be promising candidates with potential therapeutic value in H-Ras mutation-associated cancers.","In-silico identification of small molecules targeting H-Ras and in-vitro cytotoxicity with caspase-mediated apoptosis in carcinoma cells. H-Ras oncogene plays a critical role in the transformation of normal cells to a malignant phenotype through constitutive activation of the GTP bound protein leading to uncontrolled cell proliferation in several human cancers. Thus, H-Ras oncoprotein serves as an excellent target for anticancer drug discovery. To identify novel H-Ras inhibitors, we performed structure-based virtual screening of the Maybridge HitFinderâ„¢ library using Schrodinger suite. Thirty ligands from the chemical library were identified as they showed preferential in silico binding initially to H-Ras proteins with Gly12Val, Gly13Asp, and Gly12Val-Gly13Asp mutations. Absorption, distribution, metabolism, excretion, and toxicity profile confirmed drug-like properties of the compounds. Three representative molecules were tested for antiproliferative effect on T24 urinary bladder carcinoma cell line, MCF-7 breast cancer cell line and HDF-7 normal dermal fibroblast cells using 3-(4,5-dimethylthiazol-2-yl)-2,5-diphenyltetrazolium bromide assay. Two compounds (Cmpds) showed antiproliferative activity exclusively in the cancer cell lines with minimal effect on the control HDF-7 cells. The effect of compound treatment on cell cycle progression, assessed by propidium iodide (PI) staining, depicted increased arrest of T24 cell line in the sub G1 phase. Further, Annexin-V PI dual staining and pan caspase inhibitor Z-VAD-fmk indicated caspase-dependent apoptotic activity of Cmpds 1 and 3. Our findings demonstrate caspase-dependent apoptotic activity of Cmpds 1 and 3 selectively against Gly12Val mutated T24 cancer cell line implicating a potential for treatment of bladder cancer. We envisage that these molecules may be promising candidates with potential therapeutic value in H-Ras mutation-associated cancers."
0,Quantifying Drug Combination Synergy along Potency and Efficacy Axes,"Two goals motivate treating diseases with drug combinations: reduce off-target toxicity by minimizing doses (synergistic potency) and improve outcomes by escalating effect (synergistic efficacy). Established drug synergy frameworks obscure such distinction, failing to harness the potential of modern chemical libraries. We therefore developed multi-dimensional synergy of combinations (MuSyC), a formalism based on a generalized, multi-dimensional Hill equation, which decouples synergistic potency and efficacy. In mutant-EGFR-driven lung cancer, MuSyC reveals that combining a mutant-EGFR inhibitor with inhibitors of other kinases may result only in synergistic potency, whereas synergistic efficacy can be achieved by co-targeting mutant-EGFR and epigenetic regulation or microtubule polymerization. In mutant-BRAF melanoma, MuSyC determines whether a molecular correlate of BRAFi insensitivity alters a BRAF inhibitor's potency, efficacy, or both. These findings showcase MuSyC's potential to transform the enterprise of drug-combination screens by precisely guiding translation of combinations toward dose reduction, improved efficacy, or both. Meyer et al. developed a framework for measuring drug combination synergy. The framework, termed MuSyC, distinguishes between two types of synergy. The first quantifies the change in the maximal effect with the combination (synergistic efficacy), and the second measures the change in a drug's potency due to the combination (synergistic potency). By decoupling these two synergies conflated in prior methods, MuSyC rationally guides discovery and translation of drug combinations for the improvement of therapeutic efficacy and reduction of off-target toxicities via dose reduction.","Quantifying Drug Combination Synergy along Potency and Efficacy Axes. Two goals motivate treating diseases with drug combinations: reduce off-target toxicity by minimizing doses (synergistic potency) and improve outcomes by escalating effect (synergistic efficacy). Established drug synergy frameworks obscure such distinction, failing to harness the potential of modern chemical libraries. We therefore developed multi-dimensional synergy of combinations (MuSyC), a formalism based on a generalized, multi-dimensional Hill equation, which decouples synergistic potency and efficacy. In mutant-EGFR-driven lung cancer, MuSyC reveals that combining a mutant-EGFR inhibitor with inhibitors of other kinases may result only in synergistic potency, whereas synergistic efficacy can be achieved by co-targeting mutant-EGFR and epigenetic regulation or microtubule polymerization. In mutant-BRAF melanoma, MuSyC determines whether a molecular correlate of BRAFi insensitivity alters a BRAF inhibitor's potency, efficacy, or both. These findings showcase MuSyC's potential to transform the enterprise of drug-combination screens by precisely guiding translation of combinations toward dose reduction, improved efficacy, or both. Meyer et al. developed a framework for measuring drug combination synergy. The framework, termed MuSyC, distinguishes between two types of synergy. The first quantifies the change in the maximal effect with the combination (synergistic efficacy), and the second measures the change in a drug's potency due to the combination (synergistic potency). By decoupling these two synergies conflated in prior methods, MuSyC rationally guides discovery and translation of drug combinations for the improvement of therapeutic efficacy and reduction of off-target toxicities via dose reduction."
0,Cardio-Oncology Rehabilitation to Manage Cardiovascular Outcomes in Cancer Patients and Survivors: A Scientific Statement from the American Heart Association,"Cardiovascular disease is a competing cause of death in patients with cancer with early-stage disease. This elevated cardiovascular disease risk is thought to derive from both the direct effects of cancer therapies and the accumulation of risk factors such as hypertension, weight gain, cigarette smoking, and loss of cardiorespiratory fitness. Effective and viable strategies are needed to mitigate cardiovascular disease risk in this population; a multimodal model such as cardiac rehabilitation may be a potential solution. This statement from the American Heart Association provides an overview of the existing knowledge and rationale for the use of cardiac rehabilitation to provide structured exercise and ancillary services to cancer patients and survivors. This document introduces the concept of cardio-oncology rehabilitation, which includes identification of patients with cancer at high risk for cardiac dysfunction and a description of the cardiac rehabilitation infrastructure needed to address the unique exposures and complications related to cancer care. In this statement, we also discuss the need for future research to fully implement a multimodal model of cardiac rehabilitation for patients with cancer and to determine whether reimbursement of these services is clinically warranted.","Cardio-Oncology Rehabilitation to Manage Cardiovascular Outcomes in Cancer Patients and Survivors: A Scientific Statement from the American Heart Association. Cardiovascular disease is a competing cause of death in patients with cancer with early-stage disease. This elevated cardiovascular disease risk is thought to derive from both the direct effects of cancer therapies and the accumulation of risk factors such as hypertension, weight gain, cigarette smoking, and loss of cardiorespiratory fitness. Effective and viable strategies are needed to mitigate cardiovascular disease risk in this population; a multimodal model such as cardiac rehabilitation may be a potential solution. This statement from the American Heart Association provides an overview of the existing knowledge and rationale for the use of cardiac rehabilitation to provide structured exercise and ancillary services to cancer patients and survivors. This document introduces the concept of cardio-oncology rehabilitation, which includes identification of patients with cancer at high risk for cardiac dysfunction and a description of the cardiac rehabilitation infrastructure needed to address the unique exposures and complications related to cancer care. In this statement, we also discuss the need for future research to fully implement a multimodal model of cardiac rehabilitation for patients with cancer and to determine whether reimbursement of these services is clinically warranted."
0,Transient elastography for screening of liver fibrosis: Cost-effectiveness analysis from six prospective cohorts in Europe and Asia,,
0,Increased Reactivity of the Mesolimbic Reward System after Ketamine Injection in Patients with Treatment-resistant Major Depressive Disorder,,
0,"Machine Learning, Predictive Analytics, and Clinical Practice: Can the Past Inform the Present?",,
0,Frequency-splitting dynamic MRI reconstruction using multi-scale 3D convolutional sparse coding and automatic parameter selection,"In this paper, we propose a novel image reconstruction algorithm using multi-scale 3D convolutional sparse coding and a spectral decomposition technique for highly undersampled dynamic Magnetic Resonance Imaging (MRI) data. The proposed method recovers high-frequency information using a shared 3D convolution-based dictionary built progressively during the reconstruction process in an unsupervised manner, while low-frequency information is recovered using a total variation-based energy minimization method that leverages temporal coherence in dynamic MRI. Additionally, the proposed 3D dictionary is built across three different scales to more efficiently adapt to various feature sizes, and elastic net regularization is employed to promote a better approximation to the sparse input data. We also propose an automatic parameter selection technique based on a genetic algorithm to find optimal parameters for our numerical solver which is a variant of the alternating direction method of multipliers (ADMM). We demonstrate the performance of our method by comparing it with state-of-the-art methods on 15 single-coil cardiac, 7 single-coil DCE, and a multi-coil brain MRI datasets at different sampling rates (12.5%, 25% and 50%). The results show that our method significantly outperforms the other state-of-the-art methods in reconstruction quality with a comparable running time and is resilient to noise.","Frequency-splitting dynamic MRI reconstruction using multi-scale 3D convolutional sparse coding and automatic parameter selection. In this paper, we propose a novel image reconstruction algorithm using multi-scale 3D convolutional sparse coding and a spectral decomposition technique for highly undersampled dynamic Magnetic Resonance Imaging (MRI) data. The proposed method recovers high-frequency information using a shared 3D convolution-based dictionary built progressively during the reconstruction process in an unsupervised manner, while low-frequency information is recovered using a total variation-based energy minimization method that leverages temporal coherence in dynamic MRI. Additionally, the proposed 3D dictionary is built across three different scales to more efficiently adapt to various feature sizes, and elastic net regularization is employed to promote a better approximation to the sparse input data. We also propose an automatic parameter selection technique based on a genetic algorithm to find optimal parameters for our numerical solver which is a variant of the alternating direction method of multipliers (ADMM). We demonstrate the performance of our method by comparing it with state-of-the-art methods on 15 single-coil cardiac, 7 single-coil DCE, and a multi-coil brain MRI datasets at different sampling rates (12.5%, 25% and 50%). The results show that our method significantly outperforms the other state-of-the-art methods in reconstruction quality with a comparable running time and is resilient to noise."
0,"Development and validation of a deep learning system to detect glaucomatous optic neuropathy using fundus photographs (vol 87, pg 528, 2019)",,
0,A Study of High-Grade Serous Ovarian Cancer Origins Implicates the SOX18 Transcription Factor in Tumor Development,"Lawrenson et al. profile gene expression and active chromatin in âˆ¼200 ovarian and fallopian epithelial isolates and implement machine learning to demonstrate that most high-grade serous ovarian cancers (HGSOCs) derive from fallopian tube epithelial cells, but a subset may originate from ovarian epithelia. SOX18 induces mesenchymal features to drive early neoplasia in fallopian tube precursors.","A Study of High-Grade Serous Ovarian Cancer Origins Implicates the SOX18 Transcription Factor in Tumor Development. Lawrenson et al. profile gene expression and active chromatin in âˆ¼200 ovarian and fallopian epithelial isolates and implement machine learning to demonstrate that most high-grade serous ovarian cancers (HGSOCs) derive from fallopian tube epithelial cells, but a subset may originate from ovarian epithelia. SOX18 induces mesenchymal features to drive early neoplasia in fallopian tube precursors."
0,Deep Learning for Triage of Chest Radiographs: Should Every Institution Train Its Own System?,,
0,Guiding pancreatic cyst management,,
0,"""Electronic Nose"" Predicts Immunotherapy Response",,
0,G-1. Large-scale chromatin remodelling and transcriptional deregulation on der11 following translocation in mantle cell lymphoma,"Mantle cell lymphoma (MCL) is an aggressive B-cell non-Hodgkin lymphoma characterized by poor prognosis and survival rate. Its genetic hallmark is the translocation t(11;14) which leads to the overexpression of cyclin D1 (CCND1) gene which becomes juxtaposed to the immunoglobulin heavy chain (IGH) gene on the newly formed der14 chromosome. This recurrent feature is however not sufficient to promote the development of the disease as expression of CCND1 under different known IgH enhancers in transgenic mice is not sufficient for tumor development. Additional alterations are necessary to develop a malignant phenotype. When a translocation occurs, it can induce overall nuclear reorganization, epigenetic changes and altered gene expression that may contribute to oncogenesis. Here we investigated changes in nuclear positioning of gene loci and their transcription after the t(11;14) focusing our attention on the events occurring on the der11 chromosome. Methods. 3D-immunoFISH and image analysis software were used to analyze gene loci position in nuclear space. To analyze changes of transcriptional level of genes located on the der11, quantitative RT-PCR, bioinformatic analysis and data mining were performed. ChIP was carried out to analyze specific interactions between nucleolin and the genome in MCL. Results. We demonstrated that the expression of many genes located close to the translocation breakpoint was deregulated in MCL compared to other lymphomas and to B-cells from healthy donors. Most of these genes were located on the der11 after the t(11;14). We found that the der11 is relocated in close proximity to the nucleolus. Here the nucleolin, that is part of the transcriptional factor LR-1 can deregulate gene expression by direct binding to promoters. We found that the LR-1 consensus sequence and the nucleolin binding sites are significantly enriched in the regions covered by the deregulated genes compared to the rest of chromosme11 and to cells without the t(11;14). Conclusions. We identified new epigenetic events that contribute to MCL development following t(11;14).","G-1. Large-scale chromatin remodelling and transcriptional deregulation on der11 following translocation in mantle cell lymphoma. Mantle cell lymphoma (MCL) is an aggressive B-cell non-Hodgkin lymphoma characterized by poor prognosis and survival rate. Its genetic hallmark is the translocation t(11;14) which leads to the overexpression of cyclin D1 (CCND1) gene which becomes juxtaposed to the immunoglobulin heavy chain (IGH) gene on the newly formed der14 chromosome. This recurrent feature is however not sufficient to promote the development of the disease as expression of CCND1 under different known IgH enhancers in transgenic mice is not sufficient for tumor development. Additional alterations are necessary to develop a malignant phenotype. When a translocation occurs, it can induce overall nuclear reorganization, epigenetic changes and altered gene expression that may contribute to oncogenesis. Here we investigated changes in nuclear positioning of gene loci and their transcription after the t(11;14) focusing our attention on the events occurring on the der11 chromosome. Methods. 3D-immunoFISH and image analysis software were used to analyze gene loci position in nuclear space. To analyze changes of transcriptional level of genes located on the der11, quantitative RT-PCR, bioinformatic analysis and data mining were performed. ChIP was carried out to analyze specific interactions between nucleolin and the genome in MCL. Results. We demonstrated that the expression of many genes located close to the translocation breakpoint was deregulated in MCL compared to other lymphomas and to B-cells from healthy donors. Most of these genes were located on the der11 after the t(11;14). We found that the der11 is relocated in close proximity to the nucleolus. Here the nucleolin, that is part of the transcriptional factor LR-1 can deregulate gene expression by direct binding to promoters. We found that the LR-1 consensus sequence and the nucleolin binding sites are significantly enriched in the regions covered by the deregulated genes compared to the rest of chromosme11 and to cells without the t(11;14). Conclusions. We identified new epigenetic events that contribute to MCL development following t(11;14)."
0,NETWORK BIOLOGY Illuminating the dark side of machine learning,,
0,Artificial Intelligence for Cervical Precancer Screening,,
0,"Magnetic resonance imaging for prenatal estimation of birthweight in pregnancy: review of available data, techniques, and future perspectives",,
0,The MAGIC algorithm probability is a validated response biomarker of treatment of acute graft-versus-host disease,"The Mount Sinai Acute GVHD International Consortium (MAGIC) algorithm probability (MAP), derived from 2 serum biomarkers, measures damage to crypts in the gastrointestinal tract during graft-versus-host disease (GVHD). We hypothesized that changes in MAP after treatment could validate it as a response biomarker. We prospectively collected serum samples and clinical stages of acute GVHD from 615 patients receiving hematopoietic cell transplantation in 20 centers at initiation of first-line systemic treatment and 4 weeks later. We computed MAPs and clinical responses and compared their abilities to predict 6-month nonrelapse mortality (NRM) in the validation cohort (n = 367). After 4 weeks of treatment, MAPs predicted NRM better than the change in clinical symptoms in all patients and identified 2 groups with significantly different NRM in both clinical responders (40% vs 12%, P < .0001) and nonresponders (65% vs 25%, P < .0001). MAPs successfully reclassified patients for NRM risk within every clinical grade of acute GVHD after 4 weeks of treatment. At the beginning of treatment, patients with a low MAP that rose above the threshold of 0.290 after 4 weeks of treatment had a significant increase in NRM, whereas patients with a high MAP at onset that fell below that threshold after treatment had a striking decrease in NRM that translated into clear differences in overall survival. We conclude that a MAP measured before and after treatment of acute GVHD is a response biomarker that predicts long-term outcomes more accurately than change in clinical symptoms. MAPs have the potential to guide therapy for acute GVHD and may function as a useful end point in clinical trials.","The MAGIC algorithm probability is a validated response biomarker of treatment of acute graft-versus-host disease. The Mount Sinai Acute GVHD International Consortium (MAGIC) algorithm probability (MAP), derived from 2 serum biomarkers, measures damage to crypts in the gastrointestinal tract during graft-versus-host disease (GVHD). We hypothesized that changes in MAP after treatment could validate it as a response biomarker. We prospectively collected serum samples and clinical stages of acute GVHD from 615 patients receiving hematopoietic cell transplantation in 20 centers at initiation of first-line systemic treatment and 4 weeks later. We computed MAPs and clinical responses and compared their abilities to predict 6-month nonrelapse mortality (NRM) in the validation cohort (n = 367). After 4 weeks of treatment, MAPs predicted NRM better than the change in clinical symptoms in all patients and identified 2 groups with significantly different NRM in both clinical responders (40% vs 12%, P < .0001) and nonresponders (65% vs 25%, P < .0001). MAPs successfully reclassified patients for NRM risk within every clinical grade of acute GVHD after 4 weeks of treatment. At the beginning of treatment, patients with a low MAP that rose above the threshold of 0.290 after 4 weeks of treatment had a significant increase in NRM, whereas patients with a high MAP at onset that fell below that threshold after treatment had a striking decrease in NRM that translated into clear differences in overall survival. We conclude that a MAP measured before and after treatment of acute GVHD is a response biomarker that predicts long-term outcomes more accurately than change in clinical symptoms. MAPs have the potential to guide therapy for acute GVHD and may function as a useful end point in clinical trials."
0,Free Pentosidine Assessment Based on Fluorescence Measurements in Spent Dialysate,"The aim of this study was to primarily explore the relationship between free pentosidine and the fluorescence properties of spent dialysate, and also to develop a model to assess the levels of free pentosidine in spent dialysate based on the fluorescence measurements. First, 40 patients (20 females and 20 males) were examined during 40 dialysis sessions. High-pressure liquid chromatography (HPLC) was used to measure the free pentosidine concentrations from the spent dialysate. The full fluorescence spectra of the spent dialysates were recorded and single- and multi-wavelength (MW) models were developed. The average free pentosidine concentrations in the spent dialysate measured by HPLC at the start and end of the dialysis session were (mean Â± SD) 4.25 Â± 3.11 and 0.94 Â± 0.69 Î¼g/L respectively. The removal ratios (RRs) between RR-lab and RR-MW were statistically similar (p > 0.2). The concentration of free pentosidine and the RR can therefore be estimated from the spent dialysate when utilising fluorescence measurements.","Free Pentosidine Assessment Based on Fluorescence Measurements in Spent Dialysate. The aim of this study was to primarily explore the relationship between free pentosidine and the fluorescence properties of spent dialysate, and also to develop a model to assess the levels of free pentosidine in spent dialysate based on the fluorescence measurements. First, 40 patients (20 females and 20 males) were examined during 40 dialysis sessions. High-pressure liquid chromatography (HPLC) was used to measure the free pentosidine concentrations from the spent dialysate. The full fluorescence spectra of the spent dialysates were recorded and single- and multi-wavelength (MW) models were developed. The average free pentosidine concentrations in the spent dialysate measured by HPLC at the start and end of the dialysis session were (mean Â± SD) 4.25 Â± 3.11 and 0.94 Â± 0.69 Î¼g/L respectively. The removal ratios (RRs) between RR-lab and RR-MW were statistically similar (p > 0.2). The concentration of free pentosidine and the RR can therefore be estimated from the spent dialysate when utilising fluorescence measurements."
0,Promoting Trust Between Patients and Physicians in the Era of Artificial Intelligence,,
0,MRI-based radiomics signature for tumor grading of rectal carcinoma using random forest model,"The present study aimed to construct prospective models for tumor grading of rectal carcinoma by using magnetic resonance (MR)-based radiomics features. A set of 118 patients with rectal carcinoma was analyzed. After imbalance-adjustments of the data using Synthetic Minority Oversampling Technique (SMOTE), the final data set was randomized into the training set and validation set at the ratio of 3:1. The radiomics features were captured from manually segmented lesion of magnetic resonance imaging (MRI). The most related radiomics features were selected using the random forest model by calculating the Gini importance of initial extracted characteristics. A random forest classifier model was constructed using the top important features. The classifier model performance was evaluated via receive operator characteristic curve and area under the curve (AUC). A total of 1,131 radiomics features were extracted from segmented lesion. The top 50 most important features were selected to construct a random forest classifier model. The AUC values of grade 1, 2, 3, and 4 for training set were 0.918, 0.822, 0.775, and 1.000, respectively, and the corresponding AUC values for testing set were 0.717, 0.683, 0.690, and 0.827 separately. The developed feature selection method and machine learning-based prediction models using radiomics features of MRI show a relatively acceptable performance in tumor grading of rectal carcinoma and could distinguish the tumor subjects from the healthy ones, which is important for the prognosis of cancer patients.","MRI-based radiomics signature for tumor grading of rectal carcinoma using random forest model. The present study aimed to construct prospective models for tumor grading of rectal carcinoma by using magnetic resonance (MR)-based radiomics features. A set of 118 patients with rectal carcinoma was analyzed. After imbalance-adjustments of the data using Synthetic Minority Oversampling Technique (SMOTE), the final data set was randomized into the training set and validation set at the ratio of 3:1. The radiomics features were captured from manually segmented lesion of magnetic resonance imaging (MRI). The most related radiomics features were selected using the random forest model by calculating the Gini importance of initial extracted characteristics. A random forest classifier model was constructed using the top important features. The classifier model performance was evaluated via receive operator characteristic curve and area under the curve (AUC). A total of 1,131 radiomics features were extracted from segmented lesion. The top 50 most important features were selected to construct a random forest classifier model. The AUC values of grade 1, 2, 3, and 4 for training set were 0.918, 0.822, 0.775, and 1.000, respectively, and the corresponding AUC values for testing set were 0.717, 0.683, 0.690, and 0.827 separately. The developed feature selection method and machine learning-based prediction models using radiomics features of MRI show a relatively acceptable performance in tumor grading of rectal carcinoma and could distinguish the tumor subjects from the healthy ones, which is important for the prognosis of cancer patients."
0,Social determinants of health in relation to firearm-related homicides in the United States: A nationwide multilevel cross-sectional study,"Background Gun violence has shortened the average life expectancy of Americans, and better knowledge about the root causes of gun violence is crucial to its prevention. While some empirical evidence exists regarding the impacts of social and economic factors on violence and firearm homicide rates, to the author's knowledge, there has yet to be a comprehensive and comparative lagged, multilevel investigation of major social determinants of health in relation to firearm homicides and mass shootings. Methods and findings This study used negative binomial regression models and geolocated gun homicide incident data from January 1, 2015, to December 31, 2015, to explore and compare the independent associations of key state-, county-, and neighborhood-level social determinants of health-social mobility, social capital, income inequality, racial and economic segregation, and social spending-with neighborhood firearm-related homicides and mass shootings in the United States, accounting for relevant state firearm laws and a variety of state, county, and neighborhood (census tract [CT]) characteristics. Latitude and longitude coordinates on firearm-related deaths were previously collected by the Gun Violence Archive, and then linked by the British newspaper The Guardian to CTs according to 2010 Census geographies. The study population consisted of all 74,134 CTs as defined for the 2010 Census in the 48 states of the contiguous US. The final sample spanned 70,579 CTs, containing an estimated 314,247,908 individuals, or 98% of the total US population in 2015. The analyses were based on 13,060 firearm-related deaths in 2015, with 11,244 non-mass shootings taking place in 8,673 CTs and 141 mass shootings occurring in 138 CTs. For area-level social determinants, lag periods of 3 to 17 years were examined based on existing theory, empirical evidence, and data availability. County-level institutional social capital (levels of trust in institutions), social mobility, income inequality, and public welfare spending exhibited robust relationships with CT-level gun homicide rates and the total numbers of combined non-mass and mass shooting homicide incidents and non-mass shooting homicide incidents alone. A 1-standard deviation (SD) increase in institutional social capital was linked to a 19% reduction in the homicide rate (incidence rate ratio [IRR] = 0.81, 95% CI 0.73-0.91, p < 0.001) and a 17% decrease in the number of firearm homicide incidents (IRR = 0.83, 95% CI 0.73-0.95, p = 0.01). Upward social mobility was related to a 25% reduction in the gun homicide rate (IRR = 0.75, 95% CI 0.66-0.86, p < 0.001) and a 24% decrease in the number of homicide incidents (IRR = 0.76, 95% CI 0.67-0.87, p < 0.001). Meanwhile, 1-SD increases in the neighborhood percentages of residents in poverty and males living alone were associated with 26%-27% and 12% higher homicide rates, respectively. Study limitations include possible residual confounding by factors at the individual/household level, and lack of disaggregation of gun homicide data by gender and race/ethnicity. Conclusions This study finds that the rich-poor gap, level of citizens' trust in institutions, economic opportunity, and public welfare spending are all related to firearm homicide rates in the US. Further establishing the causal nature of these associations and modifying these social determinants may help to address the growing gun violence epidemic and reverse recent life expectancy declines among Americans.","Social determinants of health in relation to firearm-related homicides in the United States: A nationwide multilevel cross-sectional study. Background Gun violence has shortened the average life expectancy of Americans, and better knowledge about the root causes of gun violence is crucial to its prevention. While some empirical evidence exists regarding the impacts of social and economic factors on violence and firearm homicide rates, to the author's knowledge, there has yet to be a comprehensive and comparative lagged, multilevel investigation of major social determinants of health in relation to firearm homicides and mass shootings. Methods and findings This study used negative binomial regression models and geolocated gun homicide incident data from January 1, 2015, to December 31, 2015, to explore and compare the independent associations of key state-, county-, and neighborhood-level social determinants of health-social mobility, social capital, income inequality, racial and economic segregation, and social spending-with neighborhood firearm-related homicides and mass shootings in the United States, accounting for relevant state firearm laws and a variety of state, county, and neighborhood (census tract [CT]) characteristics. Latitude and longitude coordinates on firearm-related deaths were previously collected by the Gun Violence Archive, and then linked by the British newspaper The Guardian to CTs according to 2010 Census geographies. The study population consisted of all 74,134 CTs as defined for the 2010 Census in the 48 states of the contiguous US. The final sample spanned 70,579 CTs, containing an estimated 314,247,908 individuals, or 98% of the total US population in 2015. The analyses were based on 13,060 firearm-related deaths in 2015, with 11,244 non-mass shootings taking place in 8,673 CTs and 141 mass shootings occurring in 138 CTs. For area-level social determinants, lag periods of 3 to 17 years were examined based on existing theory, empirical evidence, and data availability. County-level institutional social capital (levels of trust in institutions), social mobility, income inequality, and public welfare spending exhibited robust relationships with CT-level gun homicide rates and the total numbers of combined non-mass and mass shooting homicide incidents and non-mass shooting homicide incidents alone. A 1-standard deviation (SD) increase in institutional social capital was linked to a 19% reduction in the homicide rate (incidence rate ratio [IRR] = 0.81, 95% CI 0.73-0.91, p < 0.001) and a 17% decrease in the number of firearm homicide incidents (IRR = 0.83, 95% CI 0.73-0.95, p = 0.01). Upward social mobility was related to a 25% reduction in the gun homicide rate (IRR = 0.75, 95% CI 0.66-0.86, p < 0.001) and a 24% decrease in the number of homicide incidents (IRR = 0.76, 95% CI 0.67-0.87, p < 0.001). Meanwhile, 1-SD increases in the neighborhood percentages of residents in poverty and males living alone were associated with 26%-27% and 12% higher homicide rates, respectively. Study limitations include possible residual confounding by factors at the individual/household level, and lack of disaggregation of gun homicide data by gender and race/ethnicity. Conclusions This study finds that the rich-poor gap, level of citizens' trust in institutions, economic opportunity, and public welfare spending are all related to firearm homicide rates in the US. Further establishing the causal nature of these associations and modifying these social determinants may help to address the growing gun violence epidemic and reverse recent life expectancy declines among Americans."
0,"LOX-1, the common therapeutic target in hypercholesterolemia: A new perspective of antiatherosclerotic action of aegeline","Background. Lectin-like oxidized low-density lipoprotein receptor-1 (LOX-1) is the major receptor for oxidized low-density lipoprotein (Ox-LDL) in the aorta of aged rats. Ox-LDL initiates LOX-1 activation in the endothelium of lipid-accumulating sites of both animal and human subjects of hypercholesterolemia. Targeting LOX-1 may provide a novel diagnostic strategy towards hypercholesterolemia and vascular diseases. Hypothesis. This study was planned to address whether aegeline (AG) could bind to LOX-1 with a higher affinity and modulate the uptake of Ox-LDL in hypercholesterolemia. Study Design. Thirty-six Wistar rats were divided into six groups. The pathology group rats were fed with high-cholesterol diet (HCD) for 45 days, and the treatment group rats were fed with HCD and aegeline/atorvastatin (AV) for the last 30 days. In vivo and in vitro experiments were carried out to assay the markers of atherosclerosis like Ox-LDL and LOX-1 levels. Histopathological examination was performed. Oil Red O staining was carried out in the IC-21 cell line. Docking studies were performed. Results. AG administration effectively brought down the lipid levels induced by HCD. The lowered levels of Ox-LDL and LOX-1 in AG-administered rats deem it to be a potent antihypercholesterolemic agent. Compared to AV, AG had a pronounced effect in downregulating the expression of lipids evidenced by Oil Red O staining. AG binds with LOX-1 at a higher affinity validated by docking. Conclusion. This study validates AG to be an effective stratagem in bringing down the lipid stress induced by HCD and can be deemed as an antihypercholesterolemic agent.","LOX-1, the common therapeutic target in hypercholesterolemia: A new perspective of antiatherosclerotic action of aegeline. Background. Lectin-like oxidized low-density lipoprotein receptor-1 (LOX-1) is the major receptor for oxidized low-density lipoprotein (Ox-LDL) in the aorta of aged rats. Ox-LDL initiates LOX-1 activation in the endothelium of lipid-accumulating sites of both animal and human subjects of hypercholesterolemia. Targeting LOX-1 may provide a novel diagnostic strategy towards hypercholesterolemia and vascular diseases. Hypothesis. This study was planned to address whether aegeline (AG) could bind to LOX-1 with a higher affinity and modulate the uptake of Ox-LDL in hypercholesterolemia. Study Design. Thirty-six Wistar rats were divided into six groups. The pathology group rats were fed with high-cholesterol diet (HCD) for 45 days, and the treatment group rats were fed with HCD and aegeline/atorvastatin (AV) for the last 30 days. In vivo and in vitro experiments were carried out to assay the markers of atherosclerosis like Ox-LDL and LOX-1 levels. Histopathological examination was performed. Oil Red O staining was carried out in the IC-21 cell line. Docking studies were performed. Results. AG administration effectively brought down the lipid levels induced by HCD. The lowered levels of Ox-LDL and LOX-1 in AG-administered rats deem it to be a potent antihypercholesterolemic agent. Compared to AV, AG had a pronounced effect in downregulating the expression of lipids evidenced by Oil Red O staining. AG binds with LOX-1 at a higher affinity validated by docking. Conclusion. This study validates AG to be an effective stratagem in bringing down the lipid stress induced by HCD and can be deemed as an antihypercholesterolemic agent."
0,Data Analytics and Machine Learning for Disease Identification in Electronic Health Records,,
0,Organ preservation in bladder cancer: an opportunity for truly personalized treatment,"Radical treatment of many solid tumours has moved from surgery to multimodal organ preservation strategies combining systemic and local treatments. Trimodality bladder-preserving treatment (TMT) comprises maximal transurethral resection of the bladder tumour followed by radiotherapy and concurrent radiosensitizing treatment, thereby sparing the urinary bladder. From the patient's perspective, the choice of maintaining quality of life without a negative effect on the chances of cure and long-term survival is attractive. In muscle-invasive bladder cancer (MIBC), the evidence shows comparable clinical outcomes between patients undergoing radical cystectomy and TMT. Despite this evidence, many patients continue to be offered radical surgery as the standard-of-care treatment. Improvements in radiotherapy techniques with adaptive radiotherapy and advances in imaging translate to increases in the accuracy of treatment delivery and reductions in long-term toxicities. With the advent of novel biomarkers promising improved prediction of treatment response, stratification of patients for different treatments on the basis of tumour biology could soon be a reality. The future of oncological treatment lies in personalized medicine with the combination of technological and biological advances leading to truly bespoke management for patients with MIBC.","Organ preservation in bladder cancer: an opportunity for truly personalized treatment. Radical treatment of many solid tumours has moved from surgery to multimodal organ preservation strategies combining systemic and local treatments. Trimodality bladder-preserving treatment (TMT) comprises maximal transurethral resection of the bladder tumour followed by radiotherapy and concurrent radiosensitizing treatment, thereby sparing the urinary bladder. From the patient's perspective, the choice of maintaining quality of life without a negative effect on the chances of cure and long-term survival is attractive. In muscle-invasive bladder cancer (MIBC), the evidence shows comparable clinical outcomes between patients undergoing radical cystectomy and TMT. Despite this evidence, many patients continue to be offered radical surgery as the standard-of-care treatment. Improvements in radiotherapy techniques with adaptive radiotherapy and advances in imaging translate to increases in the accuracy of treatment delivery and reductions in long-term toxicities. With the advent of novel biomarkers promising improved prediction of treatment response, stratification of patients for different treatments on the basis of tumour biology could soon be a reality. The future of oncological treatment lies in personalized medicine with the combination of technological and biological advances leading to truly bespoke management for patients with MIBC."
0,A Deep Neural Network for Predicting and Engineering Alternative Polyadenylation,"Alternative polyadenylation (APA) is a major driver of transcriptome diversity in human cells. Here, we use deep learning to predict APA from DNA sequence alone. We trained our model (APARENT, APA REgression NeT) on isoform expression data from over 3 million APA reporters. APARENT's predictions are highly accurate when tasked with inferring APA in synthetic and human 3'UTRs. Visualizing features learned across all network layers reveals that APARENT recognizes sequence motifs known to recruit APA regulators, discovers previously unknown sequence determinants of 3' end processing, and integrates these features into a comprehensive, interpretable, cis-regulatory code. We apply APARENT to forward engineer functional polyadenylation signals with precisely defined cleavage position and isoform usage and validate predictions experimentally. Finally, we use APARENT to quantify the impact of genetic variants on APA. Our approach detects pathogenic variants in a wide range of disease contexts, expanding our understanding of the genetic origins of disease.","A Deep Neural Network for Predicting and Engineering Alternative Polyadenylation. Alternative polyadenylation (APA) is a major driver of transcriptome diversity in human cells. Here, we use deep learning to predict APA from DNA sequence alone. We trained our model (APARENT, APA REgression NeT) on isoform expression data from over 3 million APA reporters. APARENT's predictions are highly accurate when tasked with inferring APA in synthetic and human 3'UTRs. Visualizing features learned across all network layers reveals that APARENT recognizes sequence motifs known to recruit APA regulators, discovers previously unknown sequence determinants of 3' end processing, and integrates these features into a comprehensive, interpretable, cis-regulatory code. We apply APARENT to forward engineer functional polyadenylation signals with precisely defined cleavage position and isoform usage and validate predictions experimentally. Finally, we use APARENT to quantify the impact of genetic variants on APA. Our approach detects pathogenic variants in a wide range of disease contexts, expanding our understanding of the genetic origins of disease."
0,Locomotion-dependent remapping of distributed cortical networks,,
0,"Bictegravir combined with emtricitabine and tenofovir alafenamide versus dolutegravir, abacavir, and lamivudine for initial treatment of HIV-1 infection: week 96 results from a randomised, double-blind, multicentre, phase 3, non-inferiority trial",,
0,A Hallucinogenic Serotonin-2A Receptor Agonist Reduces Visual Response Gain and Alters Temporal Dynamics in Mouse V1,"Activation of serotonin-2A receptors (5-HT2ARs) is associated with hallucinations, but impacts on sensory processing are largely unknown. Michaiel et al. demonstrate that the 5-HT2AR agonist DOI strongly reduces sensory-evoked activity and disrupts temporal dynamics. These results support models of hallucinations that propose reduced bottom-up sensory drive.","A Hallucinogenic Serotonin-2A Receptor Agonist Reduces Visual Response Gain and Alters Temporal Dynamics in Mouse V1. Activation of serotonin-2A receptors (5-HT2ARs) is associated with hallucinations, but impacts on sensory processing are largely unknown. Michaiel et al. demonstrate that the 5-HT2AR agonist DOI strongly reduces sensory-evoked activity and disrupts temporal dynamics. These results support models of hallucinations that propose reduced bottom-up sensory drive."
0,Machine Learning in Medicine,,
0,RNA splicing analysis in genomic medicine,"High-throughput next-generation sequencing technologies have led to a rapid increase in the number of sequence variants identified in clinical practice via diagnostic genetic tests. Current bioinformatic analysis pipelines fail to take adequate account of the possible splicing effects of such variants, particularly where variants fall outwith canonical splice site sequences, and consequently the pathogenicity of such variants may often be missed. The regulation of splicing is highly complex and as a result, in silico prediction tools lack sufficient sensitivity and specificity for reliable use. Variants of all kinds can be linked to aberrant splicing in disease and the need for correct identification and diagnosis grows ever more crucial as novel splice-switching antisense oligonucleotide therapies start to enter clinical usage. RT-PCR provides a useful targeted assay of the splicing effects of identified variants, while minigene assays, massive parallel reporter assays and animal models can also be used for more detailed study of a particular splicing system, given enough time and resources. However, RNA-sequencing (RNA-seq) has the potential to be used as a rapid diagnostic tool in genomic medicine. By utilising data science approaches and machine learning, it may prove possible to finally understand and interpret the 'splicing codeâ€™ and apply this knowledge in human disease diagnostics.","RNA splicing analysis in genomic medicine. High-throughput next-generation sequencing technologies have led to a rapid increase in the number of sequence variants identified in clinical practice via diagnostic genetic tests. Current bioinformatic analysis pipelines fail to take adequate account of the possible splicing effects of such variants, particularly where variants fall outwith canonical splice site sequences, and consequently the pathogenicity of such variants may often be missed. The regulation of splicing is highly complex and as a result, in silico prediction tools lack sufficient sensitivity and specificity for reliable use. Variants of all kinds can be linked to aberrant splicing in disease and the need for correct identification and diagnosis grows ever more crucial as novel splice-switching antisense oligonucleotide therapies start to enter clinical usage. RT-PCR provides a useful targeted assay of the splicing effects of identified variants, while minigene assays, massive parallel reporter assays and animal models can also be used for more detailed study of a particular splicing system, given enough time and resources. However, RNA-sequencing (RNA-seq) has the potential to be used as a rapid diagnostic tool in genomic medicine. By utilising data science approaches and machine learning, it may prove possible to finally understand and interpret the 'splicing codeâ€™ and apply this knowledge in human disease diagnostics."
0,Novel and facile criterion to assess the accuracy of WSS estimation by 4D flow MRI,,
0,Machine Learning for Anesthesiologists: A Primer: Erratum,,
0,Intraoperative Adverse Incident Classification (EAUiaiC) by the European Association of Urology ad hoc Complications Guidelines Panel,,
0,Reconstruction of the Global Neural Crest Gene Regulatory Network In Vivo,"Precise control of developmental processes is encoded in the genome in the form of gene regulatory networks (GRNs). Such multi-factorial systems are difficult to decode in vertebrates owing to their complex gene hierarchies and dynamic molecular interactions. Here we present a genome-wide in vivo reconstruction of the GRN underlying development of the multipotent neural crest (NC) embryonic cell population. By coupling NC-specific epigenomic and transcriptional profiling at population and single-cell levels with genome/epigenome engineering in vivo, we identify multiple regulatory layers governing NC ontogeny, including NC-specific enhancers and super-enhancers, novel trans-factors, and cis-signatures allowing reverse engineering of the NC-GRN at unprecedented resolution. Furthermore, identification and dissection of divergent upstream combinatorial regulatory codes has afforded new insights into opposing gene circuits that define canonical and neural NC fates early during NC ontogeny. Our integrated approach, allowing dissection of cell-type-specific regulatory circuits in vivo, has broad implications for GRN discovery and investigation.","Reconstruction of the Global Neural Crest Gene Regulatory Network In Vivo. Precise control of developmental processes is encoded in the genome in the form of gene regulatory networks (GRNs). Such multi-factorial systems are difficult to decode in vertebrates owing to their complex gene hierarchies and dynamic molecular interactions. Here we present a genome-wide in vivo reconstruction of the GRN underlying development of the multipotent neural crest (NC) embryonic cell population. By coupling NC-specific epigenomic and transcriptional profiling at population and single-cell levels with genome/epigenome engineering in vivo, we identify multiple regulatory layers governing NC ontogeny, including NC-specific enhancers and super-enhancers, novel trans-factors, and cis-signatures allowing reverse engineering of the NC-GRN at unprecedented resolution. Furthermore, identification and dissection of divergent upstream combinatorial regulatory codes has afforded new insights into opposing gene circuits that define canonical and neural NC fates early during NC ontogeny. Our integrated approach, allowing dissection of cell-type-specific regulatory circuits in vivo, has broad implications for GRN discovery and investigation."
0,Deep Learning for Mammographic Breast Density Assessment and Beyond,,
0,Machine Learning-state of the art,,
0,Patterns in the relationship between life expectancy and gross domestic product in Russia in 2005-15: a cross-sectional analysis,,
0,MultiPLIER: A Transfer Learning Framework for Transcriptomics Reveals Systemic Features of Rare Disease,"Most gene expression datasets generated by individual researchers are too small to fully benefit from unsupervised machine-learning methods. In the case of rare diseases, there may be too few cases available, even when multiple studies are combined. To address this challenge, we utilize transfer learning to extract coordinated expression patterns and use learned patterns to analyze small rare disease datasets. We trained a pathway-level information extractor (PLIER) model on a large public data compendium comprising multiple experiments, tissues, and biological conditions and then transferred the model to small datasets in an approach we call MultiPLIER. Models constructed from the public data compendium included features that aligned well to known biological factors and were more comprehensive than those constructed from individual datasets or conditions. When transferred to rare disease datasets, the models describe biological processes related to disease severity more effectively than models trained only on a given dataset. Building models of gene expression with machine-learning techniques can reveal key regulatory processes that go awry in disease. However, certain datasets are too small to support detailed models. We find that models can be trained on a large, public compendia of gene expression data and then transferred to datasets of interest, including rare disease datasets, to reveal consistent disease-associated patterns across datasets and tissues.","MultiPLIER: A Transfer Learning Framework for Transcriptomics Reveals Systemic Features of Rare Disease. Most gene expression datasets generated by individual researchers are too small to fully benefit from unsupervised machine-learning methods. In the case of rare diseases, there may be too few cases available, even when multiple studies are combined. To address this challenge, we utilize transfer learning to extract coordinated expression patterns and use learned patterns to analyze small rare disease datasets. We trained a pathway-level information extractor (PLIER) model on a large public data compendium comprising multiple experiments, tissues, and biological conditions and then transferred the model to small datasets in an approach we call MultiPLIER. Models constructed from the public data compendium included features that aligned well to known biological factors and were more comprehensive than those constructed from individual datasets or conditions. When transferred to rare disease datasets, the models describe biological processes related to disease severity more effectively than models trained only on a given dataset. Building models of gene expression with machine-learning techniques can reveal key regulatory processes that go awry in disease. However, certain datasets are too small to support detailed models. We find that models can be trained on a large, public compendia of gene expression data and then transferred to datasets of interest, including rare disease datasets, to reveal consistent disease-associated patterns across datasets and tissues."
0,Machine Learning Detection of Intracranial Aneurysms-Will It Play in Peoria?,,
0,Digital health at fifteen: more human (more needed),"There is growing appreciation that the success of digital health - whether digital tools, digital interventions or technology-based change strategies - is linked to the extent to which human factors are considered throughout design, development and implementation. A shift in focus to individuals as users and consumers of digital health highlights the capacity of the field to respond to secular developments, such as the adoption of person-centred care and consumer health technologies. We argue that this project is not only incomplete, but is fundamentally 'uncompletable' in the face of a highly dynamic landscape of both technological and human challenges. These challenges include the effects of consumerist, technology-supported care on care delivery, the rapid growth of digital users in low-income and middle-income countries and the impacts of machine learning. Digital health research will create most value by retaining a clear focus on the role of human factors in maximising health benefit, by helping health systems to anticipate and understand the person-centred effects of technology changes and by advocating strongly for the autonomy, rights and safety of consumers.","Digital health at fifteen: more human (more needed). There is growing appreciation that the success of digital health - whether digital tools, digital interventions or technology-based change strategies - is linked to the extent to which human factors are considered throughout design, development and implementation. A shift in focus to individuals as users and consumers of digital health highlights the capacity of the field to respond to secular developments, such as the adoption of person-centred care and consumer health technologies. We argue that this project is not only incomplete, but is fundamentally 'uncompletable' in the face of a highly dynamic landscape of both technological and human challenges. These challenges include the effects of consumerist, technology-supported care on care delivery, the rapid growth of digital users in low-income and middle-income countries and the impacts of machine learning. Digital health research will create most value by retaining a clear focus on the role of human factors in maximising health benefit, by helping health systems to anticipate and understand the person-centred effects of technology changes and by advocating strongly for the autonomy, rights and safety of consumers."
0,"Outcomes of Older Women With Hormone Receptor-Positive, Human Epidermal Growth Factor Receptor-Negative Metastatic Breast Cancer Treated With a CDK4/6 Inhibitor and an Aromatase Inhibitor: An FDA Pooled Analysis",,
0,Self-Organized Synchronous Calcium Transients in a Cultured Human Neural Network Derived from Cerebral Organoids,"Cerebral activity is derived from the assembly of activated cells, but it is currently difficult to study human cerebral neuronal network activities. Here, Sakaguchi et al. report self-organized and complex human neural network activity using organoid technology and drug-inducible dynamic changes of the activity that will be useful for future research on human brain function and neuropsychiatric disorders.","Self-Organized Synchronous Calcium Transients in a Cultured Human Neural Network Derived from Cerebral Organoids. Cerebral activity is derived from the assembly of activated cells, but it is currently difficult to study human cerebral neuronal network activities. Here, Sakaguchi et al. report self-organized and complex human neural network activity using organoid technology and drug-inducible dynamic changes of the activity that will be useful for future research on human brain function and neuropsychiatric disorders."
0,FtsA as a cidal target for Staphylococcus aureus: Molecular docking and dynamics studies,"Staphylococcus aureus infection is a healthcare problem to mankind for a considerable period of time. Once when it enters the bloodstream of an individual, it may potentially result in life-threatening conditions. The resistance of S. aureus to various drugs such as penicillin, methicillin, gentamicin, erythromycin, and tetracycline have been well documented. Presently vancomycin is the drug of choice for methicillin resistant S. aureus. Scientists believe that S. aureus would completely develop resistance to vancomycin as well. Therefore there is a commensurate need to develop a drug to replace vancomycin. In the current study, we have focussed on FtsA, an important and vital cell division protein, which is found only in S. aureus and in other prokaryotic cells. We have carried out virtual screening process for FtsA against ZINC database, the best hit molecules obtained from the preliminary docking studies were subjected to SYBYL X 2.0 docking. The molecules ZINC74432848, ZINC37769607, and ZINC96896268 displayed the highest C-score value of 4.89, 4.49, and 4.22, respectively. The top ranked molecule ZINC74432848 was observed to form 4 hydrogen bonds with FtsA. The simulation study reveals the greater stability of the FtsA-ZINC74432848 complex. If the in vitro and in vivo study turns out affirmative, then ZINC74432848 could be developed as a potent drug for FtsA.","FtsA as a cidal target for Staphylococcus aureus: Molecular docking and dynamics studies. Staphylococcus aureus infection is a healthcare problem to mankind for a considerable period of time. Once when it enters the bloodstream of an individual, it may potentially result in life-threatening conditions. The resistance of S. aureus to various drugs such as penicillin, methicillin, gentamicin, erythromycin, and tetracycline have been well documented. Presently vancomycin is the drug of choice for methicillin resistant S. aureus. Scientists believe that S. aureus would completely develop resistance to vancomycin as well. Therefore there is a commensurate need to develop a drug to replace vancomycin. In the current study, we have focussed on FtsA, an important and vital cell division protein, which is found only in S. aureus and in other prokaryotic cells. We have carried out virtual screening process for FtsA against ZINC database, the best hit molecules obtained from the preliminary docking studies were subjected to SYBYL X 2.0 docking. The molecules ZINC74432848, ZINC37769607, and ZINC96896268 displayed the highest C-score value of 4.89, 4.49, and 4.22, respectively. The top ranked molecule ZINC74432848 was observed to form 4 hydrogen bonds with FtsA. The simulation study reveals the greater stability of the FtsA-ZINC74432848 complex. If the in vitro and in vivo study turns out affirmative, then ZINC74432848 could be developed as a potent drug for FtsA."
0,Using poster presentation to assess large classes: a case study of a first-year undergraduate module at a South African university,"BACKGROUND: The massification of higher education is often associated with poor student engagement, poor development of their critical thinking, inadequate feedback and poor student throughput. These factors necessitate the need to devise novel, innovative methods to teach, assess and provide feedback to learners to counter the restrictions imposed due to the large class learning environments. This study was conducted to ascertain the perceptions of 1st year medical students and staff at the Nelson Mandela School of Medicine regarding the value of poster presentations as a strategy to enhance learning, assessment and feedback. METHODS: This was an exploratory observational, descriptive cross-sectional, case study. Data was collected through separate student and staff questionnaires that required participant responses on a five-point Likert scale. The data was extracted into Excel spreadsheets for quantitative analysis. RESULTS: Two-hundred- and-thirty (92%) student questionnaires were returned (NÂ =â€‰250). Most students indicated that the design and presentation of the poster had helped them to select important material (92%), understand and describe disadvantage (86%) and to make a difference in the community (92%). The students agreed that the poster assessment was an efficient (81%) and fair method (75%) that provided opportunities for meaningful feedback. Ten staff members responded to the questionnaire. Most staff members (90%) indicated that the poster presentation had allowed students to demonstrate their engagement in a meaningful and appropriate way around issues of disadvantage and HIV and agreed that the poster presentations allowed for immediate and effective feedback. CONCLUSION: Students' interactions in the tasks promoted active engagement with others and course material; the development of higher order thinking and skills which added to students' accounts of transformative learning experiences. They could describe and illustrate the difference that they had made in their chosen community. The poster presentations allowed for quick and efficient marking, immediate feedback and an opportunity to validate the students' participation. Poster presentations offered an innovative way to encourage deep meaningful engagement and learning amongst peers and facilitators. Poster presentations should be more widely considered as an innovative way of encouraging deeper engagement and learning in a large class setting.","Using poster presentation to assess large classes: a case study of a first-year undergraduate module at a South African university. BACKGROUND: The massification of higher education is often associated with poor student engagement, poor development of their critical thinking, inadequate feedback and poor student throughput. These factors necessitate the need to devise novel, innovative methods to teach, assess and provide feedback to learners to counter the restrictions imposed due to the large class learning environments. This study was conducted to ascertain the perceptions of 1st year medical students and staff at the Nelson Mandela School of Medicine regarding the value of poster presentations as a strategy to enhance learning, assessment and feedback. METHODS: This was an exploratory observational, descriptive cross-sectional, case study. Data was collected through separate student and staff questionnaires that required participant responses on a five-point Likert scale. The data was extracted into Excel spreadsheets for quantitative analysis. RESULTS: Two-hundred- and-thirty (92%) student questionnaires were returned (NÂ =â€‰250). Most students indicated that the design and presentation of the poster had helped them to select important material (92%), understand and describe disadvantage (86%) and to make a difference in the community (92%). The students agreed that the poster assessment was an efficient (81%) and fair method (75%) that provided opportunities for meaningful feedback. Ten staff members responded to the questionnaire. Most staff members (90%) indicated that the poster presentation had allowed students to demonstrate their engagement in a meaningful and appropriate way around issues of disadvantage and HIV and agreed that the poster presentations allowed for immediate and effective feedback. CONCLUSION: Students' interactions in the tasks promoted active engagement with others and course material; the development of higher order thinking and skills which added to students' accounts of transformative learning experiences. They could describe and illustrate the difference that they had made in their chosen community. The poster presentations allowed for quick and efficient marking, immediate feedback and an opportunity to validate the students' participation. Poster presentations offered an innovative way to encourage deep meaningful engagement and learning amongst peers and facilitators. Poster presentations should be more widely considered as an innovative way of encouraging deeper engagement and learning in a large class setting."
0,Use of Elastic Registration in Pulmonary MRI for the Assessment of Pulmonary Fibrosis in Patients with Systemic Sclerosis,,
0,The Splicing Code Goes Deep,"The importance of genomic sequence context in generating transcriptome diversity through RNA splicing is independently unmasked by two studies in this issue (Jaganathan et al., 2019; Baeza-Centurion et al., 2019).","The Splicing Code Goes Deep. The importance of genomic sequence context in generating transcriptome diversity through RNA splicing is independently unmasked by two studies in this issue (Jaganathan et al., 2019; Baeza-Centurion et al., 2019)."
0,"Machine learning in anaesthesia: reactive, proactive... predictive!",,
0,Paradox of age: older patients receive higher age-adjusted minimum alveolar concentration fractions of volatile anaesthetics yet display higher bispectral index values,,
0,Advances in protein structure prediction and design,"The prediction of protein three-dimensional structure from amino acid sequence has been a grand challenge problem in computational biophysics for decades, owing to its intrinsic scientific interest and also to the many potential applications for robust protein structure prediction algorithms, from genome interpretation to protein function prediction. More recently, the inverse problem - designing an amino acid sequence that will fold into a specified three-dimensional structure - has attracted growing attention as a potential route to the rational engineering of proteins with functions useful in biotechnology and medicine. Methods for the prediction and design of protein structures have advanced dramatically in the past decade. Increases in computing power and the rapid growth in protein sequence and structure databases have fuelled the development of new data-intensive and computationally demanding approaches for structure prediction. New algorithms for designing protein folds and protein-protein interfaces have been used to engineer novel high-order assemblies and to design from scratch fluorescent proteins with novel or enhanced properties, as well as signalling proteins with therapeutic potential. In this Review, we describe current approaches for protein structure prediction and design and highlight a selection of the successful applications they have enabled.","Advances in protein structure prediction and design. The prediction of protein three-dimensional structure from amino acid sequence has been a grand challenge problem in computational biophysics for decades, owing to its intrinsic scientific interest and also to the many potential applications for robust protein structure prediction algorithms, from genome interpretation to protein function prediction. More recently, the inverse problem - designing an amino acid sequence that will fold into a specified three-dimensional structure - has attracted growing attention as a potential route to the rational engineering of proteins with functions useful in biotechnology and medicine. Methods for the prediction and design of protein structures have advanced dramatically in the past decade. Increases in computing power and the rapid growth in protein sequence and structure databases have fuelled the development of new data-intensive and computationally demanding approaches for structure prediction. New algorithms for designing protein folds and protein-protein interfaces have been used to engineer novel high-order assemblies and to design from scratch fluorescent proteins with novel or enhanced properties, as well as signalling proteins with therapeutic potential. In this Review, we describe current approaches for protein structure prediction and design and highlight a selection of the successful applications they have enabled."
0,Identification of verapamil binding sites within human Kv1.5 channel using mutagenesis and docking simulation,"Background/Aims: The phenylalkylamine class of L-type Ca2+ channel antagonist verapamil prolongs the effective refractory period (ERP) of human atrium, which appears to contribute to the efficacy of verapamil in preventing reentrant-based atrial arrhythmias including atrial fibrillation. This study was designed to investigate the molecular and electrophysiological mechanism underlying the action of verapamil on human Kv1.5 (hKv1.5) channel that determines action potential duration and ERP in human atrium. Methods: Site-directed mutagenesis created 10 single-point mutations within pore region of hKv1.5 channel. Whole-cell patch-clamp method investigated the effect of verapamil on wild-type and mutant hKv1.5 channels heterologously expressed in Chinese hamster ovary cells. Docking simulation was conducted using open-state homology model of hKv1.5 channel pore. Results: Verapamil preferentially blocked hKv1.5 channel in its open state with IC50 of 2.4Â±0.6 mM (n = 6). The blocking effect of verapamil was significantly attenuated in T479A, T480A, I502A, V505A, I508A, L510A, V512A and V516A mutants, compared with wild-type hKv1.5 channel. Computer docking simulation predicted that verapamil is positioned within central cavity of channel pore and has contact with Thr479, Thr480, Val505, Ile508, Ala509, Val512, Pro513 and Val516. Conclusion: Verapamil acts as an open-channel blocker of hKv1.5 channel, presumably due to direct binding to specific amino acids within pore region of hKv1.5 channel, such as Thr479, Thr480, Val505, Ile508, Val512 and Val516. This blocking effect of verapamil on hKv1.5 channel appears to contribute at least partly to prolongation of atrial ERP and resultant antiarrhythmic action on atrial fibrillation in humans.","Identification of verapamil binding sites within human Kv1.5 channel using mutagenesis and docking simulation. Background/Aims: The phenylalkylamine class of L-type Ca2+ channel antagonist verapamil prolongs the effective refractory period (ERP) of human atrium, which appears to contribute to the efficacy of verapamil in preventing reentrant-based atrial arrhythmias including atrial fibrillation. This study was designed to investigate the molecular and electrophysiological mechanism underlying the action of verapamil on human Kv1.5 (hKv1.5) channel that determines action potential duration and ERP in human atrium. Methods: Site-directed mutagenesis created 10 single-point mutations within pore region of hKv1.5 channel. Whole-cell patch-clamp method investigated the effect of verapamil on wild-type and mutant hKv1.5 channels heterologously expressed in Chinese hamster ovary cells. Docking simulation was conducted using open-state homology model of hKv1.5 channel pore. Results: Verapamil preferentially blocked hKv1.5 channel in its open state with IC50 of 2.4Â±0.6 mM (n = 6). The blocking effect of verapamil was significantly attenuated in T479A, T480A, I502A, V505A, I508A, L510A, V512A and V516A mutants, compared with wild-type hKv1.5 channel. Computer docking simulation predicted that verapamil is positioned within central cavity of channel pore and has contact with Thr479, Thr480, Val505, Ile508, Ala509, Val512, Pro513 and Val516. Conclusion: Verapamil acts as an open-channel blocker of hKv1.5 channel, presumably due to direct binding to specific amino acids within pore region of hKv1.5 channel, such as Thr479, Thr480, Val505, Ile508, Val512 and Val516. This blocking effect of verapamil on hKv1.5 channel appears to contribute at least partly to prolongation of atrial ERP and resultant antiarrhythmic action on atrial fibrillation in humans."
0,Alpha7 nicotinic acetylcholine receptors and neural network synaptic transmission in human induced pluripotent stem cell-derived neurons,"The Î±7 nicotinic acetylcholine receptor has been extensively researched as a target for treatment of cognitive impairment in Alzheimer's disease and schizophrenia. Investigation of the Î±7 receptor is commonly performed in animals but it is critical to increase the biologically relevance of the model systems to fully capture the physiological role of the Î±7 receptor in humans. For example most humans, in contrast to animals, express the hybrid gene CHRFAM7A, the product of which modulates Î±7 receptor activity. In the present study, we used human induced pluripotent stem cell (hiPSC) derived neurons to establish a humanized Î±7 model. We established a cryobank of neural stem cells (NSCs) that could reproducibly be matured into neurons expressing neuronal markers and CHRNA7 and CHRFAM7A. The neurons responded to NMDA, GABA, and acetylcholine and exhibited synchronized spontaneous calcium oscillations. Gene expression studies and application of a range of Î±7 positive allosteric modulators (PNU-120595, TQS, JNJ-39393406 and AF58801) together with the Î±7 agonist PNU-282987 during measurement of intracellular calcium levels demonstrated the presence of functional Î±7 receptors in matured hiPSC-derived neuronal cultures. Pharmacological Î±7 activation also resulted in intracellular signaling as measured by ERK 1/2 phosphorylation and c-Fos protein expression. Moreover, PNU-120596 increased the frequency of the spontaneous calcium oscillations demonstrating implication of Î±7 receptors in human synaptic networks activity. Overall, we show that hiPSC derived neurons are an advanced in vitro model for studying human Î±7 receptor pharmacology and the involvement of this receptor in cellular processes as intracellular signaling and synaptic transmission.","Alpha7 nicotinic acetylcholine receptors and neural network synaptic transmission in human induced pluripotent stem cell-derived neurons. The Î±7 nicotinic acetylcholine receptor has been extensively researched as a target for treatment of cognitive impairment in Alzheimer's disease and schizophrenia. Investigation of the Î±7 receptor is commonly performed in animals but it is critical to increase the biologically relevance of the model systems to fully capture the physiological role of the Î±7 receptor in humans. For example most humans, in contrast to animals, express the hybrid gene CHRFAM7A, the product of which modulates Î±7 receptor activity. In the present study, we used human induced pluripotent stem cell (hiPSC) derived neurons to establish a humanized Î±7 model. We established a cryobank of neural stem cells (NSCs) that could reproducibly be matured into neurons expressing neuronal markers and CHRNA7 and CHRFAM7A. The neurons responded to NMDA, GABA, and acetylcholine and exhibited synchronized spontaneous calcium oscillations. Gene expression studies and application of a range of Î±7 positive allosteric modulators (PNU-120595, TQS, JNJ-39393406 and AF58801) together with the Î±7 agonist PNU-282987 during measurement of intracellular calcium levels demonstrated the presence of functional Î±7 receptors in matured hiPSC-derived neuronal cultures. Pharmacological Î±7 activation also resulted in intracellular signaling as measured by ERK 1/2 phosphorylation and c-Fos protein expression. Moreover, PNU-120596 increased the frequency of the spontaneous calcium oscillations demonstrating implication of Î±7 receptors in human synaptic networks activity. Overall, we show that hiPSC derived neurons are an advanced in vitro model for studying human Î±7 receptor pharmacology and the involvement of this receptor in cellular processes as intracellular signaling and synaptic transmission."
0,A deep learning framework for neuroscience,"Systems neuroscience seeks explanations for how the brain implements a wide variety of perceptual, cognitive and motor tasks. Conversely, artificial intelligence attempts to design computational systems based on the tasks they will have to solve. In artificial neural networks, the three components specified by design are the objective functions, the learning rules and the architectures. With the growing success of deep learning, which utilizes brain-inspired architectures, these three designed components have increasingly become central to how we model, engineer and optimize complex artificial learning systems. Here we argue that a greater focus on these components would also benefit systems neuroscience. We give examples of how this optimization-based framework can drive theoretical and experimental progress in neuroscience. We contend that this principled perspective on systems neuroscience will help to generate more rapid progress.","A deep learning framework for neuroscience. Systems neuroscience seeks explanations for how the brain implements a wide variety of perceptual, cognitive and motor tasks. Conversely, artificial intelligence attempts to design computational systems based on the tasks they will have to solve. In artificial neural networks, the three components specified by design are the objective functions, the learning rules and the architectures. With the growing success of deep learning, which utilizes brain-inspired architectures, these three designed components have increasingly become central to how we model, engineer and optimize complex artificial learning systems. Here we argue that a greater focus on these components would also benefit systems neuroscience. We give examples of how this optimization-based framework can drive theoretical and experimental progress in neuroscience. We contend that this principled perspective on systems neuroscience will help to generate more rapid progress."
0,"Development and implementation of a quality improvement toolkit, iron deficiency in pregnancy with maternal iron optimization (IRON MOM): A before-and-after study",,
0,"Optimal dose of selective serotonin reuptake inhibitors, venlafaxine, and mirtazapine in major depression: a systematic review and dose-response meta-analysis","Background: Depression is the single largest contributor to non-fatal health loss worldwide. Second-generation antidepressants are the first-line option for pharmacological management of depression. Optimising their use is crucial in reducing the burden of depression; however, debate about their dose dependency and their optimal target dose is ongoing. We have aimed to summarise the currently available best evidence to inform this clinical question. Methods: We did a systematic review and dose-response meta-analysis of double-blind, randomised controlled trials that examined fixed doses of five selective serotonin reuptake inhibitors (SSRIs; citalopram, escitalopram, fluoxetine, paroxetine, and sertraline), venlafaxine, or mirtazapine in the acute treatment of adults (aged 18 years or older) with major depression, identified from the Cochrane Central Register of Controlled Trials, CINAHL, Embase, LILACS, MEDLINE, PsycINFO, AMED, PSYNDEX, websites of drug licensing agencies and pharmaceutical companies, and trial registries. We imposed no language restrictions, and the search was updated until Jan 8, 2016. Doses of SSRIs were converted to fluoxetine equivalents. Trials of antidepressants for patients with depression and a serious concomitant physical illness were excluded. The main outcomes were efficacy (treatment response defined as 50% or greater reduction in depression severity), tolerability (dropouts due to adverse effects), and acceptability (dropouts for any reasons), all after a median of 8 weeks of treatment (range 4â€“12 weeks). We used a random-effects, dose-response meta-analysis model with flexible splines for SSRIs, venlafaxine, and mirtazapine. Findings: 28 554 records were identified through our search (24 524 published and 4030 unpublished records). 561 published and 121 unpublished full-text records were assessed for eligibility, and 77 studies were included (19 364 participants; mean age 42Â·5 years, SD 11Â·0; 7156 [60Â·9%] of 11 749 reported were women). For SSRIs (99 treatment groups), the dose-efficacy curve showed a gradual increase up to doses between 20 mg and 40 mg fluoxetine equivalents, and a flat to decreasing trend through the higher licensed doses up to 80 mg fluoxetine equivalents. Dropouts due to adverse effects increased steeply through the examined range. The relationship between the dose and dropouts for any reason indicated optimal acceptability for the SSRIs in the lower licensed range between 20 mg and 40 mg fluoxetine equivalents. Venlafaxine (16 treatment groups) had an initially increasing dose-efficacy relationship up to around 75â€“150 mg, followed by a more modest increase, whereas for mirtazapine (11 treatment groups) efficacy increased up to a dose of about 30 mg and then decreased. Both venlafaxine and mirtazapine showed optimal acceptability in the lower range of their licensed dose. These results were robust to several sensitivity analyses. Interpretation: For the most commonly used second-generation antidepressants, the lower range of the licensed dose achieves the optimal balance between efficacy, tolerability, and acceptability in the acute treatment of major depression. Funding: Japan Society for the Promotion of Science, Swiss National Science Foundation, and National Institute for Health Research.","Optimal dose of selective serotonin reuptake inhibitors, venlafaxine, and mirtazapine in major depression: a systematic review and dose-response meta-analysis. Background: Depression is the single largest contributor to non-fatal health loss worldwide. Second-generation antidepressants are the first-line option for pharmacological management of depression. Optimising their use is crucial in reducing the burden of depression; however, debate about their dose dependency and their optimal target dose is ongoing. We have aimed to summarise the currently available best evidence to inform this clinical question. Methods: We did a systematic review and dose-response meta-analysis of double-blind, randomised controlled trials that examined fixed doses of five selective serotonin reuptake inhibitors (SSRIs; citalopram, escitalopram, fluoxetine, paroxetine, and sertraline), venlafaxine, or mirtazapine in the acute treatment of adults (aged 18 years or older) with major depression, identified from the Cochrane Central Register of Controlled Trials, CINAHL, Embase, LILACS, MEDLINE, PsycINFO, AMED, PSYNDEX, websites of drug licensing agencies and pharmaceutical companies, and trial registries. We imposed no language restrictions, and the search was updated until Jan 8, 2016. Doses of SSRIs were converted to fluoxetine equivalents. Trials of antidepressants for patients with depression and a serious concomitant physical illness were excluded. The main outcomes were efficacy (treatment response defined as 50% or greater reduction in depression severity), tolerability (dropouts due to adverse effects), and acceptability (dropouts for any reasons), all after a median of 8 weeks of treatment (range 4â€“12 weeks). We used a random-effects, dose-response meta-analysis model with flexible splines for SSRIs, venlafaxine, and mirtazapine. Findings: 28 554 records were identified through our search (24 524 published and 4030 unpublished records). 561 published and 121 unpublished full-text records were assessed for eligibility, and 77 studies were included (19 364 participants; mean age 42Â·5 years, SD 11Â·0; 7156 [60Â·9%] of 11 749 reported were women). For SSRIs (99 treatment groups), the dose-efficacy curve showed a gradual increase up to doses between 20 mg and 40 mg fluoxetine equivalents, and a flat to decreasing trend through the higher licensed doses up to 80 mg fluoxetine equivalents. Dropouts due to adverse effects increased steeply through the examined range. The relationship between the dose and dropouts for any reason indicated optimal acceptability for the SSRIs in the lower licensed range between 20 mg and 40 mg fluoxetine equivalents. Venlafaxine (16 treatment groups) had an initially increasing dose-efficacy relationship up to around 75â€“150 mg, followed by a more modest increase, whereas for mirtazapine (11 treatment groups) efficacy increased up to a dose of about 30 mg and then decreased. Both venlafaxine and mirtazapine showed optimal acceptability in the lower range of their licensed dose. These results were robust to several sensitivity analyses. Interpretation: For the most commonly used second-generation antidepressants, the lower range of the licensed dose achieves the optimal balance between efficacy, tolerability, and acceptability in the acute treatment of major depression. Funding: Japan Society for the Promotion of Science, Swiss National Science Foundation, and National Institute for Health Research."
0,Performance of a Multigene Genomic Classifier in Thyroid Nodules with Indeterminate Cytology: A Prospective Blinded Multicenter Study,"Importance: Approximately 20% of fine-needle aspirations (FNA) of thyroid nodules have indeterminate cytology, most frequently Bethesda category III or IV. Diagnostic surgeries can be avoided for these patients if the nodules are reliably diagnosed as benign without surgery. Objective: To determine the diagnostic accuracy of a multigene classifier (GC) test (ThyroSeq v3) for cytologically indeterminate thyroid nodules. Design, Setting, and Participants: Prospective, blinded cohort study conducted at 10 medical centers, with 782 patients with 1013 nodules enrolled. Eligibility criteria were met in 256 patients with 286 nodules; central pathology review was performed on 274 nodules. Interventions: A total of 286 FNA samples from thyroid nodules underwent molecular analysis using the multigene GC (ThyroSeq v3). Main Outcomes and Measures: The primary outcome was diagnostic accuracy of the test for thyroid nodules with Bethesda III and IV cytology. The secondary outcome was prediction of cancer by specific genetic alterations in Bethesda III to V nodules. Results: Of the 286 cytologically indeterminate nodules, 206 (72%) were benign, 69 (24%) malignant, and 11 (4%) noninvasive follicular thyroid neoplasms with papillary-like nuclei (NIFTP). A total of 257 (90%) nodules (154 Bethesda III, 93 Bethesda IV, and 10 Bethesda V) had informative GC analysis, with 61% classified as negative and 39% as positive. In Bethesda III and IV nodules combined, the test demonstrated a 94% (95% CI, 86%-98%) sensitivity and 82% (95% CI, 75%-87%) specificity. With a cancer/NIFTP prevalence of 28%, the negative predictive value (NPV) was 97% (95% CI, 93%-99%) and the positive predictive value (PPV) was 66% (95% CI, 56%-75%). The observed 3% false-negative rate was similar to that of benign cytology, and the missed cancers were all low-risk tumors. Among nodules testing positive, specific groups of genetic alterations had cancer probabilities varying from 59% to 100%. Conclusions and Relevance: In this prospective, blinded, multicenter study, the multigene GC test demonstrated a high sensitivity/NPV and reasonably high specificity/PPV, which may obviate diagnostic surgery in up to 61% of patients with Bethesda III to IV indeterminate nodules, and up to 82% of all benign nodules with indeterminate cytology. Information on specific genetic alterations obtained from FNA may help inform individualized treatment of patients with a positive test result..","Performance of a Multigene Genomic Classifier in Thyroid Nodules with Indeterminate Cytology: A Prospective Blinded Multicenter Study. Importance: Approximately 20% of fine-needle aspirations (FNA) of thyroid nodules have indeterminate cytology, most frequently Bethesda category III or IV. Diagnostic surgeries can be avoided for these patients if the nodules are reliably diagnosed as benign without surgery. Objective: To determine the diagnostic accuracy of a multigene classifier (GC) test (ThyroSeq v3) for cytologically indeterminate thyroid nodules. Design, Setting, and Participants: Prospective, blinded cohort study conducted at 10 medical centers, with 782 patients with 1013 nodules enrolled. Eligibility criteria were met in 256 patients with 286 nodules; central pathology review was performed on 274 nodules. Interventions: A total of 286 FNA samples from thyroid nodules underwent molecular analysis using the multigene GC (ThyroSeq v3). Main Outcomes and Measures: The primary outcome was diagnostic accuracy of the test for thyroid nodules with Bethesda III and IV cytology. The secondary outcome was prediction of cancer by specific genetic alterations in Bethesda III to V nodules. Results: Of the 286 cytologically indeterminate nodules, 206 (72%) were benign, 69 (24%) malignant, and 11 (4%) noninvasive follicular thyroid neoplasms with papillary-like nuclei (NIFTP). A total of 257 (90%) nodules (154 Bethesda III, 93 Bethesda IV, and 10 Bethesda V) had informative GC analysis, with 61% classified as negative and 39% as positive. In Bethesda III and IV nodules combined, the test demonstrated a 94% (95% CI, 86%-98%) sensitivity and 82% (95% CI, 75%-87%) specificity. With a cancer/NIFTP prevalence of 28%, the negative predictive value (NPV) was 97% (95% CI, 93%-99%) and the positive predictive value (PPV) was 66% (95% CI, 56%-75%). The observed 3% false-negative rate was similar to that of benign cytology, and the missed cancers were all low-risk tumors. Among nodules testing positive, specific groups of genetic alterations had cancer probabilities varying from 59% to 100%. Conclusions and Relevance: In this prospective, blinded, multicenter study, the multigene GC test demonstrated a high sensitivity/NPV and reasonably high specificity/PPV, which may obviate diagnostic surgery in up to 61% of patients with Bethesda III to IV indeterminate nodules, and up to 82% of all benign nodules with indeterminate cytology. Information on specific genetic alterations obtained from FNA may help inform individualized treatment of patients with a positive test result.."
0,Prediction of comorbid diseases using weighted geometric embedding of human interactome,"Background: Comorbidity is the phenomenon of two or more diseases occurring simultaneously not by random chance and presents great challenges to accurate diagnosis and treatment. As an effort toward better understanding the genetic causes of comorbidity, in this work, we have developed a computational method to predict comorbid diseases. Two diseases sharing common genes tend to increase their comorbidity. Previous work shows that after mapping the associated genes onto the human interactome the distance between the two disease modules (subgraphs) is correlated with comorbidity. Methods: To fully incorporate structural characteristics of interactome as features into prediction of comorbidity, our method embeds the human interactome into a high dimensional geometric space with weights assigned to the network edges and uses the projection onto different dimension to ""fingerprint"" disease modules. A supervised machine learning classifier is then trained to discriminate comorbid diseases versus non-comorbid diseases. Results: In cross-validation using a benchmark dataset of more than 10,000 disease pairs, we report that our model achieves remarkable performance of ROC score = 0.90 for comorbidity threshold at relative risk RR = 0 and 0.76 for comorbidity threshold at RR = 1, and significantly outperforms the previous method and the interactome generated by annotated data. To further incorporate prior knowledge pathways association with diseases, we weight the protein-protein interaction network edges according to their frequency of occurring in those pathways in such a way that edges with higher frequency will more likely be selected in the minimum spanning tree for geometric embedding. Such weighted embedding is shown to lead to further improvement of comorbid disease prediction. Conclusion: The work demonstrates that embedding the two-dimension planar graph of human interactome into a high dimensional geometric space allows for characterizing and capturing disease modules (subgraphs formed by the disease associated genes) from multiple perspectives, and hence provides enriched features for a supervised classifier to discriminate comorbid disease pairs from non-comorbid disease pairs more accurately than based on simply the module separation.","Prediction of comorbid diseases using weighted geometric embedding of human interactome. Background: Comorbidity is the phenomenon of two or more diseases occurring simultaneously not by random chance and presents great challenges to accurate diagnosis and treatment. As an effort toward better understanding the genetic causes of comorbidity, in this work, we have developed a computational method to predict comorbid diseases. Two diseases sharing common genes tend to increase their comorbidity. Previous work shows that after mapping the associated genes onto the human interactome the distance between the two disease modules (subgraphs) is correlated with comorbidity. Methods: To fully incorporate structural characteristics of interactome as features into prediction of comorbidity, our method embeds the human interactome into a high dimensional geometric space with weights assigned to the network edges and uses the projection onto different dimension to ""fingerprint"" disease modules. A supervised machine learning classifier is then trained to discriminate comorbid diseases versus non-comorbid diseases. Results: In cross-validation using a benchmark dataset of more than 10,000 disease pairs, we report that our model achieves remarkable performance of ROC score = 0.90 for comorbidity threshold at relative risk RR = 0 and 0.76 for comorbidity threshold at RR = 1, and significantly outperforms the previous method and the interactome generated by annotated data. To further incorporate prior knowledge pathways association with diseases, we weight the protein-protein interaction network edges according to their frequency of occurring in those pathways in such a way that edges with higher frequency will more likely be selected in the minimum spanning tree for geometric embedding. Such weighted embedding is shown to lead to further improvement of comorbid disease prediction. Conclusion: The work demonstrates that embedding the two-dimension planar graph of human interactome into a high dimensional geometric space allows for characterizing and capturing disease modules (subgraphs formed by the disease associated genes) from multiple perspectives, and hence provides enriched features for a supervised classifier to discriminate comorbid disease pairs from non-comorbid disease pairs more accurately than based on simply the module separation."
0,"New Biologics for Severe Asthma: What Patients, What Agents, What Results, at What Cost?",,
0,Development of a Selective CDK7 Covalent Inhibitor Reveals Predominant Cell-Cycle Phenotype,"Cyclin-dependent kinase 7 (CDK7) regulates both cell cycle and transcription, but its precise role remains elusive. We previously described THZ1, a CDK7 inhibitor, which dramatically inhibits superenhancer-associated gene expression. However, potent CDK12/13 off-target activity obscured CDK7s contribution to this phenotype. Here, we describe the discovery of a highly selective covalent CDK7 inhibitor. YKL-5-124 causes arrest at the G1/S transition and inhibition of E2F-driven gene expression; these effects are rescued by a CDK7 mutant unable to covalently engage YKL-5-124, demonstrating on-target specificity. Unlike THZ1, treatment with YKL-5-124 resulted in no change to RNA polymerase II C-terminal domain phosphorylation; however, inhibition could be reconstituted by combining YKL-5-124 and THZ531, a selective CDK12/13 inhibitor, revealing potential redundancies in CDK control of gene transcription. These findings highlight the importance of CDK7/12/13 polypharmacology for anti-cancer activity of THZ1 and posit that selective inhibition of CDK7 may be useful for treatment of cancers marked by E2F misregulation. Olson et al. describe the development and characterization of YKL-5-124, a potent, selective, and covalent CDK7 inhibitor. YKL-5-124 displays biochemical and cellular selectivity for CDK7 over CDK12/13, structurally related kinases. CDK7 inhibition by YKL-5-124 induces a strong cell-cycle arrest and a surprisingly weak effect on RNA Pol II phosphorylation.","Development of a Selective CDK7 Covalent Inhibitor Reveals Predominant Cell-Cycle Phenotype. Cyclin-dependent kinase 7 (CDK7) regulates both cell cycle and transcription, but its precise role remains elusive. We previously described THZ1, a CDK7 inhibitor, which dramatically inhibits superenhancer-associated gene expression. However, potent CDK12/13 off-target activity obscured CDK7s contribution to this phenotype. Here, we describe the discovery of a highly selective covalent CDK7 inhibitor. YKL-5-124 causes arrest at the G1/S transition and inhibition of E2F-driven gene expression; these effects are rescued by a CDK7 mutant unable to covalently engage YKL-5-124, demonstrating on-target specificity. Unlike THZ1, treatment with YKL-5-124 resulted in no change to RNA polymerase II C-terminal domain phosphorylation; however, inhibition could be reconstituted by combining YKL-5-124 and THZ531, a selective CDK12/13 inhibitor, revealing potential redundancies in CDK control of gene transcription. These findings highlight the importance of CDK7/12/13 polypharmacology for anti-cancer activity of THZ1 and posit that selective inhibition of CDK7 may be useful for treatment of cancers marked by E2F misregulation. Olson et al. describe the development and characterization of YKL-5-124, a potent, selective, and covalent CDK7 inhibitor. YKL-5-124 displays biochemical and cellular selectivity for CDK7 over CDK12/13, structurally related kinases. CDK7 inhibition by YKL-5-124 induces a strong cell-cycle arrest and a surprisingly weak effect on RNA Pol II phosphorylation."
0,Pathological priming causes developmental gene network heterochronicity in autistic subject-derived neurons,,
0,Combined estimation of disease progression and retention on antiretroviral therapy among treated individuals with HIV in the USA: a modelling study,"Background: Accurately estimating HIV disease progression and retention on antiretroviral therapy (ART) can help inform interventions to control HIV microepidemics and mathematical models used to inform health-resource allocation decisions. Our objective was to estimate the monthly probabilities of on-ART CD4 T-cell count progression, mortality, ART dropout, and ART reinitiation using a continuous-time multistate Markov model. We also aimed to validate health-state transition probability estimates to ensure they accurately reproduced the regional HIV microepidemics across the USA. Methods: In our modelling study, we considered a cohort of patients from the HIV Research Network, a consortium of 17 adult and paediatric HIV-care providers located in the northeastern (n=8), southern (n=5), and western (n=4) regions of the USA. Individuals aged 15 years or older who were in HIV care (defined as one CD4 test and one HIV-care visit in a calendar year period) with at least one ART prescription between Jan 1, 2010, and Dec 31, 2015, were included in the analysis. We used continuous-time multistate Markov models to estimate transitions between CD4 strata and between on-ART and off-ART states. We examined and adjusted for differences in probability of transition by region, race or ethnicity, sex, HIV risk group, and other baseline clinical indicators. Findings: The median age of the 32 242 individuals included in the analysis was 44 years (interquartile range 35â€“51). Over a median follow-up of 4Â·9 years (2Â·6â€“6Â·0), 8614 (26Â·7%) of 32 242 people interrupted ART and 1325 (4Â·1%) of 32 242 people died. Women, men who have sex with men, and individuals with no previous ART experience had greater increases in CD4 cell counts, whereas black people and people who inject drugs had increased probabilities of ART dropout and faster disease progression. Regardless of CD4 strata, individuals had increased hazard for ART dropout if they were from the south (adjusted hazard ratio [aHR] range from 1Â·91, 95% CI 1Â·71â€“2Â·13, to 2Â·45, 2Â·29â€“2Â·62) or the west (aHR range from 1Â·29, 1Â·10â€“1Â·51, to 1Â·66, 1Â·51â€“1Â·82) of the USA, compared with individuals from the northeast USA. Interpretation: Our results show heterogeneities in disease progression during ART and probability of ART retention across race and ethnicity, HIV risk groups, and regions. These differences should be viewed as targets for intervention and should be incorporated in mathematical models of regional HIV microepidemics in the USA. Funding: US National Institutes of Health, Agency for Healthcare Research and Quality, and Health Resources and Services Administration.","Combined estimation of disease progression and retention on antiretroviral therapy among treated individuals with HIV in the USA: a modelling study. Background: Accurately estimating HIV disease progression and retention on antiretroviral therapy (ART) can help inform interventions to control HIV microepidemics and mathematical models used to inform health-resource allocation decisions. Our objective was to estimate the monthly probabilities of on-ART CD4 T-cell count progression, mortality, ART dropout, and ART reinitiation using a continuous-time multistate Markov model. We also aimed to validate health-state transition probability estimates to ensure they accurately reproduced the regional HIV microepidemics across the USA. Methods: In our modelling study, we considered a cohort of patients from the HIV Research Network, a consortium of 17 adult and paediatric HIV-care providers located in the northeastern (n=8), southern (n=5), and western (n=4) regions of the USA. Individuals aged 15 years or older who were in HIV care (defined as one CD4 test and one HIV-care visit in a calendar year period) with at least one ART prescription between Jan 1, 2010, and Dec 31, 2015, were included in the analysis. We used continuous-time multistate Markov models to estimate transitions between CD4 strata and between on-ART and off-ART states. We examined and adjusted for differences in probability of transition by region, race or ethnicity, sex, HIV risk group, and other baseline clinical indicators. Findings: The median age of the 32 242 individuals included in the analysis was 44 years (interquartile range 35â€“51). Over a median follow-up of 4Â·9 years (2Â·6â€“6Â·0), 8614 (26Â·7%) of 32 242 people interrupted ART and 1325 (4Â·1%) of 32 242 people died. Women, men who have sex with men, and individuals with no previous ART experience had greater increases in CD4 cell counts, whereas black people and people who inject drugs had increased probabilities of ART dropout and faster disease progression. Regardless of CD4 strata, individuals had increased hazard for ART dropout if they were from the south (adjusted hazard ratio [aHR] range from 1Â·91, 95% CI 1Â·71â€“2Â·13, to 2Â·45, 2Â·29â€“2Â·62) or the west (aHR range from 1Â·29, 1Â·10â€“1Â·51, to 1Â·66, 1Â·51â€“1Â·82) of the USA, compared with individuals from the northeast USA. Interpretation: Our results show heterogeneities in disease progression during ART and probability of ART retention across race and ethnicity, HIV risk groups, and regions. These differences should be viewed as targets for intervention and should be incorporated in mathematical models of regional HIV microepidemics in the USA. Funding: US National Institutes of Health, Agency for Healthcare Research and Quality, and Health Resources and Services Administration."
0,"Comment on ""The Limit for Artificial Intelligence's Potentiality in Surgery Doesn't Have to Be the Surgeon""",,
0,What Can We Learn from the RSNA Pediatric Bone Age Machine Learning Challenge?,,
0,Mapping the Global Chromatin Connectivity Network for Sox2 Function in Neural Stem Cell Maintenance,"Bertolini et al. report that long-range chromatin interactions in neural stem cells (NSCs) are enriched in Sox2-bound enhancers; in Sox2-deleted NSCs, interactions are reduced. Genes downregulated in Sox2-deleted cells are enriched in interactions with enhancers normally Sox2-bound. Overexpression of Socs3, a gene downregulated in mutant NSCs, rescues long-term NSC self-renewal.","Mapping the Global Chromatin Connectivity Network for Sox2 Function in Neural Stem Cell Maintenance. Bertolini et al. report that long-range chromatin interactions in neural stem cells (NSCs) are enriched in Sox2-bound enhancers; in Sox2-deleted NSCs, interactions are reduced. Genes downregulated in Sox2-deleted cells are enriched in interactions with enhancers normally Sox2-bound. Overexpression of Socs3, a gene downregulated in mutant NSCs, rescues long-term NSC self-renewal."
0,Primate Amygdala Neurons Simulate Decision Processes of Social Partners,,
0,The Management of Stroke Rehabilitation: A Synopsis of the 2019 US Department of Veterans Affairs and US Department of Defense Clinical Practice Guideline,,
0,Nonrigid reconstruction of 3D breast surfaces with a low-cost RGBD camera for surgical planning and aesthetic evaluation,,
0,Re: Robot-assisted Laparoscopic Prostatectomy Versus Open Radical Retropubic Prostatectomy: 24-month Outcomes from a Randomised Controlled Study,,
0,Data-driven Development of ROTEM and TEG Algorithms for the Management of Trauma Hemorrhage: A Prospective Observational Multicenter Study,"OBJECTIVE: Developing pragmatic data-driven algorithms for management of trauma induced coagulopathy (TIC) during trauma hemorrhage for viscoelastic hemostatic assays (VHAs). BACKGROUND: Admission data from conventional coagulation tests (CCT), rotational thrombelastometry (ROTEM) and thrombelastography (TEG) were collected prospectively at 6 European trauma centers during 2008 to 2013. METHODS: To identify significant VHA parameters capable of detecting TIC (defined as INR > 1.2), hypofibrinogenemia (< 2.0 g/L), and thrombocytopenia (< 100 x10/L), univariate regression models were constructed. Area under the curve (AUC) was calculated, and threshold values for TEG and ROTEM parameters with 70% sensitivity were included in the algorithms. RESULTS: A total of, 2287 adult trauma patients (ROTEM: 2019 and TEG: 968) were enrolled. FIBTEM clot amplitude at 5 minutes (CA5) had the largest AUC and 10 mm detected hypofibrinogenemia with 70% sensitivity. The corresponding value for functional fibrinogen (FF) TEG maximum amplitude (MA) was 19 mm. Thrombocytopenia was similarly detected using the calculated threshold EXTEM-FIBTEM CA5 30 mm. The corresponding rTEG-FF TEG MA was 46 mm. TIC was identified by EXTEM CA5 41 mm, rTEG MA 64 mm (80% sensitivity). For hyperfibrinolysis, we examined the relationship between viscoelastic lysis parameters and clinical outcomes, with resulting threshold values of 85% for EXTEM Li30 and 10% for rTEG Ly30.Based on these analyses, we constructed algorithms for ROTEM, TEG, and CCTs to be used in addition to ratio driven transfusion and tranexamic acid. CONCLUSIONS: We describe a systematic approach to define threshold parameters for ROTEM and TEG. These parameters were incorporated into algorithms to support data-driven adjustments of resuscitation with therapeutics, to optimize damage control resuscitation practice in trauma.","Data-driven Development of ROTEM and TEG Algorithms for the Management of Trauma Hemorrhage: A Prospective Observational Multicenter Study. OBJECTIVE: Developing pragmatic data-driven algorithms for management of trauma induced coagulopathy (TIC) during trauma hemorrhage for viscoelastic hemostatic assays (VHAs). BACKGROUND: Admission data from conventional coagulation tests (CCT), rotational thrombelastometry (ROTEM) and thrombelastography (TEG) were collected prospectively at 6 European trauma centers during 2008 to 2013. METHODS: To identify significant VHA parameters capable of detecting TIC (defined as INR > 1.2), hypofibrinogenemia (< 2.0 g/L), and thrombocytopenia (< 100 x10/L), univariate regression models were constructed. Area under the curve (AUC) was calculated, and threshold values for TEG and ROTEM parameters with 70% sensitivity were included in the algorithms. RESULTS: A total of, 2287 adult trauma patients (ROTEM: 2019 and TEG: 968) were enrolled. FIBTEM clot amplitude at 5 minutes (CA5) had the largest AUC and 10 mm detected hypofibrinogenemia with 70% sensitivity. The corresponding value for functional fibrinogen (FF) TEG maximum amplitude (MA) was 19 mm. Thrombocytopenia was similarly detected using the calculated threshold EXTEM-FIBTEM CA5 30 mm. The corresponding rTEG-FF TEG MA was 46 mm. TIC was identified by EXTEM CA5 41 mm, rTEG MA 64 mm (80% sensitivity). For hyperfibrinolysis, we examined the relationship between viscoelastic lysis parameters and clinical outcomes, with resulting threshold values of 85% for EXTEM Li30 and 10% for rTEG Ly30.Based on these analyses, we constructed algorithms for ROTEM, TEG, and CCTs to be used in addition to ratio driven transfusion and tranexamic acid. CONCLUSIONS: We describe a systematic approach to define threshold parameters for ROTEM and TEG. These parameters were incorporated into algorithms to support data-driven adjustments of resuscitation with therapeutics, to optimize damage control resuscitation practice in trauma."
0,Discovery of potent necroptosis inhibitors targeting RIPK1 kinase activity for the treatment of inflammatory disorder and cancer metastasis,"Necroptosis is a form of regulated necrosis controlled by receptor-interacting kinase 1 (RIPK1 or RIP1), RIPK3 (RIP3), and pseudokinase mixed lineage kinase domain-like protein (MLKL). Increasing evidence suggests that necroptosis is closely associated with pathologies including inflammatory diseases, neurodegenerative diseases, and cancer metastasis. Herein, we discovered the small-molecule PK6 and its derivatives as a novel class of necroptosis inhibitors that directly block the kinase activity of RIPK1. Optimization of PK6 led to PK68, which has improved efficacy for the inhibition of RIPK1-dependent necroptosis, with an EC50 of around 14â€“22 nM in human and mouse cells. PK68 efficiently blocks cellular activation of RIPK1, RIPK3, and MLKL upon necroptosis stimuli. PK68 displays reasonable selectivity for inhibition of RIPK1 kinase activity and favorable pharmacokinetic properties. Importantly, PK68 provides strong protection against TNF-Î±-induced systemic inflammatory response syndrome in vivo. Moreover, pre-treatment of PK68 significantly represses metastasis of both melanoma cells and lung carcinoma cells in mice. Together, our study demonstrates that PK68 is a potent and selective inhibitor of RIPK1 and also highlights its great potential for use in the treatment of inflammatory disorders and cancer metastasis.","Discovery of potent necroptosis inhibitors targeting RIPK1 kinase activity for the treatment of inflammatory disorder and cancer metastasis. Necroptosis is a form of regulated necrosis controlled by receptor-interacting kinase 1 (RIPK1 or RIP1), RIPK3 (RIP3), and pseudokinase mixed lineage kinase domain-like protein (MLKL). Increasing evidence suggests that necroptosis is closely associated with pathologies including inflammatory diseases, neurodegenerative diseases, and cancer metastasis. Herein, we discovered the small-molecule PK6 and its derivatives as a novel class of necroptosis inhibitors that directly block the kinase activity of RIPK1. Optimization of PK6 led to PK68, which has improved efficacy for the inhibition of RIPK1-dependent necroptosis, with an EC50 of around 14â€“22 nM in human and mouse cells. PK68 efficiently blocks cellular activation of RIPK1, RIPK3, and MLKL upon necroptosis stimuli. PK68 displays reasonable selectivity for inhibition of RIPK1 kinase activity and favorable pharmacokinetic properties. Importantly, PK68 provides strong protection against TNF-Î±-induced systemic inflammatory response syndrome in vivo. Moreover, pre-treatment of PK68 significantly represses metastasis of both melanoma cells and lung carcinoma cells in mice. Together, our study demonstrates that PK68 is a potent and selective inhibitor of RIPK1 and also highlights its great potential for use in the treatment of inflammatory disorders and cancer metastasis."
0,Deep Learning for Detection of Myocardial Scar Tissue: Goodbye to Gadolinium?,,
0,Organ Changes Associated with Provider-Assessed Responses in Patients with Chronic Graft-versus-Host Disease,"Assessments of overall improvement and worsening of chronic graft-versus-host disease (GVHD) manifestations by the algorithm recommended by National Institutes of Health (NIH) response criteria do not align closely with those reported by providers, particularly when patients have mixed responses with improvement in some manifestations but worsening in others. To elucidate the changes that influence provider assessment of response, we used logistic regression to generate an overall change index based on specific manifestations of chronic GVHD measured at baseline and 6 months later. We hypothesized that this overall change index would correlate strongly with overall improvement as determined by providers. The analysis included 488 patients from 2 prospective observational studies who were randomly assigned in a 3:2 ratio to discovery and replication cohorts. Changes in bilirubin and scores of the lower gastrointestinal tract, mouth, joint/fascia, lung, and skin were correlated with provider-assessed improvement, suggesting that the main NIH response measures capture relevant information. Conversely, changes in the eye, esophagus, and upper gastrointestinal tract did not correlate with provider-assessed response, suggesting that these scales could be modified or dropped from the NIH response assessment. The area under the receiver operator characteristic curve in the replication cohort was 0.72, indicating that the scoring algorithm for overall change based on NIH response measures is not well calibrated with provider-assessed response.","Organ Changes Associated with Provider-Assessed Responses in Patients with Chronic Graft-versus-Host Disease. Assessments of overall improvement and worsening of chronic graft-versus-host disease (GVHD) manifestations by the algorithm recommended by National Institutes of Health (NIH) response criteria do not align closely with those reported by providers, particularly when patients have mixed responses with improvement in some manifestations but worsening in others. To elucidate the changes that influence provider assessment of response, we used logistic regression to generate an overall change index based on specific manifestations of chronic GVHD measured at baseline and 6 months later. We hypothesized that this overall change index would correlate strongly with overall improvement as determined by providers. The analysis included 488 patients from 2 prospective observational studies who were randomly assigned in a 3:2 ratio to discovery and replication cohorts. Changes in bilirubin and scores of the lower gastrointestinal tract, mouth, joint/fascia, lung, and skin were correlated with provider-assessed improvement, suggesting that the main NIH response measures capture relevant information. Conversely, changes in the eye, esophagus, and upper gastrointestinal tract did not correlate with provider-assessed response, suggesting that these scales could be modified or dropped from the NIH response assessment. The area under the receiver operator characteristic curve in the replication cohort was 0.72, indicating that the scoring algorithm for overall change based on NIH response measures is not well calibrated with provider-assessed response."
0,Genomic data analysis workflows for tumors from patient-derived xenografts (PDXs): Challenges and guidelines,"Background: Patient-derived xenograft (PDX) models are in vivo models of human cancer that have been used for translational cancer research and therapy selection for individual patients. The Jackson Laboratory (JAX) PDX resource comprises 455 models originating from 34 different primary sites (as of 05/08/2019). The models undergo rigorous quality control and are genomically characterized to identify somatic mutations, copy number alterations, and transcriptional profiles. Bioinformatics workflows for analyzing genomic data obtained from human tumors engrafted in a mouse host (i.e., Patient-Derived Xenografts; PDXs) must address challenges such as discriminating between mouse and human sequence reads and accurately identifying somatic mutations and copy number alterations when paired non-tumor DNA from the patient is not available for comparison. Results: We report here data analysis workflows and guidelines that address these challenges and achieve reliable identification of somatic mutations, copy number alterations, and transcriptomic profiles of tumors from PDX models that lack genomic data from paired non-tumor tissue for comparison. Our workflows incorporate commonly used software and public databases but are tailored to address the specific challenges of PDX genomics data analysis through parameter tuning and customized data filters and result in improved accuracy for the detection of somatic alterations in PDX models. We also report a gene expression-based classifier that can identify EBV-transformed tumors. We validated our analytical approaches using data simulations and demonstrated the overall concordance of the genomic properties of xenograft tumors with data from primary human tumors in The Cancer Genome Atlas (TCGA). Conclusions: The analysis workflows that we have developed to accurately predict somatic profiles of tumors from PDX models that lack normal tissue for comparison enable the identification of the key oncogenic genomic and expression signatures to support model selection and/or biomarker development in therapeutic studies. A reference implementation of our analysis recommendations is available at https://github.com/TheJacksonLaboratory/PDX-Analysis-Workflows.","Genomic data analysis workflows for tumors from patient-derived xenografts (PDXs): Challenges and guidelines. Background: Patient-derived xenograft (PDX) models are in vivo models of human cancer that have been used for translational cancer research and therapy selection for individual patients. The Jackson Laboratory (JAX) PDX resource comprises 455 models originating from 34 different primary sites (as of 05/08/2019). The models undergo rigorous quality control and are genomically characterized to identify somatic mutations, copy number alterations, and transcriptional profiles. Bioinformatics workflows for analyzing genomic data obtained from human tumors engrafted in a mouse host (i.e., Patient-Derived Xenografts; PDXs) must address challenges such as discriminating between mouse and human sequence reads and accurately identifying somatic mutations and copy number alterations when paired non-tumor DNA from the patient is not available for comparison. Results: We report here data analysis workflows and guidelines that address these challenges and achieve reliable identification of somatic mutations, copy number alterations, and transcriptomic profiles of tumors from PDX models that lack genomic data from paired non-tumor tissue for comparison. Our workflows incorporate commonly used software and public databases but are tailored to address the specific challenges of PDX genomics data analysis through parameter tuning and customized data filters and result in improved accuracy for the detection of somatic alterations in PDX models. We also report a gene expression-based classifier that can identify EBV-transformed tumors. We validated our analytical approaches using data simulations and demonstrated the overall concordance of the genomic properties of xenograft tumors with data from primary human tumors in The Cancer Genome Atlas (TCGA). Conclusions: The analysis workflows that we have developed to accurately predict somatic profiles of tumors from PDX models that lack normal tissue for comparison enable the identification of the key oncogenic genomic and expression signatures to support model selection and/or biomarker development in therapeutic studies. A reference implementation of our analysis recommendations is available at https://github.com/TheJacksonLaboratory/PDX-Analysis-Workflows."
0,"Personalised mechanical ventilation tailored to lung morphology versus low positive end-expiratory pressure for patients with acute respiratory distress syndrome in France (the LIVE study): a multicentre, single-blind, randomised controlled trial",,
0,Common brain disorders are associated with heritable patterns of apparent aging of the brain,"Common risk factors for psychiatric and other brain disorders are likely to converge on biological pathways influencing the development and maintenance of brain structure and function across life. Using structural MRI data from 45,615 individuals aged 3-96 years, we demonstrate distinct patterns of apparent brain aging in several brain disorders and reveal genetic pleiotropy between apparent brain aging in healthy individuals and common brain disorders.","Common brain disorders are associated with heritable patterns of apparent aging of the brain. Common risk factors for psychiatric and other brain disorders are likely to converge on biological pathways influencing the development and maintenance of brain structure and function across life. Using structural MRI data from 45,615 individuals aged 3-96 years, we demonstrate distinct patterns of apparent brain aging in several brain disorders and reveal genetic pleiotropy between apparent brain aging in healthy individuals and common brain disorders."
0,A systematic analysis of genomics-based modeling approaches for prediction of drug response to cytotoxic chemotherapies,"Background: The availability and generation of large amounts of genomic data has led to the development of a new paradigm in cancer treatment emphasizing a precision approach at the molecular and genomic level. Statistical modeling techniques aimed at leveraging broad scale in vitro, in vivo, and clinical data for precision drug treatment has become an active area of research. As a rapidly developing discipline at the crossroads of medicine, computer science, and mathematics, techniques ranging from accepted to those on the cutting edge of artificial intelligence have been utilized. Given the diversity and complexity of these techniques a systematic understanding of fundamental modeling principles is essential to contextualize influential factors to better understand results and develop new approaches. Methods: Using data available from the Genomics of Drug Sensitivity in Cancer (GDSC) and the NCI60 we explore principle components regression, linear and non-linear support vector regression, and artificial neural networks in combination with different implementations of correlation based feature selection (CBF) on the prediction of drug response for several cytotoxic chemotherapeutic agents. Results: Our results indicate that the regression method and features used have marginal effects on Spearman correlation between the predicted and measured values as well as prediction error. Detailed analysis of these results reveal that the bulk relationship between tissue of origin and drug response is a major driving factor in model performance. Conclusion: These results display one of the challenges in building predictive models for drug response in pan-cancer models. Mainly, that bulk genotypic traits where the signal to noise ratio is high is the dominant behavior captured in these models. This suggests that improved techniques of feature selection that can discriminate individual cell response from histotype response will yield more successful pan-cancer models.","A systematic analysis of genomics-based modeling approaches for prediction of drug response to cytotoxic chemotherapies. Background: The availability and generation of large amounts of genomic data has led to the development of a new paradigm in cancer treatment emphasizing a precision approach at the molecular and genomic level. Statistical modeling techniques aimed at leveraging broad scale in vitro, in vivo, and clinical data for precision drug treatment has become an active area of research. As a rapidly developing discipline at the crossroads of medicine, computer science, and mathematics, techniques ranging from accepted to those on the cutting edge of artificial intelligence have been utilized. Given the diversity and complexity of these techniques a systematic understanding of fundamental modeling principles is essential to contextualize influential factors to better understand results and develop new approaches. Methods: Using data available from the Genomics of Drug Sensitivity in Cancer (GDSC) and the NCI60 we explore principle components regression, linear and non-linear support vector regression, and artificial neural networks in combination with different implementations of correlation based feature selection (CBF) on the prediction of drug response for several cytotoxic chemotherapeutic agents. Results: Our results indicate that the regression method and features used have marginal effects on Spearman correlation between the predicted and measured values as well as prediction error. Detailed analysis of these results reveal that the bulk relationship between tissue of origin and drug response is a major driving factor in model performance. Conclusion: These results display one of the challenges in building predictive models for drug response in pan-cancer models. Mainly, that bulk genotypic traits where the signal to noise ratio is high is the dominant behavior captured in these models. This suggests that improved techniques of feature selection that can discriminate individual cell response from histotype response will yield more successful pan-cancer models."
0,The molecular landscape of glioma in patients with Neurofibromatosis 1,"Neurofibromatosis type 1 (NF1) is a common tumor predisposition syndrome in which glioma is one of the prevalent tumors. Gliomagenesis in NF1 results in a heterogeneous spectrum of low- to high-grade neoplasms occurring during the entire lifespan of patients. The pattern of genetic and epigenetic alterations of glioma that develops in NF1 patients and the similarities with sporadic glioma remain unknown. Here, we present the molecular landscape of low- and high-grade gliomas in patients affected by NF1 (NF1-glioma). We found that the predisposing germline mutation of the NF1 gene was frequently converted to homozygosity and the somatic mutational load of NF1-glioma was influenced by age and grade. High-grade tumors harbored genetic alterations of TP53 and CDKN2A, frequent mutations of ATRX associated with Alternative Lengthening of Telomere, and were enriched in genetic alterations of transcription/chromatin regulation and PI3 kinase pathways. Low-grade tumors exhibited fewer mutations that were over-represented in genes of the MAP kinase pathway. Approximately 50% of low-grade NF1-gliomas displayed an immune signature, T lymphocyte infiltrates, and increased neo-antigen load. DNA methylation assigned NF1-glioma to LGm6, a poorly defined Isocitrate Dehydrogenase 1 wild-type subgroup enriched with ATRX mutations. Thus, the profiling of NF1-glioma defined a distinct landscape that recapitulates a subset of sporadic tumors.","The molecular landscape of glioma in patients with Neurofibromatosis 1. Neurofibromatosis type 1 (NF1) is a common tumor predisposition syndrome in which glioma is one of the prevalent tumors. Gliomagenesis in NF1 results in a heterogeneous spectrum of low- to high-grade neoplasms occurring during the entire lifespan of patients. The pattern of genetic and epigenetic alterations of glioma that develops in NF1 patients and the similarities with sporadic glioma remain unknown. Here, we present the molecular landscape of low- and high-grade gliomas in patients affected by NF1 (NF1-glioma). We found that the predisposing germline mutation of the NF1 gene was frequently converted to homozygosity and the somatic mutational load of NF1-glioma was influenced by age and grade. High-grade tumors harbored genetic alterations of TP53 and CDKN2A, frequent mutations of ATRX associated with Alternative Lengthening of Telomere, and were enriched in genetic alterations of transcription/chromatin regulation and PI3 kinase pathways. Low-grade tumors exhibited fewer mutations that were over-represented in genes of the MAP kinase pathway. Approximately 50% of low-grade NF1-gliomas displayed an immune signature, T lymphocyte infiltrates, and increased neo-antigen load. DNA methylation assigned NF1-glioma to LGm6, a poorly defined Isocitrate Dehydrogenase 1 wild-type subgroup enriched with ATRX mutations. Thus, the profiling of NF1-glioma defined a distinct landscape that recapitulates a subset of sporadic tumors."
0,The Significance of Visceral Protection in Preventing Enteroatmospheric Fistulae During Open Abdomen Treatment in Patients With Secondary Peritonitis: A Propensity Score-matched Case-control Analysis,,
0,Molecular Basis for Ligand Modulation of a Mammalian Voltage-Gated Ca2+ Channel,"The L-type voltage-gated Ca2+ (Cav) channels are modulated by various compounds exemplified by 1,4-dihydropyridines (DHP), benzothiazepines (BTZ), and phenylalkylamines (PAA), many of which have been used for characterizing channel properties and for treatment of hypertension and other disorders. Here, we report the cryoelectron microscopy (cryo-EM) structures of Cav1.1 in complex with archetypal antagonistic drugs, nifedipine, diltiazem, and verapamil, at resolutions of 2.9 Ã…, 3.0 Ã…, and 2.7 Ã…, respectively, and with a DHP agonist Bay K 8644 at 2.8 Ã…. Diltiazem and verapamil traverse the central cavity of the pore domain, directly blocking ion permeation. Although nifedipine and Bay K 8644 occupy the same fenestration site at the interface of repeats III and IV, the coordination details support previous functional observations that Bay K 8644 is less favored in the inactivated state. These structures elucidate the modes of action of different Cav ligands and establish a framework for structure-guided drug discovery. A view on how both agonists and antagonists interact with a voltage-gated calcium channel opens up avenues for understanding channel gating and new ligand design.","Molecular Basis for Ligand Modulation of a Mammalian Voltage-Gated Ca2+ Channel. The L-type voltage-gated Ca2+ (Cav) channels are modulated by various compounds exemplified by 1,4-dihydropyridines (DHP), benzothiazepines (BTZ), and phenylalkylamines (PAA), many of which have been used for characterizing channel properties and for treatment of hypertension and other disorders. Here, we report the cryoelectron microscopy (cryo-EM) structures of Cav1.1 in complex with archetypal antagonistic drugs, nifedipine, diltiazem, and verapamil, at resolutions of 2.9 Ã…, 3.0 Ã…, and 2.7 Ã…, respectively, and with a DHP agonist Bay K 8644 at 2.8 Ã…. Diltiazem and verapamil traverse the central cavity of the pore domain, directly blocking ion permeation. Although nifedipine and Bay K 8644 occupy the same fenestration site at the interface of repeats III and IV, the coordination details support previous functional observations that Bay K 8644 is less favored in the inactivated state. These structures elucidate the modes of action of different Cav ligands and establish a framework for structure-guided drug discovery. A view on how both agonists and antagonists interact with a voltage-gated calcium channel opens up avenues for understanding channel gating and new ligand design."
0,Development of Chinese gastric cancer surgery: Opportunities and challenges,"With the approaches of artificial intelligence and big data, the development of cancer genomics and updating of imaging technology, gastric cancer surgery is facing great challenges and opportunities. The main focus is on laparoscopic surgery technology, enhanced recovery after surgery, multidisciplinary comprehensive treatment, and precision medicine. Considering the common demand for reduced complication rate among doctors and patients, laparoscopic surgery has become widely popular owing to its advantages of small incision and rapid recovery. Furthermore, the development of artificial intelligence and big data has raised a new challenge in routine diagnosis and treatment. As a result, we encourage multicenter cooperation, and data standardization and sharing. At present, completion of the transition from empirical medicine to evidence-based medicine and promotion of the individualization and standardization of gastric cancer treatment are needed.","Development of Chinese gastric cancer surgery: Opportunities and challenges. With the approaches of artificial intelligence and big data, the development of cancer genomics and updating of imaging technology, gastric cancer surgery is facing great challenges and opportunities. The main focus is on laparoscopic surgery technology, enhanced recovery after surgery, multidisciplinary comprehensive treatment, and precision medicine. Considering the common demand for reduced complication rate among doctors and patients, laparoscopic surgery has become widely popular owing to its advantages of small incision and rapid recovery. Furthermore, the development of artificial intelligence and big data has raised a new challenge in routine diagnosis and treatment. As a result, we encourage multicenter cooperation, and data standardization and sharing. At present, completion of the transition from empirical medicine to evidence-based medicine and promotion of the individualization and standardization of gastric cancer treatment are needed."
0,NEURAL DEVELOPMENT Filtering out the noise in axon guidance,,
0,Breath hold effect on cardiovascular brain pulsations â€“ A multimodal magnetic resonance encephalography study,"Ultra-fast functional magnetic resonance encephalography (MREG) enables separate assessment of cardiovascular, respiratory, and vasomotor waves from brain pulsations without temporal aliasing. We examined effects of breath hold- (BH) related changes on cardiovascular brain pulsations using MREG to study the physiological nature of cerebrovascular reactivity. We used alternating 32 s BH and 88 s resting normoventilation (NV) to change brain pulsations during MREG combined with simultaneously measured respiration, continuous non-invasive blood pressure, and cortical near-infrared spectroscopy (NIRS) in healthy volunteers. Changes in classical resting-state network BOLD-like signal and cortical blood oxygenation were reproduced based on MREG and NIRS signals. Cardiovascular pulsation amplitudes of MREG signal from anterior cerebral artery, oxygenated hemoglobin concentration in frontal cortex, and blood pressure decreased after BH. MREG cardiovascular pulse amplitudes in cortical areas and sagittal sinus increased, while cerebrospinal fluid and white matter remained unchanged. Respiratory centers in the brainstem â€“ hypothalamus â€“ thalamus â€“ amygdala network showed strongest increases in cardiovascular pulsation amplitude. The spatial propagation of averaged cardiovascular impulses altered as a function of successive BH runs. The spread of cardiovascular pulse cycles exhibited a decreasing spatial similarity over time. MREG portrayed spatiotemporally accurate respiratory network activity and cardiovascular pulsation dynamics related to BH challenges at an unpreceded high temporal resolution.","Breath hold effect on cardiovascular brain pulsations â€“ A multimodal magnetic resonance encephalography study. Ultra-fast functional magnetic resonance encephalography (MREG) enables separate assessment of cardiovascular, respiratory, and vasomotor waves from brain pulsations without temporal aliasing. We examined effects of breath hold- (BH) related changes on cardiovascular brain pulsations using MREG to study the physiological nature of cerebrovascular reactivity. We used alternating 32 s BH and 88 s resting normoventilation (NV) to change brain pulsations during MREG combined with simultaneously measured respiration, continuous non-invasive blood pressure, and cortical near-infrared spectroscopy (NIRS) in healthy volunteers. Changes in classical resting-state network BOLD-like signal and cortical blood oxygenation were reproduced based on MREG and NIRS signals. Cardiovascular pulsation amplitudes of MREG signal from anterior cerebral artery, oxygenated hemoglobin concentration in frontal cortex, and blood pressure decreased after BH. MREG cardiovascular pulse amplitudes in cortical areas and sagittal sinus increased, while cerebrospinal fluid and white matter remained unchanged. Respiratory centers in the brainstem â€“ hypothalamus â€“ thalamus â€“ amygdala network showed strongest increases in cardiovascular pulsation amplitude. The spatial propagation of averaged cardiovascular impulses altered as a function of successive BH runs. The spread of cardiovascular pulse cycles exhibited a decreasing spatial similarity over time. MREG portrayed spatiotemporally accurate respiratory network activity and cardiovascular pulsation dynamics related to BH challenges at an unpreceded high temporal resolution."
0,The Cancer Genome Atlas Expression Subtypes Stratify Response to Checkpoint Inhibition in Advanced Urothelial Cancer and Identify a Subset of Patients with High Survival Probability,"Analysis of the IMvigor 210 trials involving patients with platinum-refractory or cisplatin-ineligible urothelial carcinoma who were treated with the PD-L1 inhibitor atezolizumab identified a resistance signature as an immune biomarker. Transcriptome profiling of 368 tumor samples from this trial revealed that the â€œgenomically unstableâ€ Lund subtype classification was associated with the best response. We developed and applied a novel single-patient subtype classifier based on The Cancer Genome Atlas 2017 expression-based molecular subtypes. We identified 11 patients with a neuronal subtype, with a 100% response rate in eight confirmed cases (2 complete response, 6 partial response), and 72% overall, including 3/11 patients with an unconfirmed response. The survival probability was extraordinarily high for the neuronal subtype, which represents a high-risk cohort with advanced disease, and may be secondary to low levels of TGFÎ² expression and high mutation/neoantigen burden. Patient summary: We describe a methodology for genomic classification of an individual patient's bladder cancer tumor and have identified a subtype that is associated with a high response rate to immunotherapy. This is an important step forward in identifying the right treatment for the right patient, which is the goal of personalized precision medicine. We describe a novel single-patient classifier based on The Cancer Genome Atlas 2017 scheme that identified the neuronal subtype of urothelial carcinoma as an extreme responder to anti-PD-L1 therapy. In the future, trials targeting subtype-based therapy may improve precision delivery of care for urothelial carcinoma.","The Cancer Genome Atlas Expression Subtypes Stratify Response to Checkpoint Inhibition in Advanced Urothelial Cancer and Identify a Subset of Patients with High Survival Probability. Analysis of the IMvigor 210 trials involving patients with platinum-refractory or cisplatin-ineligible urothelial carcinoma who were treated with the PD-L1 inhibitor atezolizumab identified a resistance signature as an immune biomarker. Transcriptome profiling of 368 tumor samples from this trial revealed that the â€œgenomically unstableâ€ Lund subtype classification was associated with the best response. We developed and applied a novel single-patient subtype classifier based on The Cancer Genome Atlas 2017 expression-based molecular subtypes. We identified 11 patients with a neuronal subtype, with a 100% response rate in eight confirmed cases (2 complete response, 6 partial response), and 72% overall, including 3/11 patients with an unconfirmed response. The survival probability was extraordinarily high for the neuronal subtype, which represents a high-risk cohort with advanced disease, and may be secondary to low levels of TGFÎ² expression and high mutation/neoantigen burden. Patient summary: We describe a methodology for genomic classification of an individual patient's bladder cancer tumor and have identified a subtype that is associated with a high response rate to immunotherapy. This is an important step forward in identifying the right treatment for the right patient, which is the goal of personalized precision medicine. We describe a novel single-patient classifier based on The Cancer Genome Atlas 2017 scheme that identified the neuronal subtype of urothelial carcinoma as an extreme responder to anti-PD-L1 therapy. In the future, trials targeting subtype-based therapy may improve precision delivery of care for urothelial carcinoma."
0,Propofol Anesthesia Alters Spatial and Topologic Organization of Rat Brain Metabolism,,
0,Selective inhibition of P-gp transporter by goniothalamin derivatives sensitizes resistant cancer cells to chemotherapy,"Overexpression of efflux transporters of the ATP-binding cassette (ABC) transporter family, primarily P-glycoprotein (P-gp), is a frequent cause of multidrug resistance in cancer and leads to failure of current chemotherapies. Thus, identification of selective P-gp inhibitors might provide a basis for the development of novel anticancer drug candidates. The natural product goniothalamin and 21 derivatives were characterized regarding their ability to inhibit ABC transporter function. Among the goniothalamins, selective inhibitors of P-gp were discovered. The two most potent inhibitors (R)-3 and (S)-3 displayed the ability to increase intracellular accumulation of doxorubicin, thereby sensitizing P-gp-overexpressing tumor cells to chemotherapy by decreasing doxorubicin IC50 value up to 15-fold. Molecular docking studies indicated these compounds to inhibit P-gp by acting as transporter substrates. In conclusion, our findings revealed a novel role of goniothalamin derivatives in reversing P-gp-mediated chemotherapy resistance.","Selective inhibition of P-gp transporter by goniothalamin derivatives sensitizes resistant cancer cells to chemotherapy. Overexpression of efflux transporters of the ATP-binding cassette (ABC) transporter family, primarily P-glycoprotein (P-gp), is a frequent cause of multidrug resistance in cancer and leads to failure of current chemotherapies. Thus, identification of selective P-gp inhibitors might provide a basis for the development of novel anticancer drug candidates. The natural product goniothalamin and 21 derivatives were characterized regarding their ability to inhibit ABC transporter function. Among the goniothalamins, selective inhibitors of P-gp were discovered. The two most potent inhibitors (R)-3 and (S)-3 displayed the ability to increase intracellular accumulation of doxorubicin, thereby sensitizing P-gp-overexpressing tumor cells to chemotherapy by decreasing doxorubicin IC50 value up to 15-fold. Molecular docking studies indicated these compounds to inhibit P-gp by acting as transporter substrates. In conclusion, our findings revealed a novel role of goniothalamin derivatives in reversing P-gp-mediated chemotherapy resistance."
0,Atomic insight into prion disorder: An intricate detail gained by 0.5 Î¼s molecular dynamics simulation of preventive G127V and deleterious D178V mutation in prion protein,"In this study we are looking into two contradicting mutations found in prion protein (PrP) viz G127V and D178V, that are reportedly protective and pathogenic, respectively. Despite significant advances in comprehension of the role of pathogenic mutations, the role of protective mutation in amyloid fold inhibition still lacks a substantial basis. To understand the structural basis of protective mutation, molecular dynamics simulation coupled with protein-protein docking and molecular mechanics/Poisson-Boltzmann surface area analysis was used to understand the instant structural variability brought about by these mutations alone and in combination on PrP and prion-prion complex. Atomic-scale investigations successfully revealed that the binding pattern of prion-prion varies differentially in protective and pathogenic mutations with secondary structure showing distinct contrasting patterns, which could supposedly be a critical factor for differential prion behavior in protective and pathogenic mutations. Considering the reported role of an amyloid fold in prion-prion binding, the contrasting pattern has given us a lead in comprehending the role of these mutations and has been used in this study to look for small molecules that can inhibit amyloid fold for prion-prion interaction in pathogenic mutant carrying PrP.","Atomic insight into prion disorder: An intricate detail gained by 0.5 Î¼s molecular dynamics simulation of preventive G127V and deleterious D178V mutation in prion protein. In this study we are looking into two contradicting mutations found in prion protein (PrP) viz G127V and D178V, that are reportedly protective and pathogenic, respectively. Despite significant advances in comprehension of the role of pathogenic mutations, the role of protective mutation in amyloid fold inhibition still lacks a substantial basis. To understand the structural basis of protective mutation, molecular dynamics simulation coupled with protein-protein docking and molecular mechanics/Poisson-Boltzmann surface area analysis was used to understand the instant structural variability brought about by these mutations alone and in combination on PrP and prion-prion complex. Atomic-scale investigations successfully revealed that the binding pattern of prion-prion varies differentially in protective and pathogenic mutations with secondary structure showing distinct contrasting patterns, which could supposedly be a critical factor for differential prion behavior in protective and pathogenic mutations. Considering the reported role of an amyloid fold in prion-prion binding, the contrasting pattern has given us a lead in comprehending the role of these mutations and has been used in this study to look for small molecules that can inhibit amyloid fold for prion-prion interaction in pathogenic mutant carrying PrP."
0,Mechanisms of systems memory consolidation during sleep,,
0,"Aspulvinone O, a natural inhibitor of GOT1 suppresses pancreatic ductal adenocarcinoma cells growth by interfering glutamine metabolism","Background: Distinctive from their normal counterparts, cancer cells exhibit unique metabolic dependencies on glutamine to fuel anabolic processes. Specifically, pancreatic ductal adenocarcinoma (PDAC) cells rely on an unconventional metabolic pathway catalyzed by aspartate transaminase 1 (GOT1) to rewire glutamine metabolism and support nicotinamide adenine dinucleotide phosphate (NADPH) production. Thus, the important role of GOT1 in energy metabolism and Reactive Oxygen Species (ROS) balance demonstrates that targeting GOT1 may serve as an important therapeutic target in PDAC. Methods: To assay the binding affinity between Aspulvinone O (AO) and GOT1 proteins, the virtual docking, microscale thermophoresis (MST), cellular thermal shift assay (CETSA) and drug affinity responsive target stability (DARTS) methods were employed. GOT1 was silenced in several PDAC cell lines. The level of OCR and ECR were assayed by seahorse. To evaluate the in vivo anti-tumor efficacy of AO, the xenograft model was built in CB17/scid mouse. Results: Screening of an in-house natural compound library identified the AO as a novel inhibitor of GOT1 and repressed glutamine metabolism, which sensitizes PDAC cells to oxidative stress and suppresses cell proliferation. Virtual docking analysis suggested that AO could bind to the active site of GOT1 and form obvious hydrophobic interaction with Trp141 together with hydrogen bonds with Thr110 and Ser256. Further in vitro validation, including MST, CETSA and DARTS, further demonstrated the specific combining capacity of AO. We also show that the selective inhibition of GOT1 by AO significantly reduces proliferation of PDAC in vitro and in vivo. Conclusions: Taken together, our findings identify AO as a potent bioactive inhibitor of GOT1 and a novel anti-tumour agent for PDAC therapy.","Aspulvinone O, a natural inhibitor of GOT1 suppresses pancreatic ductal adenocarcinoma cells growth by interfering glutamine metabolism. Background: Distinctive from their normal counterparts, cancer cells exhibit unique metabolic dependencies on glutamine to fuel anabolic processes. Specifically, pancreatic ductal adenocarcinoma (PDAC) cells rely on an unconventional metabolic pathway catalyzed by aspartate transaminase 1 (GOT1) to rewire glutamine metabolism and support nicotinamide adenine dinucleotide phosphate (NADPH) production. Thus, the important role of GOT1 in energy metabolism and Reactive Oxygen Species (ROS) balance demonstrates that targeting GOT1 may serve as an important therapeutic target in PDAC. Methods: To assay the binding affinity between Aspulvinone O (AO) and GOT1 proteins, the virtual docking, microscale thermophoresis (MST), cellular thermal shift assay (CETSA) and drug affinity responsive target stability (DARTS) methods were employed. GOT1 was silenced in several PDAC cell lines. The level of OCR and ECR were assayed by seahorse. To evaluate the in vivo anti-tumor efficacy of AO, the xenograft model was built in CB17/scid mouse. Results: Screening of an in-house natural compound library identified the AO as a novel inhibitor of GOT1 and repressed glutamine metabolism, which sensitizes PDAC cells to oxidative stress and suppresses cell proliferation. Virtual docking analysis suggested that AO could bind to the active site of GOT1 and form obvious hydrophobic interaction with Trp141 together with hydrogen bonds with Thr110 and Ser256. Further in vitro validation, including MST, CETSA and DARTS, further demonstrated the specific combining capacity of AO. We also show that the selective inhibition of GOT1 by AO significantly reduces proliferation of PDAC in vitro and in vivo. Conclusions: Taken together, our findings identify AO as a potent bioactive inhibitor of GOT1 and a novel anti-tumour agent for PDAC therapy."
0,"Physiologic and Clinical Assessment of Resting Physiologic Indices: Resting Full-Cycle Ratio, Diastolic Pressure-Ratio, and Instantaneous Wave-Free Ratio (vol 139, pg 889, 2018)",,
0,Allyl rhodanine azo dye derivatives: Potential antimicrobials target d-alanyl carrier protein ligase and nucleoside diphosphate kinase,"3-Allyl-5-(4-arylazo)-2-thioxothiazolidine-4-one (HLn) ligands (where n = 1 to 3) were hypothesized to have antimicrobial activities mediated through inhibition of new antimicrobial targets. The ligands (HLn) were synthesized and characterized by infrared (IR) and 1H nuclear magnetic resonance (1H NMR) spectra. The ligands (HLn) were in silico screened to their potential inhibition to models of d-alanyl carrier protein ligase (DltA) (from Bacillus cereus, PDB code 3FCE) and nucleoside diphosphate kinase (NDK) (from Staphylococcus aureus; PDB code 3Q8U). HL3 ligand has the best energy and mode of binding to both NDK and DltA, even though its binding to DltA was stronger than that to NDK. In antimicrobial activity of HL3 ligand, morphological and cytological changes in HL3-treated bacteria agreed with the in silico results. The HL3 ligand showed significant antimicrobial activity against B. cereus, S. aureus, and Fusarium oxysporium. The HL3-treated bacterial cells appeared malformed and incompletely separated. Its cell walls appeared electron-lucent and ruptured. They contained more mesosomes than normal cells. It was found that the HL3 ligand represented as a bactericide against B. cereus and S. aureusby blocking target DltA, and may target NDK.","Allyl rhodanine azo dye derivatives: Potential antimicrobials target d-alanyl carrier protein ligase and nucleoside diphosphate kinase. 3-Allyl-5-(4-arylazo)-2-thioxothiazolidine-4-one (HLn) ligands (where n = 1 to 3) were hypothesized to have antimicrobial activities mediated through inhibition of new antimicrobial targets. The ligands (HLn) were synthesized and characterized by infrared (IR) and 1H nuclear magnetic resonance (1H NMR) spectra. The ligands (HLn) were in silico screened to their potential inhibition to models of d-alanyl carrier protein ligase (DltA) (from Bacillus cereus, PDB code 3FCE) and nucleoside diphosphate kinase (NDK) (from Staphylococcus aureus; PDB code 3Q8U). HL3 ligand has the best energy and mode of binding to both NDK and DltA, even though its binding to DltA was stronger than that to NDK. In antimicrobial activity of HL3 ligand, morphological and cytological changes in HL3-treated bacteria agreed with the in silico results. The HL3 ligand showed significant antimicrobial activity against B. cereus, S. aureus, and Fusarium oxysporium. The HL3-treated bacterial cells appeared malformed and incompletely separated. Its cell walls appeared electron-lucent and ruptured. They contained more mesosomes than normal cells. It was found that the HL3 ligand represented as a bactericide against B. cereus and S. aureusby blocking target DltA, and may target NDK."
0,Antifibrotic Potential of MiR-335-3p in Hereditary Gingival Fibromatosis,,
0,"Hepatitis C Virus Infection in Patients With Cancer: Impact on Clinical Trial Enrollment, Selection of Therapy, and Prognosis",,
0,Investigation of structural stability and functionality of homodimeric gramicidin towards peptide-based drug: a molecular simulation approach,"Increasing death rates due to antibiotic resistance deteriorate the existing treatment measures. Antimicrobial peptides have turned into the emerging cure for multidrug resistance. However, the stability and functionality determine an antimicrobial peptide as a drug. Analyses of the homodimeric Î²-helical peptide, gramicidin have suggested the significant role of gramicidin-A, gramicidin-B, and gramicidin-C as antimicrobial compounds, but the structural basis for understanding the stability and functionality is insufficient to resolve multidrug resistance. To identify the best template among gramicidin types as a therapeutic product, we combined a detailed comparative static analysis and dynamic analysis along with conformational free energy and secondary structure prediction. We observed that the high intramolecular interactions and the geometrical features favored gramicidin-A among other types of gramicidin. Our analyses further revealed that the secondary structure of gramicidin-A showed Î² sheets with coils along the conformations without any disruption, thereby enhanced its membrane interactions in terms of binding free energy. In conclusion, gramicidin-A has definitely showed enhanced structural stability and functionality; this could be considered the best template for a potential therapeutic product.","Investigation of structural stability and functionality of homodimeric gramicidin towards peptide-based drug: a molecular simulation approach. Increasing death rates due to antibiotic resistance deteriorate the existing treatment measures. Antimicrobial peptides have turned into the emerging cure for multidrug resistance. However, the stability and functionality determine an antimicrobial peptide as a drug. Analyses of the homodimeric Î²-helical peptide, gramicidin have suggested the significant role of gramicidin-A, gramicidin-B, and gramicidin-C as antimicrobial compounds, but the structural basis for understanding the stability and functionality is insufficient to resolve multidrug resistance. To identify the best template among gramicidin types as a therapeutic product, we combined a detailed comparative static analysis and dynamic analysis along with conformational free energy and secondary structure prediction. We observed that the high intramolecular interactions and the geometrical features favored gramicidin-A among other types of gramicidin. Our analyses further revealed that the secondary structure of gramicidin-A showed Î² sheets with coils along the conformations without any disruption, thereby enhanced its membrane interactions in terms of binding free energy. In conclusion, gramicidin-A has definitely showed enhanced structural stability and functionality; this could be considered the best template for a potential therapeutic product."
0,Computational modeling of bicuspid aortopathy: Towards personalized risk strategies,"This paper describes current advances on the application of in-silico for the understanding of bicuspid aortopathy and future perspectives of this technology on routine clinical care. This includes the impact that artificial intelligence can provide to develop computer-based clinical decision support system and that wearable sensors can offer to remotely monitor high-risk bicuspid aortic valve (BAV) patients. First, we discussed the benefit of computational modeling by providing tangible examples of in-silico software products based on computational fluid-dynamic (CFD) and finite-element method (FEM) that are currently transforming the way we diagnose and treat cardiovascular diseases. Then, we presented recent findings on computational hemodynamic and structural mechanics of BAV to highlight the potentiality of patient-specific metrics (not-based on aortic size) to support the clinical-decision making process of BAV-associated aneurysms. Examples of BAV-related personalized healthcare solutions are illustrated.","Computational modeling of bicuspid aortopathy: Towards personalized risk strategies. This paper describes current advances on the application of in-silico for the understanding of bicuspid aortopathy and future perspectives of this technology on routine clinical care. This includes the impact that artificial intelligence can provide to develop computer-based clinical decision support system and that wearable sensors can offer to remotely monitor high-risk bicuspid aortic valve (BAV) patients. First, we discussed the benefit of computational modeling by providing tangible examples of in-silico software products based on computational fluid-dynamic (CFD) and finite-element method (FEM) that are currently transforming the way we diagnose and treat cardiovascular diseases. Then, we presented recent findings on computational hemodynamic and structural mechanics of BAV to highlight the potentiality of patient-specific metrics (not-based on aortic size) to support the clinical-decision making process of BAV-associated aneurysms. Examples of BAV-related personalized healthcare solutions are illustrated."
0,Artificial intelligence in digital pathology - new tools for diagnosis and precision oncology,"In the past decade, advances in precision oncology have resulted in an increased demand for predictive assays that enable the selection and stratification of patients for treatment. The enormous divergence of signalling and transcriptional networks mediating the crosstalk between cancer, stromal and immune cells complicates the development of functionally relevant biomarkers based on a single gene or protein. However, the result of these complex processes can be uniquely captured in the morphometric features of stained tissue specimens. The possibility of digitizing whole-slide images of tissue has led to the advent of artificial intelligence (AI) and machine learning tools in digital pathology, which enable mining of subvisual morphometric phenotypes and might, ultimately, improve patient management. In this Perspective, we critically evaluate various AI-based computational approaches for digital pathology, focusing on deep neural networks and 'hand-crafted' feature-based methodologies. We aim to provide a broad framework for incorporating AI and machine learning tools into clinical oncology, with an emphasis on biomarker development. We discuss some of the challenges relating to the use of AI, including the need for well-curated validation datasets, regulatory approval and fair reimbursement strategies. Finally, we present potential future opportunities for precision oncology.","Artificial intelligence in digital pathology - new tools for diagnosis and precision oncology. In the past decade, advances in precision oncology have resulted in an increased demand for predictive assays that enable the selection and stratification of patients for treatment. The enormous divergence of signalling and transcriptional networks mediating the crosstalk between cancer, stromal and immune cells complicates the development of functionally relevant biomarkers based on a single gene or protein. However, the result of these complex processes can be uniquely captured in the morphometric features of stained tissue specimens. The possibility of digitizing whole-slide images of tissue has led to the advent of artificial intelligence (AI) and machine learning tools in digital pathology, which enable mining of subvisual morphometric phenotypes and might, ultimately, improve patient management. In this Perspective, we critically evaluate various AI-based computational approaches for digital pathology, focusing on deep neural networks and 'hand-crafted' feature-based methodologies. We aim to provide a broad framework for incorporating AI and machine learning tools into clinical oncology, with an emphasis on biomarker development. We discuss some of the challenges relating to the use of AI, including the need for well-curated validation datasets, regulatory approval and fair reimbursement strategies. Finally, we present potential future opportunities for precision oncology."
0,Unique contributions of parvalbumin and cholinergic interneurons in organizing striatal networks during movement,,
0,Potential Liability for Physicians Using Artificial Intelligence,,
0,Spontaneous synchronization to speech reveals neural mechanisms facilitating language learning,,
0,Identification of dysregulated miRNAs in triple negative breast cancer: A meta-analysis approach,"Triple negative breast cancer (TNBC) is an aggressive subtype of breast cancer with poor clinical outcomes and lack of approved targeted therapy. Dysregulated microRNAs (miRNAs) have been considered a promising biomarker, which plays an important role in the tumorigenesis of human cancer. Due to the increase in miRNA profiling datasets of TNBC, a proper analysis is required for studying. Therefore, this study used meta-analysis to amalgamate ten miRNA profiling studies of TNBC. By the robust rank aggregation method, metasignatures of six miRNAs (4 upregulated: hsa-miR-135b-5p, hsa-miR-18a-5p, hsa-miR-9-5p and hsa-miR-522-3p; 2 downregulated: hsa-miR-190b and hsa-miR-449a) were obtained. The gene ontology analysis revealed that target genes regulated by miRNAs were associated with processes like the regulation of transcription, DNA dependent, and signal transduction. Also, it is noted from the pathway analysis that signaling and cancer pathways were associated with the progression of TNBC. A NaÃ¯ve Bayes-based classifier built with miRNA signatures discriminates TNBC and non-TNBC samples in test data set with high diagnostic sensitivity and specificity. From the analysis carried out by the study, it is suggested that the identified miRNAs are of great importance in improving the diagnostics and therapeutics for TNBC.","Identification of dysregulated miRNAs in triple negative breast cancer: A meta-analysis approach. Triple negative breast cancer (TNBC) is an aggressive subtype of breast cancer with poor clinical outcomes and lack of approved targeted therapy. Dysregulated microRNAs (miRNAs) have been considered a promising biomarker, which plays an important role in the tumorigenesis of human cancer. Due to the increase in miRNA profiling datasets of TNBC, a proper analysis is required for studying. Therefore, this study used meta-analysis to amalgamate ten miRNA profiling studies of TNBC. By the robust rank aggregation method, metasignatures of six miRNAs (4 upregulated: hsa-miR-135b-5p, hsa-miR-18a-5p, hsa-miR-9-5p and hsa-miR-522-3p; 2 downregulated: hsa-miR-190b and hsa-miR-449a) were obtained. The gene ontology analysis revealed that target genes regulated by miRNAs were associated with processes like the regulation of transcription, DNA dependent, and signal transduction. Also, it is noted from the pathway analysis that signaling and cancer pathways were associated with the progression of TNBC. A NaÃ¯ve Bayes-based classifier built with miRNA signatures discriminates TNBC and non-TNBC samples in test data set with high diagnostic sensitivity and specificity. From the analysis carried out by the study, it is suggested that the identified miRNAs are of great importance in improving the diagnostics and therapeutics for TNBC."
0,Update on Intraocular Lens Calculation Formulas,,
0,Author Correction: Do no harm: a roadmap for responsible machine learning for health care,An amendment to this paper has been published and can be accessed via a link at the top of the paper.,Author Correction: Do no harm: a roadmap for responsible machine learning for health care. An amendment to this paper has been published and can be accessed via a link at the top of the paper.
0,Association between childhood anhedonia and alterations in large-scale resting-state networks and task-evoked activation,"Importance: Anhedonia can present in children and predict detrimental clinical outcomes. Objective: To map anhedonia in children onto changes in intrinsic large-scale connectivity and task-evoked activation and to probe the specificity of these changes in anhedonia against other clinical phenotypes (low mood, anxiety, and attention-deficit/hyperactivity disorder ADHD). Design, Setting, and Participants: Functional magnetic resonance imaging (fMRI) data were from the first annual release of the Adolescent Brain Cognitive Development study, collected between September 2016 and September 2017 and analyzed between April and September 2018. Cross-sectional data of children aged 9 to 10 years from unreferred, community samples during rest (n = 2878) and during reward anticipation (n = 2874) and working memory (n = 2745) were analyzed. Main Outcomes and Measures: Alterations in fMRI data during rest, reward anticipation, and working memory were examined, using both frequentist and Bayesian approaches. Functional MRI connectivity within large-scale networks, between networks, and between networks and subcortical regions were examined during rest. Functional MRI activation were examined during reward anticipation and working memory using the monetary incentive delayed and N-back tasks, respectively. Results: Among 2878 children with adequate-quality resting-state fMRI data (mean SD age, 10.03 0.62 years; 1400 girls 48.6%), children with anhedonia (261 9.1%), compared with those without anhedonia (2617 90.9%), showed hypoconnectivity among various large-scale networks and subcortical regions, including between the arousal-related cingulo-opercular network and reward-related ventral striatum area (mean SD with anhedonia, 0.08 0.10 vs without anhedonia, 0.10 0.10; t2,876 = 3.33; P <.001; qfalse discovery rate = 0.03; lnBayes factor10 = 2.85). Such hypoconnectivity did not manifest among children with low mood (277 of 2878 9.62%), anxiety (109 of 2878 3.79%), or ADHD (459 of 2878 15.95%), suggesting specificity. Similarly, among 2874 children (mean SD age, 10.03 0.62 years; 1414 girls 49.2%) with high-quality task-evoked fMRI data, children with anhedonia (248 of 2874 8.63%) demonstrated hypoactivation during reward anticipation in various areas, including the dorsal striatum and areas of the cingulo-opercular network. This hypoactivity was not found among children with low mood (268 of 2874 9.32%), anxiety (90 of 2874 3.13%), or ADHD (473 of 2874 16.46%). Moreover, we also found context- and phenotype-specific double dissociations; while children with anhedonia showed altered activation during reward anticipation (but not working memory), those with ADHD showed altered activation during working memory (but not reward anticipation). Conclusions and Relevance: Using the Adolescent Brain Cognitive Development study data set, phenotype-specific alterations were found in intrinsic large-scale connectivity and task-evoked activation in children with anhedonia. The hypoconnectivity at rest and hypoactivation during reward anticipation complementarily map anhedonia onto aberrations in neural-cognitive processes: lack of intrinsic reward-arousal integration during rest and diminishment of extrinsic reward-arousal activity during reward anticipation. These findings help delineate the pathophysiological underpinnings of anhedonia in children.","Association between childhood anhedonia and alterations in large-scale resting-state networks and task-evoked activation. Importance: Anhedonia can present in children and predict detrimental clinical outcomes. Objective: To map anhedonia in children onto changes in intrinsic large-scale connectivity and task-evoked activation and to probe the specificity of these changes in anhedonia against other clinical phenotypes (low mood, anxiety, and attention-deficit/hyperactivity disorder ADHD). Design, Setting, and Participants: Functional magnetic resonance imaging (fMRI) data were from the first annual release of the Adolescent Brain Cognitive Development study, collected between September 2016 and September 2017 and analyzed between April and September 2018. Cross-sectional data of children aged 9 to 10 years from unreferred, community samples during rest (n = 2878) and during reward anticipation (n = 2874) and working memory (n = 2745) were analyzed. Main Outcomes and Measures: Alterations in fMRI data during rest, reward anticipation, and working memory were examined, using both frequentist and Bayesian approaches. Functional MRI connectivity within large-scale networks, between networks, and between networks and subcortical regions were examined during rest. Functional MRI activation were examined during reward anticipation and working memory using the monetary incentive delayed and N-back tasks, respectively. Results: Among 2878 children with adequate-quality resting-state fMRI data (mean SD age, 10.03 0.62 years; 1400 girls 48.6%), children with anhedonia (261 9.1%), compared with those without anhedonia (2617 90.9%), showed hypoconnectivity among various large-scale networks and subcortical regions, including between the arousal-related cingulo-opercular network and reward-related ventral striatum area (mean SD with anhedonia, 0.08 0.10 vs without anhedonia, 0.10 0.10; t2,876 = 3.33; P <.001; qfalse discovery rate = 0.03; lnBayes factor10 = 2.85). Such hypoconnectivity did not manifest among children with low mood (277 of 2878 9.62%), anxiety (109 of 2878 3.79%), or ADHD (459 of 2878 15.95%), suggesting specificity. Similarly, among 2874 children (mean SD age, 10.03 0.62 years; 1414 girls 49.2%) with high-quality task-evoked fMRI data, children with anhedonia (248 of 2874 8.63%) demonstrated hypoactivation during reward anticipation in various areas, including the dorsal striatum and areas of the cingulo-opercular network. This hypoactivity was not found among children with low mood (268 of 2874 9.32%), anxiety (90 of 2874 3.13%), or ADHD (473 of 2874 16.46%). Moreover, we also found context- and phenotype-specific double dissociations; while children with anhedonia showed altered activation during reward anticipation (but not working memory), those with ADHD showed altered activation during working memory (but not reward anticipation). Conclusions and Relevance: Using the Adolescent Brain Cognitive Development study data set, phenotype-specific alterations were found in intrinsic large-scale connectivity and task-evoked activation in children with anhedonia. The hypoconnectivity at rest and hypoactivation during reward anticipation complementarily map anhedonia onto aberrations in neural-cognitive processes: lack of intrinsic reward-arousal integration during rest and diminishment of extrinsic reward-arousal activity during reward anticipation. These findings help delineate the pathophysiological underpinnings of anhedonia in children."
0,Dynamic edge-based biomarker non-invasively predicts hepatocellular carcinoma with hepatitis B virus infection for individual patients based on blood testing,"Hepatitis B virus (HBV)-induced hepatocellular carcinoma (HCC) is a major cause of cancer-related deaths in Asia and Africa. Developing effective and non-invasive biomarkers of HCC for individual patients remains an urgent task for early diagnosis and convenient monitoring. Analyzing the transcriptomic profiles of peripheral blood mononuclear cells from both healthy donors and patients with chronic HBV infection in different states (i.e. HBV carrier, chronic hepatitis B, cirrhosis, and HCC), we identified a set of 19 candidate genes according to our algorithm of dynamic network biomarkers. These genes can both characterize different stages during HCC progression and identify cirrhosis as the critical transition stage before carcinogenesis. The interaction effects (i.e. co-expressions) of candidate genes were used to build an accurate prediction model: the so-called edge-based biomarker. Considering the convenience and robustness of biomarkers in clinical applications, we performed functional analysis, validated candidate genes in other independent samples of our collected cohort, and finally selected COL5A1, HLA-DQB1, MMP2, and CDK4 to build edge panel as prediction models. We demonstrated that the edge panel had great performance in both diagnosis and prognosis in terms of precision and specificity for HCC, especially for patients with alpha-fetoprotein-negative HCC. Our study not only provides a novel edge-based biomarker for non-invasive and effective diagnosis of HBV-associated HCC to each individual patient but also introduces a new way to integrate the interaction terms of individual molecules for clinical diagnosis and prognosis from the network and dynamics perspectives.","Dynamic edge-based biomarker non-invasively predicts hepatocellular carcinoma with hepatitis B virus infection for individual patients based on blood testing. Hepatitis B virus (HBV)-induced hepatocellular carcinoma (HCC) is a major cause of cancer-related deaths in Asia and Africa. Developing effective and non-invasive biomarkers of HCC for individual patients remains an urgent task for early diagnosis and convenient monitoring. Analyzing the transcriptomic profiles of peripheral blood mononuclear cells from both healthy donors and patients with chronic HBV infection in different states (i.e. HBV carrier, chronic hepatitis B, cirrhosis, and HCC), we identified a set of 19 candidate genes according to our algorithm of dynamic network biomarkers. These genes can both characterize different stages during HCC progression and identify cirrhosis as the critical transition stage before carcinogenesis. The interaction effects (i.e. co-expressions) of candidate genes were used to build an accurate prediction model: the so-called edge-based biomarker. Considering the convenience and robustness of biomarkers in clinical applications, we performed functional analysis, validated candidate genes in other independent samples of our collected cohort, and finally selected COL5A1, HLA-DQB1, MMP2, and CDK4 to build edge panel as prediction models. We demonstrated that the edge panel had great performance in both diagnosis and prognosis in terms of precision and specificity for HCC, especially for patients with alpha-fetoprotein-negative HCC. Our study not only provides a novel edge-based biomarker for non-invasive and effective diagnosis of HBV-associated HCC to each individual patient but also introduces a new way to integrate the interaction terms of individual molecules for clinical diagnosis and prognosis from the network and dynamics perspectives."
0,Structural determinants governing Î²-arrestin2 interaction with PDZ proteins and recruitment to CRFR1,"Î²-Arrestins are multifunctional adaptor proteins best know for their vital role in regulating G protein coupled receptor (GPCR) trafficking and signaling. Î²-arrestin2 recruitment and receptor internalization of corticotropin-releasing factor receptor 1 (CRFR1), a GPCR whose antagonists have been shown to demonstrate both anxiolytic- and antidepressant-like effects, have previously been shown to be modulated by PDZ proteins. Thus, a structural characterization of the interaction between Î²-arrestins and PDZ proteins can delineate potential mechanism of PDZ-dependent regulation of GPCR trafficking. Here, we find that the PDZ proteins PSD-95, MAGI1, and PDZK1 interact with Î²-arrestin2 in a PDZ domain-dependent manner. Further investigation of such interaction using mutational analyses revealed that mutating the alanine residue at 175 residue of Î²-arrestin2 to phenylalanine impairs interaction with PSD-95. Additionally, A175F mutant of Î²-arrestin2 shows decreased CRF-stimulated recruitment to CRFR1 and reduced receptor internalization. Thus, our findings show that the interaction between Î²-arrestins and PDZ proteins is key for CRFR1 trafficking and may be targeted to mitigate impaired CRFR1 signaling in mental and psychiatric disorders.","Structural determinants governing Î²-arrestin2 interaction with PDZ proteins and recruitment to CRFR1. Î²-Arrestins are multifunctional adaptor proteins best know for their vital role in regulating G protein coupled receptor (GPCR) trafficking and signaling. Î²-arrestin2 recruitment and receptor internalization of corticotropin-releasing factor receptor 1 (CRFR1), a GPCR whose antagonists have been shown to demonstrate both anxiolytic- and antidepressant-like effects, have previously been shown to be modulated by PDZ proteins. Thus, a structural characterization of the interaction between Î²-arrestins and PDZ proteins can delineate potential mechanism of PDZ-dependent regulation of GPCR trafficking. Here, we find that the PDZ proteins PSD-95, MAGI1, and PDZK1 interact with Î²-arrestin2 in a PDZ domain-dependent manner. Further investigation of such interaction using mutational analyses revealed that mutating the alanine residue at 175 residue of Î²-arrestin2 to phenylalanine impairs interaction with PSD-95. Additionally, A175F mutant of Î²-arrestin2 shows decreased CRF-stimulated recruitment to CRFR1 and reduced receptor internalization. Thus, our findings show that the interaction between Î²-arrestins and PDZ proteins is key for CRFR1 trafficking and may be targeted to mitigate impaired CRFR1 signaling in mental and psychiatric disorders."
0,Establishing Cerebral Organoids as Models of Human-Specific Brain Evolution,,
0,Duration of type 2 diabetes and remission rates after bariatric surgery in Sweden 2007-2015: A registry-based cohort study,,
0,Altered Neural Processing of Threat-Related Information in Children and Adolescents Exposed to Violence: A Transdiagnostic Mechanism Contributing to the Emergence of Psychopathology,,
0,The gut microbiome signatures discriminate healthy from pulmonary tuberculosis patients,"Cross talk occurs between the human gut and the lung through a gut-lung axis involving the gut microbiota. However, the signatures of the human gut microbiota after active Mycobacterium tuberculosis infection have not been fully understood. Here, we investigated changes in the gut microbiota in tuberculosis (TB) patients by shotgun sequencing the gut microbiomes of 31 healthy controls and 46 patients. We observed a dramatic changes in gut microbiota in tuberculosis patients as reflected by significant decreases in species number and microbial diversity. The gut microbiota of TB patients were mostly featured by the striking decrease of short-chain fatty acids (SCFAs)-producingbacteria as well as associated metabolic pathways. A classification model based on the abundance of three species, Haemophilus parainfluenzae, Roseburia inulinivorans, and Roseburia hominis, performed well for discriminating between healthy and diseased patients. Additionally, the healthy and diseased states can be distinguished by SNPs in the species of B. vulgatus. We present a comprehensive profile of changes in the microbiota in clinical TB patients. Our findings will shed light on the design of future diagnoses and treatments for M. tuberculosis infections.","The gut microbiome signatures discriminate healthy from pulmonary tuberculosis patients. Cross talk occurs between the human gut and the lung through a gut-lung axis involving the gut microbiota. However, the signatures of the human gut microbiota after active Mycobacterium tuberculosis infection have not been fully understood. Here, we investigated changes in the gut microbiota in tuberculosis (TB) patients by shotgun sequencing the gut microbiomes of 31 healthy controls and 46 patients. We observed a dramatic changes in gut microbiota in tuberculosis patients as reflected by significant decreases in species number and microbial diversity. The gut microbiota of TB patients were mostly featured by the striking decrease of short-chain fatty acids (SCFAs)-producingbacteria as well as associated metabolic pathways. A classification model based on the abundance of three species, Haemophilus parainfluenzae, Roseburia inulinivorans, and Roseburia hominis, performed well for discriminating between healthy and diseased patients. Additionally, the healthy and diseased states can be distinguished by SNPs in the species of B. vulgatus. We present a comprehensive profile of changes in the microbiota in clinical TB patients. Our findings will shed light on the design of future diagnoses and treatments for M. tuberculosis infections."
0,Bioinformatics Analysis of the Core Genes Related to Lupus Nephritis Through a Network and Pathway-Based Approach,"In this study, we explored the genes genetically associated with lupus nephritis (LN), and their function by bioinformatics analysis. We collected genes potentially associated with LN from National Center for Biotechnology Information Center (NCBI-Gene) and Online Mendelian Inheritance in Man (OMIM) databases. The major bioinformatics analysis linked with genes was then revealed by weighted gene co-expression network analysis (WGCNA), crosstalk analysis, functional analysis, and Pivot algorithm. Two hundred twenty-three LN-related genes were obtained by intersecting NCBI-Gene and OMIM databases. Two thousand five hundred sixty-eight LN-related proteins and 23 modules were excavated by String protein interaction network and WGCNA co-expression analysis, respectively. Pivot algorithm included no coding RNA, transcription factor and drug indicated the high-count correlation-associated modules related to cancer, kidney pathophysiological changes, and kidney injury, respectively. Gene ontology and Kyoto Encyclopedia of Genes and Genomes analysis based on 23 modules revealed LN-related genes mainly involved in immune response. Moreover, 19 genes that came from intersection of LN, arthritis, pleurisy, and myocarditis have close relationship with immune diseases and immune processes. Our results from this research may have important implications for understanding the genes underlying LN. Also, the framework proposed in this work can be used to research pathological molecular network and genes related to LN.","Bioinformatics Analysis of the Core Genes Related to Lupus Nephritis Through a Network and Pathway-Based Approach. In this study, we explored the genes genetically associated with lupus nephritis (LN), and their function by bioinformatics analysis. We collected genes potentially associated with LN from National Center for Biotechnology Information Center (NCBI-Gene) and Online Mendelian Inheritance in Man (OMIM) databases. The major bioinformatics analysis linked with genes was then revealed by weighted gene co-expression network analysis (WGCNA), crosstalk analysis, functional analysis, and Pivot algorithm. Two hundred twenty-three LN-related genes were obtained by intersecting NCBI-Gene and OMIM databases. Two thousand five hundred sixty-eight LN-related proteins and 23 modules were excavated by String protein interaction network and WGCNA co-expression analysis, respectively. Pivot algorithm included no coding RNA, transcription factor and drug indicated the high-count correlation-associated modules related to cancer, kidney pathophysiological changes, and kidney injury, respectively. Gene ontology and Kyoto Encyclopedia of Genes and Genomes analysis based on 23 modules revealed LN-related genes mainly involved in immune response. Moreover, 19 genes that came from intersection of LN, arthritis, pleurisy, and myocarditis have close relationship with immune diseases and immune processes. Our results from this research may have important implications for understanding the genes underlying LN. Also, the framework proposed in this work can be used to research pathological molecular network and genes related to LN."
0,Endoscopic Surveillance of Barrett's Esophagus Using Volumetric Laser Endomicroscopy With Artificial Intelligence Image Enhancement,,
0,Assessment and Management of Patients at Risk for Suicide: Synopsis of the 2019 U.S. Department of Veterans Affairs and U.S. Department of Defense Clinical Practice Guidelines,"Description: In May 2019, the U.S. Department of Veterans Affairs (VA) and U.S. Department of Defense (DoD) approved an update to the 2013 joint clinical practice guideline for assessing and managing patients who are at risk for suicide. This guideline provides health care providers with a framework by which to screen for, evaluate, treat, and manage the individual needs and preferences of VA and DoD patients who may be at risk for suicide. Methods: In January 2018, the VA/DoD Evidence-Based Practice Work Group convened to develop a joint VA/DoD guideline including clinical stakeholders and conforming to the National Academy of Medicine's tenets for trustworthy clinical practice guidelines. The guideline panel drafted key questions, systematically searched and evaluated the literature through April 2018, created algorithms, and advanced 22 recommendations in accordance with the GRADE (Grading of Recommendations Assessment, Development and Evaluation) system. Recommendations: This synopsis, which includes 3 clinical practice algorithms, summarizes the key recommendations of the guideline related to screening and evaluation, risk management and treatment, and other management methods. Risk management and treatment recommendations address both pharmacologic and nonpharmacologic approaches for patients with suicidal ideation and behavior. Other management methods address lethal means safety (such as restricting access to firearms, poisons, and medications and installing barriers to prevent jumping from lethal heights) and population health strategies.","Assessment and Management of Patients at Risk for Suicide: Synopsis of the 2019 U.S. Department of Veterans Affairs and U.S. Department of Defense Clinical Practice Guidelines. Description: In May 2019, the U.S. Department of Veterans Affairs (VA) and U.S. Department of Defense (DoD) approved an update to the 2013 joint clinical practice guideline for assessing and managing patients who are at risk for suicide. This guideline provides health care providers with a framework by which to screen for, evaluate, treat, and manage the individual needs and preferences of VA and DoD patients who may be at risk for suicide. Methods: In January 2018, the VA/DoD Evidence-Based Practice Work Group convened to develop a joint VA/DoD guideline including clinical stakeholders and conforming to the National Academy of Medicine's tenets for trustworthy clinical practice guidelines. The guideline panel drafted key questions, systematically searched and evaluated the literature through April 2018, created algorithms, and advanced 22 recommendations in accordance with the GRADE (Grading of Recommendations Assessment, Development and Evaluation) system. Recommendations: This synopsis, which includes 3 clinical practice algorithms, summarizes the key recommendations of the guideline related to screening and evaluation, risk management and treatment, and other management methods. Risk management and treatment recommendations address both pharmacologic and nonpharmacologic approaches for patients with suicidal ideation and behavior. Other management methods address lethal means safety (such as restricting access to firearms, poisons, and medications and installing barriers to prevent jumping from lethal heights) and population health strategies."
0,"In silico identification of natural product inhibitors for Î³-secretase activating protein, a therapeutic target for Alzheimer's disease","Alzheimer's disease (AD) is clinically characterized by the aggregation of neurotoxic amyloid-Î² (AÎ²) peptides in the brain. Î³-Secretase catalyzes the reaction of AÎ² formation. Inhibition of Î³-secretase activating protein (GSAP) reduces AÎ² production without disrupting other molecular functions and serves as a promising therapeutic target for lowering AÎ² and curing AD. Till date, no proven drug is available for curing AD because of the nonexistence of crystal/NMR structure of GSAP. Thus in the present study, for the first time, we adopted in silico method to predict the 3D structure of GSAP via comparative modeling and studied the architecture and function of GSAP through simulation studies. Docking studies with 4153 phytochemicals revealed that GSAP having a better binding affinity with macaflavanone C, (E)-1-[2,4-dihydroxy-3-(3-methylbut-2-enyl)phenyl]-3-(2,2-dimethyl-8-hydroxy-2H-benzopyran-6-yl)prop-2-en-1-one, and monachosorin B as compared with the standard drug, imatinib. Further, the molecular dynamics analysis suggested that only two phytochemicals, namely, macaflavanone C and (E)-1-[2,4-dihydroxy-3-(3-methylbut-2-enyl)phenyl]-3-(2,2-dimethyl-8-hydroxy-2H-benzopyran-6-yl)prop-2-en-1-one) significantly disrupt the original property of GSAP and also cleared the absorption, distribution, metabolism, and excretion test. These natural compounds may be utilized in future for curing AD after further investigations.","In silico identification of natural product inhibitors for Î³-secretase activating protein, a therapeutic target for Alzheimer's disease. Alzheimer's disease (AD) is clinically characterized by the aggregation of neurotoxic amyloid-Î² (AÎ²) peptides in the brain. Î³-Secretase catalyzes the reaction of AÎ² formation. Inhibition of Î³-secretase activating protein (GSAP) reduces AÎ² production without disrupting other molecular functions and serves as a promising therapeutic target for lowering AÎ² and curing AD. Till date, no proven drug is available for curing AD because of the nonexistence of crystal/NMR structure of GSAP. Thus in the present study, for the first time, we adopted in silico method to predict the 3D structure of GSAP via comparative modeling and studied the architecture and function of GSAP through simulation studies. Docking studies with 4153 phytochemicals revealed that GSAP having a better binding affinity with macaflavanone C, (E)-1-[2,4-dihydroxy-3-(3-methylbut-2-enyl)phenyl]-3-(2,2-dimethyl-8-hydroxy-2H-benzopyran-6-yl)prop-2-en-1-one, and monachosorin B as compared with the standard drug, imatinib. Further, the molecular dynamics analysis suggested that only two phytochemicals, namely, macaflavanone C and (E)-1-[2,4-dihydroxy-3-(3-methylbut-2-enyl)phenyl]-3-(2,2-dimethyl-8-hydroxy-2H-benzopyran-6-yl)prop-2-en-1-one) significantly disrupt the original property of GSAP and also cleared the absorption, distribution, metabolism, and excretion test. These natural compounds may be utilized in future for curing AD after further investigations."
0,Apolipoprotein AI) Promotes Atherosclerosis Regression in Diabetic Mice by Suppressing Myelopoiesis and Plaque Inflammation,"BACKGROUND: Despite robust cholesterol lowering, cardiovascular disease risk remains increased in patients with diabetes mellitus. Consistent with this, diabetes mellitus impairs atherosclerosis regression after cholesterol lowering in humans and mice. In mice, this is attributed in part to hyperglycemia-induced monocytosis, which increases monocyte entry into plaques despite cholesterol lowering. In addition, diabetes mellitus skews plaque macrophages toward an atherogenic inflammatory M1 phenotype instead of toward the atherosclerosis-resolving M2 state typical with cholesterol lowering. Functional high-density lipoprotein (HDL), typically low in patients with diabetes mellitus, reduces monocyte precursor proliferation in murine bone marrow and has anti-inflammatory effects on human and murine macrophages. Our study aimed to test whether raising functional HDL levels in diabetic mice prevents monocytosis, reduces the quantity and inflammation of plaque macrophages, and enhances atherosclerosis regression after cholesterol lowering. METHODS: Aortic arches containing plaques developed in Ldlr(-/-) mice were transplanted into either wild-type, diabetic wild-type, or diabetic mice transgenic for human apolipoprotein AI, which have elevated functional HDL. Recipient mice all had low levels of low-density lipoprotein cholesterol to promote plaque regression. After 2 weeks, plaques in recipient mouse aortic grafts were examined. RESULTS: Diabetic wild-type mice had impaired atherosclerosis regression, which was normalized by raising HDL levels. This benefit was linked to suppressed hyperglycemia-driven myelopoiesis, monocytosis, and neutrophilia. Increased HDL improved cholesterol efflux from bone marrow progenitors, suppressing their proliferation and monocyte and neutrophil production capacity. In addition to reducing circulating monocytes available for recruitment into plaques, in the diabetic milieu, HDL suppressed the general recruitability of monocytes to inflammatory sites and promoted plaque macrophage polarization to the M2, atherosclerosis-resolving state. There was also a decrease in plaque neutrophil extracellular traps, which are atherogenic and increased by diabetes mellitus. CONCLUSIONS: Raising apolipoprotein AI and functional levels of HDL promotes multiple favorable changes in the production of monocytes and neutrophils and in the inflammatory environment of atherosclerotic plaques of diabetic mice after cholesterol lowering and may represent a novel approach to reduce cardiovascular disease risk in people with diabetes mellitus.","Apolipoprotein AI) Promotes Atherosclerosis Regression in Diabetic Mice by Suppressing Myelopoiesis and Plaque Inflammation. BACKGROUND: Despite robust cholesterol lowering, cardiovascular disease risk remains increased in patients with diabetes mellitus. Consistent with this, diabetes mellitus impairs atherosclerosis regression after cholesterol lowering in humans and mice. In mice, this is attributed in part to hyperglycemia-induced monocytosis, which increases monocyte entry into plaques despite cholesterol lowering. In addition, diabetes mellitus skews plaque macrophages toward an atherogenic inflammatory M1 phenotype instead of toward the atherosclerosis-resolving M2 state typical with cholesterol lowering. Functional high-density lipoprotein (HDL), typically low in patients with diabetes mellitus, reduces monocyte precursor proliferation in murine bone marrow and has anti-inflammatory effects on human and murine macrophages. Our study aimed to test whether raising functional HDL levels in diabetic mice prevents monocytosis, reduces the quantity and inflammation of plaque macrophages, and enhances atherosclerosis regression after cholesterol lowering. METHODS: Aortic arches containing plaques developed in Ldlr(-/-) mice were transplanted into either wild-type, diabetic wild-type, or diabetic mice transgenic for human apolipoprotein AI, which have elevated functional HDL. Recipient mice all had low levels of low-density lipoprotein cholesterol to promote plaque regression. After 2 weeks, plaques in recipient mouse aortic grafts were examined. RESULTS: Diabetic wild-type mice had impaired atherosclerosis regression, which was normalized by raising HDL levels. This benefit was linked to suppressed hyperglycemia-driven myelopoiesis, monocytosis, and neutrophilia. Increased HDL improved cholesterol efflux from bone marrow progenitors, suppressing their proliferation and monocyte and neutrophil production capacity. In addition to reducing circulating monocytes available for recruitment into plaques, in the diabetic milieu, HDL suppressed the general recruitability of monocytes to inflammatory sites and promoted plaque macrophage polarization to the M2, atherosclerosis-resolving state. There was also a decrease in plaque neutrophil extracellular traps, which are atherogenic and increased by diabetes mellitus. CONCLUSIONS: Raising apolipoprotein AI and functional levels of HDL promotes multiple favorable changes in the production of monocytes and neutrophils and in the inflammatory environment of atherosclerotic plaques of diabetic mice after cholesterol lowering and may represent a novel approach to reduce cardiovascular disease risk in people with diabetes mellitus."
0,The practical implementation of artificial intelligence technologies in medicine,"The development of artificial intelligence (AI)-based technologies in medicine is advancing rapidly, but real-world clinical implementation has not yet become a reality. Here we review some of the key practical issues surrounding the implementation of AI into existing clinical workflows, including data sharing and privacy, transparency of algorithms, data standardization, and interoperability across multiple platforms, and concern for patient safety. We summarize the current regulatory environment in the United States and highlight comparisons with other regions in the world, notably Europe and China.","The practical implementation of artificial intelligence technologies in medicine. The development of artificial intelligence (AI)-based technologies in medicine is advancing rapidly, but real-world clinical implementation has not yet become a reality. Here we review some of the key practical issues surrounding the implementation of AI into existing clinical workflows, including data sharing and privacy, transparency of algorithms, data standardization, and interoperability across multiple platforms, and concern for patient safety. We summarize the current regulatory environment in the United States and highlight comparisons with other regions in the world, notably Europe and China."
0,Brain metabolism modulates neuronal excitability in a mouse model of pyruvate dehydrogenase deficiency,"Glucose is the ultimate substrate for most brain activities that use carbon, including synthesis of the neurotransmitters glutamate and gamma-aminobutyric acid via mitochondrial tricarboxylic acid (TCA) cycle. Brain metabolism and neuronal excitability are thus interdependent. However, the principles that govern their relationship are not always intuitive because heritable defects of brain glucose metabolism are associated with the paradoxical coexistence, in the same individual, of episodic neuronal hyperexcitation (seizures) with reduced basal cerebral electrical activity. One such prototypic disorder is pyruvate dehydrogenase (PDH) deficiency (PDHD). PDH is central to metabolism because it steers most of the glucose-derived flux into the TCA cycle. To better understand the pathophysiology of PDHD, we generated mice with brain-specific reduced PDH activity that paralleled salient human disease features, including cerebral hypotrophy, decreased amplitude electroencephalogram (EEG), and epilepsy. The mice exhibited reductions in cerebral TCA cycle flux, glutamate content, spontaneous, and electrically evoked in vivo cortical field potentials and gamma EEG oscillation amplitude. Episodic decreases in gamma oscillations preceded most epileptiform discharges, facilitating their prediction. Fast-spiking neuron excitability was decreased in brain slices, contributing to in vivo action potential burst prolongation after whisker pad stimulation. These features were partially reversed after systemic administration of acetate, which augmented cerebral TCA cycle flux, glutamate-dependent synaptic transmission, inhibition and gamma oscillations, and reduced epileptiform discharge duration. Thus, our results suggest that dysfunctional excitability in PDHD is consequent to reduced oxidative flux, which leads to decreased neuronal activation and impaired inhibition, and can be mitigated by an alternative metabolic substrate.","Brain metabolism modulates neuronal excitability in a mouse model of pyruvate dehydrogenase deficiency. Glucose is the ultimate substrate for most brain activities that use carbon, including synthesis of the neurotransmitters glutamate and gamma-aminobutyric acid via mitochondrial tricarboxylic acid (TCA) cycle. Brain metabolism and neuronal excitability are thus interdependent. However, the principles that govern their relationship are not always intuitive because heritable defects of brain glucose metabolism are associated with the paradoxical coexistence, in the same individual, of episodic neuronal hyperexcitation (seizures) with reduced basal cerebral electrical activity. One such prototypic disorder is pyruvate dehydrogenase (PDH) deficiency (PDHD). PDH is central to metabolism because it steers most of the glucose-derived flux into the TCA cycle. To better understand the pathophysiology of PDHD, we generated mice with brain-specific reduced PDH activity that paralleled salient human disease features, including cerebral hypotrophy, decreased amplitude electroencephalogram (EEG), and epilepsy. The mice exhibited reductions in cerebral TCA cycle flux, glutamate content, spontaneous, and electrically evoked in vivo cortical field potentials and gamma EEG oscillation amplitude. Episodic decreases in gamma oscillations preceded most epileptiform discharges, facilitating their prediction. Fast-spiking neuron excitability was decreased in brain slices, contributing to in vivo action potential burst prolongation after whisker pad stimulation. These features were partially reversed after systemic administration of acetate, which augmented cerebral TCA cycle flux, glutamate-dependent synaptic transmission, inhibition and gamma oscillations, and reduced epileptiform discharge duration. Thus, our results suggest that dysfunctional excitability in PDHD is consequent to reduced oxidative flux, which leads to decreased neuronal activation and impaired inhibition, and can be mitigated by an alternative metabolic substrate."
0,Acid-Base Reports Need a Text Explanation,,
0,Accurate blood pressure during patient arm movement: The Welch Allyn Connex Spot Monitor's SureBP algorithm,"Background Current blood pressure (BP) measurement guidelines specify patient requirements, including being still. Some populations of patients cannot comply. A new International Organization for Standards is being developed to test devices that claim tolerance to transport-induced motion artifacts. This study proposes the first protocol to assess BP device accuracy in the presence of patient-induced motion. Participants and methods Forty healthy volunteers (23 males) participated. The device tested was the Welch Allyn Connex Spot Monitor (CSM) using the SureBP algorithm. A reusable cuff was placed on the left arm. During inflation/deflation cycles the participant performed pronation/supination movements of the left forearm every 5 s. The CSM readings during motion were compared to the average of manual resting auscultatory estimations immediately before and after each motion cycle (bracketing). Results The CSM recorded a BP reading on the first cycle in 37 participants. It displayed a reading in all 40 participants with one repeat cycle in the other three. The meanÂ±SD for the device minus the manual BP values was 0.9Â±7.3 mmHg for systolic BP and -3.4Â±7.9 mmHg for diastolic BP. Conclusion This study represents a proposal for an automated BP device assessment in the presence of patient-induced motion. The CSM device, which uses an inflation-based algorithm, routinely produced BP values that closely matched auscultatory values bracketed immediately before and after the motion-associated cycle. The CSM should be of significant clinical value in populations in whom resting 'still' readings are not usually feasible, such as pediatric and geriatric patients, and patients in pain from injury or illness.","Accurate blood pressure during patient arm movement: The Welch Allyn Connex Spot Monitor's SureBP algorithm. Background Current blood pressure (BP) measurement guidelines specify patient requirements, including being still. Some populations of patients cannot comply. A new International Organization for Standards is being developed to test devices that claim tolerance to transport-induced motion artifacts. This study proposes the first protocol to assess BP device accuracy in the presence of patient-induced motion. Participants and methods Forty healthy volunteers (23 males) participated. The device tested was the Welch Allyn Connex Spot Monitor (CSM) using the SureBP algorithm. A reusable cuff was placed on the left arm. During inflation/deflation cycles the participant performed pronation/supination movements of the left forearm every 5 s. The CSM readings during motion were compared to the average of manual resting auscultatory estimations immediately before and after each motion cycle (bracketing). Results The CSM recorded a BP reading on the first cycle in 37 participants. It displayed a reading in all 40 participants with one repeat cycle in the other three. The meanÂ±SD for the device minus the manual BP values was 0.9Â±7.3 mmHg for systolic BP and -3.4Â±7.9 mmHg for diastolic BP. Conclusion This study represents a proposal for an automated BP device assessment in the presence of patient-induced motion. The CSM device, which uses an inflation-based algorithm, routinely produced BP values that closely matched auscultatory values bracketed immediately before and after the motion-associated cycle. The CSM should be of significant clinical value in populations in whom resting 'still' readings are not usually feasible, such as pediatric and geriatric patients, and patients in pain from injury or illness."
0,Identification and characterization of potential membrane-bound molecular drug targets of methicillin-resistant Staphylococcus aureus using in silico approaches,"Aim. To identify novel putative drug targets of methicillin-resistant S. aureus (MRSA) through subtractive proteome analysis. Methods. Identification of non-homologous proteins in the human proteome, search of MRSA essential genes and evaluation of drug target novelty were performed using a protein BLAST server. Unique metabolic pathways identification was carried out using data and tools from KEGG (Kyoto Encyclopedia of Genes and Genomes). Prediction of sub-cellular proteins localization was performed using combination of PSORT v. 3.0.2, CELLO v. 2.5, iLoc-Gpos, and Pred-Lipo tools. Homology modeling was performed using SWISS-MODEL, Phyre2, I-TASSER web-servers and the MODELLER software. Results. Proteomes of six annotated methicillin-resistant strains: MRSA ATCC BAA-1680, H-EMRSA-15, LA MRSA ST398, MRSA 252, MRSA ST772, UTSW MRSA 55 were initially analyzed. The proteome analysis of the MRSA strains in several consequent steps allowed to identify two molecular targets: diadenylate cyclase and D-alanyl-lipoteichoic acid biosynthesis (DltB) protein which meet the requirements of being essential, membrane-bound, non-homologous to human proteome, involved in unique metabolic pathways and new in terms of not having approved drugs. Using the homology modeling approach, we have built three-dimensional structures of these proteins and predicted their ligand-binding sites. Conclusions. We used classical bioinformatics approaches to identify two molecular targets of MRSA:diadenylate cyclase and DltB which can be used for further rational drug design in order to find novel therapeutic agents for treatment of multidrug resistant staphylococcal infection.","Identification and characterization of potential membrane-bound molecular drug targets of methicillin-resistant Staphylococcus aureus using in silico approaches. Aim. To identify novel putative drug targets of methicillin-resistant S. aureus (MRSA) through subtractive proteome analysis. Methods. Identification of non-homologous proteins in the human proteome, search of MRSA essential genes and evaluation of drug target novelty were performed using a protein BLAST server. Unique metabolic pathways identification was carried out using data and tools from KEGG (Kyoto Encyclopedia of Genes and Genomes). Prediction of sub-cellular proteins localization was performed using combination of PSORT v. 3.0.2, CELLO v. 2.5, iLoc-Gpos, and Pred-Lipo tools. Homology modeling was performed using SWISS-MODEL, Phyre2, I-TASSER web-servers and the MODELLER software. Results. Proteomes of six annotated methicillin-resistant strains: MRSA ATCC BAA-1680, H-EMRSA-15, LA MRSA ST398, MRSA 252, MRSA ST772, UTSW MRSA 55 were initially analyzed. The proteome analysis of the MRSA strains in several consequent steps allowed to identify two molecular targets: diadenylate cyclase and D-alanyl-lipoteichoic acid biosynthesis (DltB) protein which meet the requirements of being essential, membrane-bound, non-homologous to human proteome, involved in unique metabolic pathways and new in terms of not having approved drugs. Using the homology modeling approach, we have built three-dimensional structures of these proteins and predicted their ligand-binding sites. Conclusions. We used classical bioinformatics approaches to identify two molecular targets of MRSA:diadenylate cyclase and DltB which can be used for further rational drug design in order to find novel therapeutic agents for treatment of multidrug resistant staphylococcal infection."
0,Noise reduction in diffusion MRI using non-local self-similar information in joint x-q space,"Diffusion MRI affords valuable insights into white matter microstructures, but suffers from low signal-to-noise ratio (SNR), especially at high diffusion weighting (i.e., b-value). To avoid time-intensive repeated acquisition, post-processing algorithms are often used to reduce noise. Among existing methods, non-local means (NLM) has been shown to be particularly effective. However, most NLM algorithms for diffusion MRI focus on patch matching in the spatial domain (i.e., x-space) and disregard the fact that the data live in a combined 6D space covering both spatial domain and diffusion wavevector domain (i.e., q-space). This drawback leads to inaccurate patch matching in curved white matter structures and hence the inability to effectively use recurrent information for noise reduction. The goal of this paper is to overcome this limitation by extending NLM to the joint x-q space. Specifically, we define for each point in the x-q space a spherical patch from which we extract rotation-invariant features for patch matching. The ability to perform patch matching across q-samples allows patches from differentially orientated structures to be used for effective noise removal. Extensive experiments on synthetic, repeated-acquisition, and HCP data demonstrate that our method outperforms state-of-the-art methods, both qualitatively and quantitatively.","Noise reduction in diffusion MRI using non-local self-similar information in joint x-q space. Diffusion MRI affords valuable insights into white matter microstructures, but suffers from low signal-to-noise ratio (SNR), especially at high diffusion weighting (i.e., b-value). To avoid time-intensive repeated acquisition, post-processing algorithms are often used to reduce noise. Among existing methods, non-local means (NLM) has been shown to be particularly effective. However, most NLM algorithms for diffusion MRI focus on patch matching in the spatial domain (i.e., x-space) and disregard the fact that the data live in a combined 6D space covering both spatial domain and diffusion wavevector domain (i.e., q-space). This drawback leads to inaccurate patch matching in curved white matter structures and hence the inability to effectively use recurrent information for noise reduction. The goal of this paper is to overcome this limitation by extending NLM to the joint x-q space. Specifically, we define for each point in the x-q space a spherical patch from which we extract rotation-invariant features for patch matching. The ability to perform patch matching across q-samples allows patches from differentially orientated structures to be used for effective noise removal. Extensive experiments on synthetic, repeated-acquisition, and HCP data demonstrate that our method outperforms state-of-the-art methods, both qualitatively and quantitatively."
0,Management of Thyroid Nodules Seen on US Images: Deep Learning May Match Performance of Radiologists,"BackgroundManagement of thyroid nodules may be inconsistent between different observers and time consuming for radiologists. An artificial intelligence system that uses deep learning may improve radiology workflow for management of thyroid nodules.PurposeTo develop a deep learning algorithm that uses thyroid US images to decide whether a thyroid nodule should undergo a biopsy and to compare the performance of the algorithm with the performance of radiologists who adhere to American College of Radiology (ACR) Thyroid Imaging Reporting and Data System (TI-RADS).Materials and MethodsIn this retrospective analysis, studies in patients referred for US with subsequent fine-needle aspiration or with surgical histologic analysis used as the standard were evaluated. The study period was from August 2006 to May 2010. A multitask deep convolutional neural network was trained to provide biopsy recommendations for thyroid nodules on the basis of two orthogonal US images as the input. In the training phase, the deep learning algorithm was first evaluated by using 10-fold cross-validation. Internal validation was then performed on an independent set of 99 consecutive nodules. The sensitivity and specificity of the algorithm were compared with a consensus of three ACR TI-RADS committee experts and nine other radiologists, all of whom interpreted thyroid US images in clinical practice.ResultsIncluded were 1377 thyroid nodules in 1230 patients with complete imaging data and conclusive cytologic or histologic diagnoses. For the 99 test nodules, the proposed deep learning algorithm achieved 13 of 15 (87%: 95% confidence interval [CI]: 67%, 100%) sensitivity, the same as expert consensus (P > .99) and higher than five of nine radiologists. The specificity of the deep learning algorithm was 44 of 84 (52%; 95% CI: 42%, 62%), which was similar to expert consensus (43 of 84; 51%; 95% CI: 41%, 62%; P = .91) and higher than seven of nine other radiologists. The mean sensitivity and specificity for the nine radiologists was 83% (95% CI: 64%, 98%) and 48% (95% CI: 37%, 59%), respectively.ConclusionSensitivity and specificity of a deep learning algorithm for thyroid nodule biopsy recommendations was similar to that of expert radiologists who used American College of Radiology Thyroid Imaging and Reporting Data System guidelines.(c) RSNA, 2019Online supplemental material is available for this article.","Management of Thyroid Nodules Seen on US Images: Deep Learning May Match Performance of Radiologists. BackgroundManagement of thyroid nodules may be inconsistent between different observers and time consuming for radiologists. An artificial intelligence system that uses deep learning may improve radiology workflow for management of thyroid nodules.PurposeTo develop a deep learning algorithm that uses thyroid US images to decide whether a thyroid nodule should undergo a biopsy and to compare the performance of the algorithm with the performance of radiologists who adhere to American College of Radiology (ACR) Thyroid Imaging Reporting and Data System (TI-RADS).Materials and MethodsIn this retrospective analysis, studies in patients referred for US with subsequent fine-needle aspiration or with surgical histologic analysis used as the standard were evaluated. The study period was from August 2006 to May 2010. A multitask deep convolutional neural network was trained to provide biopsy recommendations for thyroid nodules on the basis of two orthogonal US images as the input. In the training phase, the deep learning algorithm was first evaluated by using 10-fold cross-validation. Internal validation was then performed on an independent set of 99 consecutive nodules. The sensitivity and specificity of the algorithm were compared with a consensus of three ACR TI-RADS committee experts and nine other radiologists, all of whom interpreted thyroid US images in clinical practice.ResultsIncluded were 1377 thyroid nodules in 1230 patients with complete imaging data and conclusive cytologic or histologic diagnoses. For the 99 test nodules, the proposed deep learning algorithm achieved 13 of 15 (87%: 95% confidence interval [CI]: 67%, 100%) sensitivity, the same as expert consensus (P > .99) and higher than five of nine radiologists. The specificity of the deep learning algorithm was 44 of 84 (52%; 95% CI: 42%, 62%), which was similar to expert consensus (43 of 84; 51%; 95% CI: 41%, 62%; P = .91) and higher than seven of nine other radiologists. The mean sensitivity and specificity for the nine radiologists was 83% (95% CI: 64%, 98%) and 48% (95% CI: 37%, 59%), respectively.ConclusionSensitivity and specificity of a deep learning algorithm for thyroid nodule biopsy recommendations was similar to that of expert radiologists who used American College of Radiology Thyroid Imaging and Reporting Data System guidelines.(c) RSNA, 2019Online supplemental material is available for this article."
0,Reinventing the eye exam,,
0,Micro-Net: A unified model for segmentation of various objects in microscopy images,"Object segmentation and structure localization are important steps in automated image analysis pipelines for microscopy images. We present a convolution neural network (CNN) based deep learning architecture for segmentation of objects in microscopy images. The proposed network can be used to segment cells, nuclei and glands in fluorescence microscopy and histology images after slight tuning of input parameters. The network trains at multiple resolutions of the input image, connects the intermediate layers for better localization and context and generates the output using multi-resolution deconvolution filters. The extra convolutional layers which bypass the max-pooling operation allow the network to train for variable input intensities and object size and make it robust to noisy data. We compare our results on publicly available data sets and show that the proposed network outperforms recent deep learning algorithms.","Micro-Net: A unified model for segmentation of various objects in microscopy images. Object segmentation and structure localization are important steps in automated image analysis pipelines for microscopy images. We present a convolution neural network (CNN) based deep learning architecture for segmentation of objects in microscopy images. The proposed network can be used to segment cells, nuclei and glands in fluorescence microscopy and histology images after slight tuning of input parameters. The network trains at multiple resolutions of the input image, connects the intermediate layers for better localization and context and generates the output using multi-resolution deconvolution filters. The extra convolutional layers which bypass the max-pooling operation allow the network to train for variable input intensities and object size and make it robust to noisy data. We compare our results on publicly available data sets and show that the proposed network outperforms recent deep learning algorithms."
0,"Dolutegravir versus ritonavir-boosted lopinavir both with dual nucleoside reverse transcriptase inhibitor therapy in adults with HIV-1 infection in whom first-line therapy has failed (DAWNING): an open-label, non-inferiority, phase 3b trial",,
0,Drug Discovery and Repurposing Inhibits a Major Gut Pathogen-Derived Oncogenic Toxin,"Objective: The human intestinal microbiome plays an important role in inflammatory bowel disease (IBD) and colorectal cancer (CRC) development. One of the first discovered bacterial mediators involves Bacteroides fragilis toxin (BFT, also named as fragilysin), a metalloprotease encoded by enterotoxigenic Bacteroides fragilis (ETBF) that causes barrier disruption and inflammation of the colon, leads to tumorigenesis in susceptible mice, and is enriched in the mucosa of IBD and CRC patients. Thus, targeted inhibition of BFT may benefit ETBF carrying patients. Design: By applying two complementary in silico drug design techniques, drug repositioning and molecular docking, we predicted potential BFT inhibitory compounds. Top candidates were tested in vitro on the CRC epithelial cell line HT29/c1 for their potential to inhibit key aspects of BFT activity, being epithelial morphology changes, E-cadherin cleavage (a marker for barrier function) and increased IL-8 secretion. Results: The primary bile acid and existing drug chenodeoxycholic acid (CDCA), currently used for treating gallstones, cerebrotendinous xanthomatosis, and constipation, was found to significantly inhibit all evaluated cell responses to BFT exposure. The inhibition of BFT resulted from a direct interaction between CDCA and BFT, as confirmed by an increase in the melting temperature of the BFT protein in the presence of CDCA. Conclusion: Together, our results show the potential of in silico drug discovery to combat harmful human and microbiome-derived proteins and more specifically suggests a potential for retargeting CDCA to inhibit the pro-oncogenic toxin BFT.","Drug Discovery and Repurposing Inhibits a Major Gut Pathogen-Derived Oncogenic Toxin. Objective: The human intestinal microbiome plays an important role in inflammatory bowel disease (IBD) and colorectal cancer (CRC) development. One of the first discovered bacterial mediators involves Bacteroides fragilis toxin (BFT, also named as fragilysin), a metalloprotease encoded by enterotoxigenic Bacteroides fragilis (ETBF) that causes barrier disruption and inflammation of the colon, leads to tumorigenesis in susceptible mice, and is enriched in the mucosa of IBD and CRC patients. Thus, targeted inhibition of BFT may benefit ETBF carrying patients. Design: By applying two complementary in silico drug design techniques, drug repositioning and molecular docking, we predicted potential BFT inhibitory compounds. Top candidates were tested in vitro on the CRC epithelial cell line HT29/c1 for their potential to inhibit key aspects of BFT activity, being epithelial morphology changes, E-cadherin cleavage (a marker for barrier function) and increased IL-8 secretion. Results: The primary bile acid and existing drug chenodeoxycholic acid (CDCA), currently used for treating gallstones, cerebrotendinous xanthomatosis, and constipation, was found to significantly inhibit all evaluated cell responses to BFT exposure. The inhibition of BFT resulted from a direct interaction between CDCA and BFT, as confirmed by an increase in the melting temperature of the BFT protein in the presence of CDCA. Conclusion: Together, our results show the potential of in silico drug discovery to combat harmful human and microbiome-derived proteins and more specifically suggests a potential for retargeting CDCA to inhibit the pro-oncogenic toxin BFT."
0,"A Laboratory Medicine Best Practices Systematic Review and Meta-analysis of Nucleic Acid Amplification Tests (NAATs) and Algorithms Including NAATs for the Diagnosis of Clostridioides (Clostridium) difficile in Adults (vol 32, e00032-18, 2019)",,
0,Outcomes of allogeneic haematopoietic stem cell transplantation from HLA-matched and alternative donors: a European Society for Blood and Marrow Transplantation registry retrospective analysis,,
0,Passenger Hotspot Mutations in Cancer,"Current statistical models for assessing hotspot significance do not properly account for variation in site-specific mutability, thereby yielding many false-positives. We thus (i) detail a Log-normal-Poisson (LNP) background model that accounts for this variability in a manner consistent with models of mutagenesis; (ii) use it to show that passenger hotspots arise from all common mutational processes; and (iii) apply it to a âˆ¼10,000-patient cohort to nominate driver hotspots with far fewer false-positives compared with conventional methods. Overall, we show that many cancer hotspot mutations recurring at the same genomic site across multiple tumors are actually passenger events, recurring at inherently mutable genomic sites under no positive selection.","Passenger Hotspot Mutations in Cancer. Current statistical models for assessing hotspot significance do not properly account for variation in site-specific mutability, thereby yielding many false-positives. We thus (i) detail a Log-normal-Poisson (LNP) background model that accounts for this variability in a manner consistent with models of mutagenesis; (ii) use it to show that passenger hotspots arise from all common mutational processes; and (iii) apply it to a âˆ¼10,000-patient cohort to nominate driver hotspots with far fewer false-positives compared with conventional methods. Overall, we show that many cancer hotspot mutations recurring at the same genomic site across multiple tumors are actually passenger events, recurring at inherently mutable genomic sites under no positive selection."
0,External Validation of the H2F-PEF Model in Diagnosing Patients With Heart Failure and Preserved Ejection Fraction,,
0,A Randomized Trial of a 1-Hour Troponin T Protocol in Suspected Acute Coronary Syndromes The Rapid Assessment of Possible Acute Coronary Syndrome in the Emergency Department With High-Sensitivity Troponin T Study (RAPID-TnT),,
0,Genetic algorithm as an optimization tool for the development of sponge cell culture media,"Sponges are rich sources of novel natural products. Production in cell cultures may be an option for supply of these compounds but there are currently no sponge cell lines. Because there is a lack of understanding about the precise conditions and nutritional requirements that are necessary to sustain sponge cells in vitro, there has yet to be a defined, sponge-specific nutrient medium. This study utilized a genetic algorithm approach to optimize the amino acid composition of a commercially available basal cell culture medium in order to increase the metabolic activity of cells of the marine sponge Dysidea etheria. Four generations of the algorithm were carried out in vitro in wet lab conditions and an optimal medium combination was selected for further evaluation. When compared to the basal medium control, there was a twofold increase in metabolic activity. The genetic algorithm approach can be used to optimize other components of culture media to efficiently optimize chosen parameters without the need for detailed knowledge on all possible interactions.","Genetic algorithm as an optimization tool for the development of sponge cell culture media. Sponges are rich sources of novel natural products. Production in cell cultures may be an option for supply of these compounds but there are currently no sponge cell lines. Because there is a lack of understanding about the precise conditions and nutritional requirements that are necessary to sustain sponge cells in vitro, there has yet to be a defined, sponge-specific nutrient medium. This study utilized a genetic algorithm approach to optimize the amino acid composition of a commercially available basal cell culture medium in order to increase the metabolic activity of cells of the marine sponge Dysidea etheria. Four generations of the algorithm were carried out in vitro in wet lab conditions and an optimal medium combination was selected for further evaluation. When compared to the basal medium control, there was a twofold increase in metabolic activity. The genetic algorithm approach can be used to optimize other components of culture media to efficiently optimize chosen parameters without the need for detailed knowledge on all possible interactions."
0,Predicting the Debonding of CAD/CAM Composite Resin Crowns with AI,"A preventive measure for debonding has not been established and is highly desirable to improve the survival rate of computer-aided design/computer-aided manufacturing (CAD/CAM) composite resin (CR) crowns. The aim of this study was to assess the usefulness of deep learning with a convolution neural network (CNN) method to predict the debonding probability of CAD/CAM CR crowns from 2-dimensional images captured from 3-dimensional (3D) stereolithography models of a die scanned by a 3D oral scanner. All cases of CAD/CAM CR crowns were manufactured from April 2014 to November 2015 at the Division of Prosthodontics, Osaka University Dental Hospital (Ethical Review Board at Osaka University, approval H27-E11). The data set consisted of a total of 24 cases: 12 trouble-free and 12 debonding as known labels. A total of 8,640 images were randomly divided into 6,480 training and validation images and 2,160 test images. Deep learning with a CNN method was conducted to develop a learning model to predict the debonding probability. The prediction accuracy, precision, recall, F-measure, receiver operating characteristic, and area under the curve of the learning model were assessed for the test images. Also, the mean calculation time was measured during the prediction for the test images. The prediction accuracy, precision, recall, and F-measure values of deep learning with a CNN method for the prediction of the debonding probability were 98.5%, 97.0%, 100%, and 0.985, respectively. The mean calculation time was 2 ms/step for 2,160 test images. The area under the curve was 0.998. Artificial intelligence (AI) technology-that is, the deep learning with a CNN method established in this study-demonstrated considerably good performance in terms of predicting the debonding probability of a CAD/CAM CR crown with 3D stereolithography models of a die scanned from patients.","Predicting the Debonding of CAD/CAM Composite Resin Crowns with AI. A preventive measure for debonding has not been established and is highly desirable to improve the survival rate of computer-aided design/computer-aided manufacturing (CAD/CAM) composite resin (CR) crowns. The aim of this study was to assess the usefulness of deep learning with a convolution neural network (CNN) method to predict the debonding probability of CAD/CAM CR crowns from 2-dimensional images captured from 3-dimensional (3D) stereolithography models of a die scanned by a 3D oral scanner. All cases of CAD/CAM CR crowns were manufactured from April 2014 to November 2015 at the Division of Prosthodontics, Osaka University Dental Hospital (Ethical Review Board at Osaka University, approval H27-E11). The data set consisted of a total of 24 cases: 12 trouble-free and 12 debonding as known labels. A total of 8,640 images were randomly divided into 6,480 training and validation images and 2,160 test images. Deep learning with a CNN method was conducted to develop a learning model to predict the debonding probability. The prediction accuracy, precision, recall, F-measure, receiver operating characteristic, and area under the curve of the learning model were assessed for the test images. Also, the mean calculation time was measured during the prediction for the test images. The prediction accuracy, precision, recall, and F-measure values of deep learning with a CNN method for the prediction of the debonding probability were 98.5%, 97.0%, 100%, and 0.985, respectively. The mean calculation time was 2 ms/step for 2,160 test images. The area under the curve was 0.998. Artificial intelligence (AI) technology-that is, the deep learning with a CNN method established in this study-demonstrated considerably good performance in terms of predicting the debonding probability of a CAD/CAM CR crown with 3D stereolithography models of a die scanned from patients."
0,Hepatocellular Carcinoma,,
0,"Early detection of acral melanoma: A review of clinical, dermoscopic, histopathologic, and molecular characteristics",,
0,Prediction of LC-MS/MS properties of peptides from sequence by deep learning,"Deep learning models for prediction of three key LC-MS/MS properties from peptide sequences were developed. The LC-MS/MS properties or behaviors are indexed retention times (iRT), MS1 or survey scan charge state distributions, and sequence ion intensities of HCD spectra. A common core deep supervised learning architecture, bidirectional long-short term memory (LSTM) recurrent neural networks was used to construct the three prediction models. Two featurization schemes were proposed and demonstrated to allow for efficient encoding of modifications. The iRT and charge state distribution models were trained with on order of 105 data points each. An HCD sequence ion prediction model was trained with 2 Ã— 106 experimental spectra. The iRT prediction model and HCD sequence ion prediction model provide improved accuracies over the start-of-the-art models available in literature. The MS1 charge state distribution prediction model offers excellent performance. The prediction models can be used to enhance peptide identification and quantification in data-dependent acquisition and data-independent acquisition (DIA) experiments as well as to assist MRM (multiple reaction monitoring) and PRM (parallel reaction monitoring) experiment design.","Prediction of LC-MS/MS properties of peptides from sequence by deep learning. Deep learning models for prediction of three key LC-MS/MS properties from peptide sequences were developed. The LC-MS/MS properties or behaviors are indexed retention times (iRT), MS1 or survey scan charge state distributions, and sequence ion intensities of HCD spectra. A common core deep supervised learning architecture, bidirectional long-short term memory (LSTM) recurrent neural networks was used to construct the three prediction models. Two featurization schemes were proposed and demonstrated to allow for efficient encoding of modifications. The iRT and charge state distribution models were trained with on order of 105 data points each. An HCD sequence ion prediction model was trained with 2 Ã— 106 experimental spectra. The iRT prediction model and HCD sequence ion prediction model provide improved accuracies over the start-of-the-art models available in literature. The MS1 charge state distribution prediction model offers excellent performance. The prediction models can be used to enhance peptide identification and quantification in data-dependent acquisition and data-independent acquisition (DIA) experiments as well as to assist MRM (multiple reaction monitoring) and PRM (parallel reaction monitoring) experiment design."
0,Quantification of Uncertainty in Peptide-MHC Binding Prediction Improves High-Affinity Peptide Selection for Therapeutic Design,"The computational identification of peptides that can bind the major histocompatibility complex (MHC) with high affinity is an essential step in developing personal immunotherapies and vaccines. We introduce PUFFIN, a deep residual network-based computational approach that quantifies uncertainty in peptide-MHC affinity prediction that arises from observational noise and the lack of relevant training examples. With PUFFIN's uncertainty metrics, we define binding likelihood, the probability a peptide binds to a given MHC allele at a specified affinity threshold. Compared to affinity point estimates, we find that binding likelihood correlates better with the observed affinity and reduces false positives in high-affinity peptide design. When applied to examine an existing peptide vaccine, PUFFIN identifies an alternative vaccine formulation with higher binding likelihood. PUFFIN is freely available for download at http://github.com/gifford-lab/PUFFIN. Machine-learning models that predict the binding affinity of a peptide-MHC pair are essential in peptide-based therapeutic design, but state-of-the-art methods provide point estimates of affinity that do not consider measurement noise and model uncertainty. We introduce PUFFIN, a method that quantifies the prediction uncertainty and prioritizes peptides with â€œbinding likelihoodâ€ to achieve improved accuracy in high-affinity peptide selection for therapeutic design.","Quantification of Uncertainty in Peptide-MHC Binding Prediction Improves High-Affinity Peptide Selection for Therapeutic Design. The computational identification of peptides that can bind the major histocompatibility complex (MHC) with high affinity is an essential step in developing personal immunotherapies and vaccines. We introduce PUFFIN, a deep residual network-based computational approach that quantifies uncertainty in peptide-MHC affinity prediction that arises from observational noise and the lack of relevant training examples. With PUFFIN's uncertainty metrics, we define binding likelihood, the probability a peptide binds to a given MHC allele at a specified affinity threshold. Compared to affinity point estimates, we find that binding likelihood correlates better with the observed affinity and reduces false positives in high-affinity peptide design. When applied to examine an existing peptide vaccine, PUFFIN identifies an alternative vaccine formulation with higher binding likelihood. PUFFIN is freely available for download at http://github.com/gifford-lab/PUFFIN. Machine-learning models that predict the binding affinity of a peptide-MHC pair are essential in peptide-based therapeutic design, but state-of-the-art methods provide point estimates of affinity that do not consider measurement noise and model uncertainty. We introduce PUFFIN, a method that quantifies the prediction uncertainty and prioritizes peptides with â€œbinding likelihoodâ€ to achieve improved accuracy in high-affinity peptide selection for therapeutic design."
0,A ventral CA1 to nucleus accumbens core engram circuit mediates conditioned place preference for cocaine,,
0,"Effects of fluoxetine on functional outcomes after acute stroke (FOCUS): a pragmatic, double-blind, randomised, controlled trial",,
0,"Four versus six cycles of CHOP chemotherapy in combination with six applications of rituximab in patients with aggressive B-cell lymphoma with favourable prognosis (FLYER): a randomised, phase 3, non-inferiority trial",,
0,Atom-by-atom fabrication with electron beams,,
0,miR-192/215-5p act as tumor suppressors and link Crohn's disease and colorectal cancer by targeting common metabolic pathways: An integrated informatics analysis and experimental study,"MicroRNAs have emerged as key regulators involved in a variety of biological processes. Previous studies have demonstrated that miR-192/215 participated in progression of Crohn's disease and colorectal cancer. However, their concrete relationships and regulation networks in diseases remain unclear. Here, we used bioinformatics methods to expound miR-192/215-5p macrocontrol regulatory networks shared by two diseases. For data mining and figure generation, several miRNA prediction tools, Human miRNA tissue atlas, FunRich, miRcancer, MalaCards, STRING, GEPIA, cBioPortal, GEO databases, Pathvisio, Graphpad Prism 6 software, etc. are extensively applied. miR-192/215-5p were specially distributed in colon tissues and enriched biological pathways were closely associated with human cancers. Emerging role of miR-192/215-5p and their common pathways in Crohn's disease and colorectal cancer was also analyzed. Based on results derived from multiple approaches, we identified the biological functions of miR-192/215-5p as a tumor suppressor and link Crohn's disease and colorectal cancer by targeting triglyceride synthesis and extracellular matrix remodeling pathways.","miR-192/215-5p act as tumor suppressors and link Crohn's disease and colorectal cancer by targeting common metabolic pathways: An integrated informatics analysis and experimental study. MicroRNAs have emerged as key regulators involved in a variety of biological processes. Previous studies have demonstrated that miR-192/215 participated in progression of Crohn's disease and colorectal cancer. However, their concrete relationships and regulation networks in diseases remain unclear. Here, we used bioinformatics methods to expound miR-192/215-5p macrocontrol regulatory networks shared by two diseases. For data mining and figure generation, several miRNA prediction tools, Human miRNA tissue atlas, FunRich, miRcancer, MalaCards, STRING, GEPIA, cBioPortal, GEO databases, Pathvisio, Graphpad Prism 6 software, etc. are extensively applied. miR-192/215-5p were specially distributed in colon tissues and enriched biological pathways were closely associated with human cancers. Emerging role of miR-192/215-5p and their common pathways in Crohn's disease and colorectal cancer was also analyzed. Based on results derived from multiple approaches, we identified the biological functions of miR-192/215-5p as a tumor suppressor and link Crohn's disease and colorectal cancer by targeting triglyceride synthesis and extracellular matrix remodeling pathways."
0,Treatment with a 5-day versus a 10-day schedule of decitabine in older patients with newly diagnosed acute myeloid leukaemia: a randomised phase 2 trial,,
0,Sonographic diagnosis of thyroid cancer with support of AI,,
0,Digital medicine Long data from the electrocardiogram,,
0,Use of Artificial Intelligence to Represent Emergent Systems and Augment Surgical Decision-making,,
0,Accuracy of Computer-Aided Diagnosis of Melanoma: A Meta-analysis,"Importance: The recent advances in the field of machine learning have raised expectations that computer-aided diagnosis will become the standard for the diagnosis of melanoma. Objective: To critically review the current literature and compare the diagnostic accuracy of computer-aided diagnosis with that of human experts. Data Sources: The MEDLINE, arXiv, and PubMed Central databases were searched to identify eligible studies published between January 1, 2002, and December 31, 2018. Study Selection: Studies that reported on the accuracy of automated systems for melanoma were selected. Search terms included melanoma, diagnosis, detection, computer aided, and artificial intelligence. Data Extraction and Synthesis: Evaluation of the risk of bias was performed using the QUADAS-2 tool, and quality assessment was based on predefined criteria. Data were analyzed from February 1 to March 10, 2019. Main Outcomes and Measures: Summary estimates of sensitivity and specificity and summary receiver operating characteristic curves were the primary outcomes. Results: The literature search yielded 1694 potentially eligible studies, of which 132 were included and 70 offered sufficient information for a quantitative analysis. Most studies came from the field of computer science. Prospective clinical studies were rare. Combining the results for automated systems gave a melanoma sensitivity of 0.74 (95% CI, 0.66-0.80) and a specificity of 0.84 (95% CI, 0.79-0.88). Sensitivity was lower in studies that used independent test sets than in those that did not (0.51; 95% CI, 0.34-0.69 vs 0.82; 95% CI, 0.77-0.86; P <.001); however, the specificity was similar (0.83; 95% CI, 0.71-0.91 vs 0.85; 95% CI, 0.80-0.88; P =.67). In comparison with dermatologists' diagnosis, computer-aided diagnosis showed similar sensitivities and a 10 percentage points lower specificity, but the difference was not statistically significant. Studies were heterogeneous and substantial risk of bias was found in all but 4 of the 70 studies included in the quantitative analysis. Conclusions and Relevance: Although the accuracy of computer-aided diagnosis for melanoma detection is comparable to that of experts, the real-world applicability of these systems is unknown and potentially limited owing to overfitting and the risk of bias of the studies at hand.","Accuracy of Computer-Aided Diagnosis of Melanoma: A Meta-analysis. Importance: The recent advances in the field of machine learning have raised expectations that computer-aided diagnosis will become the standard for the diagnosis of melanoma. Objective: To critically review the current literature and compare the diagnostic accuracy of computer-aided diagnosis with that of human experts. Data Sources: The MEDLINE, arXiv, and PubMed Central databases were searched to identify eligible studies published between January 1, 2002, and December 31, 2018. Study Selection: Studies that reported on the accuracy of automated systems for melanoma were selected. Search terms included melanoma, diagnosis, detection, computer aided, and artificial intelligence. Data Extraction and Synthesis: Evaluation of the risk of bias was performed using the QUADAS-2 tool, and quality assessment was based on predefined criteria. Data were analyzed from February 1 to March 10, 2019. Main Outcomes and Measures: Summary estimates of sensitivity and specificity and summary receiver operating characteristic curves were the primary outcomes. Results: The literature search yielded 1694 potentially eligible studies, of which 132 were included and 70 offered sufficient information for a quantitative analysis. Most studies came from the field of computer science. Prospective clinical studies were rare. Combining the results for automated systems gave a melanoma sensitivity of 0.74 (95% CI, 0.66-0.80) and a specificity of 0.84 (95% CI, 0.79-0.88). Sensitivity was lower in studies that used independent test sets than in those that did not (0.51; 95% CI, 0.34-0.69 vs 0.82; 95% CI, 0.77-0.86; P <.001); however, the specificity was similar (0.83; 95% CI, 0.71-0.91 vs 0.85; 95% CI, 0.80-0.88; P =.67). In comparison with dermatologists' diagnosis, computer-aided diagnosis showed similar sensitivities and a 10 percentage points lower specificity, but the difference was not statistically significant. Studies were heterogeneous and substantial risk of bias was found in all but 4 of the 70 studies included in the quantitative analysis. Conclusions and Relevance: Although the accuracy of computer-aided diagnosis for melanoma detection is comparable to that of experts, the real-world applicability of these systems is unknown and potentially limited owing to overfitting and the risk of bias of the studies at hand."
0,Safety and immunogenicity of a vaccine for extra-intestinal pathogenic Escherichia coli (ESTELLA): a phase 2 randomised controlled trial,"Background: ExPEC4V (JNJ-63871860) is a bioconjugate vaccine, containing O-antigens from Escherichia coli serotypes O1A, O2, O6A, and O25B, developed for the prevention of invasive extra-intestinal pathogenic E coli (ExPEC) disease. We aimed to assess safety, reactogenicity, and immunogenicity of ExPEC4V in healthy adults. Methods: In this phase 2 randomised, double-blind placebo-controlled study, we recruited healthy adults (â‰¥18 years with a body-mass index of 35 kg/m2 or less) between Nov 16, 2015, and Aug 8, 2017, and randomly assigned them to receive a single dose of ExPEC4V (antigen O1A:O2:O6A:O25B content 4:4:4:4 Î¼g [group 1]; 4:4:4:8 Î¼g [group 2], 8:8:8:8 Î¼g [group 3], 8:8:8:16 Î¼g [group 4], or 16:16:16:16 Î¼g [group 5]) or placebo. The primary objectives were evaluation of the safety, tolerability, and immunogenicity of ExPEC4V and determination of its dose-dependent immunogenicity 15 days after vaccination by ELISA in individuals who had received at least one vaccination dose. Antibody titres and safety evaluation were used to select two ExPEC4V doses for assessment up to day 360. This trial is registered at ClinicalTrials.gov, number NCT02546960. Findings: Of 848 enrolled participants, 843 (99%) received the ExPEC4V vaccine (757) or placebo (86) and were included in the safety analysis. Of 757 participants vaccinated with ExPEC4V, 222 (29%) had a solicited local adverse event and 325 (43%) had any solicited systemic adverse event, compared with 11 (13%) and 30 (35%) of 86 participants in the control group. Symptoms were mild-to-moderate. The most frequently reported solicited local adverse event was pain or tenderness (205 [27Â·1%] of 757 in combined ExPEC4V groups) and the most frequently reported solicited systemic adverse event was fatigue (208 [27Â·6%] of 757). Only 13 (2%) of 843 had a grade 3 event. At day 15, 80% or more of all participants achieved a two times or greater increase in serotype-specific IgG antibodies (except O25B at the lowest dose, 103 [72%] of 144). At day 360, 66% (95% CI 56Â·47â€“74Â·33) of participants in group 2 and 71% (62Â·13â€“78Â·95) of participants in group 4 selected for long-term follow-up maintained a two times or greater increase in serotype-specific antibody compared with baseline. Interpretation: EXPEC4V seemed well tolerated and elicited robust and functional antibody responses across all serotypes, doses, and age groups. For the two dosages evaluated (4:4:4:8 Î¼g and 8:8:8:16 Î¼g), the immune response persisted for 1 year. Funding: Janssen Pharmaceuticals.","Safety and immunogenicity of a vaccine for extra-intestinal pathogenic Escherichia coli (ESTELLA): a phase 2 randomised controlled trial. Background: ExPEC4V (JNJ-63871860) is a bioconjugate vaccine, containing O-antigens from Escherichia coli serotypes O1A, O2, O6A, and O25B, developed for the prevention of invasive extra-intestinal pathogenic E coli (ExPEC) disease. We aimed to assess safety, reactogenicity, and immunogenicity of ExPEC4V in healthy adults. Methods: In this phase 2 randomised, double-blind placebo-controlled study, we recruited healthy adults (â‰¥18 years with a body-mass index of 35 kg/m2 or less) between Nov 16, 2015, and Aug 8, 2017, and randomly assigned them to receive a single dose of ExPEC4V (antigen O1A:O2:O6A:O25B content 4:4:4:4 Î¼g [group 1]; 4:4:4:8 Î¼g [group 2], 8:8:8:8 Î¼g [group 3], 8:8:8:16 Î¼g [group 4], or 16:16:16:16 Î¼g [group 5]) or placebo. The primary objectives were evaluation of the safety, tolerability, and immunogenicity of ExPEC4V and determination of its dose-dependent immunogenicity 15 days after vaccination by ELISA in individuals who had received at least one vaccination dose. Antibody titres and safety evaluation were used to select two ExPEC4V doses for assessment up to day 360. This trial is registered at ClinicalTrials.gov, number NCT02546960. Findings: Of 848 enrolled participants, 843 (99%) received the ExPEC4V vaccine (757) or placebo (86) and were included in the safety analysis. Of 757 participants vaccinated with ExPEC4V, 222 (29%) had a solicited local adverse event and 325 (43%) had any solicited systemic adverse event, compared with 11 (13%) and 30 (35%) of 86 participants in the control group. Symptoms were mild-to-moderate. The most frequently reported solicited local adverse event was pain or tenderness (205 [27Â·1%] of 757 in combined ExPEC4V groups) and the most frequently reported solicited systemic adverse event was fatigue (208 [27Â·6%] of 757). Only 13 (2%) of 843 had a grade 3 event. At day 15, 80% or more of all participants achieved a two times or greater increase in serotype-specific IgG antibodies (except O25B at the lowest dose, 103 [72%] of 144). At day 360, 66% (95% CI 56Â·47â€“74Â·33) of participants in group 2 and 71% (62Â·13â€“78Â·95) of participants in group 4 selected for long-term follow-up maintained a two times or greater increase in serotype-specific antibody compared with baseline. Interpretation: EXPEC4V seemed well tolerated and elicited robust and functional antibody responses across all serotypes, doses, and age groups. For the two dosages evaluated (4:4:4:8 Î¼g and 8:8:8:16 Î¼g), the immune response persisted for 1 year. Funding: Janssen Pharmaceuticals."
0,SarcTrack,"RATIONALE: Human induced pluripotent stem cell-derived cardiomyocytes (hiPSC-CMs) in combination with CRISPR/Cas9 genome editing provide unparalleled opportunities to study cardiac biology and disease. However, sarcomeres, the fundamental units of myocyte contraction, are immature and nonlinear in hiPSC-CMs, which technically challenge accurate functional interrogation of contractile parameters in beating cells. Furthermore, existing analysis methods are relatively low-throughput, indirectly assess contractility, or only assess well-aligned sarcomeres found in mature cardiac tissues. OBJECTIVE: We aimed to develop an analysis platform that directly, rapidly, and automatically tracks sarcomeres in beating cardiomyocytes. The platform should assess sarcomere content, contraction and relaxation parameters, and beat rate. METHODS AND RESULTS: We developed SarcTrack, a MatLab software that monitors fluorescently tagged sarcomeres in hiPSC-CMs. The algorithm determines sarcomere content, sarcomere length, and returns rates of sarcomere contraction and relaxation. By rapid measurement of hundreds of sarcomeres in each hiPSC-CM, SarcTrack provides large data sets for robust statistical analyses of multiple contractile parameters. We validated SarcTrack by analyzing drug-treated hiPSC-CMs, confirming the contractility effects of compounds that directly activate (CK-1827452) or inhibit (MYK-461) myosin molecules or indirectly alter contractility (verapamil and propranolol). SarcTrack analysis of hiPSC-CMs carrying a heterozygous truncation variant in the myosin-binding protein C ( MYBPC3) gene, which causes hypertrophic cardiomyopathy, recapitulated seminal disease phenotypes including cardiac hypercontractility and diminished relaxation, abnormalities that normalized with MYK-461 treatment. CONCLUSIONS: SarcTrack provides a direct and efficient method to quantitatively assess sarcomere function. By improving existing contractility analysis methods and overcoming technical challenges associated with functional evaluation of hiPSC-CMs, SarcTrack enhances translational prospects for sarcomere-regulating therapeutics and accelerates interrogation of human cardiac genetic variants.","SarcTrack. RATIONALE: Human induced pluripotent stem cell-derived cardiomyocytes (hiPSC-CMs) in combination with CRISPR/Cas9 genome editing provide unparalleled opportunities to study cardiac biology and disease. However, sarcomeres, the fundamental units of myocyte contraction, are immature and nonlinear in hiPSC-CMs, which technically challenge accurate functional interrogation of contractile parameters in beating cells. Furthermore, existing analysis methods are relatively low-throughput, indirectly assess contractility, or only assess well-aligned sarcomeres found in mature cardiac tissues. OBJECTIVE: We aimed to develop an analysis platform that directly, rapidly, and automatically tracks sarcomeres in beating cardiomyocytes. The platform should assess sarcomere content, contraction and relaxation parameters, and beat rate. METHODS AND RESULTS: We developed SarcTrack, a MatLab software that monitors fluorescently tagged sarcomeres in hiPSC-CMs. The algorithm determines sarcomere content, sarcomere length, and returns rates of sarcomere contraction and relaxation. By rapid measurement of hundreds of sarcomeres in each hiPSC-CM, SarcTrack provides large data sets for robust statistical analyses of multiple contractile parameters. We validated SarcTrack by analyzing drug-treated hiPSC-CMs, confirming the contractility effects of compounds that directly activate (CK-1827452) or inhibit (MYK-461) myosin molecules or indirectly alter contractility (verapamil and propranolol). SarcTrack analysis of hiPSC-CMs carrying a heterozygous truncation variant in the myosin-binding protein C ( MYBPC3) gene, which causes hypertrophic cardiomyopathy, recapitulated seminal disease phenotypes including cardiac hypercontractility and diminished relaxation, abnormalities that normalized with MYK-461 treatment. CONCLUSIONS: SarcTrack provides a direct and efficient method to quantitatively assess sarcomere function. By improving existing contractility analysis methods and overcoming technical challenges associated with functional evaluation of hiPSC-CMs, SarcTrack enhances translational prospects for sarcomere-regulating therapeutics and accelerates interrogation of human cardiac genetic variants."
0,Evaluating Machine Learning Articles,,
0,Convolutional Neural Networks for Radiologic Images: A Radiologist's Guide,"Deep learning has rapidly advanced in various fields within the past few years and has recently gained particular attention in the radiology community. This article provides an introduction to deep learning technology and presents the stages that are entailed in the design process of deep learning radiology research. In addition, the article details the results of a survey of the application of deep learning-specifically, the application of convolutional neural networks-to radiologic imaging that was focused on the following five major system organs: chest, breast, brain, musculoskeletal system, and abdomen and pelvis. The survey of the studies is followed by a discussion about current challenges and future trends and their potential implications for radiology. This article may be used as a guide for radiologists planning research in the field of radiologic image analysis using convolutional neural networks.","Convolutional Neural Networks for Radiologic Images: A Radiologist's Guide. Deep learning has rapidly advanced in various fields within the past few years and has recently gained particular attention in the radiology community. This article provides an introduction to deep learning technology and presents the stages that are entailed in the design process of deep learning radiology research. In addition, the article details the results of a survey of the application of deep learning-specifically, the application of convolutional neural networks-to radiologic imaging that was focused on the following five major system organs: chest, breast, brain, musculoskeletal system, and abdomen and pelvis. The survey of the studies is followed by a discussion about current challenges and future trends and their potential implications for radiology. This article may be used as a guide for radiologists planning research in the field of radiologic image analysis using convolutional neural networks."
0,Transmembrane 4 L Six Family Member 5 Senses Arginine for mTORC1 Signaling,"The mechanistic target of rapamycin complex (mTORC1) is a signaling hub on the lysosome surface, responding to lysosomal amino acids. Although arginine is metabolically important, the physiological arginine sensor that activates mTOR remains unclear. Here, we show that transmembrane 4 L six family member 5 (TM4SF5) translocates from plasma membrane to lysosome upon arginine sufficiency and senses arginine, culminating in mTORC1/S6K1 activation. TM4SF5 bound active mTOR upon arginine sufficiency and constitutively bound amino acid transporter SLC38A9. TM4SF5 binding to the cytosolic arginine sensor Castor1 decreased upon arginine sufficiency, thus allowing TM4SF5-mediated sensing of metabolic amino acids. TM4SF5 directly bound free L-arginine via its extracellular loop possibly for the efflux, being supported by mutant study and homology and molecular docking modeling. Therefore, we propose that lysosomal TM4SF5 senses and enables arginine efflux for mTORC1/S6K1 activation, and arginine-auxotroph in hepatocellular carcinoma may be targeted by blocking the arginine sensing using anti-TM4SF5 reagents. Lysosomal arginine is involved in mTORC1/S6K1 activation for cell growth. Jung et al. identify TM4SF5 as a membrane-based sensor of physiologic levels of arginine. TM4SF5 forms a complex with mTOR and the amino acid transporter SLC38A9 on lysosomal membranes in an arginine-regulated manner, leading to arginine efflux and mTOR/S6K1 activation.","Transmembrane 4 L Six Family Member 5 Senses Arginine for mTORC1 Signaling. The mechanistic target of rapamycin complex (mTORC1) is a signaling hub on the lysosome surface, responding to lysosomal amino acids. Although arginine is metabolically important, the physiological arginine sensor that activates mTOR remains unclear. Here, we show that transmembrane 4 L six family member 5 (TM4SF5) translocates from plasma membrane to lysosome upon arginine sufficiency and senses arginine, culminating in mTORC1/S6K1 activation. TM4SF5 bound active mTOR upon arginine sufficiency and constitutively bound amino acid transporter SLC38A9. TM4SF5 binding to the cytosolic arginine sensor Castor1 decreased upon arginine sufficiency, thus allowing TM4SF5-mediated sensing of metabolic amino acids. TM4SF5 directly bound free L-arginine via its extracellular loop possibly for the efflux, being supported by mutant study and homology and molecular docking modeling. Therefore, we propose that lysosomal TM4SF5 senses and enables arginine efflux for mTORC1/S6K1 activation, and arginine-auxotroph in hepatocellular carcinoma may be targeted by blocking the arginine sensing using anti-TM4SF5 reagents. Lysosomal arginine is involved in mTORC1/S6K1 activation for cell growth. Jung et al. identify TM4SF5 as a membrane-based sensor of physiologic levels of arginine. TM4SF5 forms a complex with mTOR and the amino acid transporter SLC38A9 on lysosomal membranes in an arginine-regulated manner, leading to arginine efflux and mTOR/S6K1 activation."
0,Integrated network analysis and machine learning approach for the identification of key genes of triple-negative breast cancer,"Triple-negative breast cancer (TNBC) has attracted more attention compared with other breast cancer subtypes due to its aggressive nature, poor prognosis, and chemotherapy remains the mainstay of treatment with no other approved targeted therapy. Therefore, the study aimed to discover more promising therapeutic targets and investigating new insights of biological mechanism of TNBC. Six microarray data sets consisting of 463 non-TNBC and 405 TNBC samples were mined from Gene Expression Omnibus. The data sets were integrated by meta-analysis and identified 1075 differentially expressed genes. Protein-protein interaction network was constructed which consists of 486 nodes and 1932 edges, where 29 hub genes were obtained with high topological measures. Further, 16 features (hub genes), 12 upregulated (AURKB, CCNB2, CDC20, DDX18, EGFR, ENO1, MYC, NUP88, PLK1, PML, POLR2F, and SKP2) and four downregulated (CCND1, GLI3, SKP1, and TGFB3) were selected through machine learning correlation based feature selection method on training data set. A naÃ¯ve Bayes based classifier built using the expression profiles of 16 features (hub genes) accurately and reliably classify TNBC from non-TNBC samples in the validation test data set with a receiver operating curve of 0.93 to 0.98. Subsequently, Gene Ontology analysis revealed that the hub genes were enriched in mitotic cell cycle processes and Kyoto Encyclopedia of Genes and Genomes pathway analysis showed that they were enriched in cell cycle pathways. Thus, the identified key hub genes and pathways highlighted in the study would enhance the understanding of molecular mechanism of TNBC which may serve as potential therapeutic target.","Integrated network analysis and machine learning approach for the identification of key genes of triple-negative breast cancer. Triple-negative breast cancer (TNBC) has attracted more attention compared with other breast cancer subtypes due to its aggressive nature, poor prognosis, and chemotherapy remains the mainstay of treatment with no other approved targeted therapy. Therefore, the study aimed to discover more promising therapeutic targets and investigating new insights of biological mechanism of TNBC. Six microarray data sets consisting of 463 non-TNBC and 405 TNBC samples were mined from Gene Expression Omnibus. The data sets were integrated by meta-analysis and identified 1075 differentially expressed genes. Protein-protein interaction network was constructed which consists of 486 nodes and 1932 edges, where 29 hub genes were obtained with high topological measures. Further, 16 features (hub genes), 12 upregulated (AURKB, CCNB2, CDC20, DDX18, EGFR, ENO1, MYC, NUP88, PLK1, PML, POLR2F, and SKP2) and four downregulated (CCND1, GLI3, SKP1, and TGFB3) were selected through machine learning correlation based feature selection method on training data set. A naÃ¯ve Bayes based classifier built using the expression profiles of 16 features (hub genes) accurately and reliably classify TNBC from non-TNBC samples in the validation test data set with a receiver operating curve of 0.93 to 0.98. Subsequently, Gene Ontology analysis revealed that the hub genes were enriched in mitotic cell cycle processes and Kyoto Encyclopedia of Genes and Genomes pathway analysis showed that they were enriched in cell cycle pathways. Thus, the identified key hub genes and pathways highlighted in the study would enhance the understanding of molecular mechanism of TNBC which may serve as potential therapeutic target."
0,Development and validation of GMI signature based random survival forest prognosis model to predict clinical outcome in acute myeloid leukemia,"Background: Acute myeloid leukemia (AML) is a disease with marked molecular heterogeneity and a high early death rate. Our aim was to investigate an integrated Gene expression, Mirna and miRNA-mRNA Interactions (GMI) signature for improving risk stratification of AML. Methods: We identified differentially expressed genes by pooling a large number of 861 human AML patients and 75 normal cases. We then used miRWalk to identify the functional miRNA-mRNA regulatory module. The GMI signature based random survival forest (RSF) prognosis model was developed from training data set and evaluated in independent patient cohorts from The Cancer Genome Atlas (TCGA) dataset (N = 147). Univariate and multivariate Cox proportional hazards regression analyses were applied to evaluate the prognostic value of GMI signature. Results: We identified 139 differentially expressed genes between normal and abnormal AML samples. We discovered the functional miRNA-mRNA regulatory module which participate in the network of cancer progression. We named 23 differentially expressed genes and 16 validated target miRNAs as the GMI signature. The RSF model-based scores separated independent patient cohorts into two groups with significantly different overall survival (C-index = 0.59, hazard ratio [HR], 2.12; 95% confidence interval [CI], 1.11-4.03; p = 0.019). Similar results were obtained with reversed training and testing datasets (C-index = 0.58, hazard ratio [HR], 2.08; 95% confidence interval [CI], 1.02-4.24; p = 0.038). The GMI signature score contributed more information about recurrence than standard clinical covariates. Conclusion: The GMI signature based RSF prognosis model not only reflects regulatory relationships from identified miRNA-mRNA module but also informs patient prognosis. While in the TCGA data set the GMI signature score contributed additional information about recurrence in comparison to standard clinical covariates, further studies are needed to determine its clinical significance.","Development and validation of GMI signature based random survival forest prognosis model to predict clinical outcome in acute myeloid leukemia. Background: Acute myeloid leukemia (AML) is a disease with marked molecular heterogeneity and a high early death rate. Our aim was to investigate an integrated Gene expression, Mirna and miRNA-mRNA Interactions (GMI) signature for improving risk stratification of AML. Methods: We identified differentially expressed genes by pooling a large number of 861 human AML patients and 75 normal cases. We then used miRWalk to identify the functional miRNA-mRNA regulatory module. The GMI signature based random survival forest (RSF) prognosis model was developed from training data set and evaluated in independent patient cohorts from The Cancer Genome Atlas (TCGA) dataset (N = 147). Univariate and multivariate Cox proportional hazards regression analyses were applied to evaluate the prognostic value of GMI signature. Results: We identified 139 differentially expressed genes between normal and abnormal AML samples. We discovered the functional miRNA-mRNA regulatory module which participate in the network of cancer progression. We named 23 differentially expressed genes and 16 validated target miRNAs as the GMI signature. The RSF model-based scores separated independent patient cohorts into two groups with significantly different overall survival (C-index = 0.59, hazard ratio [HR], 2.12; 95% confidence interval [CI], 1.11-4.03; p = 0.019). Similar results were obtained with reversed training and testing datasets (C-index = 0.58, hazard ratio [HR], 2.08; 95% confidence interval [CI], 1.02-4.24; p = 0.038). The GMI signature score contributed more information about recurrence than standard clinical covariates. Conclusion: The GMI signature based RSF prognosis model not only reflects regulatory relationships from identified miRNA-mRNA module but also informs patient prognosis. While in the TCGA data set the GMI signature score contributed additional information about recurrence in comparison to standard clinical covariates, further studies are needed to determine its clinical significance."
0,Risk stratification for stroke in atrial fibrillation: a critique,,
0,Tau Positron-Emission Tomography in Former National Football League Players,,
0,Elementary motion sequence detectors in whisker somatosensory cortex,"How the somatosensory cortex (S1) encodes complex patterns of touch, such as those that occur during tactile exploration, is poorly understood. In the mouse whisker S1, temporally dense stimulation of local whisker pairs revealed that most neurons are not classical single-whisker feature detectors, but instead are strongly tuned to two-whisker sequences that involve the columnar whisker (CW) and one specific surround whisker (SW), usually in a SW-leading-CW order. Tuning was spatiotemporally precise and diverse across cells, generating a rate code for local motion vectors defined by SWâ€“CW combinations. Spatially asymmetric, sublinear suppression for suboptimal combinations and near-linearity for preferred combinations sharpened combination tuning relative to linearly predicted tuning. This resembles computation of motion direction selectivity in vision. SW-tuned neurons, misplaced in the classical whisker map, had the strongest combination tuning. Thus, each S1 column contains a rate code for local motion sequences involving the CW, thus providing a basis for higher-order feature extraction.","Elementary motion sequence detectors in whisker somatosensory cortex. How the somatosensory cortex (S1) encodes complex patterns of touch, such as those that occur during tactile exploration, is poorly understood. In the mouse whisker S1, temporally dense stimulation of local whisker pairs revealed that most neurons are not classical single-whisker feature detectors, but instead are strongly tuned to two-whisker sequences that involve the columnar whisker (CW) and one specific surround whisker (SW), usually in a SW-leading-CW order. Tuning was spatiotemporally precise and diverse across cells, generating a rate code for local motion vectors defined by SWâ€“CW combinations. Spatially asymmetric, sublinear suppression for suboptimal combinations and near-linearity for preferred combinations sharpened combination tuning relative to linearly predicted tuning. This resembles computation of motion direction selectivity in vision. SW-tuned neurons, misplaced in the classical whisker map, had the strongest combination tuning. Thus, each S1 column contains a rate code for local motion sequences involving the CW, thus providing a basis for higher-order feature extraction."
0,Application of a multi-material artifact reduction algorithm in a wide-detector CT in the evaluation of the portal venous angiography of postoperative TIPS and embolization,"Objective: To assess the effect of monochromatic images and metal artifact reduction (MAR) on the image quality of spectral CT portal venous angiography in patients with operation of after the performing transjugular intrahepatic portosystemic stent shunt(TIPS) and embolization. Methods: From December 2017 to April 2018, the examination data of 28 patients with portal hypertension due to cirrhosis who underwent portal vein angiography 1 month after TIPS and embolization were prospectively collected. After spectral CT scanning in revolution CT, the monochromatic energy levels(60 keV, 120 keV), 60 keV + 120 keV, 120kV-like + 120 keV fused images combined with MAR algorithm were reconstructed. Quantitative parameters such as image artifact index (AI) and qualitative visual evaluation scores were recorded and compared. Results: The 120 keV monochromatic images showed the lowest AI value(30.8Â±8.5, 18.2Â±4.3) and highest metal artifacts reduction effect. The 60 keV monochromatic images showed the highest AI value (57.3Â±15.7, 32.1Â±7.9) and the lowest metal artifacts reduction effect. The AI value of 60 keV + 120 keV fused images was lower than that of 60 keV images(26.2%, 24.7%). The difference of AI value between each group was statistically significant(all P<0.05). The interobserver agreement in the subjective image scores was moderate with kappa value of 0.824. The overall image quality score of 60 keV + 120 keV fused image and the noise score of 120 kV-like+120 keV were higher than the remaining groups. The differences of the subjective scores among each group were statistically significant(all P<0.05). Conclusion: The spectral CT with MAR algorithm can effectively improve the image quality of portal vein angiography after the TIPS and embolization therapy and the 60 keV + 120 keV fused images can eliminate artifacts and ensure a clear display of blood vessels.","Application of a multi-material artifact reduction algorithm in a wide-detector CT in the evaluation of the portal venous angiography of postoperative TIPS and embolization. Objective: To assess the effect of monochromatic images and metal artifact reduction (MAR) on the image quality of spectral CT portal venous angiography in patients with operation of after the performing transjugular intrahepatic portosystemic stent shunt(TIPS) and embolization. Methods: From December 2017 to April 2018, the examination data of 28 patients with portal hypertension due to cirrhosis who underwent portal vein angiography 1 month after TIPS and embolization were prospectively collected. After spectral CT scanning in revolution CT, the monochromatic energy levels(60 keV, 120 keV), 60 keV + 120 keV, 120kV-like + 120 keV fused images combined with MAR algorithm were reconstructed. Quantitative parameters such as image artifact index (AI) and qualitative visual evaluation scores were recorded and compared. Results: The 120 keV monochromatic images showed the lowest AI value(30.8Â±8.5, 18.2Â±4.3) and highest metal artifacts reduction effect. The 60 keV monochromatic images showed the highest AI value (57.3Â±15.7, 32.1Â±7.9) and the lowest metal artifacts reduction effect. The AI value of 60 keV + 120 keV fused images was lower than that of 60 keV images(26.2%, 24.7%). The difference of AI value between each group was statistically significant(all P<0.05). The interobserver agreement in the subjective image scores was moderate with kappa value of 0.824. The overall image quality score of 60 keV + 120 keV fused image and the noise score of 120 kV-like+120 keV were higher than the remaining groups. The differences of the subjective scores among each group were statistically significant(all P<0.05). Conclusion: The spectral CT with MAR algorithm can effectively improve the image quality of portal vein angiography after the TIPS and embolization therapy and the 60 keV + 120 keV fused images can eliminate artifacts and ensure a clear display of blood vessels."
0,Gait impairments in Parkinson's disease,,
0,Segmentation and classification in MRI and US fetal imaging: Recent trends and future prospects,"Fetal imaging is a burgeoning topic. New advancements in both magnetic resonance imaging and (3D) ultrasound currently allow doctors to diagnose fetal structural abnormalities such as those involved in twin-to-twin transfusion syndrome, gestational diabetes mellitus, pulmonary sequestration and hypoplasia, congenital heart disease, diaphragmatic hernia, ventriculomegaly, etc. Considering the continued breakthroughs in utero image analysis and (3D) reconstruction models, it is now possible to gain more insight into the ongoing development of the fetus. Best prenatal diagnosis performances rely on the conscious preparation of the clinicians in terms of fetal anatomy knowledge. Therefore, fetal imaging will likely span and increase its prevalence in the forthcoming years. This review covers state-of-the-art segmentation and classification methodologies for the whole fetus and, more specifically, the fetal brain, lungs, liver, heart and placenta in magnetic resonance imaging and (3D) ultrasound for the first time. Potential applications of the aforementioned methods into clinical settings are also inspected. Finally, improvements in existing approaches as well as most promising avenues to new areas of research are briefly outlined.","Segmentation and classification in MRI and US fetal imaging: Recent trends and future prospects. Fetal imaging is a burgeoning topic. New advancements in both magnetic resonance imaging and (3D) ultrasound currently allow doctors to diagnose fetal structural abnormalities such as those involved in twin-to-twin transfusion syndrome, gestational diabetes mellitus, pulmonary sequestration and hypoplasia, congenital heart disease, diaphragmatic hernia, ventriculomegaly, etc. Considering the continued breakthroughs in utero image analysis and (3D) reconstruction models, it is now possible to gain more insight into the ongoing development of the fetus. Best prenatal diagnosis performances rely on the conscious preparation of the clinicians in terms of fetal anatomy knowledge. Therefore, fetal imaging will likely span and increase its prevalence in the forthcoming years. This review covers state-of-the-art segmentation and classification methodologies for the whole fetus and, more specifically, the fetal brain, lungs, liver, heart and placenta in magnetic resonance imaging and (3D) ultrasound for the first time. Potential applications of the aforementioned methods into clinical settings are also inspected. Finally, improvements in existing approaches as well as most promising avenues to new areas of research are briefly outlined."
0,Systematic profiling identifies PDLIM2 as a novel prognostic predictor for oesophageal squamous cell carcinoma (ESCC),"Till now, no appropriate biomarkers for high-risk population screening and prognosis prediction have been identified for patients with oesophageal squamous cell carcinoma (ESCC). In this study, by the combined use of data from the Gene Expression Omnibus (GEO) datasets and The Cancer Genome Atlas (TCGA)-oesophageal carcinoma (ESCA), we aimed to screen dysregulated genes with prognostic value in ESCC and the genetic and epigenetic alterations underlying the dysregulation. About 222 genes that had at least fourfold change in ESCC compared with adjacent normal tissues were identified using the microarray data in GDS3838. Among these genes, only PDLIM2 was associated with nodal invasion and overall survival (OS) at the same time. The high PDLIM2 expression group had significantly longer OS and its expression was independently associated with better OS (HR: 0.64, 95% CI: 0.43-0.95, PÂ =Â 0.03), after adjustment for gender and pathologic stages. The expression of its exon 7/8/9/10 had the highest AUC value (0.724) and better prognostic value (HR: 0.43, 95% CI: 0.22-0.83, PÂ =Â 0.01) than total PDLIM2 expression. PDLIM2 DNA copy deletion was common in ESCC and was associated with decreased gene expression. The methylation status of two CpG sites (cg23696886 and cg20449614) in the proximal promoter region of PDLIM2 showed a moderate negative correlation with the gene expression in PDLIM2 copy neutral/amplification group. In conclusion, we infer that PDLIM2 expression might be a novel prognostic indicator for ESCC patients. Its exon 7/8/9/10 expression had the best prognostic value. Its down-regulation might be associated with gene-level copy deletion and promoter hypermethylation.","Systematic profiling identifies PDLIM2 as a novel prognostic predictor for oesophageal squamous cell carcinoma (ESCC). Till now, no appropriate biomarkers for high-risk population screening and prognosis prediction have been identified for patients with oesophageal squamous cell carcinoma (ESCC). In this study, by the combined use of data from the Gene Expression Omnibus (GEO) datasets and The Cancer Genome Atlas (TCGA)-oesophageal carcinoma (ESCA), we aimed to screen dysregulated genes with prognostic value in ESCC and the genetic and epigenetic alterations underlying the dysregulation. About 222 genes that had at least fourfold change in ESCC compared with adjacent normal tissues were identified using the microarray data in GDS3838. Among these genes, only PDLIM2 was associated with nodal invasion and overall survival (OS) at the same time. The high PDLIM2 expression group had significantly longer OS and its expression was independently associated with better OS (HR: 0.64, 95% CI: 0.43-0.95, PÂ =Â 0.03), after adjustment for gender and pathologic stages. The expression of its exon 7/8/9/10 had the highest AUC value (0.724) and better prognostic value (HR: 0.43, 95% CI: 0.22-0.83, PÂ =Â 0.01) than total PDLIM2 expression. PDLIM2 DNA copy deletion was common in ESCC and was associated with decreased gene expression. The methylation status of two CpG sites (cg23696886 and cg20449614) in the proximal promoter region of PDLIM2 showed a moderate negative correlation with the gene expression in PDLIM2 copy neutral/amplification group. In conclusion, we infer that PDLIM2 expression might be a novel prognostic indicator for ESCC patients. Its exon 7/8/9/10 expression had the best prognostic value. Its down-regulation might be associated with gene-level copy deletion and promoter hypermethylation."
0,Computational and artificial neural network based study of functional SNPs of human LEPR protein associated with reproductive function,"Genetic polymorphisms are mostly associated with inherited diseases, detecting and analyzing the biological significance of functional single-nucleotide polymorphisms (SNPs) using wet laboratory experiments is an arduous task hence the computational analysis of putative SNPs is essential before conducting a study on a large population. SNP in the leptin receptor (LEPR) could result in the retention of intracellular signalling due to the structural and functional instability of the receptor causing abnormal reproductive function in human. In this first comprehensive computational analysis of LEPR gene mutation, we have identified and analyzed the functional consequence and structural significance of the SNPs in LEPR using recently developed several computational algorithms. Thirteen deleterious mutations such as W13C, S93G, I232R, Q307H, Y354C, E497A, Q571H, R612H, K656N, T690A, T699M V741M, and L760R were identified in the LEPR gene coding region. Backpropagation algorithm has been developed to forestall the deleterious nature of SNP and to validate the outcome of the tested computational tools. From ConSurf prediction three SNPs (Q571H, R612H, and T699M) were highly conserved on LEPR protein and the most deleterious variant R612H had one hydrogen bond abolished and severely reduced protein stability. Molecular docking suggested that the mutant (R612H) LEPR had lowest binding energy than native LEPR with the ligand molecule. Thus the energetically destructive changeover of ARG to HIS in R612H could possibly affect the LEPR protein structural stability and functional constancy due to interruption in the amino acid interactions and could result in reproductive disorders in human and increases the complication in obstetric and pregnancy outcome.","Computational and artificial neural network based study of functional SNPs of human LEPR protein associated with reproductive function. Genetic polymorphisms are mostly associated with inherited diseases, detecting and analyzing the biological significance of functional single-nucleotide polymorphisms (SNPs) using wet laboratory experiments is an arduous task hence the computational analysis of putative SNPs is essential before conducting a study on a large population. SNP in the leptin receptor (LEPR) could result in the retention of intracellular signalling due to the structural and functional instability of the receptor causing abnormal reproductive function in human. In this first comprehensive computational analysis of LEPR gene mutation, we have identified and analyzed the functional consequence and structural significance of the SNPs in LEPR using recently developed several computational algorithms. Thirteen deleterious mutations such as W13C, S93G, I232R, Q307H, Y354C, E497A, Q571H, R612H, K656N, T690A, T699M V741M, and L760R were identified in the LEPR gene coding region. Backpropagation algorithm has been developed to forestall the deleterious nature of SNP and to validate the outcome of the tested computational tools. From ConSurf prediction three SNPs (Q571H, R612H, and T699M) were highly conserved on LEPR protein and the most deleterious variant R612H had one hydrogen bond abolished and severely reduced protein stability. Molecular docking suggested that the mutant (R612H) LEPR had lowest binding energy than native LEPR with the ligand molecule. Thus the energetically destructive changeover of ARG to HIS in R612H could possibly affect the LEPR protein structural stability and functional constancy due to interruption in the amino acid interactions and could result in reproductive disorders in human and increases the complication in obstetric and pregnancy outcome."
0,Challenges related to artificial intelligence research in medical imaging and the importance of image analysis competitions,"In recent years, there has been enormous interest in applying artificial intelligence (AI) to radiology. Although some of this interest may have been driven by exaggerated expectations that the technology can outperform radiologists in some tasks, there is a growing body of evidence that illustrates its limitations in medical imaging. The true potential of the technique probably lies somewhere in the middle, and AI will ultimately play a key role in medical imaging in the future. The limitless power of computers makes AI an ideal candidate to provide the standardization, consistency, and dependability needed to support radiologists in their mission to provide excellent patient care. However, important roadblocks currently limit the expansion of this field in medical imaging. This article reviews some of the challenges and potential solutions to advance the field forward, with focus on the experience gained by hosting image-based competitions.","Challenges related to artificial intelligence research in medical imaging and the importance of image analysis competitions. In recent years, there has been enormous interest in applying artificial intelligence (AI) to radiology. Although some of this interest may have been driven by exaggerated expectations that the technology can outperform radiologists in some tasks, there is a growing body of evidence that illustrates its limitations in medical imaging. The true potential of the technique probably lies somewhere in the middle, and AI will ultimately play a key role in medical imaging in the future. The limitless power of computers makes AI an ideal candidate to provide the standardization, consistency, and dependability needed to support radiologists in their mission to provide excellent patient care. However, important roadblocks currently limit the expansion of this field in medical imaging. This article reviews some of the challenges and potential solutions to advance the field forward, with focus on the experience gained by hosting image-based competitions."
0,Mechanism of imipenem resistance in metallo-Î²-lactamases expressing pathogenic bacterial spp. and identification of potential inhibitors: An in silico approach,"The World Health Organization reports that millions of people around the world are infected with antibiotic-resistant bacteria. Such resistance is more common in Pseudomonas aeruginosa, Acinetobacter baumannii, and Klebsiella pneumoniae strains because of the expression of the metallo-Î²-lactamases (MBLs) namely Imipenemase (IMP)-1, IMP-2, New Delhi metallo-Î²-lactamases-, Verona imipenemase (VIM)-4, VIM-5, and VIM-7. We did an in silico analysis to understand the resistance mechanism of imipenem at the structural level. Our modeling studies reveal that the VIM-4-imipenem complex has highest binding energy and forms a stable complex as indicated by a consensus score (C-score) value of 5.44. The intense interaction between the substrate and the Î²-lactamases leads to the increased hydrolysis of the substrate resulting in rapid hydrolysis of the antibiotic imipenem by VIM-4. Virtual screening of compounds from the ZINC database targeting VIM-4 was done, and we found compound ZINC44608383 as the high binding energy compound with the C-score value of 5.58. This compound could be exploited for inhibitor design and development. The current study helps us to understand the resistance mechanism of imipenem in MBL-expressing strains. Also, we have identified a probable inhibitor for VIM-4. We believe that our results will be useful for researchers in designing potent inhibitors for VIM-4.","Mechanism of imipenem resistance in metallo-Î²-lactamases expressing pathogenic bacterial spp. and identification of potential inhibitors: An in silico approach. The World Health Organization reports that millions of people around the world are infected with antibiotic-resistant bacteria. Such resistance is more common in Pseudomonas aeruginosa, Acinetobacter baumannii, and Klebsiella pneumoniae strains because of the expression of the metallo-Î²-lactamases (MBLs) namely Imipenemase (IMP)-1, IMP-2, New Delhi metallo-Î²-lactamases-, Verona imipenemase (VIM)-4, VIM-5, and VIM-7. We did an in silico analysis to understand the resistance mechanism of imipenem at the structural level. Our modeling studies reveal that the VIM-4-imipenem complex has highest binding energy and forms a stable complex as indicated by a consensus score (C-score) value of 5.44. The intense interaction between the substrate and the Î²-lactamases leads to the increased hydrolysis of the substrate resulting in rapid hydrolysis of the antibiotic imipenem by VIM-4. Virtual screening of compounds from the ZINC database targeting VIM-4 was done, and we found compound ZINC44608383 as the high binding energy compound with the C-score value of 5.58. This compound could be exploited for inhibitor design and development. The current study helps us to understand the resistance mechanism of imipenem in MBL-expressing strains. Also, we have identified a probable inhibitor for VIM-4. We believe that our results will be useful for researchers in designing potent inhibitors for VIM-4."
0,Gene expression profiling in blood from cerebral malaria patients and mild malaria patients living in Senegal,"Background: Plasmodium falciparum malaria remains a major health problem in Africa. The mechanisms of pathogenesis are not fully understood. Transcriptomic studies may provide new insights into molecular pathways involved in the severe form of the disease. Methods: Blood transcriptional levels were assessed in patients with cerebral malaria, non-cerebral malaria, or mild malaria by using microarray technology to look for gene expression profiles associated with clinical status. Multi-way ANOVA was used to extract differentially expressed genes. Network and pathways analyses were used to detect enrichment for biological pathways. Results: We identified a set of 443 genes that were differentially expressed in the three patient groups after applying a false discovery rate of 10%. Since the cerebral patients displayed a particular transcriptional pattern, we focused our analysis on the differences between cerebral malaria patients and mild malaria patients. We further found 842 differentially expressed genes after applying a false discovery rate of 10%. Unsupervised hierarchical clustering of cerebral malaria-informative genes led to clustering of the cerebral malaria patients. The support vector machine method allowed us to correctly classify five out of six cerebral malaria patients and six of six mild malaria patients. Furthermore, the products of the differentially expressed genes were mapped onto a human protein-protein network. This led to the identification of the proteins with the highest number of interactions, including GSK3B, RELA, and APP. The enrichment analysis of the gene functional annotation indicates that genes involved in immune signalling pathways play a role in the occurrence of cerebral malaria. These include BCR-, TCR-, TLR-, cytokine-, FcÎµRI-, and FCGR-signalling pathways and natural killer cell cytotoxicity pathways, which are involved in the activation of immune cells. In addition, our results revealed an enrichment of genes involved in Alzheimer's disease. Conclusions: In the present study, we examine a set of genes whose expression differed in cerebral malaria patients and mild malaria patients. Moreover, our results provide new insights into the potential effect of the dysregulation of gene expression in immune pathways. Host genetic variation may partly explain such alteration of gene expression. Further studies are required to investigate this in African populations.","Gene expression profiling in blood from cerebral malaria patients and mild malaria patients living in Senegal. Background: Plasmodium falciparum malaria remains a major health problem in Africa. The mechanisms of pathogenesis are not fully understood. Transcriptomic studies may provide new insights into molecular pathways involved in the severe form of the disease. Methods: Blood transcriptional levels were assessed in patients with cerebral malaria, non-cerebral malaria, or mild malaria by using microarray technology to look for gene expression profiles associated with clinical status. Multi-way ANOVA was used to extract differentially expressed genes. Network and pathways analyses were used to detect enrichment for biological pathways. Results: We identified a set of 443 genes that were differentially expressed in the three patient groups after applying a false discovery rate of 10%. Since the cerebral patients displayed a particular transcriptional pattern, we focused our analysis on the differences between cerebral malaria patients and mild malaria patients. We further found 842 differentially expressed genes after applying a false discovery rate of 10%. Unsupervised hierarchical clustering of cerebral malaria-informative genes led to clustering of the cerebral malaria patients. The support vector machine method allowed us to correctly classify five out of six cerebral malaria patients and six of six mild malaria patients. Furthermore, the products of the differentially expressed genes were mapped onto a human protein-protein network. This led to the identification of the proteins with the highest number of interactions, including GSK3B, RELA, and APP. The enrichment analysis of the gene functional annotation indicates that genes involved in immune signalling pathways play a role in the occurrence of cerebral malaria. These include BCR-, TCR-, TLR-, cytokine-, FcÎµRI-, and FCGR-signalling pathways and natural killer cell cytotoxicity pathways, which are involved in the activation of immune cells. In addition, our results revealed an enrichment of genes involved in Alzheimer's disease. Conclusions: In the present study, we examine a set of genes whose expression differed in cerebral malaria patients and mild malaria patients. Moreover, our results provide new insights into the potential effect of the dysregulation of gene expression in immune pathways. Host genetic variation may partly explain such alteration of gene expression. Further studies are required to investigate this in African populations."
0,"Assessment of Deep Generative Models for High-Resolution Synthetic Retinal Image Generation of Age-Related Macular Degeneration (vol 137, pg 258, 2019)",,
0,Structural and energetic understanding of novel natural inhibitors of Mycobacterium tuberculosis malate synthase,"Persistent infection by Mycobacterium tuberculosis requires the glyoxylate shunt. This is a bypass to the tricarboxylic acid cycle in which isocitrate lyase (ICL) and malate synthase (MS) catalyze the net incorporation of carbon during mycobacterial growth on acetate or fatty acids as the primary carbon source. To identify a potential antitubercular compound, we performed a structure-based screening of natural compounds from the ZINC database (n = 1 67 740) against the M tuberculosis MS (MtbMS) structure. The ligands were screened against MtbMS, and 354 ligands were found to have better docking score. These compounds were assessed for Lipinski and absorption, distribution, metabolism, excretion, and toxicity prediction where 15 compounds were found to fit well for redocking studies. After refinement by molecular docking and drug-likeness analysis, four potential inhibitors (ZINC1483899, ZINC1754310, ZINC2269664, and ZINC15729522) were identified. These four ligands with phenyl-diketo acid were further subjected to molecular dynamics simulation to compare the dynamics and stability of the protein structure after ligand binding. The binding energy analysis was calculated to determine the intermolecular interactions. Our results suggested that the four compounds had a binding free energy of âˆ’201.96, âˆ’242.02, âˆ’187.03, and âˆ’169.02 kJÂ·molâˆ’1, for compounds with IDs ZINC1483899, ZINC1754310, ZINC2269664, and ZINC15729522, respectively. We concluded that two compounds (ZINC1483899 and ZINC1754310) displayed considerable structural and pharmacological properties and could be probable drug candidates to fight against M tuberculosis parasites.","Structural and energetic understanding of novel natural inhibitors of Mycobacterium tuberculosis malate synthase. Persistent infection by Mycobacterium tuberculosis requires the glyoxylate shunt. This is a bypass to the tricarboxylic acid cycle in which isocitrate lyase (ICL) and malate synthase (MS) catalyze the net incorporation of carbon during mycobacterial growth on acetate or fatty acids as the primary carbon source. To identify a potential antitubercular compound, we performed a structure-based screening of natural compounds from the ZINC database (n = 1 67 740) against the M tuberculosis MS (MtbMS) structure. The ligands were screened against MtbMS, and 354 ligands were found to have better docking score. These compounds were assessed for Lipinski and absorption, distribution, metabolism, excretion, and toxicity prediction where 15 compounds were found to fit well for redocking studies. After refinement by molecular docking and drug-likeness analysis, four potential inhibitors (ZINC1483899, ZINC1754310, ZINC2269664, and ZINC15729522) were identified. These four ligands with phenyl-diketo acid were further subjected to molecular dynamics simulation to compare the dynamics and stability of the protein structure after ligand binding. The binding energy analysis was calculated to determine the intermolecular interactions. Our results suggested that the four compounds had a binding free energy of âˆ’201.96, âˆ’242.02, âˆ’187.03, and âˆ’169.02 kJÂ·molâˆ’1, for compounds with IDs ZINC1483899, ZINC1754310, ZINC2269664, and ZINC15729522, respectively. We concluded that two compounds (ZINC1483899 and ZINC1754310) displayed considerable structural and pharmacological properties and could be probable drug candidates to fight against M tuberculosis parasites."
0,Global lysine crotonylation and 2-hydroxyisobutyrylation in phenotypically different Toxoplasma gondii parasites,"Toxoplasma gondii is a unicellular protozoan parasite of the phylum Apicomplexa. The parasite repeatedly goes through a cycle of invasion, division and induction of host cell rupture, which is an obligatory process for proliferation inside warm-blooded animals. It is known that the biology of the parasite is controlled by a variety of mechanisms ranging from genomic to epigenetic to transcriptional regulation. In this study, we investigated the global protein posttranslational lysine crotonylation and 2-hy-droxyisobutyrylation of two T. gondii strains, RH and ME49, which represent distinct phenotypes for proliferation and pathogenicity in the host. Proteins with differential expression and modification patterns associated with parasite phenotypes were identified. Many proteins in T. gondii were crotonylated and 2-hydroxyisobutyrylated, and they were localized in diverse subcellular compartments involved in a wide variety of cellular functions such as motility, host invasion, metabolism and epigenetic gene regulation. These findings suggest that lysine crotonylation and 2-hydroxyisobutyrylation are ubiquitous throughout the T. gondii proteome, regulating critical functions of the modified proteins. These data provide a basis for identifying important proteins associated with parasite development and pathogenicity.","Global lysine crotonylation and 2-hydroxyisobutyrylation in phenotypically different Toxoplasma gondii parasites. Toxoplasma gondii is a unicellular protozoan parasite of the phylum Apicomplexa. The parasite repeatedly goes through a cycle of invasion, division and induction of host cell rupture, which is an obligatory process for proliferation inside warm-blooded animals. It is known that the biology of the parasite is controlled by a variety of mechanisms ranging from genomic to epigenetic to transcriptional regulation. In this study, we investigated the global protein posttranslational lysine crotonylation and 2-hy-droxyisobutyrylation of two T. gondii strains, RH and ME49, which represent distinct phenotypes for proliferation and pathogenicity in the host. Proteins with differential expression and modification patterns associated with parasite phenotypes were identified. Many proteins in T. gondii were crotonylated and 2-hydroxyisobutyrylated, and they were localized in diverse subcellular compartments involved in a wide variety of cellular functions such as motility, host invasion, metabolism and epigenetic gene regulation. These findings suggest that lysine crotonylation and 2-hydroxyisobutyrylation are ubiquitous throughout the T. gondii proteome, regulating critical functions of the modified proteins. These data provide a basis for identifying important proteins associated with parasite development and pathogenicity."
0,"Automated Triaging of Adult Chest Radiographs with Deep Artificial Neural Networks (vol 291, pg 272, 2019)",,
0,"Effects of tesamorelin on non-alcoholic fatty liver disease in HIV: a randomised, double-blind, multicentre trial",,
0,A novel Rhein derivative: Activation of Rac1/NADPH pathway enhances sensitivity of nasopharyngeal carcinoma cells to radiotherapy,"Radiation resistance and recurrent have become the major factors resulting in poor prognosis in the clinical treatment of patients with nasopharyngeal carcinoma (NPC). New strategies to enhance the efficacy of radiotherapy have been focused on the development of radiosensitizers and searching for directly targets that modulated tumor radiosensitivity. A novel potential radiosensitizer 1,8-Dihydroxy âˆ’3-(2â€²-(4â€³-methylpiperazin-1â€³-yl) ethyl-9,10-anthraquinone âˆ’3-carboxylate (RP-4) was designed and synthesized based on molecular docking technology, which was expected to regulate the radiosensitivity of tumor cells through targeting Rac1. In order to assess the radiosensitization activity of RP-4 on NPC cells, the highly differentiated CNE1 and poorly differentiated CNE2 cells NPC lines were employed. According to the results, RP-4 showed higher binding affinity toward the interaction with Rac1 than lead compounds. We found that RP-4 could inhibit cell viability and proliferation in CNE1 and CNE2 cells and significantly induced apoptosis after non-toxic concentration of RP-4 combined with 2Gy irradiation. RP-4 could effectively modulated the radiosensitivity both CNE1 cells and CNE2 cells through activating Rac1/NADPH signaling pathway and its downstream JNK/AP-1 pathway. What's more, Rac1/NADPH signaling pathway were significantly activated in Rac1-overexpressed CNE1 and CNE2 cells after treated with RP-4. Taken together, Rac1 and its downstream pathway may probably be the direct targets of RP-4 in regulating radiosensitivity of NPC cells, our finding provided a novel strategy for the development of therapeutic agents in response to tumorous radiation resistance.","A novel Rhein derivative: Activation of Rac1/NADPH pathway enhances sensitivity of nasopharyngeal carcinoma cells to radiotherapy. Radiation resistance and recurrent have become the major factors resulting in poor prognosis in the clinical treatment of patients with nasopharyngeal carcinoma (NPC). New strategies to enhance the efficacy of radiotherapy have been focused on the development of radiosensitizers and searching for directly targets that modulated tumor radiosensitivity. A novel potential radiosensitizer 1,8-Dihydroxy âˆ’3-(2â€²-(4â€³-methylpiperazin-1â€³-yl) ethyl-9,10-anthraquinone âˆ’3-carboxylate (RP-4) was designed and synthesized based on molecular docking technology, which was expected to regulate the radiosensitivity of tumor cells through targeting Rac1. In order to assess the radiosensitization activity of RP-4 on NPC cells, the highly differentiated CNE1 and poorly differentiated CNE2 cells NPC lines were employed. According to the results, RP-4 showed higher binding affinity toward the interaction with Rac1 than lead compounds. We found that RP-4 could inhibit cell viability and proliferation in CNE1 and CNE2 cells and significantly induced apoptosis after non-toxic concentration of RP-4 combined with 2Gy irradiation. RP-4 could effectively modulated the radiosensitivity both CNE1 cells and CNE2 cells through activating Rac1/NADPH signaling pathway and its downstream JNK/AP-1 pathway. What's more, Rac1/NADPH signaling pathway were significantly activated in Rac1-overexpressed CNE1 and CNE2 cells after treated with RP-4. Taken together, Rac1 and its downstream pathway may probably be the direct targets of RP-4 in regulating radiosensitivity of NPC cells, our finding provided a novel strategy for the development of therapeutic agents in response to tumorous radiation resistance."
0,Protopanaxadiol inhibits epithelialâ€“mesenchymal transition of hepatocellular carcinoma by targeting STAT3 pathway,"Diol-type ginsenosides, such as protopanaxadiol (PPD), exhibit antioxidation, anti-inflammation, and antitumor effects. However, the antitumor effect of these ginsenosides and the mechanism of PPD remain unclear. In this work, the antitumor effects of several derivatives, including PPD, Rg5, Rg3, Rh2, and Rh3, were evaluated in five different cancer cell lines. PPD demonstrated the best inhibitory effects on the proliferation and migration of the five cancer cell lines, especially the hepatocellular carcinoma (HCC) cell lines. Therefore, the mechanism of action of PPD in HCC cells was elucidated. PPD inhibited the proliferation, migration, and invasion ability of HepG2 and PLC/PRF/5 cells in a dose-dependent manner. Western blot and immunofluorescence assay showed that PPD can alter the expression of epithelialâ€“mesenchymal transition markers, increase E-cadherin expression, and decrease vimentin expression. Docking and biacore experiments revealed that STAT3 is the target protein of PPD, which formed hydrogen bonds with Gly583/Leu608/Tyr674 at the SH2 domain of STAT3. PPD inhibited the phosphorylation of STAT3 and its translocation from the cytosol to the nucleus, thereby inhibiting the expression of Twist1. PPD also inhibited tumor volume and tumor lung metastasis in PLC/PRF/5 xenograft model. In conclusion, PPD can inhibit the proliferation and metastasis of HCC cells through the STAT3/Twist1 pathway.","Protopanaxadiol inhibits epithelialâ€“mesenchymal transition of hepatocellular carcinoma by targeting STAT3 pathway. Diol-type ginsenosides, such as protopanaxadiol (PPD), exhibit antioxidation, anti-inflammation, and antitumor effects. However, the antitumor effect of these ginsenosides and the mechanism of PPD remain unclear. In this work, the antitumor effects of several derivatives, including PPD, Rg5, Rg3, Rh2, and Rh3, were evaluated in five different cancer cell lines. PPD demonstrated the best inhibitory effects on the proliferation and migration of the five cancer cell lines, especially the hepatocellular carcinoma (HCC) cell lines. Therefore, the mechanism of action of PPD in HCC cells was elucidated. PPD inhibited the proliferation, migration, and invasion ability of HepG2 and PLC/PRF/5 cells in a dose-dependent manner. Western blot and immunofluorescence assay showed that PPD can alter the expression of epithelialâ€“mesenchymal transition markers, increase E-cadherin expression, and decrease vimentin expression. Docking and biacore experiments revealed that STAT3 is the target protein of PPD, which formed hydrogen bonds with Gly583/Leu608/Tyr674 at the SH2 domain of STAT3. PPD inhibited the phosphorylation of STAT3 and its translocation from the cytosol to the nucleus, thereby inhibiting the expression of Twist1. PPD also inhibited tumor volume and tumor lung metastasis in PLC/PRF/5 xenograft model. In conclusion, PPD can inhibit the proliferation and metastasis of HCC cells through the STAT3/Twist1 pathway."
0,Brain-machine interfaces from motor to mood,,
0,The role of glucagon-like peptide-1 in reproduction: from physiology to therapeutic perspective,,
0,"Re: Georg Jancke, Firas Aljabery, Sigurdur Gudjonsson, et al. Port-site Metastases After Robot-assisted Radical Cystectomy: Is There a Publication Bias? Eur Urol 2018;73:641-2",,
0,Combination of dihydromyricetin and ondansetron strengthens antiproliferative efficiency of adriamycin in K562/ADR through downregulation of SORCIN: A new strategy of inhibiting P-glycoprotein,"Though the advancement of chemotherapy drugs alleviates the progress of cancer, long-term therapy with anticancer agents gradually leads to acquired multidrug resistance (MDR), which limits the survival outcomes in patients. It was shown that dihydromyricetin (DMY) could partly reverse MDR by suppressing P-glycoprotein (P-gp) and soluble resistance-related calcium-binding protein (SORCIN) independently. To reverse MDR more effectively, a new strategy was raised, that is, circumventing MDR by the coadministration of DMY and ondansetron (OND), a common antiemetic drug, during cancer chemotherapy. Meanwhile, the interior relation between P-gp and SORCIN was also revealed. The combination of DMY and OND strongly enhanced antiproliferative efficiency of adriamycin (ADR) because of the increasing accumulation of ADR in K562/ADR-resistant cell line. DMY could downregulate the expression of SORCIN and P-gp via the ERK/Akt pathways, whereas OND could not. In addition, it was proved that SORCIN suppressed ERK and Akt to inhibit P-gp by the silence of SORCIN, however, not vice versa. Finally, the combination of DMY, OND, and ADR led to G2/M cell cycle arrest and apoptosis via resuming P53 function and restraining relevant proteins expression. These fundamental findings provided a promising approach for further treatment of MDR.","Combination of dihydromyricetin and ondansetron strengthens antiproliferative efficiency of adriamycin in K562/ADR through downregulation of SORCIN: A new strategy of inhibiting P-glycoprotein. Though the advancement of chemotherapy drugs alleviates the progress of cancer, long-term therapy with anticancer agents gradually leads to acquired multidrug resistance (MDR), which limits the survival outcomes in patients. It was shown that dihydromyricetin (DMY) could partly reverse MDR by suppressing P-glycoprotein (P-gp) and soluble resistance-related calcium-binding protein (SORCIN) independently. To reverse MDR more effectively, a new strategy was raised, that is, circumventing MDR by the coadministration of DMY and ondansetron (OND), a common antiemetic drug, during cancer chemotherapy. Meanwhile, the interior relation between P-gp and SORCIN was also revealed. The combination of DMY and OND strongly enhanced antiproliferative efficiency of adriamycin (ADR) because of the increasing accumulation of ADR in K562/ADR-resistant cell line. DMY could downregulate the expression of SORCIN and P-gp via the ERK/Akt pathways, whereas OND could not. In addition, it was proved that SORCIN suppressed ERK and Akt to inhibit P-gp by the silence of SORCIN, however, not vice versa. Finally, the combination of DMY, OND, and ADR led to G2/M cell cycle arrest and apoptosis via resuming P53 function and restraining relevant proteins expression. These fundamental findings provided a promising approach for further treatment of MDR."
0,Identification of infants at risk of child undernutrition in India: building a predictive algorithm with data from a nationally representative survey,,
0,Low serum IGF1 is associated with hypertension and predicts early cardiovascular events in women with rheumatoid arthritis,,
0,Neural networks and deep learning: a brief introduction,,
0,Next-generation computational tools for interrogating cancer immunity,,
0,Circadian blueprint of metabolic pathways in the brain,,
0,BRCA-1 depletion impairs pro-inflammatory polarization and activation of RAW 264.7 macrophages in a NF-ÎºB-dependent mechanism,"BRCA-1 is a nuclear protein involved in DNA repair, transcriptional regulation, and cell cycle control. Its involvement in other cellular processes has been described. Here, we aimed to investigate the role of BRCA-1 in macrophages M(LPS), M(IL-4), and tumor cell-induced differentiation. We used siRNAs to knockdown BRCA-1 in RAW 264.7 macrophages exposed to LPS, IL-4, and C6 glioma cells conditioned medium (CMC6), and evaluated macrophage differentiation markers and functional phagocytic activity as well as DNA damage and cell survival in the presence and absence of BRCA-1. LPS and CMC6, but not by IL-4, increased DNA damage in macrophages, and this effect was more pronounced in BRCA-1-depleted cells, including M(IL-4). BRCA-1 depletion impaired expression of pro-inflammatory cytokines, TNF-Î± and IL-6, and reduced the phagocytic activity of macrophages in response to LPS. In CMC6-induced differentiation, BRCA-1 knockdown inhibited TNF-Î± and IL-6 expression which was accompanied by upregulation of the anti-inflammatory markers IL-10 and TGF-Î² and reduced phagocytosis. In contrast, M(IL-4) phenotype was not affected by BRCA-1 status. Molecular docking predicted that the conserved BRCA-1 domain BRCT can interact with the p65 subunit of NF-ÎºB. Immunofluorescence assays showed that BRCA-1 and p65 co-localize in the nucleus of LPS-treated macrophages and reporter gene assay showed that depletion of BRCA-1 decreased LPS and CMC6-induced NF-ÎºB transactivation. IL-4 had no effect upon NF-ÎºB. Taken together, our findings suggest a role of BRCA-1 in macrophage differentiation and phagocytosis induced by LPS and tumor cells secretoma, but not IL-4, in a mechanism associated with inhibition of NF-ÎºB.","BRCA-1 depletion impairs pro-inflammatory polarization and activation of RAW 264.7 macrophages in a NF-ÎºB-dependent mechanism. BRCA-1 is a nuclear protein involved in DNA repair, transcriptional regulation, and cell cycle control. Its involvement in other cellular processes has been described. Here, we aimed to investigate the role of BRCA-1 in macrophages M(LPS), M(IL-4), and tumor cell-induced differentiation. We used siRNAs to knockdown BRCA-1 in RAW 264.7 macrophages exposed to LPS, IL-4, and C6 glioma cells conditioned medium (CMC6), and evaluated macrophage differentiation markers and functional phagocytic activity as well as DNA damage and cell survival in the presence and absence of BRCA-1. LPS and CMC6, but not by IL-4, increased DNA damage in macrophages, and this effect was more pronounced in BRCA-1-depleted cells, including M(IL-4). BRCA-1 depletion impaired expression of pro-inflammatory cytokines, TNF-Î± and IL-6, and reduced the phagocytic activity of macrophages in response to LPS. In CMC6-induced differentiation, BRCA-1 knockdown inhibited TNF-Î± and IL-6 expression which was accompanied by upregulation of the anti-inflammatory markers IL-10 and TGF-Î² and reduced phagocytosis. In contrast, M(IL-4) phenotype was not affected by BRCA-1 status. Molecular docking predicted that the conserved BRCA-1 domain BRCT can interact with the p65 subunit of NF-ÎºB. Immunofluorescence assays showed that BRCA-1 and p65 co-localize in the nucleus of LPS-treated macrophages and reporter gene assay showed that depletion of BRCA-1 decreased LPS and CMC6-induced NF-ÎºB transactivation. IL-4 had no effect upon NF-ÎºB. Taken together, our findings suggest a role of BRCA-1 in macrophage differentiation and phagocytosis induced by LPS and tumor cells secretoma, but not IL-4, in a mechanism associated with inhibition of NF-ÎºB."
0,Quantification of Change in Iris Torsion Using a Smartphone,,
0,"Artificial intelligence, chatbots, and the future of medicine",,
0,Structure of a Signaling Cannabinoid Receptor 1-G Protein Complex,"Cannabis elicits its mood-enhancing and analgesic effects through the cannabinoid receptor 1 (CB1), a G protein-coupled receptor (GPCR) that signals primarily through the adenylyl cyclase-inhibiting heterotrimeric G protein Gi. Activation of CB1-Gi signaling pathways holds potential for treating a number of neurological disorders and is thus crucial to understand the mechanism of Gi activation by CB1. Here, we present the structure of the CB1-Gi signaling complex bound to the highly potent agonist MDMB-Fubinaca (FUB), a recently emerged illicit synthetic cannabinoid infused in street drugs that have been associated with numerous overdoses and fatalities. The structure illustrates how FUB stabilizes the receptor in an active state to facilitate nucleotide exchange in Gi. The results compose the structural framework to explain CB1 activation by different classes of ligands and provide insights into the G protein coupling and selectivity mechanisms adopted by the receptor.","Structure of a Signaling Cannabinoid Receptor 1-G Protein Complex. Cannabis elicits its mood-enhancing and analgesic effects through the cannabinoid receptor 1 (CB1), a G protein-coupled receptor (GPCR) that signals primarily through the adenylyl cyclase-inhibiting heterotrimeric G protein Gi. Activation of CB1-Gi signaling pathways holds potential for treating a number of neurological disorders and is thus crucial to understand the mechanism of Gi activation by CB1. Here, we present the structure of the CB1-Gi signaling complex bound to the highly potent agonist MDMB-Fubinaca (FUB), a recently emerged illicit synthetic cannabinoid infused in street drugs that have been associated with numerous overdoses and fatalities. The structure illustrates how FUB stabilizes the receptor in an active state to facilitate nucleotide exchange in Gi. The results compose the structural framework to explain CB1 activation by different classes of ligands and provide insights into the G protein coupling and selectivity mechanisms adopted by the receptor."
0,A comparative study of deep learning architectures on melanoma detection,"Melanoma is the most aggressive type of skin cancer, which significantly reduces the life expectancy. Early detection of melanoma can reduce the morbidity and mortality associated with skin cancer. Dermoscopic images acquired by dermoscopic instruments are used in computational analysis for skin cancer detection. However, some image quality limitations such as noises, shadows, artefacts exist that could compromise the robustness of the skin image analysis. Hence, developing an automatic intelligent system for skin cancer diagnosis with accurate detection rate is crucial. In this paper, we evaluate the performance of several state-of-the-art convolutional neural networks in dermoscopic images of skin lesions. Our experiment is conducted on a graphics processing unit (GPU)to speed up the training and deployment process. To enhance the quality of images, we employ different pre-processing steps. We also apply data augmentation methodology such as horizontal and vertical flipping techniques to address the class skewness problem. Both pre-processing and data augmentation could help to improve the final accuracy.","A comparative study of deep learning architectures on melanoma detection. Melanoma is the most aggressive type of skin cancer, which significantly reduces the life expectancy. Early detection of melanoma can reduce the morbidity and mortality associated with skin cancer. Dermoscopic images acquired by dermoscopic instruments are used in computational analysis for skin cancer detection. However, some image quality limitations such as noises, shadows, artefacts exist that could compromise the robustness of the skin image analysis. Hence, developing an automatic intelligent system for skin cancer diagnosis with accurate detection rate is crucial. In this paper, we evaluate the performance of several state-of-the-art convolutional neural networks in dermoscopic images of skin lesions. Our experiment is conducted on a graphics processing unit (GPU)to speed up the training and deployment process. To enhance the quality of images, we employ different pre-processing steps. We also apply data augmentation methodology such as horizontal and vertical flipping techniques to address the class skewness problem. Both pre-processing and data augmentation could help to improve the final accuracy."
0,Comparisons of exacerbations and mortality among regular inhaled therapies for patients with stable chronic obstructive pulmonary disease: Systematic review and Bayesian network meta-analysis,"BACKGROUND: Although exacerbation and mortality are the most important clinical outcomes of stable chronic obstructive pulmonary disease (COPD), the drug classes that are the most efficacious in reducing exacerbation and mortality among all possible inhaled drugs have not been determined. METHODS AND FINDINGS: We performed a systematic review (SR) and Bayesian network meta-analysis (NMA). We searched Medline, EMBASE, the Cochrane Central Register of Controlled Trials, ClinicalTrials.gov, the European Union Clinical Trials Register, and the official websites of pharmaceutical companies (from inception to July 9, 2019). The eligibility criteria were as follows: (1) parallel-design randomized controlled trials (RCTs); (2) adults with stable COPD; (3) comparisons among long-acting muscarinic antagonists (LAMAs), long-acting beta-agonists (LABAs), inhaled corticosteroids (ICSs), combined treatment (ICS/LAMA/LABA, LAMA/LABA, or ICS/LABA), or a placebo; and (4) study duration â‰¥ 12 weeks. This study was prospectively registered in International Prospective Register of Systematic Reviews (PROSPERO; CRD42017069087). In total, 219 trials involving 228,710 patients were included. Compared with placebo, all drug classes significantly reduced the total exacerbations and moderate to severe exacerbations. ICS/LAMA/LABA was the most efficacious treatment for reducing the exacerbation risk (odds ratio [OR] = 0.57; 95% credible interval [CrI] 0.50-0.64; posterior probability of OR > 1 [P(OR > 1)] < 0.001). In addition, in contrast to the other drug classes, ICS/LAMA/LABA and ICS/LABA were associated with a significantly higher probability of reducing mortality than placebo (OR = 0.74, 95% CrI 0.59-0.93, P[OR > 1] = 0.004; and OR = 0.86, 95% CrI 0.76-0.98, P[OR > 1] = 0.015, respectively). The results minimally changed, even in various sensitivity and covariate-adjusted meta-regression analyses. ICS/LAMA/LABA tended to lower the risk of cardiovascular mortality but did not show significant results. ICS/LAMA/LABA increased the probability of pneumonia (OR for triple therapy = 1.56; 95% CrI 1.19-2.03; P[OR > 1] = 1.000). The main limitation is that there were few RCTs including only less symptomatic patients or patients at a low risk. CONCLUSIONS: These findings suggest that triple therapy can potentially be the best option for stable COPD patients in terms of reducing exacerbation and all-cause mortality.","Comparisons of exacerbations and mortality among regular inhaled therapies for patients with stable chronic obstructive pulmonary disease: Systematic review and Bayesian network meta-analysis. BACKGROUND: Although exacerbation and mortality are the most important clinical outcomes of stable chronic obstructive pulmonary disease (COPD), the drug classes that are the most efficacious in reducing exacerbation and mortality among all possible inhaled drugs have not been determined. METHODS AND FINDINGS: We performed a systematic review (SR) and Bayesian network meta-analysis (NMA). We searched Medline, EMBASE, the Cochrane Central Register of Controlled Trials, ClinicalTrials.gov, the European Union Clinical Trials Register, and the official websites of pharmaceutical companies (from inception to July 9, 2019). The eligibility criteria were as follows: (1) parallel-design randomized controlled trials (RCTs); (2) adults with stable COPD; (3) comparisons among long-acting muscarinic antagonists (LAMAs), long-acting beta-agonists (LABAs), inhaled corticosteroids (ICSs), combined treatment (ICS/LAMA/LABA, LAMA/LABA, or ICS/LABA), or a placebo; and (4) study duration â‰¥ 12 weeks. This study was prospectively registered in International Prospective Register of Systematic Reviews (PROSPERO; CRD42017069087). In total, 219 trials involving 228,710 patients were included. Compared with placebo, all drug classes significantly reduced the total exacerbations and moderate to severe exacerbations. ICS/LAMA/LABA was the most efficacious treatment for reducing the exacerbation risk (odds ratio [OR] = 0.57; 95% credible interval [CrI] 0.50-0.64; posterior probability of OR > 1 [P(OR > 1)] < 0.001). In addition, in contrast to the other drug classes, ICS/LAMA/LABA and ICS/LABA were associated with a significantly higher probability of reducing mortality than placebo (OR = 0.74, 95% CrI 0.59-0.93, P[OR > 1] = 0.004; and OR = 0.86, 95% CrI 0.76-0.98, P[OR > 1] = 0.015, respectively). The results minimally changed, even in various sensitivity and covariate-adjusted meta-regression analyses. ICS/LAMA/LABA tended to lower the risk of cardiovascular mortality but did not show significant results. ICS/LAMA/LABA increased the probability of pneumonia (OR for triple therapy = 1.56; 95% CrI 1.19-2.03; P[OR > 1] = 1.000). The main limitation is that there were few RCTs including only less symptomatic patients or patients at a low risk. CONCLUSIONS: These findings suggest that triple therapy can potentially be the best option for stable COPD patients in terms of reducing exacerbation and all-cause mortality."
0,Combinatorial screening algorithm to engineer multiepitope subunit vaccine targeting human T-lymphotropic virus-1 infection,"Human T-lymphotropic virus (HTLV), the first human retrovirus has been discovered which is known to cause the age-old assassinating disease HTLV-1 associated myelopathy. Cancer caused by this virus is adult T cell leukemia/lymphoma which targets 10â€“20 million throughout the world. The effect of this virus extends to the fact that it causes chronic disease to the spinal cord resulting in loss of sensation and further causes blood cancer. So, to overcome the complications, we designed a subunit vaccine by the assimilation of B-cell, cytotoxic T-lymphocyte, and helper T-lymphocyte epitopes. The epitopes were joined together along with adjuvant and linkers and a vaccine was fabricated which was further subjected to 3D modeling. The physiochemical properties, allergenicity, and antigenicity were evaluated. Molecular docking and dynamics were performed with the obtained 3D model against toll like receptor (TLR-3) immune receptor. Lastly, in silico cloning was performed to ensure the expression of the designed vaccine in pET28a(+) expression vector. The future prospects of the study entailed the in vitro and in vivo experimental analysis for evaluating the immune response of the designed vaccine construct.","Combinatorial screening algorithm to engineer multiepitope subunit vaccine targeting human T-lymphotropic virus-1 infection. Human T-lymphotropic virus (HTLV), the first human retrovirus has been discovered which is known to cause the age-old assassinating disease HTLV-1 associated myelopathy. Cancer caused by this virus is adult T cell leukemia/lymphoma which targets 10â€“20 million throughout the world. The effect of this virus extends to the fact that it causes chronic disease to the spinal cord resulting in loss of sensation and further causes blood cancer. So, to overcome the complications, we designed a subunit vaccine by the assimilation of B-cell, cytotoxic T-lymphocyte, and helper T-lymphocyte epitopes. The epitopes were joined together along with adjuvant and linkers and a vaccine was fabricated which was further subjected to 3D modeling. The physiochemical properties, allergenicity, and antigenicity were evaluated. Molecular docking and dynamics were performed with the obtained 3D model against toll like receptor (TLR-3) immune receptor. Lastly, in silico cloning was performed to ensure the expression of the designed vaccine in pET28a(+) expression vector. The future prospects of the study entailed the in vitro and in vivo experimental analysis for evaluating the immune response of the designed vaccine construct."
0,Transcriptomics-Based Screening Identifies Pharmacological Inhibition of Hsp90 as a Means to Defer Aging,"Aging strongly influences human morbidity and mortality. Thus, aging-preventive compounds could greatly improve our health and lifespan. Here we screened for such compounds, known as geroprotectors, employing the power of transcriptomics to predict biological age. Using age-stratified human tissue transcriptomes and machine learning, we generated age classifiers and applied these to transcriptomic changes induced by 1,309 different compounds in human cells, ranking these compounds by their ability to induce a â€œyouthfulâ€ transcriptional state. Testing the top candidates in C. elegans, we identified two Hsp90 inhibitors, monorden and tanespimycin, which extended the animalsâ€™ lifespan and improved their health. Hsp90 inhibition induces expression of heat shock proteins known to improve protein homeostasis. Consistently, monorden treatment improved the survival of C. elegans under proteotoxic stress, and its benefits depended on the cytosolic unfolded protein response-inducing transcription factor HSF-1. Taken together, our method represents an innovative geroprotector screening approach and was able to identify a class that acts by improving protein homeostasis.","Transcriptomics-Based Screening Identifies Pharmacological Inhibition of Hsp90 as a Means to Defer Aging. Aging strongly influences human morbidity and mortality. Thus, aging-preventive compounds could greatly improve our health and lifespan. Here we screened for such compounds, known as geroprotectors, employing the power of transcriptomics to predict biological age. Using age-stratified human tissue transcriptomes and machine learning, we generated age classifiers and applied these to transcriptomic changes induced by 1,309 different compounds in human cells, ranking these compounds by their ability to induce a â€œyouthfulâ€ transcriptional state. Testing the top candidates in C. elegans, we identified two Hsp90 inhibitors, monorden and tanespimycin, which extended the animalsâ€™ lifespan and improved their health. Hsp90 inhibition induces expression of heat shock proteins known to improve protein homeostasis. Consistently, monorden treatment improved the survival of C. elegans under proteotoxic stress, and its benefits depended on the cytosolic unfolded protein response-inducing transcription factor HSF-1. Taken together, our method represents an innovative geroprotector screening approach and was able to identify a class that acts by improving protein homeostasis."
0,Unraveling the Molecular Mechanism of Action of Empagliflozin in Heart Failure With Reduced Ejection Fraction With or Without Diabetes,"The mechanism of action of empagliflozin in heart failure with reduced ejection fraction (HFrEF) was deciphered using deep learning in silico analyses together with in vivo validation. The most robust mechanism of action involved the sodium-hydrogen exchanger (NHE)-1 co-transporter with 94.7% accuracy, which was similar for diabetics and nondiabetics. Notably, direct NHE1 blockade by empagliflozin ameliorated cardiomyocyte cell death by restoring expression of X-linked inhibitor of apoptosis (XIAP) and baculoviral IAP repeat-containing protein 5 (BIRC5). These results were independent of diabetes mellitus comorbidity, suggesting that empagliflozin may emerge as a new treatment in HFrEF.","Unraveling the Molecular Mechanism of Action of Empagliflozin in Heart Failure With Reduced Ejection Fraction With or Without Diabetes. The mechanism of action of empagliflozin in heart failure with reduced ejection fraction (HFrEF) was deciphered using deep learning in silico analyses together with in vivo validation. The most robust mechanism of action involved the sodium-hydrogen exchanger (NHE)-1 co-transporter with 94.7% accuracy, which was similar for diabetics and nondiabetics. Notably, direct NHE1 blockade by empagliflozin ameliorated cardiomyocyte cell death by restoring expression of X-linked inhibitor of apoptosis (XIAP) and baculoviral IAP repeat-containing protein 5 (BIRC5). These results were independent of diabetes mellitus comorbidity, suggesting that empagliflozin may emerge as a new treatment in HFrEF."
0,Emerging imaging technologies in dermatology Part I: Basic principles,,
0,"Impact of SNPs interplay across the locus of MBL2, between MBL and Dectin-1 gene, on women's risk of developing recurrent vulvovaginal infections","Background: Human mannose binding lectin (MBL) and dendritic cell-associated C-type lectin-1 (Dectin-1) are the two prototypical PRRs of innate immunity, whose direct role in recurrent vulvovaginal infections (RVVI) defense has been defined. Previously, MBL insufficiency was proposed as a possible risk factor for the rapid progression of RVVI while, Dectin-1 was found to be playing an active role in the defense. However, the complete genetic bases for the observed low MBL levels are still lacking as our previous studies in harmony with others demonstrated the un-expected genotype-phenotype patterns. This suggested the presence of unidentified regulatory variants that may modulate sMBL levels and risk of RVVI. Therefore, the present study was designed for more inclusive locus-wide MBL2 analysis and for the possible non-linear interaction analysis of two PRRs that may impact RVVI susceptibility. Methods: The present study has extended the previous findings by investigating (1) the role of chosen additional SNPs falling in the 5â€² near region relating to sMBL levels and RVVI susceptibility, using polymerase chain reaction-restriction fragment length polymorphism, (2) interactions among SNPs within gene by comprehensive locus-wide haplotype analyses of two MBL2 blocks, (3) gene-gene interaction analyses between two PRRs, using multifactor dimensionality reduction. Results: rs11003124-G, rs7084554-C, rs36014597-G, and rs11003123-A were observed as the minor alleles in the representative North Indian cohort. RVVI cases and its types showed an appreciably high frequency of C allele, its homozygosity and heterozygosity, explaining the observed dominant mode of inheritance of rs7084554 polymorphism in contributing 1.81 fold risk of RVVI. The rs36014597 polymorphism showed the overdominant mode of inheritance, which further depicts that the carrier of a heterozygous genotype of this polymorphism had more extreme phenotype than either of its homozygous carriers in developing 4.07 fold risk of RVVI. sMBL levels significantly varied for rs11003124, rs36014597 and rs11003123 polymorphisms in bacterial vaginosis, while for rs7084554 polymorphism in mixed infection. Independent analysis of 5â€² and 3â€² haplotype blocks suggested the risk-modifying effect of all the 5â€² additional variants, Y/X secretor polymorphism and 3â€²-UTR SNP i.e. rs10824792. Combined 5â€²/3â€² haplotype analyses depicted the importance of rs36014597; an additional 5â€² variant, Y/X and rs10824792 polymorphisms from both the blocks in regulating sMBL levels and RVVI risk. Three gene-gene interaction models involving uni-variant, bi-variant and tri-variant appeared as significant predictors of RVVI risk with cross-validation consistency of 10/10, 9/10 and 5/10, respectively. Conclusions: The study presented a low-cost reproducible screening design for additional 5â€² variants i.e. rs11003124, rs7084554, rs36014597 and rs11003123 of MBL2 that can act as markers of susceptibility for RVVI or any other diseases. Two additional 5â€² variants of MBL2 i.e. rs7084554 and rs36014597 were suggested as novel molecular markers that may contribute to RVVI risk by varying sMBL levels. Variants of two blocks were found to have more of a combined effect than the independent effect in modulating RVVI susceptibility and sMBL levels. The study presented weak synergistic interaction between MBL2 and CLEC7A in association with RVVI risk. The preliminary data will establish the foundation for the investigation of within gene and between genes interaction analyses towards RVVI susceptibility.","Impact of SNPs interplay across the locus of MBL2, between MBL and Dectin-1 gene, on women's risk of developing recurrent vulvovaginal infections. Background: Human mannose binding lectin (MBL) and dendritic cell-associated C-type lectin-1 (Dectin-1) are the two prototypical PRRs of innate immunity, whose direct role in recurrent vulvovaginal infections (RVVI) defense has been defined. Previously, MBL insufficiency was proposed as a possible risk factor for the rapid progression of RVVI while, Dectin-1 was found to be playing an active role in the defense. However, the complete genetic bases for the observed low MBL levels are still lacking as our previous studies in harmony with others demonstrated the un-expected genotype-phenotype patterns. This suggested the presence of unidentified regulatory variants that may modulate sMBL levels and risk of RVVI. Therefore, the present study was designed for more inclusive locus-wide MBL2 analysis and for the possible non-linear interaction analysis of two PRRs that may impact RVVI susceptibility. Methods: The present study has extended the previous findings by investigating (1) the role of chosen additional SNPs falling in the 5â€² near region relating to sMBL levels and RVVI susceptibility, using polymerase chain reaction-restriction fragment length polymorphism, (2) interactions among SNPs within gene by comprehensive locus-wide haplotype analyses of two MBL2 blocks, (3) gene-gene interaction analyses between two PRRs, using multifactor dimensionality reduction. Results: rs11003124-G, rs7084554-C, rs36014597-G, and rs11003123-A were observed as the minor alleles in the representative North Indian cohort. RVVI cases and its types showed an appreciably high frequency of C allele, its homozygosity and heterozygosity, explaining the observed dominant mode of inheritance of rs7084554 polymorphism in contributing 1.81 fold risk of RVVI. The rs36014597 polymorphism showed the overdominant mode of inheritance, which further depicts that the carrier of a heterozygous genotype of this polymorphism had more extreme phenotype than either of its homozygous carriers in developing 4.07 fold risk of RVVI. sMBL levels significantly varied for rs11003124, rs36014597 and rs11003123 polymorphisms in bacterial vaginosis, while for rs7084554 polymorphism in mixed infection. Independent analysis of 5â€² and 3â€² haplotype blocks suggested the risk-modifying effect of all the 5â€² additional variants, Y/X secretor polymorphism and 3â€²-UTR SNP i.e. rs10824792. Combined 5â€²/3â€² haplotype analyses depicted the importance of rs36014597; an additional 5â€² variant, Y/X and rs10824792 polymorphisms from both the blocks in regulating sMBL levels and RVVI risk. Three gene-gene interaction models involving uni-variant, bi-variant and tri-variant appeared as significant predictors of RVVI risk with cross-validation consistency of 10/10, 9/10 and 5/10, respectively. Conclusions: The study presented a low-cost reproducible screening design for additional 5â€² variants i.e. rs11003124, rs7084554, rs36014597 and rs11003123 of MBL2 that can act as markers of susceptibility for RVVI or any other diseases. Two additional 5â€² variants of MBL2 i.e. rs7084554 and rs36014597 were suggested as novel molecular markers that may contribute to RVVI risk by varying sMBL levels. Variants of two blocks were found to have more of a combined effect than the independent effect in modulating RVVI susceptibility and sMBL levels. The study presented weak synergistic interaction between MBL2 and CLEC7A in association with RVVI risk. The preliminary data will establish the foundation for the investigation of within gene and between genes interaction analyses towards RVVI susceptibility."
0,YEARS Algorithm in Pregnant Patients With Suspected Pulmonary Embolism October 2019 Annals of Emergency Medicine Journal Club,,
0,A management algorithm for patients with intracranial pressure monitoring: the Seattle International Severe Traumatic Brain Injury Consensus Conference (SIBICC),"BACKGROUND: Management algorithms for adult severe traumatic brain injury (sTBI) were omitted in later editions of the Brain Trauma Foundation's sTBI Management Guidelines, as they were not evidence-based. METHODS: We used a Delphi-method-based consensus approach to address management of sTBI patients undergoing intracranial pressure (ICP) monitoring. Forty-two experienced, clinically active sTBI specialists from six continents comprised the panel. Eight surveys iterated queries and comments. An in-person meeting included whole- and small-group discussions and blinded voting. Consensus required 80% agreement. We developed heatmaps based on a traffic-light model where panelists' decision tendencies were the focus of recommendations. RESULTS: We provide comprehensive algorithms for ICP-monitor-based adult sTBI management. Consensus established 18 interventions as fundamental and ten treatments not to be used. We provide a three-tier algorithm for treating elevated ICP. Treatments within a tier are considered empirically equivalent. Higher tiers involve higher risk therapies. Tiers 1, 2, and 3 include 10, 4, and 3 interventions, respectively. We include inter-tier considerations, and recommendations for critical neuroworsening to assist the recognition and treatment of declining patients. Novel elements include guidance for autoregulation-based ICP treatment based on MAP Challenge results, and two heatmaps to guide (1) ICP-monitor removal and (2) consideration of sedation holidays for neurological examination. CONCLUSIONS: Our modern and comprehensive sTBI-management protocol is designed to assist clinicians managing sTBI patients monitored with ICP-monitors alone. Consensus-based (class III evidence), it provides management recommendations based on combined expert opinion. It reflects neither a standard-of-care nor a substitute for thoughtful individualized management.","A management algorithm for patients with intracranial pressure monitoring: the Seattle International Severe Traumatic Brain Injury Consensus Conference (SIBICC). BACKGROUND: Management algorithms for adult severe traumatic brain injury (sTBI) were omitted in later editions of the Brain Trauma Foundation's sTBI Management Guidelines, as they were not evidence-based. METHODS: We used a Delphi-method-based consensus approach to address management of sTBI patients undergoing intracranial pressure (ICP) monitoring. Forty-two experienced, clinically active sTBI specialists from six continents comprised the panel. Eight surveys iterated queries and comments. An in-person meeting included whole- and small-group discussions and blinded voting. Consensus required 80% agreement. We developed heatmaps based on a traffic-light model where panelists' decision tendencies were the focus of recommendations. RESULTS: We provide comprehensive algorithms for ICP-monitor-based adult sTBI management. Consensus established 18 interventions as fundamental and ten treatments not to be used. We provide a three-tier algorithm for treating elevated ICP. Treatments within a tier are considered empirically equivalent. Higher tiers involve higher risk therapies. Tiers 1, 2, and 3 include 10, 4, and 3 interventions, respectively. We include inter-tier considerations, and recommendations for critical neuroworsening to assist the recognition and treatment of declining patients. Novel elements include guidance for autoregulation-based ICP treatment based on MAP Challenge results, and two heatmaps to guide (1) ICP-monitor removal and (2) consideration of sedation holidays for neurological examination. CONCLUSIONS: Our modern and comprehensive sTBI-management protocol is designed to assist clinicians managing sTBI patients monitored with ICP-monitors alone. Consensus-based (class III evidence), it provides management recommendations based on combined expert opinion. It reflects neither a standard-of-care nor a substitute for thoughtful individualized management."
0,Randomized Trial of Four Treatment Approaches for Actinic Keratosis,,
0,A call for deep-learning healthcare,,
0,Analysis of Human Performance Deficiencies Associated with Surgical Adverse Events,"Importance: Potentially preventable adverse events remain a formidable cause of patient harm and health care expenditure despite advances in systems-based risk-reduction strategies. Objective: To analyze and describe the incidence of human performance deficiencies (HPDs) during the provision of surgical care to identify opportunities to enhance patient safety. Design, Setting, and Participants: This quality improvement study used a new taxonomy to inform the development and implementation of an HPD classifier tool to categorize HPDs into errors associated with cognitive, technical, and team dynamic functions. The HPD classifier tool was then used to concurrently analyze surgical adverse events in 3 adult hospital affiliates - a level I municipal trauma center, a quaternary care university hospital, and a US Veterans Administration hospital - from January 2, 2018, to June 30, 2018. Surgical trainees presented data describing all adverse events associated with surgical services at weekly hospital-based morbidity and mortality conferences. Adverse events and HPDs were classified in discussion with attending faculty and residents. Data were analyzed from July 9, 2018, to December 23, 2018. Main Outcomes and Measures: The incidence and primary and secondary causes of HPDs were classified using an HPD classifier tool. Results: A total of 188 adverse events were recorded, including 182 adverse events (96.8%) among 5365 patients who underwent surgical operations and 6 adverse events (3.2%) among patients undergoing nonoperative treatment. Among these 188 adverse events, 106 (56.4%) were associated with HPDs. Among these 106 HPD adverse events, a total of 192 HPDs (mean [SD], 1.8 [0.9] HPDs per HPD event) were identified. Human performance deficiencies were categorized as execution (98 HPDs [51.0%]), planning or problem solving (55 HPDs [28.6%]), communication (24 HPDs [12.5%]), teamwork (9 HPDs [4.7%]), and rules violation (6 HPDs [3.1%]). Human performance deficiencies most commonly presented as cognitive errors in execution of care or in case planning or problem solving (99 of 192 HPDs [51.6%]). In contrast, technical execution errors without other associated HPDs were observed in 20 of 192 HPDs (10.4%). Conclusions and Relevance: Human performance deficiencies were identified in more than half of adverse events, most commonly associated with cognitive error in the execution of care. These data provide a framework and impetus for new quality improvement initiatives incorporating cognitive training to mitigate human error in surgery..","Analysis of Human Performance Deficiencies Associated with Surgical Adverse Events. Importance: Potentially preventable adverse events remain a formidable cause of patient harm and health care expenditure despite advances in systems-based risk-reduction strategies. Objective: To analyze and describe the incidence of human performance deficiencies (HPDs) during the provision of surgical care to identify opportunities to enhance patient safety. Design, Setting, and Participants: This quality improvement study used a new taxonomy to inform the development and implementation of an HPD classifier tool to categorize HPDs into errors associated with cognitive, technical, and team dynamic functions. The HPD classifier tool was then used to concurrently analyze surgical adverse events in 3 adult hospital affiliates - a level I municipal trauma center, a quaternary care university hospital, and a US Veterans Administration hospital - from January 2, 2018, to June 30, 2018. Surgical trainees presented data describing all adverse events associated with surgical services at weekly hospital-based morbidity and mortality conferences. Adverse events and HPDs were classified in discussion with attending faculty and residents. Data were analyzed from July 9, 2018, to December 23, 2018. Main Outcomes and Measures: The incidence and primary and secondary causes of HPDs were classified using an HPD classifier tool. Results: A total of 188 adverse events were recorded, including 182 adverse events (96.8%) among 5365 patients who underwent surgical operations and 6 adverse events (3.2%) among patients undergoing nonoperative treatment. Among these 188 adverse events, 106 (56.4%) were associated with HPDs. Among these 106 HPD adverse events, a total of 192 HPDs (mean [SD], 1.8 [0.9] HPDs per HPD event) were identified. Human performance deficiencies were categorized as execution (98 HPDs [51.0%]), planning or problem solving (55 HPDs [28.6%]), communication (24 HPDs [12.5%]), teamwork (9 HPDs [4.7%]), and rules violation (6 HPDs [3.1%]). Human performance deficiencies most commonly presented as cognitive errors in execution of care or in case planning or problem solving (99 of 192 HPDs [51.6%]). In contrast, technical execution errors without other associated HPDs were observed in 20 of 192 HPDs (10.4%). Conclusions and Relevance: Human performance deficiencies were identified in more than half of adverse events, most commonly associated with cognitive error in the execution of care. These data provide a framework and impetus for new quality improvement initiatives incorporating cognitive training to mitigate human error in surgery.."
0,"Origin, Organization, Dynamics, and Function of Actin and Actomyosin Networks at the T Cell Immunological Synapse",,
0,Estimating uncertainty in MRF-based image segmentation: A perfect-MCMC approach,,
0,Plasma-based protein biomarkers can predict the risk of acute graft-versus-host disease and non-relapse mortality in patients undergoing allogeneic hematopoietic stem cell transplantation,"Predictive biomarkers for acute graft-versus-host disease (aGVHD) is currently lacking. In this study, we employed an unbiased proteome profiling method to prospectively collected plasma samples from allogeneic hematopoietic stem cell transplantation (alloHSCT) recipients to identify protein biomarkers that predict the risk of aGVHD and non-relapse mortality (NRM). In the discovery set, including five aGVHD patients and five controls, we identified seven candidate proteins. Patients with high levels of these proteins tended to exhibit a higher risk of aGVHD and NRM compared to patients with low levels in post-engraftment plasma samples from an independent validation set (n = 89). Tissue inhibitor of metalloproteinase 1, plastin-2, and regenerating islet-derived protein 3-Î± were selected as the most-predictive biomarkers via an exhaustive variable screening algorithm and were collectively used to develop a biomarker panel score ranging from 0 to 3. The biomarker panel score correlated significantly with aGVHD and NRM risk in univariable and multivariable Cox models. Furthermore, using the biomarker panel score in conjunction with clinical predictors significantly improved the discriminatory performance of the Cox model in predicting aGVHD and NRM risk. Our findings suggest that plasma-derived protein biomarkers can be used to predict aGVHD and NRM before the onset of clinical manifestations.","Plasma-based protein biomarkers can predict the risk of acute graft-versus-host disease and non-relapse mortality in patients undergoing allogeneic hematopoietic stem cell transplantation. Predictive biomarkers for acute graft-versus-host disease (aGVHD) is currently lacking. In this study, we employed an unbiased proteome profiling method to prospectively collected plasma samples from allogeneic hematopoietic stem cell transplantation (alloHSCT) recipients to identify protein biomarkers that predict the risk of aGVHD and non-relapse mortality (NRM). In the discovery set, including five aGVHD patients and five controls, we identified seven candidate proteins. Patients with high levels of these proteins tended to exhibit a higher risk of aGVHD and NRM compared to patients with low levels in post-engraftment plasma samples from an independent validation set (n = 89). Tissue inhibitor of metalloproteinase 1, plastin-2, and regenerating islet-derived protein 3-Î± were selected as the most-predictive biomarkers via an exhaustive variable screening algorithm and were collectively used to develop a biomarker panel score ranging from 0 to 3. The biomarker panel score correlated significantly with aGVHD and NRM risk in univariable and multivariable Cox models. Furthermore, using the biomarker panel score in conjunction with clinical predictors significantly improved the discriminatory performance of the Cox model in predicting aGVHD and NRM risk. Our findings suggest that plasma-derived protein biomarkers can be used to predict aGVHD and NRM before the onset of clinical manifestations."
0,Rethinking drug design in the artificial intelligence era,"Artificial intelligence (AI) tools are increasingly being applied in drug discovery. While some protagonists point to vast opportunities potentially offered by such tools, others remain sceptical, waiting for a clear impact to be shown in drug discovery projects. The reality is probably somewhere in-between these extremes, yet it is clear that AI is providing new challenges not only for the scientists involved but also for the biopharma industry and its established processes for discovering and developing new medicines. This article presents the views of a diverse group of international experts on the 'grand challenges' in small-molecule drug discovery with AI and the approaches to address them.","Rethinking drug design in the artificial intelligence era. Artificial intelligence (AI) tools are increasingly being applied in drug discovery. While some protagonists point to vast opportunities potentially offered by such tools, others remain sceptical, waiting for a clear impact to be shown in drug discovery projects. The reality is probably somewhere in-between these extremes, yet it is clear that AI is providing new challenges not only for the scientists involved but also for the biopharma industry and its established processes for discovering and developing new medicines. This article presents the views of a diverse group of international experts on the 'grand challenges' in small-molecule drug discovery with AI and the approaches to address them."
0,Segmenting and tracking cell instances with cosine embeddings and recurrent hourglass networks,,
0,Ellipsometry of human tears,,
0,Big data and machine learning algorithms for health-care delivery,"Analysis of big data by machine learning offers considerable advantages for assimilation and evaluation of large amounts of complex health-care data. However, to effectively use machine learning tools in health care, several limitations must be addressed and key issues considered, such as its clinical implementation and ethics in health-care delivery. Advantages of machine learning include flexibility and scalability compared with traditional biostatistical methods, which makes it deployable for many tasks, such as risk stratification, diagnosis and classification, and survival predictions. Another advantage of machine learning algorithms is the ability to analyse diverse data types (eg, demographic data, laboratory findings, imaging data, and doctors' free-text notes) and incorporate them into predictions for disease risk, diagnosis, prognosis, and appropriate treatments. Despite these advantages, the application of machine learning in health-care delivery also presents unique challenges that require data pre-processing, model training, and refinement of the system with respect to the actual clinical problem. Also crucial are ethical considerations, which include medico-legal implications, doctors' understanding of machine learning tools, and data privacy and security. In this Review, we discuss some of the benefits and challenges of big data and machine learning in health care.","Big data and machine learning algorithms for health-care delivery. Analysis of big data by machine learning offers considerable advantages for assimilation and evaluation of large amounts of complex health-care data. However, to effectively use machine learning tools in health care, several limitations must be addressed and key issues considered, such as its clinical implementation and ethics in health-care delivery. Advantages of machine learning include flexibility and scalability compared with traditional biostatistical methods, which makes it deployable for many tasks, such as risk stratification, diagnosis and classification, and survival predictions. Another advantage of machine learning algorithms is the ability to analyse diverse data types (eg, demographic data, laboratory findings, imaging data, and doctors' free-text notes) and incorporate them into predictions for disease risk, diagnosis, prognosis, and appropriate treatments. Despite these advantages, the application of machine learning in health-care delivery also presents unique challenges that require data pre-processing, model training, and refinement of the system with respect to the actual clinical problem. Also crucial are ethical considerations, which include medico-legal implications, doctors' understanding of machine learning tools, and data privacy and security. In this Review, we discuss some of the benefits and challenges of big data and machine learning in health care."
0,Reaching an evidence-based prognosis for personalized treatment of multiple sclerosis,,
0,LI-RADS Treatment Response Algorithm: Performance and Diagnostic Accuracy,"Background In 2017, the Liver Imaging Reporting and Data System (LI-RADS) included an algorithm for the assessment of hepatocellular carcinoma (HCC) treated with local-regional therapy. The aim of the algorithm was to enable standardized evaluation of treatment response to guide subsequent therapy. However, the performance of the algorithm has not yet been validated in the literature. Purpose To evaluate the performance of the LI-RADS 2017 Treatment Response algorithm for assessing the histopathologic viability of HCC treated with bland arterial embolization. Materials and Methods This retrospective study included patients who underwent bland arterial embolization for HCC between 2006 and 2016 and subsequent liver transplantation. Three radiologists independently assessed all treated lesions by using the CT/MRI LI-RADS 2017 Treatment Response algorithm. Radiology and posttransplant histopathology reports were then compared. Lesions were categorized on the basis of explant pathologic findings as either completely (100%) or incompletely (<100%) necrotic, and performance characteristics and predictive values for the LI-RADS Treatment Response (LR-TR) Viable and Nonviable categories were calculated for each reader. Interreader association was calculated by using the Fleiss kappa. Results A total of 45 adults (mean age, 57.1 years +/- 8.2; 13 women) with 63 total lesions were included. For predicting incomplete histopathologic tumor necrosis, the accuracy of the LR-TR Viable category for the three readers was 60%-65%, and the positive predictive value was 86%-96%. For predicting complete histopathologic tumor necrosis, the accuracy of the LR-TR Nonviable category was 67%-71%, and the negative predictive value was 81%-87%. By consensus, 17 (27%) of 63 lesions were categorized as LR-TR Equivocal, and 12 of these lesions were incompletely necrotic. Interreader association for the LR-TR category was moderate (kappa = 0.55; 95% confidence interval: 0.47, 0.67). Conclusion The Liver Imaging Reporting and Data System 2017 Treatment Response algorithm had high predictive value and moderate interreader association for the histopathologic viability of hepatocellular carcinoma treated with bland arterial embolization when lesions were assessed as Viable or Nonviable. (c) RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Gervais in this issue.","LI-RADS Treatment Response Algorithm: Performance and Diagnostic Accuracy. Background In 2017, the Liver Imaging Reporting and Data System (LI-RADS) included an algorithm for the assessment of hepatocellular carcinoma (HCC) treated with local-regional therapy. The aim of the algorithm was to enable standardized evaluation of treatment response to guide subsequent therapy. However, the performance of the algorithm has not yet been validated in the literature. Purpose To evaluate the performance of the LI-RADS 2017 Treatment Response algorithm for assessing the histopathologic viability of HCC treated with bland arterial embolization. Materials and Methods This retrospective study included patients who underwent bland arterial embolization for HCC between 2006 and 2016 and subsequent liver transplantation. Three radiologists independently assessed all treated lesions by using the CT/MRI LI-RADS 2017 Treatment Response algorithm. Radiology and posttransplant histopathology reports were then compared. Lesions were categorized on the basis of explant pathologic findings as either completely (100%) or incompletely (<100%) necrotic, and performance characteristics and predictive values for the LI-RADS Treatment Response (LR-TR) Viable and Nonviable categories were calculated for each reader. Interreader association was calculated by using the Fleiss kappa. Results A total of 45 adults (mean age, 57.1 years +/- 8.2; 13 women) with 63 total lesions were included. For predicting incomplete histopathologic tumor necrosis, the accuracy of the LR-TR Viable category for the three readers was 60%-65%, and the positive predictive value was 86%-96%. For predicting complete histopathologic tumor necrosis, the accuracy of the LR-TR Nonviable category was 67%-71%, and the negative predictive value was 81%-87%. By consensus, 17 (27%) of 63 lesions were categorized as LR-TR Equivocal, and 12 of these lesions were incompletely necrotic. Interreader association for the LR-TR category was moderate (kappa = 0.55; 95% confidence interval: 0.47, 0.67). Conclusion The Liver Imaging Reporting and Data System 2017 Treatment Response algorithm had high predictive value and moderate interreader association for the histopathologic viability of hepatocellular carcinoma treated with bland arterial embolization when lesions were assessed as Viable or Nonviable. (c) RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Gervais in this issue."
0,Targeted protein degradation: expanding the toolbox,"Proteolysis-targeting chimeras (PROTACs) and related molecules that induce targeted protein degradation by the ubiquitinâ€“proteasome system represent a new therapeutic modality and are the focus of great interest, owing to potential advantages over traditional occupancy-based inhibitors with respect to dosing, side effects, drug resistance and modulating â€˜undruggableâ€™ targets. However, the technology is still maturing, and the design elements for successful PROTAC-based drugs are currently being elucidated. Importantly, fewer than 10 of the more than 600 E3 ubiquitin ligases have so far been exploited for targeted protein degradation, and expansion of knowledge in this area is a key opportunity. Here, we briefly discuss lessons learned about targeted protein degradation in chemical biology and drug discovery and systematically review the expression profile, domain architecture and chemical tractability of human E3 ligases that could expand the toolbox for PROTAC discovery.","Targeted protein degradation: expanding the toolbox. Proteolysis-targeting chimeras (PROTACs) and related molecules that induce targeted protein degradation by the ubiquitinâ€“proteasome system represent a new therapeutic modality and are the focus of great interest, owing to potential advantages over traditional occupancy-based inhibitors with respect to dosing, side effects, drug resistance and modulating â€˜undruggableâ€™ targets. However, the technology is still maturing, and the design elements for successful PROTAC-based drugs are currently being elucidated. Importantly, fewer than 10 of the more than 600 E3 ubiquitin ligases have so far been exploited for targeted protein degradation, and expansion of knowledge in this area is a key opportunity. Here, we briefly discuss lessons learned about targeted protein degradation in chemical biology and drug discovery and systematically review the expression profile, domain architecture and chemical tractability of human E3 ligases that could expand the toolbox for PROTAC discovery."
0,Automatic graph-based method for localization of cochlear implant electrode arrays in clinical CT with sub-voxel accuracy,"Cochlear implants (CIs) are neural prosthetics that provide a sense of sound to people who experience severe to profound hearing loss. Recent studies have demonstrated a correlation between hearing outcomes and intra-cochlear locations of CI electrodes. Our group has been conducting investigations on this correlation and has been developing an image-guided cochlear implant programming (IGCIP) system to program CI devices to improve hearing outcomes. One crucial step that has not been automated in IGCIP is the localization of CI electrodes in clinical CTs. Existing methods for CI electrode localization do not generalize well on large-scale datasets of clinical CTs implanted with different brands of CI arrays. In this paper, we propose a novel method for localizing different brands of CI electrodes in clinical CTs. We firstly generate the candidate electrode positions at sub-voxel resolution in a whole head CT by thresholding an up-sampled feature image and voxel-thinning the result. Then, we use a graph-based path-finding algorithm to find a fixed-length path that consists of a subset of the candidates as the localization result. Validation on a large-scale dataset of clinical CTs shows that our proposed method outperforms the state-of-art CI electrode localization methods and achieves a mean error of 0.12mm when compared to expert manual localization results. This represents a crucial step in translating IGCIP from the laboratory to large-scale clinical use.","Automatic graph-based method for localization of cochlear implant electrode arrays in clinical CT with sub-voxel accuracy. Cochlear implants (CIs) are neural prosthetics that provide a sense of sound to people who experience severe to profound hearing loss. Recent studies have demonstrated a correlation between hearing outcomes and intra-cochlear locations of CI electrodes. Our group has been conducting investigations on this correlation and has been developing an image-guided cochlear implant programming (IGCIP) system to program CI devices to improve hearing outcomes. One crucial step that has not been automated in IGCIP is the localization of CI electrodes in clinical CTs. Existing methods for CI electrode localization do not generalize well on large-scale datasets of clinical CTs implanted with different brands of CI arrays. In this paper, we propose a novel method for localizing different brands of CI electrodes in clinical CTs. We firstly generate the candidate electrode positions at sub-voxel resolution in a whole head CT by thresholding an up-sampled feature image and voxel-thinning the result. Then, we use a graph-based path-finding algorithm to find a fixed-length path that consists of a subset of the candidates as the localization result. Validation on a large-scale dataset of clinical CTs shows that our proposed method outperforms the state-of-art CI electrode localization methods and achieves a mean error of 0.12mm when compared to expert manual localization results. This represents a crucial step in translating IGCIP from the laboratory to large-scale clinical use."
0,A deep learning model to predict a diagnosis of Alzheimer disease by using18F-FDG PET of the brain,"Purpose: To develop and validate a deep learning algorithm that predicts the final diagnosis of Alzheimer disease (AD), mild cognitive impairment, or neither at fluorine 18 (18F) fluorodeoxyglucose (FDG) PET of the brain and compare its performance to that of radiologic readers. Materials and Methods: Prospective18F-FDG PET brain images from the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) (2109 imaging studies from 2005 to 2017, 1002 patients) and retrospective independent test set (40 imaging studies from 2006 to 2016, 40 patients) were collected. Final clinical diagnosis at follow-up was recorded. Convolutional neural network of InceptionV3 architecture was trained on 90% of ADNI data set and tested on the remaining 10%, as well as the independent test set, with performance compared to radiologic readers. Model was analyzed with sensitivity, specificity, receiver operating characteristic (ROC), saliency map, and t-distributed stochastic neighbor embedding. Results: The algorithm achieved area under the ROC curve of 0.98 (95% confidence interval: 0.94, 1.00) when evaluated on predicting the final clinical diagnosis of AD in the independent test set (82% specificity at 100% sensitivity), an average of 75.8 months prior to the final diagnosis, which in ROC space outperformed reader performance (57% [four of seven] sensitivity, 91% [30 of 33] specificity; P , .05). Saliency map demonstrated attention to known areas of interest but with focus on the entire brain. Conclusion: By using fluorine 18 fluorodeoxyglucose PET of the brain, a deep learning algorithm developed for early prediction of Alzheimer disease achieved 82% specificity at 100% sensitivity, an average of 75.8 months prior to the final diagnosis.","A deep learning model to predict a diagnosis of Alzheimer disease by using18F-FDG PET of the brain. Purpose: To develop and validate a deep learning algorithm that predicts the final diagnosis of Alzheimer disease (AD), mild cognitive impairment, or neither at fluorine 18 (18F) fluorodeoxyglucose (FDG) PET of the brain and compare its performance to that of radiologic readers. Materials and Methods: Prospective18F-FDG PET brain images from the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) (2109 imaging studies from 2005 to 2017, 1002 patients) and retrospective independent test set (40 imaging studies from 2006 to 2016, 40 patients) were collected. Final clinical diagnosis at follow-up was recorded. Convolutional neural network of InceptionV3 architecture was trained on 90% of ADNI data set and tested on the remaining 10%, as well as the independent test set, with performance compared to radiologic readers. Model was analyzed with sensitivity, specificity, receiver operating characteristic (ROC), saliency map, and t-distributed stochastic neighbor embedding. Results: The algorithm achieved area under the ROC curve of 0.98 (95% confidence interval: 0.94, 1.00) when evaluated on predicting the final clinical diagnosis of AD in the independent test set (82% specificity at 100% sensitivity), an average of 75.8 months prior to the final diagnosis, which in ROC space outperformed reader performance (57% [four of seven] sensitivity, 91% [30 of 33] specificity; P , .05). Saliency map demonstrated attention to known areas of interest but with focus on the entire brain. Conclusion: By using fluorine 18 fluorodeoxyglucose PET of the brain, a deep learning algorithm developed for early prediction of Alzheimer disease achieved 82% specificity at 100% sensitivity, an average of 75.8 months prior to the final diagnosis."
0,Randomized Phase II Trial Comparing Site-Specific Treatment Based on Gene Expression Profiling With Carboplatin and Paclitaxel for Patients With Cancer of Unknown Primary Site,,
0,Stress resilience is promoted by a Zfp189-driven transcriptional network in prefrontal cortex,,
0,Secondary metabolite as therapeutic agent from endophytic fungi Alternaria longipes strain VITN14G of mangrove plant Avicennia officinalis,"Endophytic fungi, especially from mangrove plants, are rich source of secondary metabolites, which plays a major role in various pharmacological actions preferably in cancer and bacterial infections. To perceive its role in antidiabetic activity we isolated and tested the metabolites derived from a novel strain Alternaria longipes strain VITN14G obtained from mangrove plant Avicennia officinalis. The crude extract was analyzed for antidiabetic activity and subjected to column chromatography. The isolated fractions were screened in vitro for Î±-glucosidase and Î±-amylase inhibitory activities. The cytotoxicity of the isolated fractions was studied on L929 cell lines. Following which, the screened fraction 2 was allowed for structure elucidation using gas chromatography-mass spectrometry, one-dimensional, two-dimensionalÂ nuclear magnetic resonance spectroscopy, ultraviolet, and Fourier-transform infrared analysis. The binding energies of the isolated fraction 2 with glycolytic enzymes were calculated by molecular docking studies using AutoDock Vina. The isolated fraction 2 identified as 2,4,6-triphenylaniline, showed no significant difference in Î±-amylase inhibition rates and a significant difference of 10% in Î±-glucosidase inhibition rates than that of the standard drug acarbose. Further, the cytotoxicity assay of the isolated fraction 2 resulted in a cell viability of 73.96%. Supportingly, in silico studies showed 2,4,6-triphenylaniline to produce a stronger binding affinity toward the glycolytic enzyme targets. The compound 2,4,6-triphenylaniline isolated from A. longipes strain VITN14G exhibited satisfactory antidiabetic activity for type 2 diabetes in vitro, which will further be confirmed by in vivo studies. Successful outcome of the study will result in a natural substitute for existing synthetic antidiabetic drugs.","Secondary metabolite as therapeutic agent from endophytic fungi Alternaria longipes strain VITN14G of mangrove plant Avicennia officinalis. Endophytic fungi, especially from mangrove plants, are rich source of secondary metabolites, which plays a major role in various pharmacological actions preferably in cancer and bacterial infections. To perceive its role in antidiabetic activity we isolated and tested the metabolites derived from a novel strain Alternaria longipes strain VITN14G obtained from mangrove plant Avicennia officinalis. The crude extract was analyzed for antidiabetic activity and subjected to column chromatography. The isolated fractions were screened in vitro for Î±-glucosidase and Î±-amylase inhibitory activities. The cytotoxicity of the isolated fractions was studied on L929 cell lines. Following which, the screened fraction 2 was allowed for structure elucidation using gas chromatography-mass spectrometry, one-dimensional, two-dimensionalÂ nuclear magnetic resonance spectroscopy, ultraviolet, and Fourier-transform infrared analysis. The binding energies of the isolated fraction 2 with glycolytic enzymes were calculated by molecular docking studies using AutoDock Vina. The isolated fraction 2 identified as 2,4,6-triphenylaniline, showed no significant difference in Î±-amylase inhibition rates and a significant difference of 10% in Î±-glucosidase inhibition rates than that of the standard drug acarbose. Further, the cytotoxicity assay of the isolated fraction 2 resulted in a cell viability of 73.96%. Supportingly, in silico studies showed 2,4,6-triphenylaniline to produce a stronger binding affinity toward the glycolytic enzyme targets. The compound 2,4,6-triphenylaniline isolated from A. longipes strain VITN14G exhibited satisfactory antidiabetic activity for type 2 diabetes in vitro, which will further be confirmed by in vivo studies. Successful outcome of the study will result in a natural substitute for existing synthetic antidiabetic drugs."
0,The Kidney Failure Risk Equation for prediction of end stage renal disease in UK primary care: An external validation and clinical impact projection cohort study,"BACKGROUND: The Kidney Failure Risk Equation (KFRE) uses the 4 variables of age, sex, urine albumin-to-creatinine ratio (ACR), and estimated glomerular filtration rate (eGFR) in individuals with chronic kidney disease (CKD) to predict the risk of end stage renal disease (ESRD), i.e., the need for dialysis or a kidney transplant, within 2 and 5 years. Currently, national guideline writers in the UK and other countries are evaluating the role of the KFRE in renal referrals from primary care to secondary care, but the KFRE has had limited external validation in primary care. The study's objectives were therefore to externally validate the KFRE's prediction of ESRD events in primary care, perform model recalibration if necessary, and assess its projected impact on referral rates to secondary care renal services. METHODS AND FINDINGS: Individuals with 2 or more Chronic Kidney Disease Epidemiology Collaboration (CKD-EPI) eGFR values < 60 ml/min/1.73 m2 more than 90 days apart and a urine ACR or protein-to-creatinine ratio measurement between 1 December 2004 and 1 November 2016 were included in the cohort. The cohort included 35,539 (5.6%) individuals (57.5% female, mean age 75.9 years, median CKD-EPI eGFR 51 ml/min/1.73 m2, median ACR 3.2 mg/mmol) from a total adult practice population of 630,504. Overall, 176 (0.50%) and 429 (1.21%) ESRD events occurred within 2 and 5 years, respectively. Median length of follow-up was 4.7 years (IQR 2.8 to 6.6). Model discrimination was excellent for both 2-year (C-statistic 0.932, 95% CI 0.909 to 0.954) and 5-year (C-statistic 0.924, 95% 0.909 to 0.938) ESRD prediction. The KFRE overpredicted risk in lower (<20%) risk groups. Reducing the model's baseline risk improved calibration for both 2- and 5-year risk for lower risk groups, but led to some underprediction of risk in higher risk groups. Compared to current criteria, using referral criteria based on a KFRE-calculated 5-year ESRD risk of >/=5% and/or an ACR of >/=70 mg/mmol reduced the number of individuals eligible for referral who did not develop ESRD, increased the likelihood of referral eligibility in those who did develop ESRD, and referred the latter at a younger age and with a higher eGFR. The main limitation of the current study is that the cohort is from one region of the UK and therefore may not be representative of primary care CKD care in other countries. CONCLUSIONS: In this cohort, the recalibrated KFRE accurately predicted the risk of ESRD at 2 and 5 years in primary care. Its introduction into primary care for referrals to secondary care renal services may lead to a reduction in unnecessary referrals, and earlier referrals in those who go on to develop ESRD. However, further validation studies in more diverse cohorts of the clinical impact projections and suggested referral criteria are required before the latter can be clinically implemented.","The Kidney Failure Risk Equation for prediction of end stage renal disease in UK primary care: An external validation and clinical impact projection cohort study. BACKGROUND: The Kidney Failure Risk Equation (KFRE) uses the 4 variables of age, sex, urine albumin-to-creatinine ratio (ACR), and estimated glomerular filtration rate (eGFR) in individuals with chronic kidney disease (CKD) to predict the risk of end stage renal disease (ESRD), i.e., the need for dialysis or a kidney transplant, within 2 and 5 years. Currently, national guideline writers in the UK and other countries are evaluating the role of the KFRE in renal referrals from primary care to secondary care, but the KFRE has had limited external validation in primary care. The study's objectives were therefore to externally validate the KFRE's prediction of ESRD events in primary care, perform model recalibration if necessary, and assess its projected impact on referral rates to secondary care renal services. METHODS AND FINDINGS: Individuals with 2 or more Chronic Kidney Disease Epidemiology Collaboration (CKD-EPI) eGFR values < 60 ml/min/1.73 m2 more than 90 days apart and a urine ACR or protein-to-creatinine ratio measurement between 1 December 2004 and 1 November 2016 were included in the cohort. The cohort included 35,539 (5.6%) individuals (57.5% female, mean age 75.9 years, median CKD-EPI eGFR 51 ml/min/1.73 m2, median ACR 3.2 mg/mmol) from a total adult practice population of 630,504. Overall, 176 (0.50%) and 429 (1.21%) ESRD events occurred within 2 and 5 years, respectively. Median length of follow-up was 4.7 years (IQR 2.8 to 6.6). Model discrimination was excellent for both 2-year (C-statistic 0.932, 95% CI 0.909 to 0.954) and 5-year (C-statistic 0.924, 95% 0.909 to 0.938) ESRD prediction. The KFRE overpredicted risk in lower (<20%) risk groups. Reducing the model's baseline risk improved calibration for both 2- and 5-year risk for lower risk groups, but led to some underprediction of risk in higher risk groups. Compared to current criteria, using referral criteria based on a KFRE-calculated 5-year ESRD risk of >/=5% and/or an ACR of >/=70 mg/mmol reduced the number of individuals eligible for referral who did not develop ESRD, increased the likelihood of referral eligibility in those who did develop ESRD, and referred the latter at a younger age and with a higher eGFR. The main limitation of the current study is that the cohort is from one region of the UK and therefore may not be representative of primary care CKD care in other countries. CONCLUSIONS: In this cohort, the recalibrated KFRE accurately predicted the risk of ESRD at 2 and 5 years in primary care. Its introduction into primary care for referrals to secondary care renal services may lead to a reduction in unnecessary referrals, and earlier referrals in those who go on to develop ESRD. However, further validation studies in more diverse cohorts of the clinical impact projections and suggested referral criteria are required before the latter can be clinically implemented."
0,Next-generation sequencing in Charcot-Marie-Tooth disease: opportunities and challenges,,
0,A single centre prospective cohort study addressing the effect of a rule-in/rule-out troponin algorithm on routine clinical practice,"Aims: In 2015, the European Society of Cardiology introduced new guidelines for the diagnosis of acute coronary syndromes in patients presenting without persistent ST-segment elevation. These guidelines included the use of high-sensitivity troponin assays for â€˜rule-inâ€™ and â€˜rule-outâ€™ of acute myocardial injury at presentation (using a â€˜0 hourâ€™ blood test). Whilst these algorithms have been extensively validated in prospective diagnostic studies, the outcome of their implementation in routine clinical practice has not been described. The present study describes the change in the patient journey resulting from implementation of such an algorithm in a busy innercity Emergency Department. Methods and results: Data were prospectively collected from electronic records at a large Central London hospital over seven months spanning the periods before, during and after the introduction of a new high-sensitivity troponin rapid diagnostic algorithm modelled on the European Society of Cardiology guideline. Over 213 days, 4644 patients had high-sensitivity troponin T measured in the Emergency Department. Of these patients, 40.4% could be â€˜ruled-outâ€™ based on the high-sensitivity troponin T concentration at presentation, whilst 7.6% could be â€˜ruled-inâ€™. Adoption of the algorithm into clinical practice was associated with a 37.5% increase of repeat high-sensitivity troponin T measurements within 1.5 h for those patients classified as â€˜intermediate riskâ€™ on presentation. Conclusions: Introduction of a 0 hour â€˜rule-inâ€™ and â€˜rule-outâ€™ algorithm in routine clinical practice enables rapid triage of 48% of patients, and is associated with more rapid repeat testing in intermediate risk patients.","A single centre prospective cohort study addressing the effect of a rule-in/rule-out troponin algorithm on routine clinical practice. Aims: In 2015, the European Society of Cardiology introduced new guidelines for the diagnosis of acute coronary syndromes in patients presenting without persistent ST-segment elevation. These guidelines included the use of high-sensitivity troponin assays for â€˜rule-inâ€™ and â€˜rule-outâ€™ of acute myocardial injury at presentation (using a â€˜0 hourâ€™ blood test). Whilst these algorithms have been extensively validated in prospective diagnostic studies, the outcome of their implementation in routine clinical practice has not been described. The present study describes the change in the patient journey resulting from implementation of such an algorithm in a busy innercity Emergency Department. Methods and results: Data were prospectively collected from electronic records at a large Central London hospital over seven months spanning the periods before, during and after the introduction of a new high-sensitivity troponin rapid diagnostic algorithm modelled on the European Society of Cardiology guideline. Over 213 days, 4644 patients had high-sensitivity troponin T measured in the Emergency Department. Of these patients, 40.4% could be â€˜ruled-outâ€™ based on the high-sensitivity troponin T concentration at presentation, whilst 7.6% could be â€˜ruled-inâ€™. Adoption of the algorithm into clinical practice was associated with a 37.5% increase of repeat high-sensitivity troponin T measurements within 1.5 h for those patients classified as â€˜intermediate riskâ€™ on presentation. Conclusions: Introduction of a 0 hour â€˜rule-inâ€™ and â€˜rule-outâ€™ algorithm in routine clinical practice enables rapid triage of 48% of patients, and is associated with more rapid repeat testing in intermediate risk patients."
0,Can personalized use of NSAIDs be a reality in the clinic?,"The use of NSAIDs in rheumatology could be improved by an appropriate risk scoring system that accounts for adverse events such as bleeding and thrombosis. Such a risk score has now been developed using data from the PRECISION trial, but is this score ready to be applied in clinical practice?","Can personalized use of NSAIDs be a reality in the clinic?. The use of NSAIDs in rheumatology could be improved by an appropriate risk scoring system that accounts for adverse events such as bleeding and thrombosis. Such a risk score has now been developed using data from the PRECISION trial, but is this score ready to be applied in clinical practice?"
0,Patch-based adaptive weighting with segmentation and scale (PAWSS) for visual tracking in surgical video,,
0,Robotic-assisted training after stroke: RATULS advances science,,
0,Matrine attenuates high-fat diet-induced in vivo and ox-LDL-induced in vitro vascular injury by regulating the PKCÎ±/eNOS and PI3K/Akt/eNOS pathways,"Lipid metabolism disorders lead to vascular endothelial injury. Matrine is an alkaloid that has been used to improve obesity and diabetes and for the treatment of hepatitis B. However, its effect on lipid metabolism disorders and vascular injury is unclear. Here, we investigated the effect of matrine on high-fat diet fed mice and oxidized low-density lipoprotein (ox-LDL)-induced human umbilical vein endothelial cells (HUVECs). Computational virtual docking analyses, phosphoinositide 3-kinase (PI3K) and protein kinase C-Î± (PKCÎ±) inhibitors were used to localize matrine in vascular injuries. The results showed that matrine-treated mice were more resistant to abnormal lipid metabolism and inflammation than vehicle-treated mice and exhibited significantly alleviated ox-LDL-stimulated dysfunction of HUVECs, restored diminished nitric oxide release, decreased reactive oxygen species generation and increased expression phosphorylation of AKT-Ser473 and endothelial nitric oxide synthase (eNOS)-Ser1177. Matrine not only up-regulates eNOS-Ser1177 but also down-regulates eNOS-Thr495, a PKCÎ±-controlled negative regulator of eNOS. Using computational virtual docking analyses and biochemical assays, matrine was also shown to influence eNOS/NO via PKCÎ± inhibition. Moreover, the protective effects of matrine were significantly abolished by the simultaneous application of PKCÎ± and the PI3K inhibitor. Matrine may thus be potentially employed as a novel therapeutic strategy against high-fat diet-induced vascular injury.","Matrine attenuates high-fat diet-induced in vivo and ox-LDL-induced in vitro vascular injury by regulating the PKCÎ±/eNOS and PI3K/Akt/eNOS pathways. Lipid metabolism disorders lead to vascular endothelial injury. Matrine is an alkaloid that has been used to improve obesity and diabetes and for the treatment of hepatitis B. However, its effect on lipid metabolism disorders and vascular injury is unclear. Here, we investigated the effect of matrine on high-fat diet fed mice and oxidized low-density lipoprotein (ox-LDL)-induced human umbilical vein endothelial cells (HUVECs). Computational virtual docking analyses, phosphoinositide 3-kinase (PI3K) and protein kinase C-Î± (PKCÎ±) inhibitors were used to localize matrine in vascular injuries. The results showed that matrine-treated mice were more resistant to abnormal lipid metabolism and inflammation than vehicle-treated mice and exhibited significantly alleviated ox-LDL-stimulated dysfunction of HUVECs, restored diminished nitric oxide release, decreased reactive oxygen species generation and increased expression phosphorylation of AKT-Ser473 and endothelial nitric oxide synthase (eNOS)-Ser1177. Matrine not only up-regulates eNOS-Ser1177 but also down-regulates eNOS-Thr495, a PKCÎ±-controlled negative regulator of eNOS. Using computational virtual docking analyses and biochemical assays, matrine was also shown to influence eNOS/NO via PKCÎ± inhibition. Moreover, the protective effects of matrine were significantly abolished by the simultaneous application of PKCÎ± and the PI3K inhibitor. Matrine may thus be potentially employed as a novel therapeutic strategy against high-fat diet-induced vascular injury."
0,A novel optical tissue clearing protocol for mouse skeletal muscle to visualize endplates in their tissue context,"Neuromuscular junctions (NMJs) mediate skeletal muscle contractions and play an important role in several neuromuscular disorders when their morphology and function are compromised. However, due to their small size and sparse distribution throughout the comparatively large, inherently opaque muscle tissue the analysis of NMJ morphology has been limited to teased fiber preparations, longitudinal muscle sections, and flat muscles. Consequently, whole mount analyses of NMJ morphology, numbers, their distribution, and assignment to a given muscle fiber have also been impossible to determine in muscle types that are frequently used in experimental paradigms. This impossibility is exacerbated by the lack of optical tissue clearing techniques that are compatible with clear and persistent NMJ stains. Here, we present MYOCLEAR, a novel and highly reproducible muscle tissue clearing protocol. Based on hydrogel-based tissue clearing methods, this protocol permits the labeling and detection of all NMJs in adult hindleg extensor digitorum longus muscles from wildtype and diseased mice. The method is also applicable to adult mouse diaphragm muscles and can be used for different staining agents, including toxins, lectins, antibodies, and nuclear dyes. It will be useful in understanding the distribution, morphological features, and muscle tissue context of NMJs in hindleg muscle whole mounts for biomedical and basic research.","A novel optical tissue clearing protocol for mouse skeletal muscle to visualize endplates in their tissue context. Neuromuscular junctions (NMJs) mediate skeletal muscle contractions and play an important role in several neuromuscular disorders when their morphology and function are compromised. However, due to their small size and sparse distribution throughout the comparatively large, inherently opaque muscle tissue the analysis of NMJ morphology has been limited to teased fiber preparations, longitudinal muscle sections, and flat muscles. Consequently, whole mount analyses of NMJ morphology, numbers, their distribution, and assignment to a given muscle fiber have also been impossible to determine in muscle types that are frequently used in experimental paradigms. This impossibility is exacerbated by the lack of optical tissue clearing techniques that are compatible with clear and persistent NMJ stains. Here, we present MYOCLEAR, a novel and highly reproducible muscle tissue clearing protocol. Based on hydrogel-based tissue clearing methods, this protocol permits the labeling and detection of all NMJs in adult hindleg extensor digitorum longus muscles from wildtype and diseased mice. The method is also applicable to adult mouse diaphragm muscles and can be used for different staining agents, including toxins, lectins, antibodies, and nuclear dyes. It will be useful in understanding the distribution, morphological features, and muscle tissue context of NMJs in hindleg muscle whole mounts for biomedical and basic research."
0,A network clustering based feature selection strategy for classifying autism spectrum disorder,"Background: Advanced non-invasive neuroimaging techniques offer new approaches to study functions and structures of human brains. Whole-brain functional networks obtained from resting state functional magnetic resonance imaging has been widely used to study brain diseases like autism spectrum disorder (ASD). Auto-classification of ASD has become an important issue. Existing classification methods for ASD are based on features extracted from the whole-brain functional networks, which may be not discriminant enough for good performance. Methods: In this study, we propose a network clustering based feature selection strategy for classifying ASD. In our proposed method, we first apply symmetric non-negative matrix factorization to divide brain networks into four modules. Then we extract features from one of four modules called default mode network (DMN) and use them to train several classifiers for ASD classification. Results: The computational experiments show that our proposed method achieves better performances than those trained with features extracted from the whole brain network. Conclusion: It is a good strategy to train the classifiers for ASD based on features from the default mode subnetwork.","A network clustering based feature selection strategy for classifying autism spectrum disorder. Background: Advanced non-invasive neuroimaging techniques offer new approaches to study functions and structures of human brains. Whole-brain functional networks obtained from resting state functional magnetic resonance imaging has been widely used to study brain diseases like autism spectrum disorder (ASD). Auto-classification of ASD has become an important issue. Existing classification methods for ASD are based on features extracted from the whole-brain functional networks, which may be not discriminant enough for good performance. Methods: In this study, we propose a network clustering based feature selection strategy for classifying ASD. In our proposed method, we first apply symmetric non-negative matrix factorization to divide brain networks into four modules. Then we extract features from one of four modules called default mode network (DMN) and use them to train several classifiers for ASD classification. Results: The computational experiments show that our proposed method achieves better performances than those trained with features extracted from the whole brain network. Conclusion: It is a good strategy to train the classifiers for ASD based on features from the default mode subnetwork."
0,A clinical algorithm to diagnose differences of sex development,"The diagnosis and management of children born with ambiguous genitalia is challenging for clinicians. Such differences of sex development (DSDs) are congenital conditions in which chromosomal, gonadal, or anatomical sex is atypical. The aetiology of DSDs is very heterogenous and a precise diagnosis is essential for management of genetic, endocrine, surgical, reproductive, and psychosocial issues. In this Review, we outline a step-by-step approach, compiled in a diagnostic algorithm, for the clinical assessment and molecular diagnosis of a patient with ambiguity of the external genitalia on initial presentation. We appraise established and emerging technologies and their effect on diagnosis, and discuss current controversies.","A clinical algorithm to diagnose differences of sex development. The diagnosis and management of children born with ambiguous genitalia is challenging for clinicians. Such differences of sex development (DSDs) are congenital conditions in which chromosomal, gonadal, or anatomical sex is atypical. The aetiology of DSDs is very heterogenous and a precise diagnosis is essential for management of genetic, endocrine, surgical, reproductive, and psychosocial issues. In this Review, we outline a step-by-step approach, compiled in a diagnostic algorithm, for the clinical assessment and molecular diagnosis of a patient with ambiguity of the external genitalia on initial presentation. We appraise established and emerging technologies and their effect on diagnosis, and discuss current controversies."
0,Reply,,
0,Mechanisms and diagnostic evaluation of persistent or recurrent angina following percutaneous coronary revascularization,,
0,Pyridine derivatives as anticancer lead compounds with Fatty Acid Synthase as the target: An in silico-guided in vitro study,"For the past few decades, structure-based drug discovery (SBDD) has become an inevitable technique in the drug development process for screening hit compounds against therapeutic targets. Here, we have successfully used the SBDD approach viz. virtual high-throughput screening to identify potential inhibitors against the Ketoacyl synthase (KS) domain of Fatty acid synthase (FASN). Overexpression of FASN, and subsequent enhancement of de novo lipogenesis is a key survival strategy of cancer cells. Hence, targeting lipid metabolism using FASN inhibitors has been considered as a promising method to induce metabolic stress, thereby posing a survival disadvantage to cancer cells. In the present study, we have successfully identified eight FASN inhibitors from Asinex Elite database by implementing in silico tools. Five of the hit compounds share a common ring structure, which enables characteristic binding interactions with FASN-KS. Among them, in vitro validation showed that SFA 22637550 possesses significant FASN inhibitory activity and antiproliferative effect in human cancer cells of various origins. The maximum sensitivity was exhibited towards HepG2 hepatocellular carcinoma cells (IC50 = 28 ÂµM). The mode of cell death was found to be apoptosis with a significant increase in SubG0 population without affecting any other phases of the cell cycle. The current study puts forward an excellent core structure for the development of potent FASN inhibitors for successfully targeting cancer cell metabolism, thereby causing selective cell death.","Pyridine derivatives as anticancer lead compounds with Fatty Acid Synthase as the target: An in silico-guided in vitro study. For the past few decades, structure-based drug discovery (SBDD) has become an inevitable technique in the drug development process for screening hit compounds against therapeutic targets. Here, we have successfully used the SBDD approach viz. virtual high-throughput screening to identify potential inhibitors against the Ketoacyl synthase (KS) domain of Fatty acid synthase (FASN). Overexpression of FASN, and subsequent enhancement of de novo lipogenesis is a key survival strategy of cancer cells. Hence, targeting lipid metabolism using FASN inhibitors has been considered as a promising method to induce metabolic stress, thereby posing a survival disadvantage to cancer cells. In the present study, we have successfully identified eight FASN inhibitors from Asinex Elite database by implementing in silico tools. Five of the hit compounds share a common ring structure, which enables characteristic binding interactions with FASN-KS. Among them, in vitro validation showed that SFA 22637550 possesses significant FASN inhibitory activity and antiproliferative effect in human cancer cells of various origins. The maximum sensitivity was exhibited towards HepG2 hepatocellular carcinoma cells (IC50 = 28 ÂµM). The mode of cell death was found to be apoptosis with a significant increase in SubG0 population without affecting any other phases of the cell cycle. The current study puts forward an excellent core structure for the development of potent FASN inhibitors for successfully targeting cancer cell metabolism, thereby causing selective cell death."
0,Predictors of perioperative complications in paediatric cranial vault reconstruction surgery: a multicentre observational study from the Pediatric Craniofacial Collaborative Group,"BACKGROUND: The current incidence of major complications in paediatric craniofacial surgery in North America has not been accurately defined. In this report, the Pediatric Craniofacial Collaborative Group evaluates the incidence and determines the independent predictors of major perioperative complications using a multicentre database. METHODS: The Pediatric Craniofacial Surgery Perioperative Registry was queried for subjects undergoing complex cranial vault reconstruction surgery over a 5-year period. Major perioperative complications were identified through a structured a priori consensus process. Logistic regression was applied to identify predictors of a major perioperative complication with bootstrapping to evaluate discrimination accuracy and provide internal validity of the multivariable model. RESULTS: A total of 1814 patients from 33 institutions in the US and Canada were analysed; 15% were reported to have a major perioperative complication. Multivariable predictors included ASA physical status 3 or 4 (P=0.005), craniofacial syndrome (P=0.008), antifibrinolytic administered (P=0.003), blood product transfusion >50 ml kg(-1) (P<0.001), and surgery duration over 5 h (P<0.001). Bootstrapping indicated that the predictive algorithm had good internal validity and excellent discrimination and model performance. A perioperative complication was estimated to increase the hospital length of stay by an average of 3 days (P<0.001). CONCLUSIONS: The predictive algorithm can be used as a prognostic tool to risk stratify patients and thereby potentially reduce morbidity and mortality. Craniofacial teams can utilise these predictors of complications to identify high-risk patients. Based on this information, further prospective quality improvement initiatives may decrease complications, and reduce morbidity and mortality.","Predictors of perioperative complications in paediatric cranial vault reconstruction surgery: a multicentre observational study from the Pediatric Craniofacial Collaborative Group. BACKGROUND: The current incidence of major complications in paediatric craniofacial surgery in North America has not been accurately defined. In this report, the Pediatric Craniofacial Collaborative Group evaluates the incidence and determines the independent predictors of major perioperative complications using a multicentre database. METHODS: The Pediatric Craniofacial Surgery Perioperative Registry was queried for subjects undergoing complex cranial vault reconstruction surgery over a 5-year period. Major perioperative complications were identified through a structured a priori consensus process. Logistic regression was applied to identify predictors of a major perioperative complication with bootstrapping to evaluate discrimination accuracy and provide internal validity of the multivariable model. RESULTS: A total of 1814 patients from 33 institutions in the US and Canada were analysed; 15% were reported to have a major perioperative complication. Multivariable predictors included ASA physical status 3 or 4 (P=0.005), craniofacial syndrome (P=0.008), antifibrinolytic administered (P=0.003), blood product transfusion >50 ml kg(-1) (P<0.001), and surgery duration over 5 h (P<0.001). Bootstrapping indicated that the predictive algorithm had good internal validity and excellent discrimination and model performance. A perioperative complication was estimated to increase the hospital length of stay by an average of 3 days (P<0.001). CONCLUSIONS: The predictive algorithm can be used as a prognostic tool to risk stratify patients and thereby potentially reduce morbidity and mortality. Craniofacial teams can utilise these predictors of complications to identify high-risk patients. Based on this information, further prospective quality improvement initiatives may decrease complications, and reduce morbidity and mortality."
0,MS signature TH cell subset,,
0,Biomarker Test for Myalgic Encephalomyelitis/Chronic Fatigue Syndrome,,
0,Continuous thermodilution to assess absolute flow and microvascular resistance: validation in humans using O-15 H2O positron emission tomography,,
0,Liver and Lung Transplant Advances,,
0,Machine Learning-state of the art The critical role that machine learning can play in advancing cardiology was outlined at a packed session at ESC 2019,,
0,PROTEOFORMER 2.0: Further developments in the ribosome profiling-assisted proteogenomic hunt for new proteoforms,"PROTEOFORMER is a pipeline that enables the automated processing of data derived from ribosome profiling (RIBO-seq, i.e. The sequencing of ribosome-protected mRNA fragments). As such, genome-wide ribosome occupancies lead to the delineation of data-specific translation product candidates and these can improve the mass spectrometry-based identification. Since its first publication, different upgrades, new features and extensions have been added to the PROTEOFORMER pipeline. Some of the most important upgrades include P-site offset calculation during mapping, comprehensive data preexploration, the introduction of two alternative proteoform calling strategies and extended pipeline output features. These novelties are illustrated by analyzing ribosome profiling data of human HCT116 and Jurkat data. The different proteoform calling strategies are used alongside one another and in the end combined together with reference sequences from UniProt. Matching mass spectrometry data are searched against this extended search space with MaxQuant. Overall, besides annotated proteoforms, this pipeline leads to the identification and validation of different categories of new proteoforms, including translation products of up- and downstream open reading frames, 5' and 3' extended and truncated proteoforms, single amino acid variants, splice variants and translation products of so-called noncoding regions. Further, proof-of-concept is reported for the improvement of spectrum matching by including Prosit, a deep neural network strategy that adds extra fragmentation spectrum intensity features to the analysis. In the light of ribosome profiling-driven proteogenomics, it is shown that this allows validating the spectrum matches of newly identified proteoforms with elevated stringency. These updates and novel conclusions provide new insights and lessons for the ribosome profiling-based proteogenomic research field.","PROTEOFORMER 2.0: Further developments in the ribosome profiling-assisted proteogenomic hunt for new proteoforms. PROTEOFORMER is a pipeline that enables the automated processing of data derived from ribosome profiling (RIBO-seq, i.e. The sequencing of ribosome-protected mRNA fragments). As such, genome-wide ribosome occupancies lead to the delineation of data-specific translation product candidates and these can improve the mass spectrometry-based identification. Since its first publication, different upgrades, new features and extensions have been added to the PROTEOFORMER pipeline. Some of the most important upgrades include P-site offset calculation during mapping, comprehensive data preexploration, the introduction of two alternative proteoform calling strategies and extended pipeline output features. These novelties are illustrated by analyzing ribosome profiling data of human HCT116 and Jurkat data. The different proteoform calling strategies are used alongside one another and in the end combined together with reference sequences from UniProt. Matching mass spectrometry data are searched against this extended search space with MaxQuant. Overall, besides annotated proteoforms, this pipeline leads to the identification and validation of different categories of new proteoforms, including translation products of up- and downstream open reading frames, 5' and 3' extended and truncated proteoforms, single amino acid variants, splice variants and translation products of so-called noncoding regions. Further, proof-of-concept is reported for the improvement of spectrum matching by including Prosit, a deep neural network strategy that adds extra fragmentation spectrum intensity features to the analysis. In the light of ribosome profiling-driven proteogenomics, it is shown that this allows validating the spectrum matches of newly identified proteoforms with elevated stringency. These updates and novel conclusions provide new insights and lessons for the ribosome profiling-based proteogenomic research field."
0,Attributable Failure of First-line Cancer Treatment and Incremental Costs Associated With Smoking by Patients With Cancer,"Importance: Previous studies have shown that continued smoking among patients with cancer can increase overall and cancer-specific mortality, risk for second primary cancer, and risk for toxic effects of cancer treatment. To our knowledge, there have been no efforts to estimate additional costs for cancer treatment attributed to smoking. Objective: To model attributable incremental costs of subsequent cancer treatment associated with continued smoking by patients with cancer. Design, Setting, and Participants: For this economic evaluation, a model was developed in 2018 using data from a 2014 US Surgeon General's report that considered expected failure rates of first-line cancer treatment in nonsmoking patients, smoking prevalence, odds ratio of first-line cancer treatment failure attributed to smoking compared with nonsmoking, and cost of cancer treatment after failure of first-line cancer treatment. Main Outcomes and Measures: Attributable failures of first-line cancer treatment and incremental cost for subsequent treatment associated with continued smoking among patients with cancer. Results: Attributable treatment failures were higher under conditions in which high first-line cure rates were expected in nonsmoking patients compared with conditions in which low cure rates were expected. Peak attributable failures occurred under the conditions in which expected cure rates among nonsmoking patients ranged from 50% to 65%. Under the conditions of a 30% expected treatment failure rate among nonsmoking patients, 20% smoking prevalence, 60% increased risk of failure of first-line cancer treatment, and $100â€¯000 mean added cost of treating a first-line cancer treatment failure, the additional incremental cost per 1000 total patients was estimated to be $2.1 million, reflecting an additional cost of $10â€¯678 per smoking patient. Extrapolation of cost to 1.6 million patients with cancer diagnosed annually reflects a potential $3.4 billion in incremental cost. Conclusions and Relevance: The findings suggest that continued smoking among patients with cancer and the increase in attributable first-line cancer treatment failure is associated with significant incremental costs for subsequent cancer treatments. Additional work appears to be needed to identify optimal methods to mitigate these incremental costs.","Attributable Failure of First-line Cancer Treatment and Incremental Costs Associated With Smoking by Patients With Cancer. Importance: Previous studies have shown that continued smoking among patients with cancer can increase overall and cancer-specific mortality, risk for second primary cancer, and risk for toxic effects of cancer treatment. To our knowledge, there have been no efforts to estimate additional costs for cancer treatment attributed to smoking. Objective: To model attributable incremental costs of subsequent cancer treatment associated with continued smoking by patients with cancer. Design, Setting, and Participants: For this economic evaluation, a model was developed in 2018 using data from a 2014 US Surgeon General's report that considered expected failure rates of first-line cancer treatment in nonsmoking patients, smoking prevalence, odds ratio of first-line cancer treatment failure attributed to smoking compared with nonsmoking, and cost of cancer treatment after failure of first-line cancer treatment. Main Outcomes and Measures: Attributable failures of first-line cancer treatment and incremental cost for subsequent treatment associated with continued smoking among patients with cancer. Results: Attributable treatment failures were higher under conditions in which high first-line cure rates were expected in nonsmoking patients compared with conditions in which low cure rates were expected. Peak attributable failures occurred under the conditions in which expected cure rates among nonsmoking patients ranged from 50% to 65%. Under the conditions of a 30% expected treatment failure rate among nonsmoking patients, 20% smoking prevalence, 60% increased risk of failure of first-line cancer treatment, and $100â€¯000 mean added cost of treating a first-line cancer treatment failure, the additional incremental cost per 1000 total patients was estimated to be $2.1 million, reflecting an additional cost of $10â€¯678 per smoking patient. Extrapolation of cost to 1.6 million patients with cancer diagnosed annually reflects a potential $3.4 billion in incremental cost. Conclusions and Relevance: The findings suggest that continued smoking among patients with cancer and the increase in attributable first-line cancer treatment failure is associated with significant incremental costs for subsequent cancer treatments. Additional work appears to be needed to identify optimal methods to mitigate these incremental costs."
0,Internet-delivered psychological treatments: from innovation to implementation,,
0,Reproducing the molecular subclassification of peripheral T-cell lymphoma-NOS by immunohistochemistry,"Peripheral T-cell lymphoma (PTCL) is a heterogeneous group of mature T-cell malignancies; approximately one-third of cases are designated as PTCL-not otherwise specified (PTCL-NOS). Using gene-expression profiling (GEP), we have previously defined 2 major molecular subtypes of PTCL-NOS, PTCL-GATA3 and PTCL-TBX21, which have distinct biological differences in oncogenic pathways and prognosis. In the current study, we generated an immunohistochemistry (IHC) algorithm to identify the 2 subtypes in paraffin tissue using antibodies to key transcriptional factors (GATA3 and TBX21) and their target proteins (CCR4 and CXCR3). In a training cohort of 49 cases of PTCL-NOS with corresponding GEP data, the 2 subtypes identified by the IHC algorithm matched the GEP results with high sensitivity (85%) and showed a significant difference in overall survival (OS) (P = .03). The IHC algorithm classification showed high interobserver reproducibility among pathologists and was validated in a second PTCL-NOS cohort (n = 124), where a significant difference in OS between the PTCL-GATA3 and PTCL-TBX21 subtypes was confirmed (P = .003). In multivariate analysis, a high International Prognostic Index score (3-5) and the PTCL-GATA3 subtype identified by IHC were independent adverse predictors of OS (P = .0015). Additionally, the 2 IHC-defined subtypes were significantly associated with distinct morphological features (P < .001), and there was a significant enrichment of an activated CD8+ cytotoxic phenotype in the PTCL-TBX21 subtype (P = .03). The IHC algorithm will aid in identifying the 2 subtypes in clinical practice, which will aid the future clinical management of patients and facilitate risk stratification in clinical trials.","Reproducing the molecular subclassification of peripheral T-cell lymphoma-NOS by immunohistochemistry. Peripheral T-cell lymphoma (PTCL) is a heterogeneous group of mature T-cell malignancies; approximately one-third of cases are designated as PTCL-not otherwise specified (PTCL-NOS). Using gene-expression profiling (GEP), we have previously defined 2 major molecular subtypes of PTCL-NOS, PTCL-GATA3 and PTCL-TBX21, which have distinct biological differences in oncogenic pathways and prognosis. In the current study, we generated an immunohistochemistry (IHC) algorithm to identify the 2 subtypes in paraffin tissue using antibodies to key transcriptional factors (GATA3 and TBX21) and their target proteins (CCR4 and CXCR3). In a training cohort of 49 cases of PTCL-NOS with corresponding GEP data, the 2 subtypes identified by the IHC algorithm matched the GEP results with high sensitivity (85%) and showed a significant difference in overall survival (OS) (P = .03). The IHC algorithm classification showed high interobserver reproducibility among pathologists and was validated in a second PTCL-NOS cohort (n = 124), where a significant difference in OS between the PTCL-GATA3 and PTCL-TBX21 subtypes was confirmed (P = .003). In multivariate analysis, a high International Prognostic Index score (3-5) and the PTCL-GATA3 subtype identified by IHC were independent adverse predictors of OS (P = .0015). Additionally, the 2 IHC-defined subtypes were significantly associated with distinct morphological features (P < .001), and there was a significant enrichment of an activated CD8+ cytotoxic phenotype in the PTCL-TBX21 subtype (P = .03). The IHC algorithm will aid in identifying the 2 subtypes in clinical practice, which will aid the future clinical management of patients and facilitate risk stratification in clinical trials."
0,PTEN Suppresses Glycolysis by Dephosphorylating and Inhibiting Autophosphorylated PGK1,"The PTEN tumor suppressor is frequently mutated or deleted in cancer and regulates glucose metabolism through the PI3K-AKT pathway. However, whether PTEN directly regulates glycolysis in tumor cells is unclear. We demonstrate here that PTEN directly interacts with phosphoglycerate kinase 1 (PGK1). PGK1 functions not only as a glycolytic enzyme but also as a protein kinase intermolecularly autophosphorylating itself at Y324 for activation. The protein phosphatase activity of PTEN dephosphorylates and inhibits autophosphorylated PGK1, thereby inhibiting glycolysis, ATP production, and brain tumor cell proliferation. In addition, knockin expression of a PGK1 Y324F mutant inhibits brain tumor formation. Analyses of human glioblastoma specimens reveals that PGK1 Y324 phosphorylation levels inversely correlate with PTEN expression status and are positively associated with poor prognosis in glioblastoma patients. This work highlights the instrumental role of PGK1 autophosphorylation in its activation and PTEN protein phosphatase activity in governing glycolysis and tumorigenesis.","PTEN Suppresses Glycolysis by Dephosphorylating and Inhibiting Autophosphorylated PGK1. The PTEN tumor suppressor is frequently mutated or deleted in cancer and regulates glucose metabolism through the PI3K-AKT pathway. However, whether PTEN directly regulates glycolysis in tumor cells is unclear. We demonstrate here that PTEN directly interacts with phosphoglycerate kinase 1 (PGK1). PGK1 functions not only as a glycolytic enzyme but also as a protein kinase intermolecularly autophosphorylating itself at Y324 for activation. The protein phosphatase activity of PTEN dephosphorylates and inhibits autophosphorylated PGK1, thereby inhibiting glycolysis, ATP production, and brain tumor cell proliferation. In addition, knockin expression of a PGK1 Y324F mutant inhibits brain tumor formation. Analyses of human glioblastoma specimens reveals that PGK1 Y324 phosphorylation levels inversely correlate with PTEN expression status and are positively associated with poor prognosis in glioblastoma patients. This work highlights the instrumental role of PGK1 autophosphorylation in its activation and PTEN protein phosphatase activity in governing glycolysis and tumorigenesis."
0,Integrated Physiological and Biochemical Assessments for the Prediction of Growth of Abdominal Aortic Aneurysms in Humans,,
0,Safety and Efficacy of Antithrombotic Strategies in Patients with Atrial Fibrillation Undergoing Percutaneous Coronary Intervention: A Network Meta-analysis of Randomized Controlled Trials,"Importance: The antithrombotic treatment of patients with atrial fibrillation (AF) and coronary artery disease, in particular with acute coronary syndrome (ACS) and/or percutaneous coronary intervention (PCI), poses a significant treatment dilemma in clinical practice. Objective: To study the safety and efficacy of different antithrombotic regimens using a network meta-analysis of randomized controlled trials in this population. Data Sources: PubMed, EMBASE, EBSCO, and Cochrane databases were searched to identify randomized controlled trials comparing antithrombotic regimens. Study Selection: Four randomized studies were included (n = 10026; WOEST, PIONEER AF-PCI, RE-DUAL PCI, and AUGUSTUS). Data Extraction and Synthesis: The Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines were used in this systematic review and network meta-analysis between 4 regimens using a Bayesian random-effects model. A pre hoc statistical analysis plan was written, and the review protocol was registered at PROSPERO. Data were analyzed between November 2018 and February 2019. Main Outcomes and Measures: The primary safety outcome was Thrombolysis in Myocardial Infarction (TIMI) major bleeding; secondary safety outcomes were combined TIMI major and minor bleeding, trial-defined primary bleeding events, intracranial hemorrhage, and hospitalization. The primary efficacy outcome was trial-defined major adverse cardiovascular events (MACE); secondary efficacy outcomes were individual components of MACE. Results: The overall prevalence of ACS varied from 28% to 61%. The mean age ranged from 70 to 72 years; 20% to 29% of the trial population were women; and most patients were at high risk for thromboembolic and bleeding events. Compared with a regimen of vitamin K antagonist (VKA) plus dual antiplatelet therapy (DAPT; P2Y12 inhibitor plus aspirin), the odds ratios (ORs) for TIMI major bleeding were 0.58 (95% CI, 0.31-1.08) for VKA plus P2Y12 inhibitor, 0.49 (95% CI, 0.30-0.82) for non-VKA oral anticoagulant (NOAC) plus P2Y12 inhibitor, and 0.70 (95% CI, 0.38-1.23) for NOAC plus DAPT. Compared with VKA plus DAPT, the ORs for MACE were 0.96 (95% CI, 0.60-1.46) for VKA plus P2Y12 inhibitor, 1.02 (95% CI, 0.71-1.47) for NOAC plus P2Y12 inhibitor, and 0.94 (95% CI, 0.60-1.45) for NOAC plus DAPT. Conclusions and Relevance: A regimen of NOACs plus P2Y12 inhibitor was associated with less bleeding compared with VKAs plus DAPT. Strategies omitting aspirin caused less bleeding, including intracranial bleeding, without significant difference in MACE, compared with strategies including aspirin. Our results support the use of NOAC plus P2Y12 inhibitor as the preferred regimen post-percutaneous coronary intervention for these high-risk patients with AF. A regimen of VKA plus DAPT should generally be avoided.","Safety and Efficacy of Antithrombotic Strategies in Patients with Atrial Fibrillation Undergoing Percutaneous Coronary Intervention: A Network Meta-analysis of Randomized Controlled Trials. Importance: The antithrombotic treatment of patients with atrial fibrillation (AF) and coronary artery disease, in particular with acute coronary syndrome (ACS) and/or percutaneous coronary intervention (PCI), poses a significant treatment dilemma in clinical practice. Objective: To study the safety and efficacy of different antithrombotic regimens using a network meta-analysis of randomized controlled trials in this population. Data Sources: PubMed, EMBASE, EBSCO, and Cochrane databases were searched to identify randomized controlled trials comparing antithrombotic regimens. Study Selection: Four randomized studies were included (n = 10026; WOEST, PIONEER AF-PCI, RE-DUAL PCI, and AUGUSTUS). Data Extraction and Synthesis: The Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines were used in this systematic review and network meta-analysis between 4 regimens using a Bayesian random-effects model. A pre hoc statistical analysis plan was written, and the review protocol was registered at PROSPERO. Data were analyzed between November 2018 and February 2019. Main Outcomes and Measures: The primary safety outcome was Thrombolysis in Myocardial Infarction (TIMI) major bleeding; secondary safety outcomes were combined TIMI major and minor bleeding, trial-defined primary bleeding events, intracranial hemorrhage, and hospitalization. The primary efficacy outcome was trial-defined major adverse cardiovascular events (MACE); secondary efficacy outcomes were individual components of MACE. Results: The overall prevalence of ACS varied from 28% to 61%. The mean age ranged from 70 to 72 years; 20% to 29% of the trial population were women; and most patients were at high risk for thromboembolic and bleeding events. Compared with a regimen of vitamin K antagonist (VKA) plus dual antiplatelet therapy (DAPT; P2Y12 inhibitor plus aspirin), the odds ratios (ORs) for TIMI major bleeding were 0.58 (95% CI, 0.31-1.08) for VKA plus P2Y12 inhibitor, 0.49 (95% CI, 0.30-0.82) for non-VKA oral anticoagulant (NOAC) plus P2Y12 inhibitor, and 0.70 (95% CI, 0.38-1.23) for NOAC plus DAPT. Compared with VKA plus DAPT, the ORs for MACE were 0.96 (95% CI, 0.60-1.46) for VKA plus P2Y12 inhibitor, 1.02 (95% CI, 0.71-1.47) for NOAC plus P2Y12 inhibitor, and 0.94 (95% CI, 0.60-1.45) for NOAC plus DAPT. Conclusions and Relevance: A regimen of NOACs plus P2Y12 inhibitor was associated with less bleeding compared with VKAs plus DAPT. Strategies omitting aspirin caused less bleeding, including intracranial bleeding, without significant difference in MACE, compared with strategies including aspirin. Our results support the use of NOAC plus P2Y12 inhibitor as the preferred regimen post-percutaneous coronary intervention for these high-risk patients with AF. A regimen of VKA plus DAPT should generally be avoided."
0,Dynamic Cortical Connectivity during General Anesthesia in Healthy Volunteers,,
0,Volumetric Ca2+ Imaging in the Mouse Brain Using Hybrid Multiplexed Sculpted Light Microscopy,,
0,Super-resolution reconstruction of neonatal brain magnetic resonance images via residual structured sparse representation,,
0,Structural and functional features of lysine acetylation of plant and animal tubulins,"The study of the genome and the proteome of different species and representatives of distinct kingdoms, especially detection of proteome via wide-scaled analyses has various challenges and pitfalls. Attempts to combine all available information together and isolate some common features for determination of the pathway and their mechanism of action generally have a highly complicated nature. However, microtubule (MT) monomers are highly conserved protein structures, and microtubules are structurally conserved from Homo sapiens to Arabidopsis thaliana. The interaction of MT elements with microtubule-associated proteins and post-translational modifiers is fully dependent on protein interfaces, and almost all MT modifications are well described except acetylation. Crystallography and interactome data using different approaches were combined to identify conserved proteins important in acetylation of microtubules. Application of computational methods and comparative analysis of binding modes generated a robust predictive model of acetylation of the Ïµ-amino group of Lys40 in Î±-tubulins. In turn, the model discarded some probable mechanisms of interaction between elements of interest. Reconstruction of unresolved protein structures was carried out with modeling by homology to the existing crystal structure (PDBID: 1Z2B) from B. taurus using Swiss-model server, followed by a molecular dynamics simulation. Docking of the human tubulin fragment with Lys40 into the active site of Î±-tubulin acetyltransferase, reproduces the binding mode of peptidomimetic from X-ray structure (PDBID: 4PK3).","Structural and functional features of lysine acetylation of plant and animal tubulins. The study of the genome and the proteome of different species and representatives of distinct kingdoms, especially detection of proteome via wide-scaled analyses has various challenges and pitfalls. Attempts to combine all available information together and isolate some common features for determination of the pathway and their mechanism of action generally have a highly complicated nature. However, microtubule (MT) monomers are highly conserved protein structures, and microtubules are structurally conserved from Homo sapiens to Arabidopsis thaliana. The interaction of MT elements with microtubule-associated proteins and post-translational modifiers is fully dependent on protein interfaces, and almost all MT modifications are well described except acetylation. Crystallography and interactome data using different approaches were combined to identify conserved proteins important in acetylation of microtubules. Application of computational methods and comparative analysis of binding modes generated a robust predictive model of acetylation of the Ïµ-amino group of Lys40 in Î±-tubulins. In turn, the model discarded some probable mechanisms of interaction between elements of interest. Reconstruction of unresolved protein structures was carried out with modeling by homology to the existing crystal structure (PDBID: 1Z2B) from B. taurus using Swiss-model server, followed by a molecular dynamics simulation. Docking of the human tubulin fragment with Lys40 into the active site of Î±-tubulin acetyltransferase, reproduces the binding mode of peptidomimetic from X-ray structure (PDBID: 4PK3)."
0,Cardiomyocyte Homeodomain-Interacting Protein Kinase 2 Maintains Basal Cardiac Function via Extracellular Signal-Regulated Kinase Signaling,,
0,Computer-aided assessment of liver fibrosis progression in patients with chronic hepatitis B: an exploratory research,"Objective: To establish automatic liver fibrosis classification models by using traditional machine learning and deep learning methods and preliminaryly evaluate the efficiency. Methods: Gray scale ultrasound images and corresponding elastic images of 354 patients, 247 males and 107 females, mean age (54Â±12) years undergoing partial hepatectomy in Zhongshan Hospital of Fudan University from November 2014 to January 2016 were enrolled in this study. By using traditional machine learning and deep learning methods, an automatic classification model of liver fibrosis stages (S0 to S4) were established through feature extraction and classification of ultrasound image data sets and the accuracy in different classification categories of each model were calculated, by using liver biopsy as the reference standard. Results: Pathological examination showed 73 cases in pathological stage S0, 40 cases in S1, 49 cases in S2, 41 cases in S3, and 151 cases in S4. The traditional machine classification model based on support vector machine (SVM) classifier and sparse representation classifier and the deep learning classification model based on LeNet-5 neural network, their accuracy rates in the two categories (S0/S1/S2 and S3/S4) were 89.8%, 91.8% and 90.7% respectively; the accuracy rates in the three categories (S0/S1 and S2/S3 and S4) were 75.3%, 79.4% and 82.8% respectively; the accuracy in the three categories (S0 and S1/S2/S3 and S4) were 79.3%, 82.7% and 87.2% respectively. Conclusions: Computer-aided assessment of liver fibrosis progression in patients with chronic hepatitis B has a high accuracy, and can achieve a more detailed classification. This method is expected to be applied in the non-invasive evaluation of liver fibrosis in patients with hepatitis B in clinical work in the future.","Computer-aided assessment of liver fibrosis progression in patients with chronic hepatitis B: an exploratory research. Objective: To establish automatic liver fibrosis classification models by using traditional machine learning and deep learning methods and preliminaryly evaluate the efficiency. Methods: Gray scale ultrasound images and corresponding elastic images of 354 patients, 247 males and 107 females, mean age (54Â±12) years undergoing partial hepatectomy in Zhongshan Hospital of Fudan University from November 2014 to January 2016 were enrolled in this study. By using traditional machine learning and deep learning methods, an automatic classification model of liver fibrosis stages (S0 to S4) were established through feature extraction and classification of ultrasound image data sets and the accuracy in different classification categories of each model were calculated, by using liver biopsy as the reference standard. Results: Pathological examination showed 73 cases in pathological stage S0, 40 cases in S1, 49 cases in S2, 41 cases in S3, and 151 cases in S4. The traditional machine classification model based on support vector machine (SVM) classifier and sparse representation classifier and the deep learning classification model based on LeNet-5 neural network, their accuracy rates in the two categories (S0/S1/S2 and S3/S4) were 89.8%, 91.8% and 90.7% respectively; the accuracy rates in the three categories (S0/S1 and S2/S3 and S4) were 75.3%, 79.4% and 82.8% respectively; the accuracy in the three categories (S0 and S1/S2/S3 and S4) were 79.3%, 82.7% and 87.2% respectively. Conclusions: Computer-aided assessment of liver fibrosis progression in patients with chronic hepatitis B has a high accuracy, and can achieve a more detailed classification. This method is expected to be applied in the non-invasive evaluation of liver fibrosis in patients with hepatitis B in clinical work in the future."
0,Towards individualized therapy for metastatic renal cell carcinoma,,
0,"Comparison of a clinical-laboratory algorithm, 4t and heparin-induced thrombocytopenia expert probability scores in the diagnosis of heparin-induced thrombocytopenia in the critical care setting","Background: Several scoring systems are utilized to calculate the pre-test probability of heparin-induced thrombocytopenia (HIT). We hypothesize that a clinical-laboratory algorithm combining the 4Ts score with the optical density (OD) of anti-PF4-heparin antibody is more accurate than either the 4Ts or HIT expert probability (HEP) scores in the critical care setting. Methods: A single-institution retrospective review of adult patients admitted to the intensive care unit (ICU) that were evaluated for HIT was conducted. Two reviewers independently rated the proposed algorithm, 4Ts and HEP score. Summary, univariate and area under receiver operator characteristic analyses were performed. Results: A total of 88 patients with a mean (SD) age of 62 (15) years were included. The sensitivity, positive predictive value and negative predictive value were superior in our clinical-laboratory algorithm compared to the 4Ts score â‰¥ 4 and the HEP score â‰¥ 2. The algorithmâ€™s specificity was non-inferior to the 4Ts score and HEP score. There was no significant difference between our clinical-laboratory algorithm and the 4Ts score or the HEP score in predicting HIT. Conclusion: Our study confirms that the combination of clinical and laboratory criteria is crucial in the presumable diagnosis of HIT. This is the first study that validates different HIT scores in an isolated ICU population.","Comparison of a clinical-laboratory algorithm, 4t and heparin-induced thrombocytopenia expert probability scores in the diagnosis of heparin-induced thrombocytopenia in the critical care setting. Background: Several scoring systems are utilized to calculate the pre-test probability of heparin-induced thrombocytopenia (HIT). We hypothesize that a clinical-laboratory algorithm combining the 4Ts score with the optical density (OD) of anti-PF4-heparin antibody is more accurate than either the 4Ts or HIT expert probability (HEP) scores in the critical care setting. Methods: A single-institution retrospective review of adult patients admitted to the intensive care unit (ICU) that were evaluated for HIT was conducted. Two reviewers independently rated the proposed algorithm, 4Ts and HEP score. Summary, univariate and area under receiver operator characteristic analyses were performed. Results: A total of 88 patients with a mean (SD) age of 62 (15) years were included. The sensitivity, positive predictive value and negative predictive value were superior in our clinical-laboratory algorithm compared to the 4Ts score â‰¥ 4 and the HEP score â‰¥ 2. The algorithmâ€™s specificity was non-inferior to the 4Ts score and HEP score. There was no significant difference between our clinical-laboratory algorithm and the 4Ts score or the HEP score in predicting HIT. Conclusion: Our study confirms that the combination of clinical and laboratory criteria is crucial in the presumable diagnosis of HIT. This is the first study that validates different HIT scores in an isolated ICU population."
0,In vitro and in silico molecular interaction of multiphase nanoparticles containing inositol hexaphosphate and jacalin: Therapeutic potential against colon cancer cells (HCT-15),"Inositol hexaphosphate (IP6) is a natural constituent found in almost all cereals and legumes. It is known to cause numerous antiangiogenic manifestations. Notwithstanding its great potential, it is underutilized due to the chelation and rapid excretion from the body. Jacalin is another natural constituent obtained from seeds of jackfruit and can target disaccharides overexpressed in tumor cells. The current study was in-quested to develop and evaluate a surface-modified gold nanoparticulate system containing IP6 and jacalin which may maximize the apoptotic effect of IP6 against HCT-15 cell lines. IP6 loaded jacalin-pectin-gold nanoparticles (IJP-GNPs) were developed through reduction followed by incubation method. The developed formulation was tested for various in vitro and in silico studies to investigate its potential. HCT-15 cells when exposed to IJP-GNP resulted in significant apoptotic effects in dose as well as time-dependent manner, as measured using 3-(4,5-dimethylthiazol-2-yl)-2,5-diphenyltetrazolium bromide, micronucleus, and reactive oxygen species assay. IJP-GNP displayed cell cycle arrest at the G0/G1 phase. To further explore the mechanism of chemoprevention, in silico studies were performed. The docking results revealed that the interactive behavior of IP6, P-GNP, and jacalin could target and inhibit the tumor formation activity, supported by in vitro studies. Taken together, all the findings suggested that IP6 loaded nanoparticles may increase the hope of future drug delivery strategy for targeting colon cancer.","In vitro and in silico molecular interaction of multiphase nanoparticles containing inositol hexaphosphate and jacalin: Therapeutic potential against colon cancer cells (HCT-15). Inositol hexaphosphate (IP6) is a natural constituent found in almost all cereals and legumes. It is known to cause numerous antiangiogenic manifestations. Notwithstanding its great potential, it is underutilized due to the chelation and rapid excretion from the body. Jacalin is another natural constituent obtained from seeds of jackfruit and can target disaccharides overexpressed in tumor cells. The current study was in-quested to develop and evaluate a surface-modified gold nanoparticulate system containing IP6 and jacalin which may maximize the apoptotic effect of IP6 against HCT-15 cell lines. IP6 loaded jacalin-pectin-gold nanoparticles (IJP-GNPs) were developed through reduction followed by incubation method. The developed formulation was tested for various in vitro and in silico studies to investigate its potential. HCT-15 cells when exposed to IJP-GNP resulted in significant apoptotic effects in dose as well as time-dependent manner, as measured using 3-(4,5-dimethylthiazol-2-yl)-2,5-diphenyltetrazolium bromide, micronucleus, and reactive oxygen species assay. IJP-GNP displayed cell cycle arrest at the G0/G1 phase. To further explore the mechanism of chemoprevention, in silico studies were performed. The docking results revealed that the interactive behavior of IP6, P-GNP, and jacalin could target and inhibit the tumor formation activity, supported by in vitro studies. Taken together, all the findings suggested that IP6 loaded nanoparticles may increase the hope of future drug delivery strategy for targeting colon cancer."
0,Why we need a small data paradigm,,
0,Cases in Precision Medicine: The Role of Pharmacogenetics in Precision Prescribing,Pharmacogenetics may help physicians deliver individualized treatments based on how a person's genes affect a drug's effects and metabolism. This information can help prevent adverse events or improve drug efficacy by enabling the physician to optimize dosage or to avoid a medication with adverse reactions and to prescribe an alternative therapy. This article discusses the current clinical utility of pharmacogenetic testing in the context of a patient who requires anticoagulation with warfarin.,Cases in Precision Medicine: The Role of Pharmacogenetics in Precision Prescribing. Pharmacogenetics may help physicians deliver individualized treatments based on how a person's genes affect a drug's effects and metabolism. This information can help prevent adverse events or improve drug efficacy by enabling the physician to optimize dosage or to avoid a medication with adverse reactions and to prescribe an alternative therapy. This article discusses the current clinical utility of pharmacogenetic testing in the context of a patient who requires anticoagulation with warfarin.
0,Uncovering thousands of new peptides with sequence-mask-search hybrid de novo peptide sequencing framework,"Typical analyses of mass spectrometry data only identify amino acid sequences that exist in reference databases. This restricts the possibility of discovering new peptides such as those that contain uncharacterized mutations or originate from unexpected processing of RNAs and proteins. De novo peptide sequencing approaches address this limitation but often suffer from low accuracy and require extensive validation by experts. Here, we develop SMSNet, a deep learning-based de novo peptide sequencing framework that achieves >95% amino acid accuracy while retaining good identification coverage. Applications of SMSNet on landmark proteomics and peptidomics studies reveal over 10,000 previously uncharacterized HLA antigens and phosphopeptides, and in conjunction with database-search methods, expand the coverage of peptide identification by almost 30%. The power to accurately identify new peptides of SMSNet would make it an invaluable tool for any future proteomics and peptidomics studies, including tumor neoantigen discovery, antibody sequencing, and proteome characterization of nonmodel organisms.","Uncovering thousands of new peptides with sequence-mask-search hybrid de novo peptide sequencing framework. Typical analyses of mass spectrometry data only identify amino acid sequences that exist in reference databases. This restricts the possibility of discovering new peptides such as those that contain uncharacterized mutations or originate from unexpected processing of RNAs and proteins. De novo peptide sequencing approaches address this limitation but often suffer from low accuracy and require extensive validation by experts. Here, we develop SMSNet, a deep learning-based de novo peptide sequencing framework that achieves >95% amino acid accuracy while retaining good identification coverage. Applications of SMSNet on landmark proteomics and peptidomics studies reveal over 10,000 previously uncharacterized HLA antigens and phosphopeptides, and in conjunction with database-search methods, expand the coverage of peptide identification by almost 30%. The power to accurately identify new peptides of SMSNet would make it an invaluable tool for any future proteomics and peptidomics studies, including tumor neoantigen discovery, antibody sequencing, and proteome characterization of nonmodel organisms."
0,iHuman: a futuristic vision for the human experience,,
0,The diagnostic accuracy of dermoscopy for basal cell carcinoma: A systematic review and meta-analysis,"Background: Dermoscopy is a noninvasive technique for the diagnosis of skin lesions. Its accuracy for basal cell carcinoma (BCC) has not been systematically studied. Objective: We sought to systematically investigate the accuracy of dermoscopy for the diagnosis of BCC compared with examination with the naked eye. Methods: A systematic review of studies reporting the accuracy of naked eye examination and dermoscopy for the diagnosis of BCC was conducted. A meta-analysis for sensitivity and specificity was performed using a bivariate mixed-effects logistic regression modeling framework. Results: Seventeen studies were identified. The pooled sensitivity and specificity of dermoscopy for the diagnosis of BCC were 91.2% and 95%, respectively. In studies comparing test performance, adding dermoscopy to naked eye examination improved sensitivity from 66.9% to 85% (P =.0001) and specificity from 97.2% to 98.2% (P =.006). The sensitivity and specificity of dermoscopy were higher for pigmented than nonpigmented BCC. Sensitivity increased when dermoscopy was performed by experts and when the diagnosis was based on in-person dermoscopy as opposed to dermoscopic photographs. Limitations: Significant heterogeneity among studies with a medium-to-high risk of bias. Conclusion: Dermoscopy is a sensitive and specific add-on tool for the diagnosis of BCC. It is especially valuable for pigmented BCC.","The diagnostic accuracy of dermoscopy for basal cell carcinoma: A systematic review and meta-analysis. Background: Dermoscopy is a noninvasive technique for the diagnosis of skin lesions. Its accuracy for basal cell carcinoma (BCC) has not been systematically studied. Objective: We sought to systematically investigate the accuracy of dermoscopy for the diagnosis of BCC compared with examination with the naked eye. Methods: A systematic review of studies reporting the accuracy of naked eye examination and dermoscopy for the diagnosis of BCC was conducted. A meta-analysis for sensitivity and specificity was performed using a bivariate mixed-effects logistic regression modeling framework. Results: Seventeen studies were identified. The pooled sensitivity and specificity of dermoscopy for the diagnosis of BCC were 91.2% and 95%, respectively. In studies comparing test performance, adding dermoscopy to naked eye examination improved sensitivity from 66.9% to 85% (P =.0001) and specificity from 97.2% to 98.2% (P =.006). The sensitivity and specificity of dermoscopy were higher for pigmented than nonpigmented BCC. Sensitivity increased when dermoscopy was performed by experts and when the diagnosis was based on in-person dermoscopy as opposed to dermoscopic photographs. Limitations: Significant heterogeneity among studies with a medium-to-high risk of bias. Conclusion: Dermoscopy is a sensitive and specific add-on tool for the diagnosis of BCC. It is especially valuable for pigmented BCC."
0,AI added to the curriculum for doctors-to-be,,
0,A Laboratory Medicine Best Practices Systematic Review and Meta-analysis of Nucleic Acid Amplification Tests (NAATs) and Algorithms Including NAATs for the Diagnosis of Clostridioides (Clostridium) difficile in Adults,"The evidence base for the optimal laboratory diagnosis of Clostridioides (Clostridium) difficile in adults is currently unresolved due to the uncertain performance characteristics and various combinations of tests. This systematic review evaluates the diagnostic accuracy of laboratory testing algorithms that include nucleic acid amplification tests (NAATs) to detect the presence of C. difficile The systematic review and meta-analysis included eligible studies (those that had PICO [population, intervention, comparison, outcome] elements) that assessed the diagnostic accuracy of NAAT alone or following glutamate dehydrogenase (GDH) enzyme immunoassays (EIAs) or GDH EIAs plus C. difficile toxin EIAs (toxin). The diagnostic yield of NAAT for repeat testing after an initial negative result was also assessed. Two hundred thirty-eight studies met inclusion criteria. Seventy-two of these studies had sufficient data for meta-analysis. The strength of evidence ranged from high to insufficient. The uses of NAAT only, GDH-positive EIA followed by NAAT, and GDH-positive/toxin-negative EIA followed by NAAT are all recommended as American Society for Microbiology (ASM) best practices for the detection of the C. difficile toxin gene or organism. Meta-analysis of published evidence supports the use of testing algorithms that use NAAT alone or in combination with GDH or GDH plus toxin EIA to detect the presence of C. difficile in adults. There is insufficient evidence to recommend against repeat testing of the sample using NAAT after an initial negative result due to a lack of evidence of harm (i.e., financial, length of stay, or delay of treatment) as specified by the Laboratory Medicine Best Practices (LMBP) systematic review method in making such an assessment. Findings from this systematic review provide clarity to diagnostic testing strategies and highlight gaps, such as low numbers of GDH/toxin/PCR studies, in existing evidence on diagnostic performance, which can be used to guide future clinical research studies.","A Laboratory Medicine Best Practices Systematic Review and Meta-analysis of Nucleic Acid Amplification Tests (NAATs) and Algorithms Including NAATs for the Diagnosis of Clostridioides (Clostridium) difficile in Adults. The evidence base for the optimal laboratory diagnosis of Clostridioides (Clostridium) difficile in adults is currently unresolved due to the uncertain performance characteristics and various combinations of tests. This systematic review evaluates the diagnostic accuracy of laboratory testing algorithms that include nucleic acid amplification tests (NAATs) to detect the presence of C. difficile The systematic review and meta-analysis included eligible studies (those that had PICO [population, intervention, comparison, outcome] elements) that assessed the diagnostic accuracy of NAAT alone or following glutamate dehydrogenase (GDH) enzyme immunoassays (EIAs) or GDH EIAs plus C. difficile toxin EIAs (toxin). The diagnostic yield of NAAT for repeat testing after an initial negative result was also assessed. Two hundred thirty-eight studies met inclusion criteria. Seventy-two of these studies had sufficient data for meta-analysis. The strength of evidence ranged from high to insufficient. The uses of NAAT only, GDH-positive EIA followed by NAAT, and GDH-positive/toxin-negative EIA followed by NAAT are all recommended as American Society for Microbiology (ASM) best practices for the detection of the C. difficile toxin gene or organism. Meta-analysis of published evidence supports the use of testing algorithms that use NAAT alone or in combination with GDH or GDH plus toxin EIA to detect the presence of C. difficile in adults. There is insufficient evidence to recommend against repeat testing of the sample using NAAT after an initial negative result due to a lack of evidence of harm (i.e., financial, length of stay, or delay of treatment) as specified by the Laboratory Medicine Best Practices (LMBP) systematic review method in making such an assessment. Findings from this systematic review provide clarity to diagnostic testing strategies and highlight gaps, such as low numbers of GDH/toxin/PCR studies, in existing evidence on diagnostic performance, which can be used to guide future clinical research studies."
0,Identification of a gene set associated with colorectal cancer in microarray data using the entropy method,"Objective: We sought to apply Shannonâ€™s entropy to determine colorectal cancer genes in a microarray dataset. Materials and Methods: In the retrospective study, 36 samples were analysed, 18 colorectal carcinoma and 18 paired normal tissue samples. After identification of the gene fold-changes, we used the entropy theory to identify an effective gene set. These genes were subsequently categorised into homogenous clusters. Results: We assessed 36 tissue samples. The entropy theory was used to select a set of 29 genes from 3128 genes that had fold-changes greater than one, which provided the most information on colorectal cancer. This study shows that all genes fall into a cluster, except for the R08183 gene. Conclusion: This study has identified several genes associated with colon cancer using the entropy method, which were not detected by custom methods. Therefore, we suggest that the entropy theory should be used to identify genes associated with cancers in a microarray dataset.","Identification of a gene set associated with colorectal cancer in microarray data using the entropy method. Objective: We sought to apply Shannonâ€™s entropy to determine colorectal cancer genes in a microarray dataset. Materials and Methods: In the retrospective study, 36 samples were analysed, 18 colorectal carcinoma and 18 paired normal tissue samples. After identification of the gene fold-changes, we used the entropy theory to identify an effective gene set. These genes were subsequently categorised into homogenous clusters. Results: We assessed 36 tissue samples. The entropy theory was used to select a set of 29 genes from 3128 genes that had fold-changes greater than one, which provided the most information on colorectal cancer. This study shows that all genes fall into a cluster, except for the R08183 gene. Conclusion: This study has identified several genes associated with colon cancer using the entropy method, which were not detected by custom methods. Therefore, we suggest that the entropy theory should be used to identify genes associated with cancers in a microarray dataset."
0,Machine Learning Reveals the Texture of Regional Lung Ventilation at CT,,
0,Prognostic value of aberrantly expressed methylation gene profiles in lung squamous cell carcinoma: A study based on The Cancer Genome Atlas,"Currently, research on genome-scale epigenetic modifications for studying the pathogenesis of lung cancer is lacking. Aberrant DNA methylation, as the most common and important modification in epigenetics, is an important means of regulating genomic function and can be used as a biomarker for the diagnosis and prognosis of lung squamous cell carcinoma (LUSC). In this paper, methylation information and gene expression data from patients with LUSC were extracted from the TCGA database. Univariate and multivariate COX analyses were used to screen abnormally methylated genes related to the prognosis of LUSC. The relationship between key DNA methylation sites and the transcriptional expression of LUSC-related genes was explored. A prognostic risk model constructed by four abnormally methylated genes (VAX1, CH25H, AdCyAP1, and Irx1) was used to predict the prognosis of LUSC patients. Also, the methylation levels of the key gene IRX1 are significantly correlated with the prognosis and correlated with the methylation of the site cg09232937 and cg10530883. This study is based on high-throughput data mining and provides an effective bioinformatics basis for further understanding the pathogenesis and prognosis of LUSC, which has important theoretical significance for follow-up studies on LUSC.","Prognostic value of aberrantly expressed methylation gene profiles in lung squamous cell carcinoma: A study based on The Cancer Genome Atlas. Currently, research on genome-scale epigenetic modifications for studying the pathogenesis of lung cancer is lacking. Aberrant DNA methylation, as the most common and important modification in epigenetics, is an important means of regulating genomic function and can be used as a biomarker for the diagnosis and prognosis of lung squamous cell carcinoma (LUSC). In this paper, methylation information and gene expression data from patients with LUSC were extracted from the TCGA database. Univariate and multivariate COX analyses were used to screen abnormally methylated genes related to the prognosis of LUSC. The relationship between key DNA methylation sites and the transcriptional expression of LUSC-related genes was explored. A prognostic risk model constructed by four abnormally methylated genes (VAX1, CH25H, AdCyAP1, and Irx1) was used to predict the prognosis of LUSC patients. Also, the methylation levels of the key gene IRX1 are significantly correlated with the prognosis and correlated with the methylation of the site cg09232937 and cg10530883. This study is based on high-throughput data mining and provides an effective bioinformatics basis for further understanding the pathogenesis and prognosis of LUSC, which has important theoretical significance for follow-up studies on LUSC."
0,Training the next generation of Africa's doctors: why medical schools should embrace the team-based learning pedagogy,"BACKGROUND: As far back as 1995, the Cape Town Declaration on training Africa's future doctor recognized the need for medical schools to adopt active-learning strategies in order to nurture holistic development of the doctor. However, medical education in Africa remains largely stuck with traditional pedagogies that emphasize the 'hard skills' such as knowledge and clinical acumen while doing little to develop 'soft skills' such as effective communication, teamwork, critical thinking or life-long learning skills. By reviewing literature on Africa's epidemiologic and demographic transitions, we establish the need for increasing the output of well-trained doctors in order to match the continent's complex current and future healthcare needs. Challenges that bedevil African medical education such as outdated curricula, limited educational infrastructure and chronic resource constraints are presented and discussed. Furthermore, increased student enrollments, a trend observed at many schools, coupled with chronic faculty shortages have inadvertently presented specific barriers against the success of small-group active-learning strategies such as Problem-Based and Case-Based Learning. We argue that Team-Based Learning (TBL) offers a robust alternative for delivering holistic medical education in the current setting. TBL is instructor-driven and embodies key attributes that foster development of both 'hard' and 'soft' skills. We elaborate on advantages that TBL is likely to bring to the African medical education landscape, including increased learner enthusiasm and creativity, accountability, peer mentorship, deep learning and better knowledge retention. As with all new pedagogical methods, challenges anticipated during initial implementation of TBL are discussed followed by the limited pilot experiences with TBL in Africa. CONCLUSION: For its ability to enable a student-centered, active learning experience delivered at minimum cost, we encourage individual instructors and African medical schools at large, to adopt TBL as a complementary strategy towards realizing the goal of training Africa's fit-for-purpose doctor.","Training the next generation of Africa's doctors: why medical schools should embrace the team-based learning pedagogy. BACKGROUND: As far back as 1995, the Cape Town Declaration on training Africa's future doctor recognized the need for medical schools to adopt active-learning strategies in order to nurture holistic development of the doctor. However, medical education in Africa remains largely stuck with traditional pedagogies that emphasize the 'hard skills' such as knowledge and clinical acumen while doing little to develop 'soft skills' such as effective communication, teamwork, critical thinking or life-long learning skills. By reviewing literature on Africa's epidemiologic and demographic transitions, we establish the need for increasing the output of well-trained doctors in order to match the continent's complex current and future healthcare needs. Challenges that bedevil African medical education such as outdated curricula, limited educational infrastructure and chronic resource constraints are presented and discussed. Furthermore, increased student enrollments, a trend observed at many schools, coupled with chronic faculty shortages have inadvertently presented specific barriers against the success of small-group active-learning strategies such as Problem-Based and Case-Based Learning. We argue that Team-Based Learning (TBL) offers a robust alternative for delivering holistic medical education in the current setting. TBL is instructor-driven and embodies key attributes that foster development of both 'hard' and 'soft' skills. We elaborate on advantages that TBL is likely to bring to the African medical education landscape, including increased learner enthusiasm and creativity, accountability, peer mentorship, deep learning and better knowledge retention. As with all new pedagogical methods, challenges anticipated during initial implementation of TBL are discussed followed by the limited pilot experiences with TBL in Africa. CONCLUSION: For its ability to enable a student-centered, active learning experience delivered at minimum cost, we encourage individual instructors and African medical schools at large, to adopt TBL as a complementary strategy towards realizing the goal of training Africa's fit-for-purpose doctor."
0,The Pancreas as a Site of Metastasis or Second Primary in Patients with Small Bowel Neuroendocrine Tumors,"Background: The small bowel and pancreas are the most common primary sites of neuroendocrine tumors (NETs) giving rise to metastatic disease. Some patients with small bowel NETs (SBNETs) present with synchronous or metachronous pancreatic NETs (PNETs), and it is unclear whether these are separate primaries or metastases from one site to the other. Methods: A surgical NET database including patients undergoing operations for SBNETs or PNETs was reviewed. Patients with synchronous or metachronous tumors in both the small bowel and pancreas were identified, and available tissues from primary tumors and metastases were examined using a 4-gene quantitative polymerase chain reaction (qPCR) and immunohistochemistry (IHC) panel developed for evaluating NETs of unknown primary. Results: Of 338 patients undergoing exploration, 11 had NETs in both the small bowel and pancreas. Tissues from 11 small bowel tumors, 9 pancreatic tumors, and 10 metastases were analyzed. qPCR and IHC data revealed that three patients had separate SBNET and PNET primaries, and five patients had SBNETs that metastasized to the pancreas. Pancreatic tissue was unavailable in two patients, and qPCR and IHC gave discrepant results in one patient. Conclusions: NETs in both the small bowel and pancreas were found in 3% of our patients. In nearly two-thirds of evaluable patients, the pancreatic tumor was a metastasis from the SBNET primary, while in the remaining one-third of patients it represented a separate primary. Determining the origin of these tumors can help guide the choice of systemic therapy and surgical management.","The Pancreas as a Site of Metastasis or Second Primary in Patients with Small Bowel Neuroendocrine Tumors. Background: The small bowel and pancreas are the most common primary sites of neuroendocrine tumors (NETs) giving rise to metastatic disease. Some patients with small bowel NETs (SBNETs) present with synchronous or metachronous pancreatic NETs (PNETs), and it is unclear whether these are separate primaries or metastases from one site to the other. Methods: A surgical NET database including patients undergoing operations for SBNETs or PNETs was reviewed. Patients with synchronous or metachronous tumors in both the small bowel and pancreas were identified, and available tissues from primary tumors and metastases were examined using a 4-gene quantitative polymerase chain reaction (qPCR) and immunohistochemistry (IHC) panel developed for evaluating NETs of unknown primary. Results: Of 338 patients undergoing exploration, 11 had NETs in both the small bowel and pancreas. Tissues from 11 small bowel tumors, 9 pancreatic tumors, and 10 metastases were analyzed. qPCR and IHC data revealed that three patients had separate SBNET and PNET primaries, and five patients had SBNETs that metastasized to the pancreas. Pancreatic tissue was unavailable in two patients, and qPCR and IHC gave discrepant results in one patient. Conclusions: NETs in both the small bowel and pancreas were found in 3% of our patients. In nearly two-thirds of evaluable patients, the pancreatic tumor was a metastasis from the SBNET primary, while in the remaining one-third of patients it represented a separate primary. Determining the origin of these tumors can help guide the choice of systemic therapy and surgical management."
0,A methodology for generating four-dimensional arterial spin labeling MR angiography virtual phantoms,,
0,Multiple neurosteroid and cholesterol binding sites in voltage-dependent anion channel-1 determined by photo-affinity labeling,"Voltage-dependent anion channel-1 (VDAC1) is a mitochondrial porin that is implicated in cellular metabolism and apoptosis, and modulated by numerous small molecules including lipids. VDAC1 binds sterols, including cholesterol and neurosteroids such as allopregnanolone. Biochemical and computational studies suggest that VDAC1 binds multiple cholesterol molecules, but photolabeling studies have identified only a single cholesterol and neurosteroid binding site at E73. To identify all the binding sites of neurosteroids in VDAC1, we apply photo-affinity labeling using two sterol-based photolabeling reagents with complementary photochemistry: 5Î±-6-AziP which contains an aliphatic diazirine, and KK200 which contains a trifluoromethyl-phenyldiazirine (TPD) group. 5Î±-6-AziP and KK200 photolabel multiple residues within an E73 pocket confirming the presence of this site and mapping sterol orientation within this pocket. In addition, KK200 photolabels four other sites consistent with the finding that VDAC1 co-purifies with five cholesterol molecules. Both allopregnanolone and cholesterol competitively prevent photolabeling at E73 and three other sites indicating that these are common sterol binding sites shared by both neurosteroids and cholesterol. Binding at the functionally important residue E73 suggests a possible role for sterols in regulating VDAC1 signaling and interaction with partner proteins.","Multiple neurosteroid and cholesterol binding sites in voltage-dependent anion channel-1 determined by photo-affinity labeling. Voltage-dependent anion channel-1 (VDAC1) is a mitochondrial porin that is implicated in cellular metabolism and apoptosis, and modulated by numerous small molecules including lipids. VDAC1 binds sterols, including cholesterol and neurosteroids such as allopregnanolone. Biochemical and computational studies suggest that VDAC1 binds multiple cholesterol molecules, but photolabeling studies have identified only a single cholesterol and neurosteroid binding site at E73. To identify all the binding sites of neurosteroids in VDAC1, we apply photo-affinity labeling using two sterol-based photolabeling reagents with complementary photochemistry: 5Î±-6-AziP which contains an aliphatic diazirine, and KK200 which contains a trifluoromethyl-phenyldiazirine (TPD) group. 5Î±-6-AziP and KK200 photolabel multiple residues within an E73 pocket confirming the presence of this site and mapping sterol orientation within this pocket. In addition, KK200 photolabels four other sites consistent with the finding that VDAC1 co-purifies with five cholesterol molecules. Both allopregnanolone and cholesterol competitively prevent photolabeling at E73 and three other sites indicating that these are common sterol binding sites shared by both neurosteroids and cholesterol. Binding at the functionally important residue E73 suggests a possible role for sterols in regulating VDAC1 signaling and interaction with partner proteins."
0,Recommendations for the management of hemophagocytic lymphohistiocytosis in adults,,
0,Receptor tyrosine kinases (RTKs) consociate in regulatory clusters in Alzheimerâ€™s disease and type 2 diabetes,"Alzheimerâ€™s disease (AD) and type 2 diabetes (T2D) share the common hallmark of insulin resistance. It is conjectured that receptor tyrosine kinases (RTKs) play definitive roles in the process. To decipher the signaling overlap behind this phenotypic resemblance, the activity status of RTKs is probed in post-mortem AD and T2D tissues and cell models. Activities of only about one-third changed in a similar fashion, whereas about half of them showed opposite outcomes when exposed to contrasting signals akin to AD and T2D. Interestingly, irrespective of disease type, RTKs with enhanced and compromised activities clustered distinctly, indicating separate levels of regulations. Similar regulatory mechanisms within an activity cluster could be inferred, which have potential to impact future therapeutic developments.","Receptor tyrosine kinases (RTKs) consociate in regulatory clusters in Alzheimerâ€™s disease and type 2 diabetes. Alzheimerâ€™s disease (AD) and type 2 diabetes (T2D) share the common hallmark of insulin resistance. It is conjectured that receptor tyrosine kinases (RTKs) play definitive roles in the process. To decipher the signaling overlap behind this phenotypic resemblance, the activity status of RTKs is probed in post-mortem AD and T2D tissues and cell models. Activities of only about one-third changed in a similar fashion, whereas about half of them showed opposite outcomes when exposed to contrasting signals akin to AD and T2D. Interestingly, irrespective of disease type, RTKs with enhanced and compromised activities clustered distinctly, indicating separate levels of regulations. Similar regulatory mechanisms within an activity cluster could be inferred, which have potential to impact future therapeutic developments."
0,Evaluating global and local sequence alignment methods for comparing patient medical records,"BACKGROUND: Sequence alignment is a way of arranging sequences (e.g., DNA, RNA, protein, natural language, financial data, or medical events) to identify the relatedness between two or more sequences and regions of similarity. For Electronic Health Records (EHR) data, sequence alignment helps to identify patients of similar disease trajectory for more relevant and precise prognosis, diagnosis and treatment of patients. METHODS: We tested two cutting-edge global sequence alignment methods, namely dynamic time warping (DTW) and Needleman-Wunsch algorithm (NWA), together with their local modifications, DTW for Local alignment (DTWL) and Smith-Waterman algorithm (SWA), for aligning patient medical records. We also used 4 sets of synthetic patient medical records generated from a large real-world EHR database as gold standard data, to objectively evaluate these sequence alignment algorithms. RESULTS: For global sequence alignments, 47 out of 80 DTW alignments and 11 out of 80 NWA alignments had superior similarity scores than reference alignments while the rest 33 DTW alignments and 69 NWA alignments had the same similarity scores as reference alignments. Forty-six out of 80 DTW alignments had better similarity scores than NWA alignments with the rest 34 cases having the equal similarity scores from both algorithms. For local sequence alignments, 70 out of 80 DTWL alignments and 68 out of 80 SWA alignments had larger coverage and higher similarity scores than reference alignments while the rest DTWL alignments and SWA alignments received the same coverage and similarity scores as reference alignments. Six out of 80 DTWL alignmentsâ€‰showed larger coverage and higher similarity scores than SWA alignments. Thirty DTWL alignments had the equal coverage but better similarity scores than SWA. DTWL and SWA received the equal coverage and similarity scores for the rest 44 cases. CONCLUSIONS: DTW, NWA, DTWL and SWA outperformed the reference alignments. DTW (or DTWL) seems to align better than NWA (or SWA) by inserting new daily events and identifying more similarities between patient medical records. The evaluation results could provide valuable information on the strengths and weakness of these sequence alignment methods for future development of sequence alignment methods and patient similarity-based studies.","Evaluating global and local sequence alignment methods for comparing patient medical records. BACKGROUND: Sequence alignment is a way of arranging sequences (e.g., DNA, RNA, protein, natural language, financial data, or medical events) to identify the relatedness between two or more sequences and regions of similarity. For Electronic Health Records (EHR) data, sequence alignment helps to identify patients of similar disease trajectory for more relevant and precise prognosis, diagnosis and treatment of patients. METHODS: We tested two cutting-edge global sequence alignment methods, namely dynamic time warping (DTW) and Needleman-Wunsch algorithm (NWA), together with their local modifications, DTW for Local alignment (DTWL) and Smith-Waterman algorithm (SWA), for aligning patient medical records. We also used 4 sets of synthetic patient medical records generated from a large real-world EHR database as gold standard data, to objectively evaluate these sequence alignment algorithms. RESULTS: For global sequence alignments, 47 out of 80 DTW alignments and 11 out of 80 NWA alignments had superior similarity scores than reference alignments while the rest 33 DTW alignments and 69 NWA alignments had the same similarity scores as reference alignments. Forty-six out of 80 DTW alignments had better similarity scores than NWA alignments with the rest 34 cases having the equal similarity scores from both algorithms. For local sequence alignments, 70 out of 80 DTWL alignments and 68 out of 80 SWA alignments had larger coverage and higher similarity scores than reference alignments while the rest DTWL alignments and SWA alignments received the same coverage and similarity scores as reference alignments. Six out of 80 DTWL alignmentsâ€‰showed larger coverage and higher similarity scores than SWA alignments. Thirty DTWL alignments had the equal coverage but better similarity scores than SWA. DTWL and SWA received the equal coverage and similarity scores for the rest 44 cases. CONCLUSIONS: DTW, NWA, DTWL and SWA outperformed the reference alignments. DTW (or DTWL) seems to align better than NWA (or SWA) by inserting new daily events and identifying more similarities between patient medical records. The evaluation results could provide valuable information on the strengths and weakness of these sequence alignment methods for future development of sequence alignment methods and patient similarity-based studies."
0,Breast MRI and X-ray mammography registration using gradient values,,
0,Single-Cell Reconstruction of Emerging Population Activity in an Entire Developing Circuit,,
0,Deep learning and medical diagnosis,,
0,Sexual Dysfunction and Sex Hormone Abnormalities in Patients With Cirrhosis: Review of Pathogenesis and Management,,
0,A central hydrophobic E1 region controls the pH range of hepatitis C virus membrane fusion and susceptibility to fusion inhibitors,,
0,Machine-learning-based patient-specific prediction models for knee osteoarthritis,"Osteoarthritis (OA) is an extremely common musculoskeletal disease. However, current guidelines are not well suited for diagnosing patients in the early stages of disease and do not discriminate patients for whom the disease might progress rapidly. The most important hurdle in OA management is identifying and classifying patients who will benefit most from treatment. Further efforts are needed in patient subgrouping and developing prediction models. Conventional statistical modelling approaches exist; however, these models are limited in the amount of information they can adequately process. Comprehensive patient-specific prediction models need to be developed. Approaches such as data mining and machine learning should aid in the development of such models. Although a challenging task, technology is now available that should enable subgrouping of patients with OA and lead to improved clinical decision-making and precision medicine.","Machine-learning-based patient-specific prediction models for knee osteoarthritis. Osteoarthritis (OA) is an extremely common musculoskeletal disease. However, current guidelines are not well suited for diagnosing patients in the early stages of disease and do not discriminate patients for whom the disease might progress rapidly. The most important hurdle in OA management is identifying and classifying patients who will benefit most from treatment. Further efforts are needed in patient subgrouping and developing prediction models. Conventional statistical modelling approaches exist; however, these models are limited in the amount of information they can adequately process. Comprehensive patient-specific prediction models need to be developed. Approaches such as data mining and machine learning should aid in the development of such models. Although a challenging task, technology is now available that should enable subgrouping of patients with OA and lead to improved clinical decision-making and precision medicine."
0,Physiological Signature of Memory Age in the Prefrontal-Hippocampal Circuit,"The long-term storage of episodic memory requires communication between prefrontal cortex and hippocampus. However, how consolidation alters dynamic interactions between these regions during subsequent recall remains unexplored. Here we perform simultaneous electrophysiological recordings from anterior cingulate cortex (ACC) and hippocampal CA1 in mice during recall of recent and remote contextual fear memory. We find that, in contrast to recent memory, remote memory recall is accompanied by increased ACC-CA1 synchronization at multiple frequency bands. The augmented ACC-CA1 interaction is associated with strengthened coupling among distally spaced CA1 neurons, suggesting an ACC-driven organization of a sparse code. This robust shift in physiology permits a support vector machine classifier to accurately determine memory age on the basis of the ACC-CA1 synchronization pattern. Our findings reveal that memory consolidation alters the dynamic coupling of the prefrontal-hippocampal circuit and results in a physiological signature of memory age.","Physiological Signature of Memory Age in the Prefrontal-Hippocampal Circuit. The long-term storage of episodic memory requires communication between prefrontal cortex and hippocampus. However, how consolidation alters dynamic interactions between these regions during subsequent recall remains unexplored. Here we perform simultaneous electrophysiological recordings from anterior cingulate cortex (ACC) and hippocampal CA1 in mice during recall of recent and remote contextual fear memory. We find that, in contrast to recent memory, remote memory recall is accompanied by increased ACC-CA1 synchronization at multiple frequency bands. The augmented ACC-CA1 interaction is associated with strengthened coupling among distally spaced CA1 neurons, suggesting an ACC-driven organization of a sparse code. This robust shift in physiology permits a support vector machine classifier to accurately determine memory age on the basis of the ACC-CA1 synchronization pattern. Our findings reveal that memory consolidation alters the dynamic coupling of the prefrontal-hippocampal circuit and results in a physiological signature of memory age."
0,"Re: Eugene Shkolyar, Xiao Jia, Timothy C. Chang, et al. Augmented Bladder Tumor Detection Using Deep Learning. Eur Urol 2019;76:714-8",,
0,"Effectiveness of polypill for primary and secondary prevention of cardiovascular diseases (PolyIran): a pragmatic, cluster-randomised trial",,
0,"Deep medicine, artificial intelligence, and the practising clinician",,
0,Pneumatosis Cystoides Intestinalis Secondary to Use of an alpha-Glucosidase Inhibitor,,
0,"Efficacy and safety of dolutegravir-rilpivirine for maintenance of virological suppression in adults with HIV-1:100-week data from the randomised, open-label, phase 3 SWORD-1 and SWORD-2 studies",,
0,Interpretable deep neural network for cancer survival analysis by integrating genomic and clinical data,"Background: Understanding the complex biological mechanisms of cancer patient survival using genomic and clinical data is vital, not only to develop new treatments for patients, but also to improve survival prediction. However, highly nonlinear and high-dimension, low-sample size (HDLSS) data cause computational challenges to applying conventional survival analysis. Results: We propose a novel biologically interpretable pathway-based sparse deep neural network, named Cox-PASNet, which integrates high-dimensional gene expression data and clinical data on a simple neural network architecture for survival analysis. Cox-PASNet is biologically interpretable where nodes in the neural network correspond to biological genes and pathways, while capturing the nonlinear and hierarchical effects of biological pathways associated with cancer patient survival. We also propose a heuristic optimization solution to train Cox-PASNet with HDLSS data. Cox-PASNet was intensively evaluated by comparing the predictive performance of current state-of-the-art methods on glioblastoma multiforme (GBM) and ovarian serous cystadenocarcinoma (OV) cancer. In the experiments, Cox-PASNet showed out-performance, compared to the benchmarking methods. Moreover, the neural network architecture of Cox-PASNet was biologically interpreted, and several significant prognostic factors of genes and biological pathways were identified. Conclusions: Cox-PASNet models biological mechanisms in the neural network by incorporating biological pathway databases and sparse coding. The neural network of Cox-PASNet can identify nonlinear and hierarchical associations of genomic and clinical data to cancer patient survival. The open-source code of Cox-PASNet in PyTorch implemented for training, evaluation, and model interpretation is available at: https://github.com/DataX-JieHao/Cox-PASNet.","Interpretable deep neural network for cancer survival analysis by integrating genomic and clinical data. Background: Understanding the complex biological mechanisms of cancer patient survival using genomic and clinical data is vital, not only to develop new treatments for patients, but also to improve survival prediction. However, highly nonlinear and high-dimension, low-sample size (HDLSS) data cause computational challenges to applying conventional survival analysis. Results: We propose a novel biologically interpretable pathway-based sparse deep neural network, named Cox-PASNet, which integrates high-dimensional gene expression data and clinical data on a simple neural network architecture for survival analysis. Cox-PASNet is biologically interpretable where nodes in the neural network correspond to biological genes and pathways, while capturing the nonlinear and hierarchical effects of biological pathways associated with cancer patient survival. We also propose a heuristic optimization solution to train Cox-PASNet with HDLSS data. Cox-PASNet was intensively evaluated by comparing the predictive performance of current state-of-the-art methods on glioblastoma multiforme (GBM) and ovarian serous cystadenocarcinoma (OV) cancer. In the experiments, Cox-PASNet showed out-performance, compared to the benchmarking methods. Moreover, the neural network architecture of Cox-PASNet was biologically interpreted, and several significant prognostic factors of genes and biological pathways were identified. Conclusions: Cox-PASNet models biological mechanisms in the neural network by incorporating biological pathway databases and sparse coding. The neural network of Cox-PASNet can identify nonlinear and hierarchical associations of genomic and clinical data to cancer patient survival. The open-source code of Cox-PASNet in PyTorch implemented for training, evaluation, and model interpretation is available at: https://github.com/DataX-JieHao/Cox-PASNet."
0,Diagnosis of Suspected Pulmonary Embolism in Pregnancy,,
0,Estimation of the global prevalence and burden of obstructive sleep apnoea: a literature-based analysis,,
0,Multi-Dimensional Screening Strategy for Drug Repurposing with Statistical Framework-A New Road to Influenza Drug discovery,"Influenza virus is known for its intermittent outbreaks affecting billions of people worldwide. Several neuraminidase inhibitors have been used in practice to overcome this situation. However, advent of new resistant mutants has limited its clinical utilization. In the recent years drug repurposing technique has attained the limelight as it is cost effective and reduces the time consumed for drug discovery. Here, we present multi-dimensional repurposing strategy that integrates the results of ligand-, energy-, receptor cavity, and shape-based pharmacophore algorithm to effectively identify novel drug candidate for influenza. The pharmacophore hypotheses were generated by utilizing the PHASE module of SchrÃ¶dinger. The generated hypotheses such as AADP, AADDD, and DDRRNH, respectively, for ligand-, e-pharmacophore and receptor cavity based approach alongside shape of oseltamivir were successfully utilized to screen the DrugBank database. Subsequently, these models were evaluated for their differentiating ability using Enrichment calculation. Receiver operating curve and enrichment factors from the analysis indicate that the models possess better capability to screen actives from decoy set of molecules. Eventually, the hits retrieved from different hypotheses were subjected to molecular docking using Glide module of SchrÃ¶dinger Suite. The results of different algorithms were then combined to eliminate false positive hits and to demonstrate reliable prediction performance than existing approaches. Of note, Pearson's correlation coefficients were calculated to examine the extent of correlation between the glide score and IC50 values. Further, the interaction profile, pharmacokinetic, and pharmacodynamics properties were analyzed for the hit compounds. The results from our analysis showed that alprostadil (DB00770) exhibits better binding affinity toward NA protein than the existing drug molecules. The biological activity of the hit was also predicted using PASS algorithm that renders the antiviral activity of the compound. Further, the results were validated using mutation analysis and molecular dynamic simulation studies. Indeed, this integrative filtering is able to exceed accuracy of other state-of-the-art methods for the drug discovery.","Multi-Dimensional Screening Strategy for Drug Repurposing with Statistical Framework-A New Road to Influenza Drug discovery. Influenza virus is known for its intermittent outbreaks affecting billions of people worldwide. Several neuraminidase inhibitors have been used in practice to overcome this situation. However, advent of new resistant mutants has limited its clinical utilization. In the recent years drug repurposing technique has attained the limelight as it is cost effective and reduces the time consumed for drug discovery. Here, we present multi-dimensional repurposing strategy that integrates the results of ligand-, energy-, receptor cavity, and shape-based pharmacophore algorithm to effectively identify novel drug candidate for influenza. The pharmacophore hypotheses were generated by utilizing the PHASE module of SchrÃ¶dinger. The generated hypotheses such as AADP, AADDD, and DDRRNH, respectively, for ligand-, e-pharmacophore and receptor cavity based approach alongside shape of oseltamivir were successfully utilized to screen the DrugBank database. Subsequently, these models were evaluated for their differentiating ability using Enrichment calculation. Receiver operating curve and enrichment factors from the analysis indicate that the models possess better capability to screen actives from decoy set of molecules. Eventually, the hits retrieved from different hypotheses were subjected to molecular docking using Glide module of SchrÃ¶dinger Suite. The results of different algorithms were then combined to eliminate false positive hits and to demonstrate reliable prediction performance than existing approaches. Of note, Pearson's correlation coefficients were calculated to examine the extent of correlation between the glide score and IC50 values. Further, the interaction profile, pharmacokinetic, and pharmacodynamics properties were analyzed for the hit compounds. The results from our analysis showed that alprostadil (DB00770) exhibits better binding affinity toward NA protein than the existing drug molecules. The biological activity of the hit was also predicted using PASS algorithm that renders the antiviral activity of the compound. Further, the results were validated using mutation analysis and molecular dynamic simulation studies. Indeed, this integrative filtering is able to exceed accuracy of other state-of-the-art methods for the drug discovery."
1,Evaluating reinforcement learning agents for anatomical landmark detection,"Automatic detection of anatomical landmarks is an important step for a wide range of applications in medical image analysis. Manual annotation of landmarks is a tedious task and prone to observer errors. In this paper, we evaluate novel deep reinforcement learning (RL) strategies to train agents that can precisely and robustly localize target landmarks in medical scans. An artificial RL agent learns to identify the optimal path to the landmark by interacting with an environment, in our case 3D images. Furthermore, we investigate the use of fixed- and multi-scale search strategies with novel hierarchical action steps in a coarse-to-fine manner. Several deep Q-network (DQN) architectures are evaluated for detecting multiple landmarks using three different medical imaging datasets: fetal head ultrasound (US), adult brain and cardiac magnetic resonance imaging (MRI). The performance of our agents surpasses state-of-the-art supervised and RL methods. Our experiments also show that multi-scale search strategies perform significantly better than fixed-scale agents in images with large field of view and noisy background such as in cardiac MRI. Moreover, the novel hierarchical steps can significantly speed up the searching process by a factor of 4-5 times.","Evaluating reinforcement learning agents for anatomical landmark detection. Automatic detection of anatomical landmarks is an important step for a wide range of applications in medical image analysis. Manual annotation of landmarks is a tedious task and prone to observer errors. In this paper, we evaluate novel deep reinforcement learning (RL) strategies to train agents that can precisely and robustly localize target landmarks in medical scans. An artificial RL agent learns to identify the optimal path to the landmark by interacting with an environment, in our case 3D images. Furthermore, we investigate the use of fixed- and multi-scale search strategies with novel hierarchical action steps in a coarse-to-fine manner. Several deep Q-network (DQN) architectures are evaluated for detecting multiple landmarks using three different medical imaging datasets: fetal head ultrasound (US), adult brain and cardiac magnetic resonance imaging (MRI). The performance of our agents surpasses state-of-the-art supervised and RL methods. Our experiments also show that multi-scale search strategies perform significantly better than fixed-scale agents in images with large field of view and noisy background such as in cardiac MRI. Moreover, the novel hierarchical steps can significantly speed up the searching process by a factor of 4-5 times."
1,A deep network for tissue microstructure estimation using modified LSTM units,,
1,Automated segmentation of knee bone and cartilage combining statistical shape knowledge and convolutional neural networks: Data from the Osteoarthritis Initiative,"We present a method for the automated segmentation of knee bones and cartilage from magnetic resonance imaging (MRI) that combines a priori knowledge of anatomical shape with Convolutional Neural Networks (CNNs). The proposed approach incorporates 3D Statistical Shape Models (SSMs) as well as 2D and 3D CNNs to achieve a robust and accurate segmentation of even highly pathological knee structures. The shape models and neural networks employed are trained using data from the Osteoarthritis Initiative (OAI) and the MICCAI grand challenge ""Segmentation of Knee Images 2010"" (SKI10), respectively. We evaluate our method on 40 validation and 50 submission datasets from the SKI10 challenge. For the first time, an accuracy equivalent to the inter-observer variability of human readers is achieved in this challenge. Moreover, the quality of the proposed method is thoroughly assessed using various measures for data from the OAI, i.e. 507 manual segmentations of bone and cartilage, and 88 additional manual segmentations of cartilage. Our method yields sub-voxel accuracy for both OAI datasets. We make the 507 manual segmentations as well as our experimental setup publicly available to further aid research in the field of medical image segmentation. In conclusion, combining localized classification via CNNs with statistical anatomical knowledge via SSMs results in a state-of-the-art segmentation method for knee bones and cartilage from MRI data.","Automated segmentation of knee bone and cartilage combining statistical shape knowledge and convolutional neural networks: Data from the Osteoarthritis Initiative. We present a method for the automated segmentation of knee bones and cartilage from magnetic resonance imaging (MRI) that combines a priori knowledge of anatomical shape with Convolutional Neural Networks (CNNs). The proposed approach incorporates 3D Statistical Shape Models (SSMs) as well as 2D and 3D CNNs to achieve a robust and accurate segmentation of even highly pathological knee structures. The shape models and neural networks employed are trained using data from the Osteoarthritis Initiative (OAI) and the MICCAI grand challenge ""Segmentation of Knee Images 2010"" (SKI10), respectively. We evaluate our method on 40 validation and 50 submission datasets from the SKI10 challenge. For the first time, an accuracy equivalent to the inter-observer variability of human readers is achieved in this challenge. Moreover, the quality of the proposed method is thoroughly assessed using various measures for data from the OAI, i.e. 507 manual segmentations of bone and cartilage, and 88 additional manual segmentations of cartilage. Our method yields sub-voxel accuracy for both OAI datasets. We make the 507 manual segmentations as well as our experimental setup publicly available to further aid research in the field of medical image segmentation. In conclusion, combining localized classification via CNNs with statistical anatomical knowledge via SSMs results in a state-of-the-art segmentation method for knee bones and cartilage from MRI data."
1,Machine learning analysis of DNA methylation profiles distinguishes primary lung squamous cell carcinomas from head and neck metastases,"Head and neck squamous cell carcinoma (HNSC) patients are at risk of suffering from both pulmonary metastases or a second squamous cell carcinoma of the lung (LUSC). Differentiating pulmonary metastases from primary lung cancers is of high clinical importance, but not possible in most cases with current diagnostics. To address this, we performed DNA methylation profiling of primary tumors and trained three different machine learning methods to distinguish metastatic HNSC from primary LUSC. We developed an artificial neural network that correctly classified 96.4% of the cases in a validation cohort of 279 patients with HNSC and LUSC as well as normal lung controls, outperforming support vector machines (95.7%) and random forests (87.8%). Prediction accuracies of more than 99% were achieved for 92.1% (neural network), 90% (support vector machine), and 43% (random forest) of these cases by applying thresholds to the resulting probability scores and excluding samples with low confidence. As independent clinical validation of the approach, we analyzed a series of 51 patients with a history of HNSC and a second lung tumor, demonstrating the correct classifications based on clinicopathological properties. In summary, our approach may facilitate the reliable diagnostic differentiation of pulmonary metastases of HNSC from primary LUSC to guide therapeutic decisions.","Machine learning analysis of DNA methylation profiles distinguishes primary lung squamous cell carcinomas from head and neck metastases. Head and neck squamous cell carcinoma (HNSC) patients are at risk of suffering from both pulmonary metastases or a second squamous cell carcinoma of the lung (LUSC). Differentiating pulmonary metastases from primary lung cancers is of high clinical importance, but not possible in most cases with current diagnostics. To address this, we performed DNA methylation profiling of primary tumors and trained three different machine learning methods to distinguish metastatic HNSC from primary LUSC. We developed an artificial neural network that correctly classified 96.4% of the cases in a validation cohort of 279 patients with HNSC and LUSC as well as normal lung controls, outperforming support vector machines (95.7%) and random forests (87.8%). Prediction accuracies of more than 99% were achieved for 92.1% (neural network), 90% (support vector machine), and 43% (random forest) of these cases by applying thresholds to the resulting probability scores and excluding samples with low confidence. As independent clinical validation of the approach, we analyzed a series of 51 patients with a history of HNSC and a second lung tumor, demonstrating the correct classifications based on clinicopathological properties. In summary, our approach may facilitate the reliable diagnostic differentiation of pulmonary metastases of HNSC from primary LUSC to guide therapeutic decisions."
1,Predicting skilled delivery service use in Ethiopia: dual application of logistic regression and machine learning algorithms,"BACKGROUND: Skilled assistance during childbirth is essential to reduce maternal deaths. However, in Ethiopia, which is among the six countries contributing to more than half of the global maternal deaths, the coverage of births attended by skilled health personnel remains very low. The aim of this study was to identify determinants and develop a predictive model for skilled delivery service use in Ethiopia by applying logistic regression and machine-learning techniques. METHODS: Data from the 2016 Ethiopian Demographic and Health Survey (EDHS) was used for this study. Statistical Package for Social Sciences (SPSS) and Waikato Environment for Knowledge Analysis (WEKA) tools were used for logistic regression and model building respectively. Classification algorithms namely J48, NaÃ¯ve Bayes, Support Vector Machine (SVM), and Artificial Neural Network (ANN) were used for model development. The validation of the predictive models was assessed using accuracy, sensitivity, specificity, and area under Receiver Operating Characteristics (ROC) curve. RESULTS: Only 27.7% women received skilled delivery assistance in Ethiopia. First antenatal care (ANC) [AORâ€‰=â€‰1.83, 95% CI (1.24-2.69)], birth order [AORâ€‰=â€‰0.22, 95% CI (0.11-0.46)], television ownership [AORâ€‰=â€‰6.83, 95% CI (2.52-18.52)], contraceptive use [AORâ€‰=â€‰1.92, 95% CI (1.26-2.97)], cost needed for healthcare [AORâ€‰=â€‰2.17, 95% CI (1.47-3.21)], age at first birth [AORâ€‰=â€‰1.96, 95% CI (1.31-2.94)], and age at first sex [AORâ€‰=â€‰2.72, 95% CI (1.55-4.76)] were determinants for utilizing skilled delivery services during the childbirth. Predictive models were developed and the J48 model had superior predictive accuracy (98%), sensitivity (96%), specificity (99%) and, the area under ROC (98%). CONCLUSIONS: First ANC and contraceptive uses were among the determinants of utilization of skilled delivery services. A predictive model was developed to forecast the likelihood of a pregnant woman seeking skilled delivery assistance; therefore, the predictive model can help to decide targeted interventions for a pregnant woman to ensure skilled assistance at childbirth. The model developed through the J48 algorithm has better predictive accuracy. Web-based application can be build based on results of this study.","Predicting skilled delivery service use in Ethiopia: dual application of logistic regression and machine learning algorithms. BACKGROUND: Skilled assistance during childbirth is essential to reduce maternal deaths. However, in Ethiopia, which is among the six countries contributing to more than half of the global maternal deaths, the coverage of births attended by skilled health personnel remains very low. The aim of this study was to identify determinants and develop a predictive model for skilled delivery service use in Ethiopia by applying logistic regression and machine-learning techniques. METHODS: Data from the 2016 Ethiopian Demographic and Health Survey (EDHS) was used for this study. Statistical Package for Social Sciences (SPSS) and Waikato Environment for Knowledge Analysis (WEKA) tools were used for logistic regression and model building respectively. Classification algorithms namely J48, NaÃ¯ve Bayes, Support Vector Machine (SVM), and Artificial Neural Network (ANN) were used for model development. The validation of the predictive models was assessed using accuracy, sensitivity, specificity, and area under Receiver Operating Characteristics (ROC) curve. RESULTS: Only 27.7% women received skilled delivery assistance in Ethiopia. First antenatal care (ANC) [AORâ€‰=â€‰1.83, 95% CI (1.24-2.69)], birth order [AORâ€‰=â€‰0.22, 95% CI (0.11-0.46)], television ownership [AORâ€‰=â€‰6.83, 95% CI (2.52-18.52)], contraceptive use [AORâ€‰=â€‰1.92, 95% CI (1.26-2.97)], cost needed for healthcare [AORâ€‰=â€‰2.17, 95% CI (1.47-3.21)], age at first birth [AORâ€‰=â€‰1.96, 95% CI (1.31-2.94)], and age at first sex [AORâ€‰=â€‰2.72, 95% CI (1.55-4.76)] were determinants for utilizing skilled delivery services during the childbirth. Predictive models were developed and the J48 model had superior predictive accuracy (98%), sensitivity (96%), specificity (99%) and, the area under ROC (98%). CONCLUSIONS: First ANC and contraceptive uses were among the determinants of utilization of skilled delivery services. A predictive model was developed to forecast the likelihood of a pregnant woman seeking skilled delivery assistance; therefore, the predictive model can help to decide targeted interventions for a pregnant woman to ensure skilled assistance at childbirth. The model developed through the J48 algorithm has better predictive accuracy. Web-based application can be build based on results of this study."
1,"qFIBS: A Novel Automated Technique for Quantitative Evaluation of Fibrosis, Inflammation, Ballooning, and Steatosis in Patients With Nonalcoholic Steatohepatitis",,
1,Automated detection and classification of thyroid nodules in ultrasound images using clinical-knowledge-guided convolutional neural networks,"Accurate diagnosis of thyroid nodules using ultrasonography is a valuable but tough task even for experienced radiologists, considering both benign and malignant nodules have heterogeneous appearances. Computer-aided diagnosis (CAD) methods could potentially provide objective suggestions to assist radiologists. However, the performance of existing learning-based approaches is still limited, for direct application of general learning models often ignores critical domain knowledge related to the specific nodule diagnosis. In this study, we propose a novel deep-learning-based CAD system, guided by task-specific prior knowledge, for automated nodule detection and classification in ultrasound images. Our proposed CAD system consists of two stages. First, a multi-scale region-based detection network is designed to learn pyramidal features for detecting nodules at different feature scales. The region proposals are constrained by the prior knowledge about size and shape distributions of real nodules. Then, a multi-branch classification network is proposed to integrate multi-view diagnosis-oriented features, in which each network branch captures and enhances one specific group of characteristics that were generally used by radiologists. We evaluated and compared our method with the state-of-the-art CAD methods and experienced radiologists on two datasets, i.e. Dataset I and Dataset II. The detection and diagnostic accuracy on Dataset I were 97.5% and 97.1%, respectively. Besides, our CAD system also achieved better performance than experienced radiologists on Dataset II, with improvements of accuracy for 8%. The experimental results demonstrate that our proposed method is effective in the discrimination of thyroid nodules.","Automated detection and classification of thyroid nodules in ultrasound images using clinical-knowledge-guided convolutional neural networks. Accurate diagnosis of thyroid nodules using ultrasonography is a valuable but tough task even for experienced radiologists, considering both benign and malignant nodules have heterogeneous appearances. Computer-aided diagnosis (CAD) methods could potentially provide objective suggestions to assist radiologists. However, the performance of existing learning-based approaches is still limited, for direct application of general learning models often ignores critical domain knowledge related to the specific nodule diagnosis. In this study, we propose a novel deep-learning-based CAD system, guided by task-specific prior knowledge, for automated nodule detection and classification in ultrasound images. Our proposed CAD system consists of two stages. First, a multi-scale region-based detection network is designed to learn pyramidal features for detecting nodules at different feature scales. The region proposals are constrained by the prior knowledge about size and shape distributions of real nodules. Then, a multi-branch classification network is proposed to integrate multi-view diagnosis-oriented features, in which each network branch captures and enhances one specific group of characteristics that were generally used by radiologists. We evaluated and compared our method with the state-of-the-art CAD methods and experienced radiologists on two datasets, i.e. Dataset I and Dataset II. The detection and diagnostic accuracy on Dataset I were 97.5% and 97.1%, respectively. Besides, our CAD system also achieved better performance than experienced radiologists on Dataset II, with improvements of accuracy for 8%. The experimental results demonstrate that our proposed method is effective in the discrimination of thyroid nodules."
1,Recent advances in Swedish and Spanish medical entity recognition in clinical texts using deep neural approaches,"BACKGROUND: Text mining and natural language processing of clinical text, such as notes from electronic health records, requires specific consideration of the specialized characteristics of these texts. Deep learning methods could potentially mitigate domain specific challenges such as limited access to in-domain tools and data sets. METHODS: A bi-directional Long Short-Term Memory network is applied to clinical notes in Spanish and Swedish for the task of medical named entity recognition. Several types of embeddings, both generated from in-domain and out-of-domain text corpora, and a number of generation and combination strategies for embeddings have been evaluated in order to investigate different input representations and the influence of domain on the final results. RESULTS: For Spanish, a micro averaged F1-score of 75.25 was obtained and for Swedish, the corresponding score was 76.04. The best results for both languages were achieved using embeddings generated from in-domain corpora extracted from electronic health records, but embeddings generated from related domains were also found to be beneficial. CONCLUSIONS: A recurrent neural network with in-domain embeddings improved the medical named entity recognition compared to shallow learning methods, showing this combination to be suitable for entity recognition in clinical text for both languages.","Recent advances in Swedish and Spanish medical entity recognition in clinical texts using deep neural approaches. BACKGROUND: Text mining and natural language processing of clinical text, such as notes from electronic health records, requires specific consideration of the specialized characteristics of these texts. Deep learning methods could potentially mitigate domain specific challenges such as limited access to in-domain tools and data sets. METHODS: A bi-directional Long Short-Term Memory network is applied to clinical notes in Spanish and Swedish for the task of medical named entity recognition. Several types of embeddings, both generated from in-domain and out-of-domain text corpora, and a number of generation and combination strategies for embeddings have been evaluated in order to investigate different input representations and the influence of domain on the final results. RESULTS: For Spanish, a micro averaged F1-score of 75.25 was obtained and for Swedish, the corresponding score was 76.04. The best results for both languages were achieved using embeddings generated from in-domain corpora extracted from electronic health records, but embeddings generated from related domains were also found to be beneficial. CONCLUSIONS: A recurrent neural network with in-domain embeddings improved the medical named entity recognition compared to shallow learning methods, showing this combination to be suitable for entity recognition in clinical text for both languages."
1,Hierarchical sequence labeling for extracting BEL statements from biomedical literature,"BACKGROUND: Extracting relations between bio-entities from biomedical literature is often a challenging task and also an essential step towards biomedical knowledge expansion. The BioCreative community has organized a shared task to evaluate the robustness of the causal relationship extraction algorithms in Biological Expression Language (BEL) from biomedical literature. METHOD: We first map the sentence-level BEL statements in the BC-V training corpus to the corresponding text segments, thus generating hierarchically tagged training instances. A hierarchical sequence labeling model was afterwards induced from these training instances and applied to the test sentences in order to construct the BEL statements. RESULTS: The experimental results on extracting BEL statements from BioCreative V Track 4 test corpus show that our method achieves promising performance with an overall F-measure of 31.6%. Furthermore, it has the potential to be enhanced by adopting more advanced machine learning approaches. CONCLUSION: We propose a framework for hierarchical relation extraction using hierarchical sequence labeling on the instance-level training corpus derived from the original sentence-level corpus via word alignment. Its main advantage is that we can make full use of the original training corpus to induce the sequence labelers and then apply them to the test corpus.","Hierarchical sequence labeling for extracting BEL statements from biomedical literature. BACKGROUND: Extracting relations between bio-entities from biomedical literature is often a challenging task and also an essential step towards biomedical knowledge expansion. The BioCreative community has organized a shared task to evaluate the robustness of the causal relationship extraction algorithms in Biological Expression Language (BEL) from biomedical literature. METHOD: We first map the sentence-level BEL statements in the BC-V training corpus to the corresponding text segments, thus generating hierarchically tagged training instances. A hierarchical sequence labeling model was afterwards induced from these training instances and applied to the test sentences in order to construct the BEL statements. RESULTS: The experimental results on extracting BEL statements from BioCreative V Track 4 test corpus show that our method achieves promising performance with an overall F-measure of 31.6%. Furthermore, it has the potential to be enhanced by adopting more advanced machine learning approaches. CONCLUSION: We propose a framework for hierarchical relation extraction using hierarchical sequence labeling on the instance-level training corpus derived from the original sentence-level corpus via word alignment. Its main advantage is that we can make full use of the original training corpus to induce the sequence labelers and then apply them to the test corpus."
1,Survival outcome prediction in cervical cancer: Cox models vs deep-learning model,"BACKGROUND: Historically, the Cox proportional hazard regression model has been the mainstay for survival analyses in oncologic research. The Cox proportional hazard regression model generally is used based on an assumption of linear association. However, it is likely that, in reality, there are many clinicopathologic features that exhibit a nonlinear association in biomedicine. OBJECTIVE: The purpose of this study was to compare the deep-learning neural network model and the Cox proportional hazard regression model in the prediction of survival in women with cervical cancer. STUDY DESIGN: This was a retrospective pilot study of consecutive cases of newly diagnosed stage I-IV cervical cancer from 2000-2014. A total of 40 features that included patient demographics, vital signs, laboratory test results, tumor characteristics, and treatment types were assessed for analysis and grouped into 3 feature sets. The deep-learning neural network model was compared with the Cox proportional hazard regression model and 3 other survival analysis models for progression-free survival and overall survival. Mean absolute error and concordance index were used to assess the performance of these 5 models. RESULTS: There were 768 women included in the analysis. The median age was 49 years, and the majority were Hispanic (71.7%). The majority of tumors were squamous (75.3%) and stage I (48.7%). The median follow-up time was 40.2 months; there were 241 events for recurrence and progression and 170 deaths during the follow-up period. The deep-learning model showed promising results in the prediction of progression-free survival when compared with the Cox proportional hazard regression model (mean absolute error, 29.3 vs 316.2). The deep-learning model also outperformed all the other models, including the Cox proportional hazard regression model, for overall survival (mean absolute error, Cox proportional hazard regression vs deep-learning, 43.6 vs 30.7). The performance of the deep-learning model further improved when more features were included (concordance index for progression-free survival: 0.695 for 20 features, 0.787 for 36 features, and 0.795 for 40 features). There were 10 features for progression-free survival and 3 features for overall survival that demonstrated significance only in the deep-learning model, but not in the Cox proportional hazard regression model. There were no features for progression-free survival and 3 features for overall survival that demonstrated significance only in the Cox proportional hazard regression model, but not in the deep-learning model. CONCLUSION: Our study suggests that the deep-learning neural network model may be a useful analytic tool for survival prediction in women with cervical cancer because it exhibited superior performance compared with the Cox proportional hazard regression model. This novel analytic approach may provide clinicians with meaningful survival information that potentially could be integrated into treatment decision-making and planning. Further validation studies are necessary to support this pilot study.","Survival outcome prediction in cervical cancer: Cox models vs deep-learning model. BACKGROUND: Historically, the Cox proportional hazard regression model has been the mainstay for survival analyses in oncologic research. The Cox proportional hazard regression model generally is used based on an assumption of linear association. However, it is likely that, in reality, there are many clinicopathologic features that exhibit a nonlinear association in biomedicine. OBJECTIVE: The purpose of this study was to compare the deep-learning neural network model and the Cox proportional hazard regression model in the prediction of survival in women with cervical cancer. STUDY DESIGN: This was a retrospective pilot study of consecutive cases of newly diagnosed stage I-IV cervical cancer from 2000-2014. A total of 40 features that included patient demographics, vital signs, laboratory test results, tumor characteristics, and treatment types were assessed for analysis and grouped into 3 feature sets. The deep-learning neural network model was compared with the Cox proportional hazard regression model and 3 other survival analysis models for progression-free survival and overall survival. Mean absolute error and concordance index were used to assess the performance of these 5 models. RESULTS: There were 768 women included in the analysis. The median age was 49 years, and the majority were Hispanic (71.7%). The majority of tumors were squamous (75.3%) and stage I (48.7%). The median follow-up time was 40.2 months; there were 241 events for recurrence and progression and 170 deaths during the follow-up period. The deep-learning model showed promising results in the prediction of progression-free survival when compared with the Cox proportional hazard regression model (mean absolute error, 29.3 vs 316.2). The deep-learning model also outperformed all the other models, including the Cox proportional hazard regression model, for overall survival (mean absolute error, Cox proportional hazard regression vs deep-learning, 43.6 vs 30.7). The performance of the deep-learning model further improved when more features were included (concordance index for progression-free survival: 0.695 for 20 features, 0.787 for 36 features, and 0.795 for 40 features). There were 10 features for progression-free survival and 3 features for overall survival that demonstrated significance only in the deep-learning model, but not in the Cox proportional hazard regression model. There were no features for progression-free survival and 3 features for overall survival that demonstrated significance only in the Cox proportional hazard regression model, but not in the deep-learning model. CONCLUSION: Our study suggests that the deep-learning neural network model may be a useful analytic tool for survival prediction in women with cervical cancer because it exhibited superior performance compared with the Cox proportional hazard regression model. This novel analytic approach may provide clinicians with meaningful survival information that potentially could be integrated into treatment decision-making and planning. Further validation studies are necessary to support this pilot study."
1,A clinical text classification paradigm using weak supervision and deep representation,"BACKGROUND: Automatic clinical text classification is a natural language processing (NLP) technology that unlocks information embedded in clinical narratives. Machine learning approaches have been shown to be effective for clinical text classification tasks. However, a successful machine learning model usually requires extensive human efforts to create labeled training data and conduct feature engineering. In this study, we propose a clinical text classification paradigm using weak supervision and deep representation to reduce theseÂ human efforts. METHODS: We develop a rule-based NLP algorithm to automatically generate labels for the training data, and then use the pre-trained word embeddings as deep representation features for training machine learning models. Since machine learning is trained on labels generated by the automatic NLPÂ algorithm, this training process is called weak supervision. We evaluat the paradigm effectiveness on two institutional case studies at Mayo Clinic: smoking status classification and proximal femur (hip) fracture classification, and one case study using a public dataset: the i2b2 2006 smoking status classification shared task. We test four widely used machine learning models, namely, Support Vector Machine (SVM), Random Forest (RF), Multilayer Perceptron Neural Networks (MLPNN), and Convolutional Neural Networks (CNN), using this paradigm. Precision, recall, and F1 score are used as metrics to evaluate performance. RESULTS: CNN achieves the best performance in both institutional tasks (F1 score: 0.92 for Mayo Clinic smoking status classification and 0.97 for fracture classification). We show that word embeddings significantly outperform tf-idf and topic modeling features in the paradigm, and that CNN captures additional patterns from the weak supervision compared to the rule-based NLP algorithms. We also observe two drawbacks of the proposed paradigm that CNN is more sensitive to the size of training data, and that the proposed paradigm might not be effective for complex multiclass classification tasks. CONCLUSION: The proposed clinical text classification paradigm could reduce human efforts of labeled training data creation and feature engineering for applying machine learning to clinical text classification by leveraging weak supervision and deep representation. The experimental experiments have validated the effectiveness of paradigm by two institutional and one shared clinical text classification tasks.","A clinical text classification paradigm using weak supervision and deep representation. BACKGROUND: Automatic clinical text classification is a natural language processing (NLP) technology that unlocks information embedded in clinical narratives. Machine learning approaches have been shown to be effective for clinical text classification tasks. However, a successful machine learning model usually requires extensive human efforts to create labeled training data and conduct feature engineering. In this study, we propose a clinical text classification paradigm using weak supervision and deep representation to reduce theseÂ human efforts. METHODS: We develop a rule-based NLP algorithm to automatically generate labels for the training data, and then use the pre-trained word embeddings as deep representation features for training machine learning models. Since machine learning is trained on labels generated by the automatic NLPÂ algorithm, this training process is called weak supervision. We evaluat the paradigm effectiveness on two institutional case studies at Mayo Clinic: smoking status classification and proximal femur (hip) fracture classification, and one case study using a public dataset: the i2b2 2006 smoking status classification shared task. We test four widely used machine learning models, namely, Support Vector Machine (SVM), Random Forest (RF), Multilayer Perceptron Neural Networks (MLPNN), and Convolutional Neural Networks (CNN), using this paradigm. Precision, recall, and F1 score are used as metrics to evaluate performance. RESULTS: CNN achieves the best performance in both institutional tasks (F1 score: 0.92 for Mayo Clinic smoking status classification and 0.97 for fracture classification). We show that word embeddings significantly outperform tf-idf and topic modeling features in the paradigm, and that CNN captures additional patterns from the weak supervision compared to the rule-based NLP algorithms. We also observe two drawbacks of the proposed paradigm that CNN is more sensitive to the size of training data, and that the proposed paradigm might not be effective for complex multiclass classification tasks. CONCLUSION: The proposed clinical text classification paradigm could reduce human efforts of labeled training data creation and feature engineering for applying machine learning to clinical text classification by leveraging weak supervision and deep representation. The experimental experiments have validated the effectiveness of paradigm by two institutional and one shared clinical text classification tasks."
1,Machine learning in medicine: a practical introduction,"BACKGROUND: Following visible successes on a wide range of predictive tasks, machine learning techniques are attracting substantial interest from medical researchers and clinicians. We address the need for capacity development in this area by providing a conceptual introduction to machine learning alongside a practical guide to developing and evaluating predictive algorithms using freely-available open source software and public domain data. METHODS: We demonstrate the use of machine learning techniques by developing three predictive models for cancer diagnosis using descriptions of nuclei sampled from breast masses. These algorithms include regularized General Linear Model regression (GLMs), Support Vector Machines (SVMs) with a radial basis function kernel, and single-layer Artificial Neural Networks. The publicly-available dataset describing the breast mass samples (N=683) was randomly split into evaluation (n=456) and validation (n=227) samples. We trained algorithms on data from the evaluation sample before they were used to predict the diagnostic outcome in the validation dataset. We compared the predictions made on the validation datasets with the real-world diagnostic decisions to calculate the accuracy, sensitivity, and specificity of the three models. We explored the use of averaging and voting ensembles to improve predictive performance. We provide a step-by-step guide to developing algorithms using the open-source R statistical programming environment. RESULTS: The trained algorithms were able to classify cell nuclei with high accuracy (.94 -.96), sensitivity (.97 -.99), and specificity (.85 -.94). Maximum accuracy (.96) and area under the curve (.97) was achieved using the SVM algorithm. Prediction performance increased marginally (accuracy =.97, sensitivity =.99, specificity =.95) when algorithms were arranged into a voting ensemble. CONCLUSIONS: We use a straightforward example to demonstrate the theory and practice of machine learning for clinicians and medical researchers. The principals which we demonstrate here can be readily applied to other complex tasks including natural language processing and image recognition.","Machine learning in medicine: a practical introduction. BACKGROUND: Following visible successes on a wide range of predictive tasks, machine learning techniques are attracting substantial interest from medical researchers and clinicians. We address the need for capacity development in this area by providing a conceptual introduction to machine learning alongside a practical guide to developing and evaluating predictive algorithms using freely-available open source software and public domain data. METHODS: We demonstrate the use of machine learning techniques by developing three predictive models for cancer diagnosis using descriptions of nuclei sampled from breast masses. These algorithms include regularized General Linear Model regression (GLMs), Support Vector Machines (SVMs) with a radial basis function kernel, and single-layer Artificial Neural Networks. The publicly-available dataset describing the breast mass samples (N=683) was randomly split into evaluation (n=456) and validation (n=227) samples. We trained algorithms on data from the evaluation sample before they were used to predict the diagnostic outcome in the validation dataset. We compared the predictions made on the validation datasets with the real-world diagnostic decisions to calculate the accuracy, sensitivity, and specificity of the three models. We explored the use of averaging and voting ensembles to improve predictive performance. We provide a step-by-step guide to developing algorithms using the open-source R statistical programming environment. RESULTS: The trained algorithms were able to classify cell nuclei with high accuracy (.94 -.96), sensitivity (.97 -.99), and specificity (.85 -.94). Maximum accuracy (.96) and area under the curve (.97) was achieved using the SVM algorithm. Prediction performance increased marginally (accuracy =.97, sensitivity =.99, specificity =.95) when algorithms were arranged into a voting ensemble. CONCLUSIONS: We use a straightforward example to demonstrate the theory and practice of machine learning for clinicians and medical researchers. The principals which we demonstrate here can be readily applied to other complex tasks including natural language processing and image recognition."
1,A modality-adaptive method for segmenting brain tumors and organs-at-risk in radiation therapy planning,,
1,Monitoring Disease Progression With a Quantitative Severity Scale for Retinopathy of Prematurity Using Deep Learning,"Importance: Retinopathy of prematurity (ROP) is a leading cause of childhood blindness worldwide, but clinical diagnosis is subjective and qualitative. Objective: To describe a quantitative ROP severity score derived using a deep learning algorithm designed to evaluate plus disease and to assess its utility for objectively monitoring ROP progression. Design, Setting, and Participants: This retrospective cohort study included images from 5255 clinical examinations of 871 premature infants who met the ROP screening criteria of the Imaging and Informatics in ROP (i-ROP) Consortium, which comprises 9 tertiary care centers in North America, from July 1, 2011, to December 31, 2016. Data analysis was performed from July 2017 to May 2018. Exposure: A deep learning algorithm was used to assign a continuous ROP vascular severity score from 1 (most normal) to 9 (most severe) at each examination based on a single posterior photograph compared with a reference standard diagnosis (RSD) simplified into 4 categories: no ROP, mild ROP, type 2 ROP or pre-plus disease, or type 1 ROP. Disease course was assessed longitudinally across multiple examinations for all patients. Main Outcomes and Measures: Mean ROP vascular severity score progression over time compared with the RSD. Results: A total of 5255 clinical examinations from 871 infants (mean [SD] gestational age, 27.0 [2.0] weeks; 493 [56.6%] male; mean [SD] birth weight, 949 [271] g) were analyzed. The median severity scores for each category were as follows: 1.1 (interquartile range [IQR], 1.0-1.5) (no ROP), 1.5 (IQR, 1.1-3.4) (mild ROP), 4.6 (IQR, 2.4-5.3) (type 2 and pre-plus), and 7.5 (IQR, 5.0-8.7) (treatment-requiring ROP) (P < .001). When the long-term differences in the median severity scores across time between the eyes progressing to treatment and those who did not eventually require treatment were compared, the median score was higher in the treatment group by 0.06 at 30 to 32 weeks, 0.75 at 32 to 34 weeks, 3.56 at 34 to 36 weeks, 3.71 at 36 to 38 weeks, and 3.24 at 38 to 40 weeks postmenstrual age (P < .001 for all comparisons). Conclusions and Relevance: The findings suggest that the proposed ROP vascular severity score is associated with category of disease at a given point in time and clinical progression of ROP in premature infants. Automated image analysis may be used to quantify clinical disease progression and identify infants at high risk for eventually developing treatment-requiring ROP. This finding has implications for quality and delivery of ROP care and for future approaches to disease classification.","Monitoring Disease Progression With a Quantitative Severity Scale for Retinopathy of Prematurity Using Deep Learning. Importance: Retinopathy of prematurity (ROP) is a leading cause of childhood blindness worldwide, but clinical diagnosis is subjective and qualitative. Objective: To describe a quantitative ROP severity score derived using a deep learning algorithm designed to evaluate plus disease and to assess its utility for objectively monitoring ROP progression. Design, Setting, and Participants: This retrospective cohort study included images from 5255 clinical examinations of 871 premature infants who met the ROP screening criteria of the Imaging and Informatics in ROP (i-ROP) Consortium, which comprises 9 tertiary care centers in North America, from July 1, 2011, to December 31, 2016. Data analysis was performed from July 2017 to May 2018. Exposure: A deep learning algorithm was used to assign a continuous ROP vascular severity score from 1 (most normal) to 9 (most severe) at each examination based on a single posterior photograph compared with a reference standard diagnosis (RSD) simplified into 4 categories: no ROP, mild ROP, type 2 ROP or pre-plus disease, or type 1 ROP. Disease course was assessed longitudinally across multiple examinations for all patients. Main Outcomes and Measures: Mean ROP vascular severity score progression over time compared with the RSD. Results: A total of 5255 clinical examinations from 871 infants (mean [SD] gestational age, 27.0 [2.0] weeks; 493 [56.6%] male; mean [SD] birth weight, 949 [271] g) were analyzed. The median severity scores for each category were as follows: 1.1 (interquartile range [IQR], 1.0-1.5) (no ROP), 1.5 (IQR, 1.1-3.4) (mild ROP), 4.6 (IQR, 2.4-5.3) (type 2 and pre-plus), and 7.5 (IQR, 5.0-8.7) (treatment-requiring ROP) (P < .001). When the long-term differences in the median severity scores across time between the eyes progressing to treatment and those who did not eventually require treatment were compared, the median score was higher in the treatment group by 0.06 at 30 to 32 weeks, 0.75 at 32 to 34 weeks, 3.56 at 34 to 36 weeks, 3.71 at 36 to 38 weeks, and 3.24 at 38 to 40 weeks postmenstrual age (P < .001 for all comparisons). Conclusions and Relevance: The findings suggest that the proposed ROP vascular severity score is associated with category of disease at a given point in time and clinical progression of ROP in premature infants. Automated image analysis may be used to quantify clinical disease progression and identify infants at high risk for eventually developing treatment-requiring ROP. This finding has implications for quality and delivery of ROP care and for future approaches to disease classification."
1,Automatic spondylolisthesis grading from MRIs across modalities using faster adversarial recognition network,"Grading spondylolisthesis into several stages from MRI images is challenging because detecting critical vertebrae and locating landmarks in images of different characteristics is difficult. We propose Faster Adversarial Recognition (FAR) network to accurately perform spondylolisthesis grading by excellently detecting critical vertebrae without the need of locating the landmarks. The FAR network introduces the adversarial scheme by using a multi-task recognition network as the generator and an adversarial module as the discriminator. The multi-task recognition network (generator) is an integrated network that can reliably perform multi-scale hierarchical feature learning, critical vertebrae detection, detected vertebrae classification, bounding box regression, and spondylolisthesis grading in a hybrid supervised manner. The adversarial module (discriminator) takes the detection results as inputs to supervise the generative network by leveraging the high-order statistics of the distribution of the detected bounding box coordinates. The FAR network is evaluated to be accurate and robust in spondylolisthesis grading (training accuracy: 0.9883 Â± 0.0094, testing accuracy: 0.8933 Â± 0.0276) for MRI images of different modalities, which can be attributed to the excellent critical vertebrae detection (detection mAP75 for training: 1 Â± 0, for testing: 0.9636 Â± 0.0180, and IoU (Intersection-over-union) â‰¥ 0.9/0.8 for most detections with their corresponding ground truth in the training/testing dataset). This accuracy is comparable to that of the physicians and outperforms other state-of-the-art methods. These results indicate the potential of our framework to perform spondylolisthesis grading for clinical diagnosis.","Automatic spondylolisthesis grading from MRIs across modalities using faster adversarial recognition network. Grading spondylolisthesis into several stages from MRI images is challenging because detecting critical vertebrae and locating landmarks in images of different characteristics is difficult. We propose Faster Adversarial Recognition (FAR) network to accurately perform spondylolisthesis grading by excellently detecting critical vertebrae without the need of locating the landmarks. The FAR network introduces the adversarial scheme by using a multi-task recognition network as the generator and an adversarial module as the discriminator. The multi-task recognition network (generator) is an integrated network that can reliably perform multi-scale hierarchical feature learning, critical vertebrae detection, detected vertebrae classification, bounding box regression, and spondylolisthesis grading in a hybrid supervised manner. The adversarial module (discriminator) takes the detection results as inputs to supervise the generative network by leveraging the high-order statistics of the distribution of the detected bounding box coordinates. The FAR network is evaluated to be accurate and robust in spondylolisthesis grading (training accuracy: 0.9883 Â± 0.0094, testing accuracy: 0.8933 Â± 0.0276) for MRI images of different modalities, which can be attributed to the excellent critical vertebrae detection (detection mAP75 for training: 1 Â± 0, for testing: 0.9636 Â± 0.0180, and IoU (Intersection-over-union) â‰¥ 0.9/0.8 for most detections with their corresponding ground truth in the training/testing dataset). This accuracy is comparable to that of the physicians and outperforms other state-of-the-art methods. These results indicate the potential of our framework to perform spondylolisthesis grading for clinical diagnosis."
1,Unsupervised tumor detection in Dynamic PET/CT imaging of the prostate,,
1,CT male pelvic organ segmentation using fully convolutional networks with boundary sensitive representation,,
1,Novel drug-independent sedation level estimation based on machine learning of quantitative frontal electroencephalogram features in healthy volunteers,"BACKGROUND: Sedation indicators based on a single quantitative EEG (QEEG) feature have been criticised for their limited performance. We hypothesised that integration of multiple QEEG features into a single sedation-level estimator using a machine learning algorithm could reliably predict levels of sedation, independent of the sedative drug used. METHODS: In total, 102 subjects receiving propofol (N=36; 16 male/20 female), sevoflurane (N=36; 16 male/20 female), or dexmedetomidine (N=30; 15 male/15 female) were included in this study of healthy volunteers. Sedation level was assessed using the Modified Observer's Assessment of Alertness/Sedation (MOAA/S) score. We used 44 QEEG features estimated from the EEG data in a logistic regression algorithm, and an elastic-net regularisation method was used for feature selection. The area under the receiver operator characteristic curve (AUC) was used to assess the performance of the logistic regression model. RESULTS: The performances obtained when the system was trained and tested as drug-dependent mode to distinguish between awake and sedated states (mean AUC [standard deviation]) were propofol=0.97 (0.03), sevoflurane=0.74 (0.25), and dexmedetomidine=0.77 (0.10). The drug-independent system resulted in mean AUC=0.83 (0.17) to discriminate between the awake and sedated states. CONCLUSIONS: The incorporation of large numbers of QEEG features and machine learning algorithms is feasible for next-generation monitors of sedation level. Different QEEG features were selected for propofol, sevoflurane, and dexmedetomidine groups, but the sedation-level estimator maintained a high performance for predicting MOAA/S independent of the drug used. CLINICAL TRIAL REGISTRATION: NCT02043938; NCT03143972.","Novel drug-independent sedation level estimation based on machine learning of quantitative frontal electroencephalogram features in healthy volunteers. BACKGROUND: Sedation indicators based on a single quantitative EEG (QEEG) feature have been criticised for their limited performance. We hypothesised that integration of multiple QEEG features into a single sedation-level estimator using a machine learning algorithm could reliably predict levels of sedation, independent of the sedative drug used. METHODS: In total, 102 subjects receiving propofol (N=36; 16 male/20 female), sevoflurane (N=36; 16 male/20 female), or dexmedetomidine (N=30; 15 male/15 female) were included in this study of healthy volunteers. Sedation level was assessed using the Modified Observer's Assessment of Alertness/Sedation (MOAA/S) score. We used 44 QEEG features estimated from the EEG data in a logistic regression algorithm, and an elastic-net regularisation method was used for feature selection. The area under the receiver operator characteristic curve (AUC) was used to assess the performance of the logistic regression model. RESULTS: The performances obtained when the system was trained and tested as drug-dependent mode to distinguish between awake and sedated states (mean AUC [standard deviation]) were propofol=0.97 (0.03), sevoflurane=0.74 (0.25), and dexmedetomidine=0.77 (0.10). The drug-independent system resulted in mean AUC=0.83 (0.17) to discriminate between the awake and sedated states. CONCLUSIONS: The incorporation of large numbers of QEEG features and machine learning algorithms is feasible for next-generation monitors of sedation level. Different QEEG features were selected for propofol, sevoflurane, and dexmedetomidine groups, but the sedation-level estimator maintained a high performance for predicting MOAA/S independent of the drug used. CLINICAL TRIAL REGISTRATION: NCT02043938; NCT03143972."
1,Comparison of Machine Learning Optimal Classification Trees with the Pediatric Emergency Care Applied Research Network Head Trauma Decision Rules,"Importance: Computed tomographic (CT) scanning is the standard for the rapid diagnosis of intracranial injury, but it is costly and exposes patients to ionizing radiation. The Pediatric Emergency Care Applied Research Network (PECARN) rules for identifying children with minor head trauma who are at very low risk of clinically important traumatic brain injury (ciTBI) are widely used to triage CT imaging. Objective: To examine whether optimal classification trees (OCTs), which are novel machine-learning classifiers, improve on PECARN rules' predictive accuracy. Design, Setting, and Participants: A secondary analysis of prospective, publicly available data on emergency department visits for head trauma used by the PECARN group to develop their tool was conducted to derive OCT-based prediction rules for ciTBI in a development cohort and compare their predictive performance vs the PECARN rules in a validation cohort among children who were younger than 2 years and 2 years or older. Data on 42412 children with head trauma and without severely altered mental status who were examined between June 1, 2004, and September 30, 2006, were gathered from 25 emergency departments in North America participating in PECARN. Data analysis was conducted from September 15, 2016, to December 18, 2018. Main Outcomes and Measures: The outcome was ciTBI, with predictive performance measured by estimating the sensitivity, specificity, positive predictive value, negative predictive value, positive likelihood ratio, and negative likelihood ratio for the OCT and the PECARN rules. The OCT and PECARN rules' performance was compared by estimating ratios for each measure. Results: Of the 42412 children (15 996 [37.7%] girls) included in the analysis, 10718 were younger than 2 years (25.3%; mean [SD] age, 11.6 [0.6] months) and 31694 were 2 years or older (74.7%; age, 9.1 [4.9] years). Compared with PECARN rules, OCTs misclassified 0 vs 1 child with ciTBI in the younger and 10 vs 9 children with ciTBI in the older cohort, and correctly identified more children with very low risk of ciTBI in the younger (7605 vs 5701) and older (20 594 vs 18 134) cohorts. In the validation cohorts, compared with the PECARN rules, the OCTs had statistically significantly better specificity (in the younger cohort: 69.3%; 95% CI, 67.4%-71.2% vs 52.8%; 95% CI, 50.8%-54.9%; in the older cohort: 65.6%; 95% CI, 64.5%-66.8% vs 57.6%; 95% CI, 56.4%-58.8%), positive predictive value (odds ratios, 1.54; 95% CI, 1.36-1.74 and 1.23; 95% CI, 1.17-1.30, in younger and older children, respectively), and positive likelihood ratio (risk ratios, 1.54; 95% CI, 1.36-1.74 and 1.23; 95% CI, 1.17-1.30, in younger and older children, respectively). There were no statistically significant differences in the sensitivity, negative predictive value, and negative likelihood ratio between the 2 sets of rules. Conclusions and Relevance: If implemented, OCTs may help reduce the number of unnecessary CT scans, without missing more patients with ciTBI than the PECARN rules..","Comparison of Machine Learning Optimal Classification Trees with the Pediatric Emergency Care Applied Research Network Head Trauma Decision Rules. Importance: Computed tomographic (CT) scanning is the standard for the rapid diagnosis of intracranial injury, but it is costly and exposes patients to ionizing radiation. The Pediatric Emergency Care Applied Research Network (PECARN) rules for identifying children with minor head trauma who are at very low risk of clinically important traumatic brain injury (ciTBI) are widely used to triage CT imaging. Objective: To examine whether optimal classification trees (OCTs), which are novel machine-learning classifiers, improve on PECARN rules' predictive accuracy. Design, Setting, and Participants: A secondary analysis of prospective, publicly available data on emergency department visits for head trauma used by the PECARN group to develop their tool was conducted to derive OCT-based prediction rules for ciTBI in a development cohort and compare their predictive performance vs the PECARN rules in a validation cohort among children who were younger than 2 years and 2 years or older. Data on 42412 children with head trauma and without severely altered mental status who were examined between June 1, 2004, and September 30, 2006, were gathered from 25 emergency departments in North America participating in PECARN. Data analysis was conducted from September 15, 2016, to December 18, 2018. Main Outcomes and Measures: The outcome was ciTBI, with predictive performance measured by estimating the sensitivity, specificity, positive predictive value, negative predictive value, positive likelihood ratio, and negative likelihood ratio for the OCT and the PECARN rules. The OCT and PECARN rules' performance was compared by estimating ratios for each measure. Results: Of the 42412 children (15 996 [37.7%] girls) included in the analysis, 10718 were younger than 2 years (25.3%; mean [SD] age, 11.6 [0.6] months) and 31694 were 2 years or older (74.7%; age, 9.1 [4.9] years). Compared with PECARN rules, OCTs misclassified 0 vs 1 child with ciTBI in the younger and 10 vs 9 children with ciTBI in the older cohort, and correctly identified more children with very low risk of ciTBI in the younger (7605 vs 5701) and older (20 594 vs 18 134) cohorts. In the validation cohorts, compared with the PECARN rules, the OCTs had statistically significantly better specificity (in the younger cohort: 69.3%; 95% CI, 67.4%-71.2% vs 52.8%; 95% CI, 50.8%-54.9%; in the older cohort: 65.6%; 95% CI, 64.5%-66.8% vs 57.6%; 95% CI, 56.4%-58.8%), positive predictive value (odds ratios, 1.54; 95% CI, 1.36-1.74 and 1.23; 95% CI, 1.17-1.30, in younger and older children, respectively), and positive likelihood ratio (risk ratios, 1.54; 95% CI, 1.36-1.74 and 1.23; 95% CI, 1.17-1.30, in younger and older children, respectively). There were no statistically significant differences in the sensitivity, negative predictive value, and negative likelihood ratio between the 2 sets of rules. Conclusions and Relevance: If implemented, OCTs may help reduce the number of unnecessary CT scans, without missing more patients with ciTBI than the PECARN rules.."
1,Evaluation of an Algorithm for Identifying Ocular Conditions in Electronic Health Record Data,"Importance: For research involving big data, researchers must accurately identify patients with ocular diseases or phenotypes of interest. Reliance on administrative billing codes alone for this purpose is limiting. Objective: To develop a method to accurately identify the presence or absence of ocular conditions of interest using electronic health record (EHR) data. Design, Setting, and Participants: This study is a retrospective analysis of the EHR data of patients (n = 122339) in the Sight Outcomes Research Collaborative Ophthalmology Data Repository who received eye care at participating academic medical centers between August 1, 2012, and August 31, 2017. An algorithm that searches structured and unstructured (free-text) EHR data for conditions of interest was developed and then tested to determine how well it could detect the presence or absence of exfoliation syndrome (XFS). The algorithm was trained to search for evidence of XFS among a sample of patients with and without XFS (n = 200) by reviewing International Classification of Diseases, Ninth Revision or International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-9 or ICD-10) billing codes, the patient's problem list, and text within the ocular examination section and unstructured (free-text) data in the EHR. The likelihood that each patient had XFS was estimated using logistic least absolute shrinkage and selection operator (LASSO) regression. The EHR data of all patients were run through the algorithm to generate an XFS probability score for each patient. The algorithm was validated with review of EHRs by glaucoma specialists. Main Outcomes and Measures: Positive predictive value (PPV) and negative predictive value (NPV) of the algorithm were computed as the proportion of patients correctly classified with XFS or without XFS. Results: This study included 122 339 patients, with a mean (SD) age of 52.4 (25.1) years. Of these patients, 69 002 (56.4%) were female and 99 579 (81.4%) were white. The algorithm assigned a less than 10% probability of XFS for 121 085 patients (99.0%) as well as an XFS probability score of more than 75% for 543 patients (0.4%), more than 90% for 353 patients (0.3%), and more than 99% for 83 patients (0.07%). Validated by glaucoma specialists, the algorithm had a PPV of 95.0% (95% CI, 89.5%-97.7%) and an NPV of 100% (95% CI, 91.2%-100%). When there was ICD-9 or ICD-10 billing code documentation of XFS, in 86% or 96% of the records, respectively, evidence of XFS was also recorded elsewhere in the EHR. Conversely, when there was clinical examination or free-text evidence of XFS, it was documented with ICD-9 codes only approximately 40% of the time and even less often with ICD-10 codes. Conclusions and Relevance: The algorithm developed, tested, and validated in this study appears to be better at identifying the presence or absence of XFS in EHR data than the conventional approach of assessing only billing codes; such an algorithm may enhance the ability of investigators to use EHR data to study patients with ocular diseases.","Evaluation of an Algorithm for Identifying Ocular Conditions in Electronic Health Record Data. Importance: For research involving big data, researchers must accurately identify patients with ocular diseases or phenotypes of interest. Reliance on administrative billing codes alone for this purpose is limiting. Objective: To develop a method to accurately identify the presence or absence of ocular conditions of interest using electronic health record (EHR) data. Design, Setting, and Participants: This study is a retrospective analysis of the EHR data of patients (n = 122339) in the Sight Outcomes Research Collaborative Ophthalmology Data Repository who received eye care at participating academic medical centers between August 1, 2012, and August 31, 2017. An algorithm that searches structured and unstructured (free-text) EHR data for conditions of interest was developed and then tested to determine how well it could detect the presence or absence of exfoliation syndrome (XFS). The algorithm was trained to search for evidence of XFS among a sample of patients with and without XFS (n = 200) by reviewing International Classification of Diseases, Ninth Revision or International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-9 or ICD-10) billing codes, the patient's problem list, and text within the ocular examination section and unstructured (free-text) data in the EHR. The likelihood that each patient had XFS was estimated using logistic least absolute shrinkage and selection operator (LASSO) regression. The EHR data of all patients were run through the algorithm to generate an XFS probability score for each patient. The algorithm was validated with review of EHRs by glaucoma specialists. Main Outcomes and Measures: Positive predictive value (PPV) and negative predictive value (NPV) of the algorithm were computed as the proportion of patients correctly classified with XFS or without XFS. Results: This study included 122 339 patients, with a mean (SD) age of 52.4 (25.1) years. Of these patients, 69 002 (56.4%) were female and 99 579 (81.4%) were white. The algorithm assigned a less than 10% probability of XFS for 121 085 patients (99.0%) as well as an XFS probability score of more than 75% for 543 patients (0.4%), more than 90% for 353 patients (0.3%), and more than 99% for 83 patients (0.07%). Validated by glaucoma specialists, the algorithm had a PPV of 95.0% (95% CI, 89.5%-97.7%) and an NPV of 100% (95% CI, 91.2%-100%). When there was ICD-9 or ICD-10 billing code documentation of XFS, in 86% or 96% of the records, respectively, evidence of XFS was also recorded elsewhere in the EHR. Conversely, when there was clinical examination or free-text evidence of XFS, it was documented with ICD-9 codes only approximately 40% of the time and even less often with ICD-10 codes. Conclusions and Relevance: The algorithm developed, tested, and validated in this study appears to be better at identifying the presence or absence of XFS in EHR data than the conventional approach of assessing only billing codes; such an algorithm may enhance the ability of investigators to use EHR data to study patients with ocular diseases."
1,Automated diagnosis of breast ultrasonography images using deep neural networks,"Ultrasonography images of breast mass aid in the detection and diagnosis of breast cancer. Manually analyzing ultrasonography images is time-consuming, exhausting and subjective. Automated analyzing such images is desired. In this study, we develop an automated breast cancer diagnosis model for ultrasonography images. Traditional methods of automated ultrasonography images analysis employ hand-crafted features to classify images, and lack robustness to the variation in the shapes, size and texture of breast lesions, leading to low sensitivity in clinical applications. To overcome these shortcomings, we propose a method to diagnose breast ultrasonography images using deep convolutional neural networks with multi-scale kernels and skip connections. Our method consists of two components: the first one is to determine whether there are malignant tumors in the image, and the second one is to recognize solid nodules. In order to let the two networks work in a collaborative way, a region enhance mechanism based on class activation maps is proposed. The mechanism helps to improve classification accuracy and sensitivity for both networks. A cross training algorithm is introduced to train the networks. We construct a large annotated dataset containing a total of 8145 breast ultrasonography images to train and evaluate the models. All of the annotations are proven by pathological records. The proposed method is compared with two state-of-the-art approaches, and outperforms both of them by a large margin. Experimental results show that our approach achieves a performance comparable to human sonographers and can be applied to clinical scenarios.","Automated diagnosis of breast ultrasonography images using deep neural networks. Ultrasonography images of breast mass aid in the detection and diagnosis of breast cancer. Manually analyzing ultrasonography images is time-consuming, exhausting and subjective. Automated analyzing such images is desired. In this study, we develop an automated breast cancer diagnosis model for ultrasonography images. Traditional methods of automated ultrasonography images analysis employ hand-crafted features to classify images, and lack robustness to the variation in the shapes, size and texture of breast lesions, leading to low sensitivity in clinical applications. To overcome these shortcomings, we propose a method to diagnose breast ultrasonography images using deep convolutional neural networks with multi-scale kernels and skip connections. Our method consists of two components: the first one is to determine whether there are malignant tumors in the image, and the second one is to recognize solid nodules. In order to let the two networks work in a collaborative way, a region enhance mechanism based on class activation maps is proposed. The mechanism helps to improve classification accuracy and sensitivity for both networks. A cross training algorithm is introduced to train the networks. We construct a large annotated dataset containing a total of 8145 breast ultrasonography images to train and evaluate the models. All of the annotations are proven by pathological records. The proposed method is compared with two state-of-the-art approaches, and outperforms both of them by a large margin. Experimental results show that our approach achieves a performance comparable to human sonographers and can be applied to clinical scenarios."
1,MILD-Net: Minimal information loss dilated network for gland instance segmentation in colon histology images,"The analysis of glandular morphology within colon histopathology images is an important step in determining the grade of colon cancer. Despite the importance of this task, manual segmentation is laborious, time-consuming and can suffer from subjectivity among pathologists. The rise of computational pathology has led to the development of automated methods for gland segmentation that aim to overcome the challenges of manual segmentation. However, this task is non-trivial due to the large variability in glandular appearance and the difficulty in differentiating between certain glandular and non-glandular histological structures. Furthermore, a measure of uncertainty is essential for diagnostic decision making. To address these challenges, we propose a fully convolutional neural network that counters the loss of information caused by max-pooling by re-introducing the original image at multiple points within the network. We also use atrous spatial pyramid pooling with varying dilation rates for preserving the resolution and multi-level aggregation. To incorporate uncertainty, we introduce random transformations during test time for an enhanced segmentation result that simultaneously generates an uncertainty map, highlighting areas of ambiguity. We show that this map can be used to define a metric for disregarding predictions with high uncertainty. The proposed network achieves state-of-the-art performance on the GlaS challenge dataset and on a second independent colorectal adenocarcinoma dataset. In addition, we perform gland instance segmentation on whole-slide images from two further datasets to highlight the generalisability of our method. As an extension, we introduce MILD-Net(+) for simultaneous gland and lumen segmentation, to increase the diagnostic power of the network.","MILD-Net: Minimal information loss dilated network for gland instance segmentation in colon histology images. The analysis of glandular morphology within colon histopathology images is an important step in determining the grade of colon cancer. Despite the importance of this task, manual segmentation is laborious, time-consuming and can suffer from subjectivity among pathologists. The rise of computational pathology has led to the development of automated methods for gland segmentation that aim to overcome the challenges of manual segmentation. However, this task is non-trivial due to the large variability in glandular appearance and the difficulty in differentiating between certain glandular and non-glandular histological structures. Furthermore, a measure of uncertainty is essential for diagnostic decision making. To address these challenges, we propose a fully convolutional neural network that counters the loss of information caused by max-pooling by re-introducing the original image at multiple points within the network. We also use atrous spatial pyramid pooling with varying dilation rates for preserving the resolution and multi-level aggregation. To incorporate uncertainty, we introduce random transformations during test time for an enhanced segmentation result that simultaneously generates an uncertainty map, highlighting areas of ambiguity. We show that this map can be used to define a metric for disregarding predictions with high uncertainty. The proposed network achieves state-of-the-art performance on the GlaS challenge dataset and on a second independent colorectal adenocarcinoma dataset. In addition, we perform gland instance segmentation on whole-slide images from two further datasets to highlight the generalisability of our method. As an extension, we introduce MILD-Net(+) for simultaneous gland and lumen segmentation, to increase the diagnostic power of the network."
1,Clinical-grade computational pathology using weakly supervised deep learning on whole slide images,"The development of decision support systems for pathology and their deployment in clinical practice have been hindered by the need for large manually annotated datasets. To overcome this problem, we present a multiple instance learning-based deep learning system that uses only the reported diagnoses as labels for training, thereby avoiding expensive and time-consuming pixel-wise manual annotations. We evaluated this framework at scale on a dataset of 44,732 whole slide images from 15,187 patients without any form of data curation. Tests on prostate cancer, basal cell carcinoma and breast cancer metastases to axillary lymph nodes resulted in areas under the curve above 0.98 for all cancer types. Its clinical application would allow pathologists to exclude 65-75% of slides while retaining 100% sensitivity. Our results show that this system has the ability to train accurate classification models at unprecedented scale, laying the foundation for the deployment of computational decision support systems in clinical practice.","Clinical-grade computational pathology using weakly supervised deep learning on whole slide images. The development of decision support systems for pathology and their deployment in clinical practice have been hindered by the need for large manually annotated datasets. To overcome this problem, we present a multiple instance learning-based deep learning system that uses only the reported diagnoses as labels for training, thereby avoiding expensive and time-consuming pixel-wise manual annotations. We evaluated this framework at scale on a dataset of 44,732 whole slide images from 15,187 patients without any form of data curation. Tests on prostate cancer, basal cell carcinoma and breast cancer metastases to axillary lymph nodes resulted in areas under the curve above 0.98 for all cancer types. Its clinical application would allow pathologists to exclude 65-75% of slides while retaining 100% sensitivity. Our results show that this system has the ability to train accurate classification models at unprecedented scale, laying the foundation for the deployment of computational decision support systems in clinical practice."
1,Visualizing Deep Learning Models for the Detection of Referable Diabetic Retinopathy and Glaucoma,"Importance: Convolutional neural networks have recently been applied to ophthalmic diseases; however, the rationale for the outputs generated by these systems is inscrutable to clinicians. A visualization tool is needed that would enable clinicians to understand important exposure variables in real time. Objective: To systematically visualize the convolutional neural networks of 2 validated deep learning models for the detection of referable diabetic retinopathy (DR) and glaucomatous optic neuropathy (GON). Design, Setting, and Participants: The GON and referable DR algorithms were previously developed and validated (holdout method) using 48116 and 66790 retinal photographs, respectively, derived from a third-party database (LabelMe) of deidentified photographs from various clinical settings in China. In the present cross-sectional study, a random sample of 100 true-positive photographs and all false-positive cases from each of the GON and DR validation data sets were selected. All data were collected from March to June 2017. The original color fundus images were processed using an adaptive kernel visualization technique. The images were preprocessed by applying a sliding window with a size of 28 x 28 pixels and a stride of 3 pixels to crop images into smaller subimages to produce a feature map. Threshold scales were adjusted to optimal levels for each model to generate heat maps highlighting localized landmarks on the input image. A single optometrist allocated each image to predefined categories based on the generated heat map. Main Outcomes and Measures: Visualization regions of the fundus. Results: In the GON data set, 90 of 100 true-positive cases (90%; 95% CI, 82%-95%) and 15 of 22 false-positive cases (68%; 95% CI, 45%-86%) displayed heat map visualization within regions of the optic nerve head only. Lesions typically seen in cases of referable DR (exudate, hemorrhage, or vessel abnormality) were identified as the most important prognostic regions in 96 of 100 true-positive DR cases (96%; 95% CI, 90%-99%). In 39 of 46 false-positive DR cases (85%; 95% CI, 71%-94%), the heat map displayed visualization of nontraditional fundus regions with or without retinal venules. Conclusions and Relevance: These findings suggest that this visualization method can highlight traditional regions in disease diagnosis, substantiating the validity of the deep learning models investigated. This visualization technique may promote the clinical adoption of these models.","Visualizing Deep Learning Models for the Detection of Referable Diabetic Retinopathy and Glaucoma. Importance: Convolutional neural networks have recently been applied to ophthalmic diseases; however, the rationale for the outputs generated by these systems is inscrutable to clinicians. A visualization tool is needed that would enable clinicians to understand important exposure variables in real time. Objective: To systematically visualize the convolutional neural networks of 2 validated deep learning models for the detection of referable diabetic retinopathy (DR) and glaucomatous optic neuropathy (GON). Design, Setting, and Participants: The GON and referable DR algorithms were previously developed and validated (holdout method) using 48116 and 66790 retinal photographs, respectively, derived from a third-party database (LabelMe) of deidentified photographs from various clinical settings in China. In the present cross-sectional study, a random sample of 100 true-positive photographs and all false-positive cases from each of the GON and DR validation data sets were selected. All data were collected from March to June 2017. The original color fundus images were processed using an adaptive kernel visualization technique. The images were preprocessed by applying a sliding window with a size of 28 x 28 pixels and a stride of 3 pixels to crop images into smaller subimages to produce a feature map. Threshold scales were adjusted to optimal levels for each model to generate heat maps highlighting localized landmarks on the input image. A single optometrist allocated each image to predefined categories based on the generated heat map. Main Outcomes and Measures: Visualization regions of the fundus. Results: In the GON data set, 90 of 100 true-positive cases (90%; 95% CI, 82%-95%) and 15 of 22 false-positive cases (68%; 95% CI, 45%-86%) displayed heat map visualization within regions of the optic nerve head only. Lesions typically seen in cases of referable DR (exudate, hemorrhage, or vessel abnormality) were identified as the most important prognostic regions in 96 of 100 true-positive DR cases (96%; 95% CI, 90%-99%). In 39 of 46 false-positive DR cases (85%; 95% CI, 71%-94%), the heat map displayed visualization of nontraditional fundus regions with or without retinal venules. Conclusions and Relevance: These findings suggest that this visualization method can highlight traditional regions in disease diagnosis, substantiating the validity of the deep learning models investigated. This visualization technique may promote the clinical adoption of these models."
1,CATARACTS: Challenge on automatic tool annotation for cataRACT surgery,"Surgical tool detection is attracting increasing attention from the medical image analysis community. The goal generally is not to precisely locate tools in images, but rather to indicate which tools are being used by the surgeon at each instant. The main motivation for annotating tool usage is to design efficient solutions for surgical workflow analysis, with potential applications in report generation, surgical training and even real-time decision support. Most existing tool annotation algorithms focus on laparoscopic surgeries. However, with 19 million interventions per year, the most common surgical procedure in the world is cataract surgery. The CATARACTS challenge was organized in 2017 to evaluate tool annotation algorithms in the specific context of cataract surgery. It relies on more than nine hours of videos, from 50 cataract surgeries, in which the presence of 21 surgical tools was manually annotated by two experts. With 14 participating teams, this challenge can be considered a success. As might be expected, the submitted solutions are based on deep learning. This paper thoroughly evaluates these solutions: in particular, the quality of their annotations are compared to that of human interpretations. Next, lessons learnt from the differential analysis of these solutions are discussed. We expect that they will guide the design of efficient surgery monitoring tools in the near future.","CATARACTS: Challenge on automatic tool annotation for cataRACT surgery. Surgical tool detection is attracting increasing attention from the medical image analysis community. The goal generally is not to precisely locate tools in images, but rather to indicate which tools are being used by the surgeon at each instant. The main motivation for annotating tool usage is to design efficient solutions for surgical workflow analysis, with potential applications in report generation, surgical training and even real-time decision support. Most existing tool annotation algorithms focus on laparoscopic surgeries. However, with 19 million interventions per year, the most common surgical procedure in the world is cataract surgery. The CATARACTS challenge was organized in 2017 to evaluate tool annotation algorithms in the specific context of cataract surgery. It relies on more than nine hours of videos, from 50 cataract surgeries, in which the presence of 21 surgical tools was manually annotated by two experts. With 14 participating teams, this challenge can be considered a success. As might be expected, the submitted solutions are based on deep learning. This paper thoroughly evaluates these solutions: in particular, the quality of their annotations are compared to that of human interpretations. Next, lessons learnt from the differential analysis of these solutions are discussed. We expect that they will guide the design of efficient surgery monitoring tools in the near future."
1,TOP-GAN: Stain-free cancer cell classification using deep learning with a small training set,"We propose a new deep learning approach for medical imaging that copes with the problem of a small training set, the main bottleneck of deep learning, and apply it for classification of healthy and cancer cell lines acquired by quantitative phase imaging. The proposed method, called transferring of pre-trained generative adversarial network (TOP-GAN), is hybridization between transfer learning and generative adversarial networks (GANs). Healthy cells and cancer cells of different metastatic potential have been imaged by low-coherence off-axis holography. After the acquisition, the optical path delay maps of the cells are extracted and directly used as inputs to the networks. In order to cope with the small number of classified images, we use GANs to train a large number of unclassified images from another cell type (sperm cells). After this preliminary training, we change the last layers of the network and design automatic classifiers for the correct cell type (healthy/primary cancer/metastatic cancer) with 90-99% accuracies, although small training sets of down to several images are used. These results are better in comparison to other classic methods that aim at coping with the same problem of a small training set. We believe that our approach makes the combination of holographic microscopy and deep learning networks more accessible to the medical field by enabling a rapid, automatic and accurate classification in stain-free imaging flow cytometry. Furthermore, our approach is expected to be applicable to many other medical image classification tasks, suffering from a small training set.","TOP-GAN: Stain-free cancer cell classification using deep learning with a small training set. We propose a new deep learning approach for medical imaging that copes with the problem of a small training set, the main bottleneck of deep learning, and apply it for classification of healthy and cancer cell lines acquired by quantitative phase imaging. The proposed method, called transferring of pre-trained generative adversarial network (TOP-GAN), is hybridization between transfer learning and generative adversarial networks (GANs). Healthy cells and cancer cells of different metastatic potential have been imaged by low-coherence off-axis holography. After the acquisition, the optical path delay maps of the cells are extracted and directly used as inputs to the networks. In order to cope with the small number of classified images, we use GANs to train a large number of unclassified images from another cell type (sperm cells). After this preliminary training, we change the last layers of the network and design automatic classifiers for the correct cell type (healthy/primary cancer/metastatic cancer) with 90-99% accuracies, although small training sets of down to several images are used. These results are better in comparison to other classic methods that aim at coping with the same problem of a small training set. We believe that our approach makes the combination of holographic microscopy and deep learning networks more accessible to the medical field by enabling a rapid, automatic and accurate classification in stain-free imaging flow cytometry. Furthermore, our approach is expected to be applicable to many other medical image classification tasks, suffering from a small training set."
1,From Machine to Machine: An OCT-Trained Deep Learning Algorithm for Objective Quantification of Glaucomatous Damage in Fundus Photographs,"PURPOSE: Previous approaches using deep learning (DL) algorithms to classify glaucomatous damage on fundus photographs have been limited by the requirement for human labeling of a reference training set. We propose a new approach using quantitative spectral-domain (SD) OCT data to train a DL algorithm to quantify glaucomatous structural damage on optic disc photographs. DESIGN: Cross-sectional study. PARTICIPANTS: A total of 32 820 pairs of optic disc photographs and SD OCT retinal nerve fiber layer (RNFL) scans from 2312 eyes of 1198 participants. METHODS: The sample was divided randomly into validation plus training (80%) and test (20%) sets, with randomization performed at the patient level. A DL convolutional neural network was trained to assess optic disc photographs and predict SD OCT average RNFL thickness. MAIN OUTCOME MEASURES: The DL algorithm performance was evaluated in the test sample by evaluating correlation and agreement between the predictions and actual SD OCT measurements. We also assessed the ability to discriminate eyes with glaucomatous visual field loss from healthy eyes with area under the receiver operating characteristic (ROC) curves. RESULTS: The mean prediction of average RNFL thickness from all 6292 optic disc photographs in the test set was 83.3+/-14.5 mum, whereas the mean average RNFL thickness from all corresponding SD OCT scans was 82.5+/-16.8 mum (P = 0.164). There was a very strong correlation between predicted and observed RNFL thickness values (Pearson r = 0.832; R(2) = 69.3%; P < 0.001), with mean absolute error of the predictions of 7.39 mum. The area under the ROC curves for discriminating glaucomatous from healthy eyes with the DL predictions and actual SD OCT average RNFL thickness measurements were 0.944 (95% confidence interval [CI], 0.912-0.966) and 0.940 (95% CI, 0.902-0.966), respectively (P = 0.724). CONCLUSIONS: We introduced a novel DL approach to assess fundus photographs and provide quantitative information about the amount of neural damage that can be used to diagnose and stage glaucoma. In addition, training neural networks to predict SD OCT data objectively represents a new approach that overcomes limitations of human labeling and could be useful in other areas of ophthalmology.","From Machine to Machine: An OCT-Trained Deep Learning Algorithm for Objective Quantification of Glaucomatous Damage in Fundus Photographs. PURPOSE: Previous approaches using deep learning (DL) algorithms to classify glaucomatous damage on fundus photographs have been limited by the requirement for human labeling of a reference training set. We propose a new approach using quantitative spectral-domain (SD) OCT data to train a DL algorithm to quantify glaucomatous structural damage on optic disc photographs. DESIGN: Cross-sectional study. PARTICIPANTS: A total of 32 820 pairs of optic disc photographs and SD OCT retinal nerve fiber layer (RNFL) scans from 2312 eyes of 1198 participants. METHODS: The sample was divided randomly into validation plus training (80%) and test (20%) sets, with randomization performed at the patient level. A DL convolutional neural network was trained to assess optic disc photographs and predict SD OCT average RNFL thickness. MAIN OUTCOME MEASURES: The DL algorithm performance was evaluated in the test sample by evaluating correlation and agreement between the predictions and actual SD OCT measurements. We also assessed the ability to discriminate eyes with glaucomatous visual field loss from healthy eyes with area under the receiver operating characteristic (ROC) curves. RESULTS: The mean prediction of average RNFL thickness from all 6292 optic disc photographs in the test set was 83.3+/-14.5 mum, whereas the mean average RNFL thickness from all corresponding SD OCT scans was 82.5+/-16.8 mum (P = 0.164). There was a very strong correlation between predicted and observed RNFL thickness values (Pearson r = 0.832; R(2) = 69.3%; P < 0.001), with mean absolute error of the predictions of 7.39 mum. The area under the ROC curves for discriminating glaucomatous from healthy eyes with the DL predictions and actual SD OCT average RNFL thickness measurements were 0.944 (95% confidence interval [CI], 0.912-0.966) and 0.940 (95% CI, 0.902-0.966), respectively (P = 0.724). CONCLUSIONS: We introduced a novel DL approach to assess fundus photographs and provide quantitative information about the amount of neural damage that can be used to diagnose and stage glaucoma. In addition, training neural networks to predict SD OCT data objectively represents a new approach that overcomes limitations of human labeling and could be useful in other areas of ophthalmology."
1,Discovering hierarchical common brain networks via multimodal deep belief network,,
1,Weakly supervised mitosis detection in breast histopathology images using concentric loss,"Developing new deep learning methods for medical image analysis is a prevalent research topic in machine learning. In this paper, we propose a deep learning scheme with a novel loss function for weakly supervised breast cancer diagnosis. According to the Nottingham Grading System, mitotic count plays an important role in breast cancer diagnosis and grading. To determine the cancer grade, pathologists usually need to manually count mitosis from a great deal of histopathology images, which is a very tedious and time-consuming task. This paper proposes an automatic method for detecting mitosis. We regard the mitosis detection task as a semantic segmentation problem and use a deep fully convolutional network to address it. Different from conventional training data used in semantic segmentation system, the training label of mitosis data is usually in the format of centroid pixel, rather than all the pixels belonging to a mitosis. The centroid label is a kind of weak label, which is much easier to annotate and can save the effort of pathologists a lot. However, technically this weak label is not sufficient for training a mitosis segmentation model. To tackle this problem, we expand the single-pixel label to a novel label with concentric circles, where the inside circle is a mitotic region and the ring around the inside circle is a ""middle ground"". During the training stage, we do not compute the loss of the ring region because it may have the presence of both mitotic and non-mitotic pixels. This new loss termed as ""concentric loss"" is able to make the semantic segmentation network be trained with the weakly annotated mitosis data. On the generated segmentation map from the segmentation model, we filter out low confidence and obtain mitotic cells. On the challenging ICPR 2014 MITOSIS dataset and AMIDA13 dataset, we achieve a 0.562 F-score and 0.673 F-score respectively, outperforming all previous approaches significantly. On the latest TUPAC16 dataset, we obtain a F-score of 0.669, which is also the state-of-the-art result. The excellent results quantitatively demonstrate the effectiveness of the proposed mitosis segmentation network with the concentric loss. All of our code has been made publicly available at https://github.com/ChaoLi977/SegMitos_mitosis_detection.","Weakly supervised mitosis detection in breast histopathology images using concentric loss. Developing new deep learning methods for medical image analysis is a prevalent research topic in machine learning. In this paper, we propose a deep learning scheme with a novel loss function for weakly supervised breast cancer diagnosis. According to the Nottingham Grading System, mitotic count plays an important role in breast cancer diagnosis and grading. To determine the cancer grade, pathologists usually need to manually count mitosis from a great deal of histopathology images, which is a very tedious and time-consuming task. This paper proposes an automatic method for detecting mitosis. We regard the mitosis detection task as a semantic segmentation problem and use a deep fully convolutional network to address it. Different from conventional training data used in semantic segmentation system, the training label of mitosis data is usually in the format of centroid pixel, rather than all the pixels belonging to a mitosis. The centroid label is a kind of weak label, which is much easier to annotate and can save the effort of pathologists a lot. However, technically this weak label is not sufficient for training a mitosis segmentation model. To tackle this problem, we expand the single-pixel label to a novel label with concentric circles, where the inside circle is a mitotic region and the ring around the inside circle is a ""middle ground"". During the training stage, we do not compute the loss of the ring region because it may have the presence of both mitotic and non-mitotic pixels. This new loss termed as ""concentric loss"" is able to make the semantic segmentation network be trained with the weakly annotated mitosis data. On the generated segmentation map from the segmentation model, we filter out low confidence and obtain mitotic cells. On the challenging ICPR 2014 MITOSIS dataset and AMIDA13 dataset, we achieve a 0.562 F-score and 0.673 F-score respectively, outperforming all previous approaches significantly. On the latest TUPAC16 dataset, we obtain a F-score of 0.669, which is also the state-of-the-art result. The excellent results quantitatively demonstrate the effectiveness of the proposed mitosis segmentation network with the concentric loss. All of our code has been made publicly available at https://github.com/ChaoLi977/SegMitos_mitosis_detection."
1,Prediction of cognitive impairment via deep learning trained with multi-center neuropsychological test data,"BACKGROUND: Neuropsychological tests (NPTs) are important tools for informing diagnoses of cognitive impairment (CI). However, interpreting NPTs requires specialists and is thus time-consuming. To streamline the application of NPTs in clinical settings, we developed and evaluated the accuracy of a machine learning algorithm using multi-center NPT data. METHODS: Multi-center data were obtained from 14,926 formal neuropsychological assessments (Seoul Neuropsychological Screening Battery), which were classified into normal cognition (NC), mild cognitive impairment (MCI) and Alzheimer's disease dementia (ADD). We trained a machine learning model with artificial neural network algorithm using TensorFlow (https://www.tensorflow.org) to distinguish cognitive state with the 46-variable data and measured prediction accuracies from 10 randomly selected datasets. The features of the NPT were listed in order of their contribution to the outcome using Recursive Feature Elimination. RESULTS: The ten times mean accuracies of identifying CI (MCI and ADD) achieved by 96.66â€‰Â±â€‰0.52% of the balanced dataset and 97.23â€‰Â±â€‰0.32% of the clinic-based dataset, and the accuracies for predicting cognitive states (NC, MCI or ADD) were 95.49â€‰Â±â€‰0.53 and 96.34â€‰Â±â€‰1.03%. The sensitivity to the detection CI and MCI in the balanced dataset were 96.0 and 96.0%, and the specificity were 96.8 and 97.4%, respectively. The 'time orientation' and '3-word recall' score of MMSE were highly ranked features in predicting CI and cognitive state. The twelve features reduced from 46 variable of NPTs with age and education had contributed to more than 90% accuracy in predicting cognitive impairment. CONCLUSIONS: The machine learning algorithm for NPTs has suggested potential use as a reference in differentiating cognitive impairment in the clinical setting.","Prediction of cognitive impairment via deep learning trained with multi-center neuropsychological test data. BACKGROUND: Neuropsychological tests (NPTs) are important tools for informing diagnoses of cognitive impairment (CI). However, interpreting NPTs requires specialists and is thus time-consuming. To streamline the application of NPTs in clinical settings, we developed and evaluated the accuracy of a machine learning algorithm using multi-center NPT data. METHODS: Multi-center data were obtained from 14,926 formal neuropsychological assessments (Seoul Neuropsychological Screening Battery), which were classified into normal cognition (NC), mild cognitive impairment (MCI) and Alzheimer's disease dementia (ADD). We trained a machine learning model with artificial neural network algorithm using TensorFlow (https://www.tensorflow.org) to distinguish cognitive state with the 46-variable data and measured prediction accuracies from 10 randomly selected datasets. The features of the NPT were listed in order of their contribution to the outcome using Recursive Feature Elimination. RESULTS: The ten times mean accuracies of identifying CI (MCI and ADD) achieved by 96.66â€‰Â±â€‰0.52% of the balanced dataset and 97.23â€‰Â±â€‰0.32% of the clinic-based dataset, and the accuracies for predicting cognitive states (NC, MCI or ADD) were 95.49â€‰Â±â€‰0.53 and 96.34â€‰Â±â€‰1.03%. The sensitivity to the detection CI and MCI in the balanced dataset were 96.0 and 96.0%, and the specificity were 96.8 and 97.4%, respectively. The 'time orientation' and '3-word recall' score of MMSE were highly ranked features in predicting CI and cognitive state. The twelve features reduced from 46 variable of NPTs with age and education had contributed to more than 90% accuracy in predicting cognitive impairment. CONCLUSIONS: The machine learning algorithm for NPTs has suggested potential use as a reference in differentiating cognitive impairment in the clinical setting."
1,Perinodular and Intranodular Radiomic Features on Lung CT Images Distinguish Adenocarcinomas from Granulomas,"Purpose To evaluate ability of radiomic (computer-extracted imaging) features to distinguish non-small cell lung cancer adenocarcinomas from granulomas at noncontrast CT. Materials and Methods For this retrospective study, screening or standard diagnostic noncontrast CT images were collected for 290 patients (mean age, 68 years; range, 18-92 years; 125 men [mean age, 67 years; range, 18-90 years] and 165 women [mean age, 68 years; range, 33-92 years]) from two institutions between 2007 and 2013. Histopathologic analysis was available for one nodule per patient. Corresponding nodule of interest was identified on axial CT images by a radiologist with manual annotation. Nodule shape, wavelet (Gabor), and texture-based (Haralick and Laws energy) features were extracted from intra- and perinodular regions. Features were pruned to train machine learning classifiers with 145 patients. In a test set of 145 patients, classifier results were compared against a convolutional neural network (CNN) and diagnostic readings of two radiologists. Results Support vector machine classifier with intranodular radiomic features achieved an area under the receiver operating characteristic curve (AUC) of 0.75 on the test set. Combining radiomics of intranodular with perinodular regions improved the AUC to 0.80. On the same test set, CNN resulted in an AUC of 0.76. Radiologist readers achieved AUCs of 0.61 and 0.60, respectively. Conclusion Radiomic features from intranodular and perinodular regions of nodules can distinguish non-small cell lung cancer adenocarcinomas from benign granulomas at noncontrast CT. (c) RSNA, 2018 Online supplemental material is available for this article. See also the editorial by Nishino in this issue.","Perinodular and Intranodular Radiomic Features on Lung CT Images Distinguish Adenocarcinomas from Granulomas. Purpose To evaluate ability of radiomic (computer-extracted imaging) features to distinguish non-small cell lung cancer adenocarcinomas from granulomas at noncontrast CT. Materials and Methods For this retrospective study, screening or standard diagnostic noncontrast CT images were collected for 290 patients (mean age, 68 years; range, 18-92 years; 125 men [mean age, 67 years; range, 18-90 years] and 165 women [mean age, 68 years; range, 33-92 years]) from two institutions between 2007 and 2013. Histopathologic analysis was available for one nodule per patient. Corresponding nodule of interest was identified on axial CT images by a radiologist with manual annotation. Nodule shape, wavelet (Gabor), and texture-based (Haralick and Laws energy) features were extracted from intra- and perinodular regions. Features were pruned to train machine learning classifiers with 145 patients. In a test set of 145 patients, classifier results were compared against a convolutional neural network (CNN) and diagnostic readings of two radiologists. Results Support vector machine classifier with intranodular radiomic features achieved an area under the receiver operating characteristic curve (AUC) of 0.75 on the test set. Combining radiomics of intranodular with perinodular regions improved the AUC to 0.80. On the same test set, CNN resulted in an AUC of 0.76. Radiologist readers achieved AUCs of 0.61 and 0.60, respectively. Conclusion Radiomic features from intranodular and perinodular regions of nodules can distinguish non-small cell lung cancer adenocarcinomas from benign granulomas at noncontrast CT. (c) RSNA, 2018 Online supplemental material is available for this article. See also the editorial by Nishino in this issue."
1,Improving rare disease classification using imperfect knowledge graph,"BACKGROUND: Accurately recognizing rare diseases based on symptom description is an important task in patient triage, early risk stratification, and target therapies. However, due to the very nature of rare diseases, the lack of historical data poses a great challenge to machine learning-based approaches. On the other hand, medical knowledge in automatically constructed knowledge graphs (KGs) has the potential to compensate the lack of labeled training examples. This work aims to develop a rare disease classification algorithm that makes effective use of a knowledge graph, even when the graph is imperfect. METHOD: We develop a text classification algorithm that represents a document as a combination of a ""bag of words"" and a ""bag of knowledge terms,"" where a ""knowledge term"" is a term shared between the document and the subgraph of KG relevant to the disease classification task. We use two Chinese disease diagnosis corpora to evaluate the algorithm. The first one, HaoDaiFu, contains 51,374 chief complaints categorized into 805 diseases. The second data set, ChinaRe, contains 86,663 patient descriptions categorized into 44 disease categories. RESULTS: On the two evaluation data sets, the proposed algorithm delivers robust performance and outperforms a wide range of baselines, including resampling, deep learning, and feature selection approaches. Both classification-based metric (macro-averaged F1 score) and ranking-based metric (mean reciprocal rank) are used in evaluation. CONCLUSION: Medical knowledge in large-scale knowledge graphs can be effectively leveraged to improve rare diseases classification models, even when the knowledge graph is incomplete.","Improving rare disease classification using imperfect knowledge graph. BACKGROUND: Accurately recognizing rare diseases based on symptom description is an important task in patient triage, early risk stratification, and target therapies. However, due to the very nature of rare diseases, the lack of historical data poses a great challenge to machine learning-based approaches. On the other hand, medical knowledge in automatically constructed knowledge graphs (KGs) has the potential to compensate the lack of labeled training examples. This work aims to develop a rare disease classification algorithm that makes effective use of a knowledge graph, even when the graph is imperfect. METHOD: We develop a text classification algorithm that represents a document as a combination of a ""bag of words"" and a ""bag of knowledge terms,"" where a ""knowledge term"" is a term shared between the document and the subgraph of KG relevant to the disease classification task. We use two Chinese disease diagnosis corpora to evaluate the algorithm. The first one, HaoDaiFu, contains 51,374 chief complaints categorized into 805 diseases. The second data set, ChinaRe, contains 86,663 patient descriptions categorized into 44 disease categories. RESULTS: On the two evaluation data sets, the proposed algorithm delivers robust performance and outperforms a wide range of baselines, including resampling, deep learning, and feature selection approaches. Both classification-based metric (macro-averaged F1 score) and ranking-based metric (mean reciprocal rank) are used in evaluation. CONCLUSION: Medical knowledge in large-scale knowledge graphs can be effectively leveraged to improve rare diseases classification models, even when the knowledge graph is incomplete."
1,Quantitative analysis of neural foramina in the lumbar spine: An imaging informatics and machine learning study,"Purpose: To use machine learning tools and leverage big data informatics to statistically model the variation in the area of lumbar neural foramina in a large asymptomatic population. Materials and Methods: By using an electronic health record and imaging archive, lumbar MRI studies in 645 male (mean age, 50.07 years) and 511 female (mean age, 48.23 years) patients between 20 and 80 years old were identified. Machine learning algorithms were used to delineate lumbar neural foramina autonomously and measure their areas. The relationship between neural foraminal area and patient age, sex, and height was studied by using multivariable linear regression. Results: Neural foraminal areas correlated directly with patient height and inversely with patient age. The associations involved were statistically significant (P <.01). Conclusion: By using machine learning and big data techniques, a linear model encoding variation in lumbar neural foraminal areas in asymptomatic individuals has been established. This model can be used to make quantitative assessments of neural foraminal areas in patients by comparing them to the age-, sex-, and height-adjusted population averages.","Quantitative analysis of neural foramina in the lumbar spine: An imaging informatics and machine learning study. Purpose: To use machine learning tools and leverage big data informatics to statistically model the variation in the area of lumbar neural foramina in a large asymptomatic population. Materials and Methods: By using an electronic health record and imaging archive, lumbar MRI studies in 645 male (mean age, 50.07 years) and 511 female (mean age, 48.23 years) patients between 20 and 80 years old were identified. Machine learning algorithms were used to delineate lumbar neural foramina autonomously and measure their areas. The relationship between neural foraminal area and patient age, sex, and height was studied by using multivariable linear regression. Results: Neural foraminal areas correlated directly with patient height and inversely with patient age. The associations involved were statistically significant (P <.01). Conclusion: By using machine learning and big data techniques, a linear model encoding variation in lumbar neural foraminal areas in asymptomatic individuals has been established. This model can be used to make quantitative assessments of neural foraminal areas in patients by comparing them to the age-, sex-, and height-adjusted population averages."
1,Prevalence and Predictability of Low-Yield Inpatient Laboratory Diagnostic Tests,"Importance: Laboratory testing is an important target for high-value care initiatives, constituting the highest volume of medical procedures. Prior studies have found that up to half of all inpatient laboratory tests may be medically unnecessary, but a systematic method to identify these unnecessary tests in individual cases is lacking. Objective: To systematically identify low-yield inpatient laboratory testing through personalized predictions. Design, Setting, and Participants: In this retrospective diagnostic study with multivariable prediction models, 116637 inpatients treated at Stanford University Hospital from January 1, 2008, to December 31, 2017, a total of 60929 inpatients treated at University of Michigan from January 1, 2015, to December 31, 2018, and 13940 inpatients treated at the University of California, San Francisco from January 1 to December 31, 2018, were assessed. Main Outcomes and Measures: Diagnostic accuracy measures, including sensitivity, specificity, negative predictive values (NPVs), positive predictive values (PPVs), and area under the receiver operating characteristic curve (AUROC), of machine learning models when predicting whether inpatient laboratory tests yield a normal result as defined by local laboratory reference ranges. Results: In the recent data sets (July 1, 2014, to June 30, 2017) from Stanford University Hospital (including 22664 female inpatients with a mean [SD] age of 58.8 [19.0] years and 22016 male inpatients with a mean [SD] age of 59.0 [18.1] years), among the top 20 highest-volume tests, 792397 were repeats of orders within 24 hours, including tests that are physiologically unlikely to yield new information that quickly (eg, white blood cell differential, glycated hemoglobin, and serum albumin level). The best-performing machine learning models predicted normal results with an AUROC of 0.90 or greater for 12 stand-alone laboratory tests (eg, sodium AUROC, 0.92 [95% CI, 0.91-0.93]; sensitivity, 98%; specificity, 35%; PPV, 66%; NPV, 93%; lactate dehydrogenase AUROC, 0.93 [95% CI, 0.93-0.94]; sensitivity, 96%; specificity, 65%; PPV, 71%; NPV, 95%; and troponin I AUROC, 0.92 [95% CI, 0.91-0.93]; sensitivity, 88%; specificity, 79%; PPV, 67%; NPV, 93%) and 10 common laboratory test components (eg, hemoglobin AUROC, 0.94 [95% CI, 0.92-0.95]; sensitivity, 99%; specificity, 17%; PPV, 90%; NPV, 81%; creatinine AUROC, 0.96 [95% CI, 0.96-0.97]; sensitivity, 93%; specificity, 83%; PPV, 79%; NPV, 94%; and urea nitrogen AUROC, 0.95 [95% CI, 0.94, 0.96]; sensitivity, 87%; specificity, 89%; PPV, 77%; NPV 94%). Conclusions and Relevance: The findings suggest that low-yield diagnostic testing is common and can be systematically identified through data-driven methods and patient context-aware predictions. Implementing machine learning models appear to be able to quantify the level of uncertainty and expected information gained from diagnostic tests explicitly, with the potential to encourage useful testing and discourage low-value testing that incurs direct costs and indirect harms..","Prevalence and Predictability of Low-Yield Inpatient Laboratory Diagnostic Tests. Importance: Laboratory testing is an important target for high-value care initiatives, constituting the highest volume of medical procedures. Prior studies have found that up to half of all inpatient laboratory tests may be medically unnecessary, but a systematic method to identify these unnecessary tests in individual cases is lacking. Objective: To systematically identify low-yield inpatient laboratory testing through personalized predictions. Design, Setting, and Participants: In this retrospective diagnostic study with multivariable prediction models, 116637 inpatients treated at Stanford University Hospital from January 1, 2008, to December 31, 2017, a total of 60929 inpatients treated at University of Michigan from January 1, 2015, to December 31, 2018, and 13940 inpatients treated at the University of California, San Francisco from January 1 to December 31, 2018, were assessed. Main Outcomes and Measures: Diagnostic accuracy measures, including sensitivity, specificity, negative predictive values (NPVs), positive predictive values (PPVs), and area under the receiver operating characteristic curve (AUROC), of machine learning models when predicting whether inpatient laboratory tests yield a normal result as defined by local laboratory reference ranges. Results: In the recent data sets (July 1, 2014, to June 30, 2017) from Stanford University Hospital (including 22664 female inpatients with a mean [SD] age of 58.8 [19.0] years and 22016 male inpatients with a mean [SD] age of 59.0 [18.1] years), among the top 20 highest-volume tests, 792397 were repeats of orders within 24 hours, including tests that are physiologically unlikely to yield new information that quickly (eg, white blood cell differential, glycated hemoglobin, and serum albumin level). The best-performing machine learning models predicted normal results with an AUROC of 0.90 or greater for 12 stand-alone laboratory tests (eg, sodium AUROC, 0.92 [95% CI, 0.91-0.93]; sensitivity, 98%; specificity, 35%; PPV, 66%; NPV, 93%; lactate dehydrogenase AUROC, 0.93 [95% CI, 0.93-0.94]; sensitivity, 96%; specificity, 65%; PPV, 71%; NPV, 95%; and troponin I AUROC, 0.92 [95% CI, 0.91-0.93]; sensitivity, 88%; specificity, 79%; PPV, 67%; NPV, 93%) and 10 common laboratory test components (eg, hemoglobin AUROC, 0.94 [95% CI, 0.92-0.95]; sensitivity, 99%; specificity, 17%; PPV, 90%; NPV, 81%; creatinine AUROC, 0.96 [95% CI, 0.96-0.97]; sensitivity, 93%; specificity, 83%; PPV, 79%; NPV, 94%; and urea nitrogen AUROC, 0.95 [95% CI, 0.94, 0.96]; sensitivity, 87%; specificity, 89%; PPV, 77%; NPV 94%). Conclusions and Relevance: The findings suggest that low-yield diagnostic testing is common and can be systematically identified through data-driven methods and patient context-aware predictions. Implementing machine learning models appear to be able to quantify the level of uncertainty and expected information gained from diagnostic tests explicitly, with the potential to encourage useful testing and discourage low-value testing that incurs direct costs and indirect harms.."
1,Clinical text classification with rule-based features and knowledge-guided convolutional neural networks,"BACKGROUND: Clinical text classification is an fundamental problem in medical natural language processing. Existing studies have cocnventionally focused on rules or knowledge sources-based feature engineering, but only a limited number of studies have exploited effective representation learning capability of deep learning methods. METHODS: In this study, we propose a new approach which combines rule-based features and knowledge-guided deep learning models for effective disease classification. Critical Steps of our method include recognizing trigger phrases, predicting classes with very few examples using trigger phrases and training a convolutional neural network (CNN) with word embeddings and Unified Medical Language System (UMLS) entity embeddings. RESULTS: We evaluated our method on the 2008 Integrating Informatics with Biology and the Bedside (i2b2) obesity challenge. The results demonstrate that our method outperforms the state-of-the-art methods. CONCLUSION: We showed that CNN model is powerful for learning effective hidden features, and CUIs embeddings are helpful for building clinical text representations. This shows integrating domain knowledge into CNN models is promising.","Clinical text classification with rule-based features and knowledge-guided convolutional neural networks. BACKGROUND: Clinical text classification is an fundamental problem in medical natural language processing. Existing studies have cocnventionally focused on rules or knowledge sources-based feature engineering, but only a limited number of studies have exploited effective representation learning capability of deep learning methods. METHODS: In this study, we propose a new approach which combines rule-based features and knowledge-guided deep learning models for effective disease classification. Critical Steps of our method include recognizing trigger phrases, predicting classes with very few examples using trigger phrases and training a convolutional neural network (CNN) with word embeddings and Unified Medical Language System (UMLS) entity embeddings. RESULTS: We evaluated our method on the 2008 Integrating Informatics with Biology and the Bedside (i2b2) obesity challenge. The results demonstrate that our method outperforms the state-of-the-art methods. CONCLUSION: We showed that CNN model is powerful for learning effective hidden features, and CUIs embeddings are helpful for building clinical text representations. This shows integrating domain knowledge into CNN models is promising."
1,Temporally constrained ICA with threshold and its application to fMRI data,"BACKGROUND: Although independent component analysis (ICA) has been widely applied to functional magnetic resonance imaging (fMRI) data to reveal spatially independent brain networks, the order indetermination of ICA leads to the problem of target component selection. The temporally constrained independent component analysis (TCICA) is capable of automatically extracting the desired spatially independent components by adding the temporal prior information of the task to the mixing matrix for fMRI data analysis. However, the TCICA method can only extract a single component that tends to be a mix of multiple task-related components when there exist several independent components related to one task. METHODS: In this study, we proposed a TCICA with threshold (TCICA-Thres) method that performed TCICA outside the threshold and performed FastICA inside the threshold to automatically extract all the target components related to one task. The proposed approach was tested using simulated fMRI data and was applied to a real fMRI experiment using 13 subjects. Additionally, the performance of TCICA-Thres was compared with that of FastICA and TCICA. RESULTS: The results from the simulation and the fMRI data demonstrated that TCICA-Thres better extracted the task-related components than TCICA. Moreover, TCICA-Thres outperformed FastICA in robustness to noise, spatial detection power and computational time. CONCLUSIONS: The proposed TCICA-Thres solves the limitations of TCICA and extends the application of TCICA in fMRI data analysis.","Temporally constrained ICA with threshold and its application to fMRI data. BACKGROUND: Although independent component analysis (ICA) has been widely applied to functional magnetic resonance imaging (fMRI) data to reveal spatially independent brain networks, the order indetermination of ICA leads to the problem of target component selection. The temporally constrained independent component analysis (TCICA) is capable of automatically extracting the desired spatially independent components by adding the temporal prior information of the task to the mixing matrix for fMRI data analysis. However, the TCICA method can only extract a single component that tends to be a mix of multiple task-related components when there exist several independent components related to one task. METHODS: In this study, we proposed a TCICA with threshold (TCICA-Thres) method that performed TCICA outside the threshold and performed FastICA inside the threshold to automatically extract all the target components related to one task. The proposed approach was tested using simulated fMRI data and was applied to a real fMRI experiment using 13 subjects. Additionally, the performance of TCICA-Thres was compared with that of FastICA and TCICA. RESULTS: The results from the simulation and the fMRI data demonstrated that TCICA-Thres better extracted the task-related components than TCICA. Moreover, TCICA-Thres outperformed FastICA in robustness to noise, spatial detection power and computational time. CONCLUSIONS: The proposed TCICA-Thres solves the limitations of TCICA and extends the application of TCICA in fMRI data analysis."
1,Representation learning for clinical time series prediction tasks in electronic health records,"BACKGROUND: Electronic health records (EHRs) provide possibilities to improve patient care and facilitate clinical research. However, there are many challenges faced by the applications of EHRs, such as temporality, high dimensionality, sparseness, noise, random error and systematic bias. In particular, temporal information is difficult to effectively use by traditional machine learning methods while the sequential information of EHRs is very useful. METHOD: In this paper, we propose a general-purpose patient representation learning approach to summarize sequential EHRs. Specifically, a recurrent neural network based denoising autoencoder (RNN-DAE) is employed to encode inhospital records of each patient into a low dimensional dense vector. RESULTS: Based on EHR data collected from Shuguang Hospital affiliated to Shanghai University of Traditional Chinese Medicine, we experimentally evaluate our proposed RNN-DAE method on both mortality prediction task and comorbidity prediction task. Extensive experimental results show that our proposed RNN-DAE method outperforms existing methods. In addition, we apply the ""Deep Feature"" represented by our proposed RNN-DAE method to track similar patients with t-SNE, which also achieves some interesting observations. CONCLUSION: We propose an effective unsupervised RNN-DAE method to summarize patient sequential information in EHR data. Our proposed RNN-DAE method is useful on both mortality prediction task and comorbidity prediction task.","Representation learning for clinical time series prediction tasks in electronic health records. BACKGROUND: Electronic health records (EHRs) provide possibilities to improve patient care and facilitate clinical research. However, there are many challenges faced by the applications of EHRs, such as temporality, high dimensionality, sparseness, noise, random error and systematic bias. In particular, temporal information is difficult to effectively use by traditional machine learning methods while the sequential information of EHRs is very useful. METHOD: In this paper, we propose a general-purpose patient representation learning approach to summarize sequential EHRs. Specifically, a recurrent neural network based denoising autoencoder (RNN-DAE) is employed to encode inhospital records of each patient into a low dimensional dense vector. RESULTS: Based on EHR data collected from Shuguang Hospital affiliated to Shanghai University of Traditional Chinese Medicine, we experimentally evaluate our proposed RNN-DAE method on both mortality prediction task and comorbidity prediction task. Extensive experimental results show that our proposed RNN-DAE method outperforms existing methods. In addition, we apply the ""Deep Feature"" represented by our proposed RNN-DAE method to track similar patients with t-SNE, which also achieves some interesting observations. CONCLUSION: We propose an effective unsupervised RNN-DAE method to summarize patient sequential information in EHR data. Our proposed RNN-DAE method is useful on both mortality prediction task and comorbidity prediction task."
1,Incorporating medical code descriptions for diagnosis prediction in healthcare,"BACKGROUND: Diagnosis aims to predict the future health status of patients according to their historical electronic health records (EHR), which is an important yet challenging task in healthcare informatics. Existing diagnosis prediction approaches mainly employ recurrent neural networks (RNN) with attention mechanisms to make predictions. However, these approaches ignore the importance of code descriptions, i.e., the medical definitions of diagnosis codes. We believe that taking diagnosis code descriptions into account can help the state-of-the-art models not only to learn meaning code representations, but also to improve the predictive performance, especially when the EHR data are insufficient. METHODS: We propose a simple, but general diagnosis prediction framework, which includes two basic components: diagnosis code embedding and predictive model. To learn the interpretable code embeddings, we apply convolutional neural networks (CNN) to model medical descriptions of diagnosis codes extracted from online medical websites. The learned medical embedding matrix is used to embed the input visits into vector representations, which are fed into the predictive models. Any existing diagnosis prediction approach (referred to as the base model) can be cast into the proposed framework as the predictive model (called the enhanced model). RESULTS: We conduct experiments on two real medical datasets: the MIMIC-III dataset and the Heart Failure claim dataset. Experimental results show that the enhanced diagnosis prediction approaches significantly improve the prediction performance. Moreover, we validate the effectiveness of the proposed framework with insufficient EHR data. Finally, we visualize the learned medical code embeddings to show the interpretability of the proposed framework. CONCLUSIONS: Given the historical visit records of a patient, the proposed framework is able to predict the next visit information by incorporating medical code descriptions.","Incorporating medical code descriptions for diagnosis prediction in healthcare. BACKGROUND: Diagnosis aims to predict the future health status of patients according to their historical electronic health records (EHR), which is an important yet challenging task in healthcare informatics. Existing diagnosis prediction approaches mainly employ recurrent neural networks (RNN) with attention mechanisms to make predictions. However, these approaches ignore the importance of code descriptions, i.e., the medical definitions of diagnosis codes. We believe that taking diagnosis code descriptions into account can help the state-of-the-art models not only to learn meaning code representations, but also to improve the predictive performance, especially when the EHR data are insufficient. METHODS: We propose a simple, but general diagnosis prediction framework, which includes two basic components: diagnosis code embedding and predictive model. To learn the interpretable code embeddings, we apply convolutional neural networks (CNN) to model medical descriptions of diagnosis codes extracted from online medical websites. The learned medical embedding matrix is used to embed the input visits into vector representations, which are fed into the predictive models. Any existing diagnosis prediction approach (referred to as the base model) can be cast into the proposed framework as the predictive model (called the enhanced model). RESULTS: We conduct experiments on two real medical datasets: the MIMIC-III dataset and the Heart Failure claim dataset. Experimental results show that the enhanced diagnosis prediction approaches significantly improve the prediction performance. Moreover, we validate the effectiveness of the proposed framework with insufficient EHR data. Finally, we visualize the learned medical code embeddings to show the interpretability of the proposed framework. CONCLUSIONS: Given the historical visit records of a patient, the proposed framework is able to predict the next visit information by incorporating medical code descriptions."
1,Towards cross-modal organ translation and segmentation: A cycle- and shape-consistent generative adversarial network,"Synthesized medical images have several important applications. For instance, they can be used as an intermedium in cross-modality image registration or used as augmented training samples to boost the generalization capability of a classifier. In this work, we propose a generic cross-modality synthesis approach with the following targets: 1) synthesizing realistic looking 2D/3D images without needing paired training data, 2) ensuring consistent anatomical structures, which could be changed by geometric distortion in cross-modality synthesis and 3) more importantly, improving volume segmentation by using synthetic data for modalities with limited training samples. We show that these goals can be achieved with an end-to-end 2D/3D convolutional neural network (CNN) composed of mutually-beneficial generators and segmentors for image synthesis and segmentation tasks. The generators are trained with an adversarial loss, a cycle-consistency loss, and also a shape-consistency loss (supervised by segmentors) to reduce the geometric distortion. From the segmentation view, the segmentors are boosted by synthetic data from generators in an online manner. Generators and segmentors prompt each other alternatively in an end-to-end training fashion. We validate our proposed method on three datasets, including cardiovascular CT and magnetic resonance imaging (MRI), abdominal CT and MRI, and mammography X-rays from different data domains, showing both tasks are beneficial to each other and coupling these two tasks results in better performance than solving them exclusively.","Towards cross-modal organ translation and segmentation: A cycle- and shape-consistent generative adversarial network. Synthesized medical images have several important applications. For instance, they can be used as an intermedium in cross-modality image registration or used as augmented training samples to boost the generalization capability of a classifier. In this work, we propose a generic cross-modality synthesis approach with the following targets: 1) synthesizing realistic looking 2D/3D images without needing paired training data, 2) ensuring consistent anatomical structures, which could be changed by geometric distortion in cross-modality synthesis and 3) more importantly, improving volume segmentation by using synthetic data for modalities with limited training samples. We show that these goals can be achieved with an end-to-end 2D/3D convolutional neural network (CNN) composed of mutually-beneficial generators and segmentors for image synthesis and segmentation tasks. The generators are trained with an adversarial loss, a cycle-consistency loss, and also a shape-consistency loss (supervised by segmentors) to reduce the geometric distortion. From the segmentation view, the segmentors are boosted by synthetic data from generators in an online manner. Generators and segmentors prompt each other alternatively in an end-to-end training fashion. We validate our proposed method on three datasets, including cardiovascular CT and magnetic resonance imaging (MRI), abdominal CT and MRI, and mammography X-rays from different data domains, showing both tasks are beneficial to each other and coupling these two tasks results in better performance than solving them exclusively."
1,Learning to detect lymphocytes in immunohistochemistry with deep learning,"The immune system is of critical importance in the development of cancer. The evasion of destruction by the immune system is one of the emerging hallmarks of cancer. We have built a dataset of 171,166 manually annotated CD3(+) and CD8(+) cells, which we used to train deep learning algorithms for automatic detection of lymphocytes in histopathology images to better quantify immune response. Moreover, we investigate the effectiveness of four deep learning based methods when different subcompartments of the whole-slide image are considered: normal tissue areas, areas with immune cell clusters, and areas containing artifacts. We have compared the proposed methods in breast, colon and prostate cancer tissue slides collected from nine different medical centers. Finally, we report the results of an observer study on lymphocyte quantification, which involved four pathologists from different medical centers, and compare their performance with the automatic detection. The results give insights on the applicability of the proposed methods for clinical use. U-Net obtained the highest performance with an F1-score of 0.78 and the highest agreement with manual evaluation (kappa=0.72), whereas the average pathologists agreement with reference standard was kappa=0.64. The test set and the automatic evaluation procedure are publicly available at lyon19.grand-challenge.org.","Learning to detect lymphocytes in immunohistochemistry with deep learning. The immune system is of critical importance in the development of cancer. The evasion of destruction by the immune system is one of the emerging hallmarks of cancer. We have built a dataset of 171,166 manually annotated CD3(+) and CD8(+) cells, which we used to train deep learning algorithms for automatic detection of lymphocytes in histopathology images to better quantify immune response. Moreover, we investigate the effectiveness of four deep learning based methods when different subcompartments of the whole-slide image are considered: normal tissue areas, areas with immune cell clusters, and areas containing artifacts. We have compared the proposed methods in breast, colon and prostate cancer tissue slides collected from nine different medical centers. Finally, we report the results of an observer study on lymphocyte quantification, which involved four pathologists from different medical centers, and compare their performance with the automatic detection. The results give insights on the applicability of the proposed methods for clinical use. U-Net obtained the highest performance with an F1-score of 0.78 and the highest agreement with manual evaluation (kappa=0.72), whereas the average pathologists agreement with reference standard was kappa=0.64. The test set and the automatic evaluation procedure are publicly available at lyon19.grand-challenge.org."
1,Learning to detect chest radiographs containing pulmonary lesions using visual attention networks,"Machine learning approaches hold great potential for the automated detection of lung nodules on chest radiographs, but training algorithms requires very large amounts of manually annotated radiographs, which are difficult to obtain. The increasing availability of PACS (Picture Archiving and Communication System), is laying the technological foundations needed to make available large volumes of clinical data and images from hospital archives. Binary labels indicating whether a radiograph contains a pulmonary lesion can be extracted at scale, using natural language processing algorithms. In this study, we propose two novel neural networks for the detection of chest radiographs containing pulmonary lesions. Both architectures make use of a large number of weakly-labelled images combined with a smaller number of manually annotated x-rays. The annotated lesions are used during training to deliver a type of visual attention feedback informing the networks about their lesion localisation performance. The first architecture extracts saliency maps from high-level convolutional layers and compares the inferred position of a lesion against the true position when this information is available; a localisation error is then back-propagated along with the softmax classification error. The second approach consists of a recurrent attention model that learns to observe a short sequence of smaller image portions through reinforcement learning; the reward function penalises the exploration of areas, within an image, that are unlikely to contain nodules. Using a repository of over 430,000 historical chest radiographs, we present and discuss the proposed methods over related architectures that use either weakly-labelled or annotated images only.","Learning to detect chest radiographs containing pulmonary lesions using visual attention networks. Machine learning approaches hold great potential for the automated detection of lung nodules on chest radiographs, but training algorithms requires very large amounts of manually annotated radiographs, which are difficult to obtain. The increasing availability of PACS (Picture Archiving and Communication System), is laying the technological foundations needed to make available large volumes of clinical data and images from hospital archives. Binary labels indicating whether a radiograph contains a pulmonary lesion can be extracted at scale, using natural language processing algorithms. In this study, we propose two novel neural networks for the detection of chest radiographs containing pulmonary lesions. Both architectures make use of a large number of weakly-labelled images combined with a smaller number of manually annotated x-rays. The annotated lesions are used during training to deliver a type of visual attention feedback informing the networks about their lesion localisation performance. The first architecture extracts saliency maps from high-level convolutional layers and compares the inferred position of a lesion against the true position when this information is available; a localisation error is then back-propagated along with the softmax classification error. The second approach consists of a recurrent attention model that learns to observe a short sequence of smaller image portions through reinforcement learning; the reward function penalises the exploration of areas, within an image, that are unlikely to contain nodules. Using a repository of over 430,000 historical chest radiographs, we present and discuss the proposed methods over related architectures that use either weakly-labelled or annotated images only."
1,Attention-Based Deep Neural Networks for Detection of Cancerous and Precancerous Esophagus Tissue on Histopathological Slides,"Importance: Deep learning-based methods, such as the sliding window approach for cropped-image classification and heuristic aggregation for whole-slide inference, for analyzing histological patterns in high-resolution microscopy images have shown promising results. These approaches, however, require a laborious annotation process and are fragmented. Objective: To evaluate a novel deep learning method that uses tissue-level annotations for high-resolution histological image analysis for Barrett esophagus (BE) and esophageal adenocarcinoma detection. Design, Setting, and Participants: This diagnostic study collected deidentified high-resolution histological images (N = 379) for training a new model composed of a convolutional neural network and a grid-based attention network. Histological images of patients who underwent endoscopic esophagus and gastroesophageal junction mucosal biopsy between January 1, 2016, and December 31, 2018, at Dartmouth-Hitchcock Medical Center (Lebanon, New Hampshire) were collected. Main Outcomes and Measures: The model was evaluated on an independent testing set of 123 histological images with 4 classes: normal, BE-no-dysplasia, BE-with-dysplasia, and adenocarcinoma. Performance of this model was measured and compared with that of the current state-of-the-art sliding window approach using the following standard machine learning metrics: accuracy, recall, precision, and F1 score. Results: Of the independent testing set of 123 histological images, 30 (24.4%) were in the BE-no-dysplasia class, 14 (11.4%) in the BE-with-dysplasia class, 21 (17.1%) in the adenocarcinoma class, and 58 (47.2%) in the normal class. Classification accuracies of the proposed model were 0.85 (95% CI, 0.81-0.90) for the BE-no-dysplasia class, 0.89 (95% CI, 0.84-0.92) for the BE-with-dysplasia class, and 0.88 (95% CI, 0.84-0.92) for the adenocarcinoma class. The proposed model achieved a mean accuracy of 0.83 (95% CI, 0.80-0.86) and marginally outperformed the sliding window approach on the same testing set. The F1 scores of the attention-based model were at least 8% higher for each class compared with the sliding window approach: 0.68 (95% CI, 0.61-0.75) vs 0.61 (95% CI, 0.53-0.68) for the normal class, 0.72 (95% CI, 0.63-0.80) vs 0.58 (95% CI, 0.45-0.69) for the BE-no-dysplasia class, 0.30 (95% CI, 0.11-0.48) vs 0.22 (95% CI, 0.11-0.33) for the BE-with-dysplasia class, and 0.67 (95% CI, 0.54-0.77) vs 0.58 (95% CI, 0.44-0.70) for the adenocarcinoma class. However, this outperformance was not statistically significant. Conclusions and Relevance: Results of this study suggest that the proposed attention-based deep neural network framework for BE and esophageal adenocarcinoma detection is important because it is based solely on tissue-level annotations, unlike existing methods that are based on regions of interest. This new model is expected to open avenues for applying deep learning to digital pathology.","Attention-Based Deep Neural Networks for Detection of Cancerous and Precancerous Esophagus Tissue on Histopathological Slides. Importance: Deep learning-based methods, such as the sliding window approach for cropped-image classification and heuristic aggregation for whole-slide inference, for analyzing histological patterns in high-resolution microscopy images have shown promising results. These approaches, however, require a laborious annotation process and are fragmented. Objective: To evaluate a novel deep learning method that uses tissue-level annotations for high-resolution histological image analysis for Barrett esophagus (BE) and esophageal adenocarcinoma detection. Design, Setting, and Participants: This diagnostic study collected deidentified high-resolution histological images (N = 379) for training a new model composed of a convolutional neural network and a grid-based attention network. Histological images of patients who underwent endoscopic esophagus and gastroesophageal junction mucosal biopsy between January 1, 2016, and December 31, 2018, at Dartmouth-Hitchcock Medical Center (Lebanon, New Hampshire) were collected. Main Outcomes and Measures: The model was evaluated on an independent testing set of 123 histological images with 4 classes: normal, BE-no-dysplasia, BE-with-dysplasia, and adenocarcinoma. Performance of this model was measured and compared with that of the current state-of-the-art sliding window approach using the following standard machine learning metrics: accuracy, recall, precision, and F1 score. Results: Of the independent testing set of 123 histological images, 30 (24.4%) were in the BE-no-dysplasia class, 14 (11.4%) in the BE-with-dysplasia class, 21 (17.1%) in the adenocarcinoma class, and 58 (47.2%) in the normal class. Classification accuracies of the proposed model were 0.85 (95% CI, 0.81-0.90) for the BE-no-dysplasia class, 0.89 (95% CI, 0.84-0.92) for the BE-with-dysplasia class, and 0.88 (95% CI, 0.84-0.92) for the adenocarcinoma class. The proposed model achieved a mean accuracy of 0.83 (95% CI, 0.80-0.86) and marginally outperformed the sliding window approach on the same testing set. The F1 scores of the attention-based model were at least 8% higher for each class compared with the sliding window approach: 0.68 (95% CI, 0.61-0.75) vs 0.61 (95% CI, 0.53-0.68) for the normal class, 0.72 (95% CI, 0.63-0.80) vs 0.58 (95% CI, 0.45-0.69) for the BE-no-dysplasia class, 0.30 (95% CI, 0.11-0.48) vs 0.22 (95% CI, 0.11-0.33) for the BE-with-dysplasia class, and 0.67 (95% CI, 0.54-0.77) vs 0.58 (95% CI, 0.44-0.70) for the adenocarcinoma class. However, this outperformance was not statistically significant. Conclusions and Relevance: Results of this study suggest that the proposed attention-based deep neural network framework for BE and esophageal adenocarcinoma detection is important because it is based solely on tissue-level annotations, unlike existing methods that are based on regions of interest. This new model is expected to open avenues for applying deep learning to digital pathology."
1,Removing segmentation inconsistencies with semi-supervised non-adjacency constraint,,
1,Direct automated quantitative measurement of spine by cascade amplifier regression network with manifold regularization,,
1,Ultra-short term HRV features as surrogates of short term HRV: a case study on mental stress detection in real life,"BACKGROUND: This paper suggests a method to assess the extent to which ultra-short Heart Rate Variability (HRV) features (less than 5â€‰min) can be considered as valid surrogates of short HRV features (nominally 5â€‰min). Short term HRV analysis has been widely investigated for mental stress assessment, whereas the validity of ultra-short HRV features remains unclear. Therefore, this study proposes a method to explore the extent to which HRV excerpts can be shortened without losing their ability to automatically detect mental stress. METHODS: ECGs were acquired from 42 healthy subjects during a university examination and resting condition. 23 features were extracted from HRV excerpts of different lengths (i.e., 30â€‰s, 1â€‰min, 2â€‰min, 3â€‰min, and 5â€‰min). Significant differences between rest and stress phases were investigated using non-parametric statistical tests at different time-scales. Features extracted from each ultra-short length were compared with the standard short HRV features, assumed as the benchmark, via Spearman's rank correlation analysis and Bland-Altman plots during rest and stress phases. Using data-driven machine learning approaches, a model aiming to detect mental stress was trained, validated and tested using short HRV features, and assessed on the ultra-short HRV features. RESULTS: Six out of 23 ultra-short HRV features (MeanNN, StdNN, MeanHR, StdHR, HF, and SD2) displayed consistency across all of the excerpt lengths (i.e., from 5 to 1â€‰min) and 3 out of those 6 ultra-short HRV features (MeanNN, StdHR, and HF) achieved good performance (accuracy above 88%) when employed in a well-dimensioned automatic classifier. CONCLUSION: This study concluded that 6 ultra-short HRV features are valid surrogates of short HRV features for mental stress investigation.","Ultra-short term HRV features as surrogates of short term HRV: a case study on mental stress detection in real life. BACKGROUND: This paper suggests a method to assess the extent to which ultra-short Heart Rate Variability (HRV) features (less than 5â€‰min) can be considered as valid surrogates of short HRV features (nominally 5â€‰min). Short term HRV analysis has been widely investigated for mental stress assessment, whereas the validity of ultra-short HRV features remains unclear. Therefore, this study proposes a method to explore the extent to which HRV excerpts can be shortened without losing their ability to automatically detect mental stress. METHODS: ECGs were acquired from 42 healthy subjects during a university examination and resting condition. 23 features were extracted from HRV excerpts of different lengths (i.e., 30â€‰s, 1â€‰min, 2â€‰min, 3â€‰min, and 5â€‰min). Significant differences between rest and stress phases were investigated using non-parametric statistical tests at different time-scales. Features extracted from each ultra-short length were compared with the standard short HRV features, assumed as the benchmark, via Spearman's rank correlation analysis and Bland-Altman plots during rest and stress phases. Using data-driven machine learning approaches, a model aiming to detect mental stress was trained, validated and tested using short HRV features, and assessed on the ultra-short HRV features. RESULTS: Six out of 23 ultra-short HRV features (MeanNN, StdNN, MeanHR, StdHR, HF, and SD2) displayed consistency across all of the excerpt lengths (i.e., from 5 to 1â€‰min) and 3 out of those 6 ultra-short HRV features (MeanNN, StdHR, and HF) achieved good performance (accuracy above 88%) when employed in a well-dimensioned automatic classifier. CONCLUSION: This study concluded that 6 ultra-short HRV features are valid surrogates of short HRV features for mental stress investigation."
1,Keratinocytic Skin Cancer Detection on the Face Using Region-Based Convolutional Neural Network,"Importance: Detection of cutaneous cancer on the face using deep-learning algorithms has been challenging because various anatomic structures create curves and shades that confuse the algorithm and can potentially lead to false-positive results. Objective: To evaluate whether an algorithm can automatically locate suspected areas and predict the probability of a lesion being malignant. Design, Setting, and Participants: Region-based convolutional neural network technology was used to create 924538 possible lesions by extracting nodular benign lesions from 182348 clinical photographs. After manually or automatically annotating these possible lesions based on image findings, convolutional neural networks were trained with 1106886 image crops to locate and diagnose cancer. Validation data sets (2844 images from 673 patients; mean [SD] age, 58.2 [19.9] years; 308 men [45.8%]; 185 patients with malignant tumors, 305 with benign tumors, and 183 free of tumor) were obtained from 3 hospitals between January 1, 2010, and September 30, 2018. Main Outcomes and Measures: The area under the receiver operating characteristic curve, F1 score (mean of precision and recall; range, 0.000-1.000), and Youden index score (sensitivity + specificity -1; 0%-100%) were used to compare the performance of the algorithm with that of the participants. Results: The algorithm analyzed a mean (SD) of 4.2 (2.4) photographs per patient and reported the malignancy score according to the highest malignancy output. The area under the receiver operating characteristic curve for the validation data set (673 patients) was 0.910. At a high-sensitivity cutoff threshold, the sensitivity and specificity of the model with the 673 patients were 76.8% and 90.6%, respectively. With the test partition (325 images; 80 patients), the performance of the algorithm was compared with the performance of 13 board-certified dermatologists, 34 dermatology residents, 20 nondermatologic physicians, and 52 members of the general public with no medical background. When the disease screening performance was evaluated at high sensitivity areas using the F1 score and Youden index score, the algorithm showed a higher F1 score (0.831 vs 0.653 [0.126], P < .001) and Youden index score (0.675 vs 0.417 [0.124], P < .001) than that of nondermatologic physicians. The accuracy of the algorithm was comparable with that of dermatologists (F1 score, 0.831 vs 0.835 [0.040]; Youden index score, 0.675 vs 0.671 [0.100]). Conclusions and Relevance: The results of the study suggest that the algorithm could localize and diagnose skin cancer without preselection of suspicious lesions by dermatologists.","Keratinocytic Skin Cancer Detection on the Face Using Region-Based Convolutional Neural Network. Importance: Detection of cutaneous cancer on the face using deep-learning algorithms has been challenging because various anatomic structures create curves and shades that confuse the algorithm and can potentially lead to false-positive results. Objective: To evaluate whether an algorithm can automatically locate suspected areas and predict the probability of a lesion being malignant. Design, Setting, and Participants: Region-based convolutional neural network technology was used to create 924538 possible lesions by extracting nodular benign lesions from 182348 clinical photographs. After manually or automatically annotating these possible lesions based on image findings, convolutional neural networks were trained with 1106886 image crops to locate and diagnose cancer. Validation data sets (2844 images from 673 patients; mean [SD] age, 58.2 [19.9] years; 308 men [45.8%]; 185 patients with malignant tumors, 305 with benign tumors, and 183 free of tumor) were obtained from 3 hospitals between January 1, 2010, and September 30, 2018. Main Outcomes and Measures: The area under the receiver operating characteristic curve, F1 score (mean of precision and recall; range, 0.000-1.000), and Youden index score (sensitivity + specificity -1; 0%-100%) were used to compare the performance of the algorithm with that of the participants. Results: The algorithm analyzed a mean (SD) of 4.2 (2.4) photographs per patient and reported the malignancy score according to the highest malignancy output. The area under the receiver operating characteristic curve for the validation data set (673 patients) was 0.910. At a high-sensitivity cutoff threshold, the sensitivity and specificity of the model with the 673 patients were 76.8% and 90.6%, respectively. With the test partition (325 images; 80 patients), the performance of the algorithm was compared with the performance of 13 board-certified dermatologists, 34 dermatology residents, 20 nondermatologic physicians, and 52 members of the general public with no medical background. When the disease screening performance was evaluated at high sensitivity areas using the F1 score and Youden index score, the algorithm showed a higher F1 score (0.831 vs 0.653 [0.126], P < .001) and Youden index score (0.675 vs 0.417 [0.124], P < .001) than that of nondermatologic physicians. The accuracy of the algorithm was comparable with that of dermatologists (F1 score, 0.831 vs 0.835 [0.040]; Youden index score, 0.675 vs 0.671 [0.100]). Conclusions and Relevance: The results of the study suggest that the algorithm could localize and diagnose skin cancer without preselection of suspicious lesions by dermatologists."
1,Automatic detection and diagnosis of sacroiliitis in CT scans as incidental findings,,
1,Use of electronic health record data and machine learning to identify candidates for HIV pre-exposure prophylaxis: a modelling study,"BACKGROUND: The limitations of existing HIV risk prediction tools are a barrier to implementation of pre-exposure prophylaxis (PrEP). We developed and validated an HIV prediction model to identify potential PrEP candidates in a large health-care system. METHODS: Our study population was HIV-uninfected adult members of Kaiser Permanente Northern California, a large integrated health-care system, who were not yet using PrEP and had at least 2 years of previous health plan enrolment with at least one outpatient visit from Jan 1, 2007, to Dec 31, 2017. Using 81 electronic health record (EHR) variables, we applied least absolute shrinkage and selection operator (LASSO) regression to predict incident HIV diagnosis within 3 years on a subset of patients who entered the cohort in 2007-14 (development dataset), assessing ten-fold cross-validated area under the receiver operating characteristic curve (AUC) and 95% CIs. We compared the full model to simpler models including only men who have sex with men (MSM) status and sexually transmitted infection (STI) positivity, testing, and treatment. Models were validated prospectively with data from an independent set of patients who entered the cohort in 2015-17. We computed predicted probabilities of incident HIV diagnosis within 3 years (risk scores), categorised as low risk (<0.05%), moderate risk (0.05% to <0.20%), high risk (0.20% to <1.0%), and very high risk (>/=1.0%), for all patients in the validation dataset. FINDINGS: Of 3 750 664 patients in 2007-17 (3 143 963 in the development dataset and 606 701 in the validation dataset), there were 784 incident HIV cases within 3 years of baseline. The LASSO procedure retained 44 predictors in the full model, with an AUC of 0.86 (95% CI 0.85-0.87) for incident HIV cases in 2007-14. Model performance remained high in the validation dataset (AUC 0.84, 0.80-0.89). The full model outperformed simpler models including only MSM status and STI positivity. For the full model, flagging 13 463 (2.2%) patients with high or very high HIV risk scores in the validation dataset identified 32 (38.6%) of the 83 incident HIV cases, including 32 (46.4%) of 69 male cases and none of the 14 female cases. The full model had equivalent sensitivity by race whereas simpler models identified fewer black than white HIV cases. INTERPRETATION: Prediction models using EHR data can identify patients at high risk of HIV acquisition who could benefit from PrEP. Future studies should optimise EHR-based HIV risk prediction tools and evaluate their effect on prescription of PrEP. FUNDING: Kaiser Permanente Community Benefit Research Program and the US National Institutes of Health.","Use of electronic health record data and machine learning to identify candidates for HIV pre-exposure prophylaxis: a modelling study. BACKGROUND: The limitations of existing HIV risk prediction tools are a barrier to implementation of pre-exposure prophylaxis (PrEP). We developed and validated an HIV prediction model to identify potential PrEP candidates in a large health-care system. METHODS: Our study population was HIV-uninfected adult members of Kaiser Permanente Northern California, a large integrated health-care system, who were not yet using PrEP and had at least 2 years of previous health plan enrolment with at least one outpatient visit from Jan 1, 2007, to Dec 31, 2017. Using 81 electronic health record (EHR) variables, we applied least absolute shrinkage and selection operator (LASSO) regression to predict incident HIV diagnosis within 3 years on a subset of patients who entered the cohort in 2007-14 (development dataset), assessing ten-fold cross-validated area under the receiver operating characteristic curve (AUC) and 95% CIs. We compared the full model to simpler models including only men who have sex with men (MSM) status and sexually transmitted infection (STI) positivity, testing, and treatment. Models were validated prospectively with data from an independent set of patients who entered the cohort in 2015-17. We computed predicted probabilities of incident HIV diagnosis within 3 years (risk scores), categorised as low risk (<0.05%), moderate risk (0.05% to <0.20%), high risk (0.20% to <1.0%), and very high risk (>/=1.0%), for all patients in the validation dataset. FINDINGS: Of 3 750 664 patients in 2007-17 (3 143 963 in the development dataset and 606 701 in the validation dataset), there were 784 incident HIV cases within 3 years of baseline. The LASSO procedure retained 44 predictors in the full model, with an AUC of 0.86 (95% CI 0.85-0.87) for incident HIV cases in 2007-14. Model performance remained high in the validation dataset (AUC 0.84, 0.80-0.89). The full model outperformed simpler models including only MSM status and STI positivity. For the full model, flagging 13 463 (2.2%) patients with high or very high HIV risk scores in the validation dataset identified 32 (38.6%) of the 83 incident HIV cases, including 32 (46.4%) of 69 male cases and none of the 14 female cases. The full model had equivalent sensitivity by race whereas simpler models identified fewer black than white HIV cases. INTERPRETATION: Prediction models using EHR data can identify patients at high risk of HIV acquisition who could benefit from PrEP. Future studies should optimise EHR-based HIV risk prediction tools and evaluate their effect on prescription of PrEP. FUNDING: Kaiser Permanente Community Benefit Research Program and the US National Institutes of Health."
1,Metal artifact reduction for the segmentation of the intra cochlear anatomy in CT images of the ear with 3D-conditional GANs,,
1,Improvement of fully automated airway segmentation on volumetric computed tomographic images using a 2.5 dimensional convolutional neural net,"We propose a novel airway segmentation method in volumetric chest computed tomography (CT) and evaluate its performance on multiple datasets. The segmentation is performed voxel-by-voxel by a 2.5D convolutional neural net (2.5D CNN) trained in a supervised manner. To enhance the accuracy of the segmented airway tree, we simultaneously took three adjacent slices in each of the orthogonal directions including axial, sagittal, and coronal and fine-tuned the parameters that influence the tree length and the number of leakage. The gold standard of airway segmentation was generated by a semi-automated method using AVIEW. The 2.5D CNN was trained and evaluated on a subset of inspiratory thoracic CT scans taken from the Korean obstructive lung disease study, which includes normal subjects and chronic obstructive pulmonary disease patients. The reliability and further practicality of our proposed method was demonstrated in multiple datasets. In eight test datasets collected by the same imaging protocol, the percentage detected tree length, false positive rate, and Dice similarity coefficient of our method were 92.16%, 7.74%, and 0.8997+/-0.0892, respectively. In 20 test datasets of the EXACT'09 challenge, the percentage detected tree length was 60.1% and the false positive rate was 4.56%. Our fully automated (end-to-end) segmentation method could be applied in radiologic practice.","Improvement of fully automated airway segmentation on volumetric computed tomographic images using a 2.5 dimensional convolutional neural net. We propose a novel airway segmentation method in volumetric chest computed tomography (CT) and evaluate its performance on multiple datasets. The segmentation is performed voxel-by-voxel by a 2.5D convolutional neural net (2.5D CNN) trained in a supervised manner. To enhance the accuracy of the segmented airway tree, we simultaneously took three adjacent slices in each of the orthogonal directions including axial, sagittal, and coronal and fine-tuned the parameters that influence the tree length and the number of leakage. The gold standard of airway segmentation was generated by a semi-automated method using AVIEW. The 2.5D CNN was trained and evaluated on a subset of inspiratory thoracic CT scans taken from the Korean obstructive lung disease study, which includes normal subjects and chronic obstructive pulmonary disease patients. The reliability and further practicality of our proposed method was demonstrated in multiple datasets. In eight test datasets collected by the same imaging protocol, the percentage detected tree length, false positive rate, and Dice similarity coefficient of our method were 92.16%, 7.74%, and 0.8997+/-0.0892, respectively. In 20 test datasets of the EXACT'09 challenge, the percentage detected tree length was 60.1% and the false positive rate was 4.56%. Our fully automated (end-to-end) segmentation method could be applied in radiologic practice."
1,Automated classification of dense calcium tissues in gray-scale intravascular ultrasound images using a deep belief network,"BACKGROUND: IVUS is widely used to quantitatively assess coronary artery disease. The purpose of this study was to automatically characterize dense calcium (DC) tissue in the gray scale intravascular ultrasound (IVUS) images using the image textural features. METHODS: A total of 316â€‰Gy-scale IVUS and corresponding virtual histology images from 26 patients with acute coronary syndrome who underwent IVUS along with X-ray angiography between October 2009 to September 2014 were retrospectively acquired and analyzed. One expert performed all procedures and assessed their IVUS scans. After image acquisition, the DC candidate and corresponding acoustic shadow regions were automatically determined. Then, nine image-base feature groups were extracted from the DC candidates. In order to reduce the dimensionalities, principal component analysis (PCA) was performed, and selected feature sets were utilized as an input for a deep belief network. Classification results were validated using 10-fold cross validation. RESULTS: The dimensionality of the feature map was efficiently reduced by 50% (from 66 to 33) without any performance decrease using PCA method. Sensitivity, specificity, and accuracy of the proposed method were 92.8â€‰Â±â€‰0.1%, 85.1â€‰Â±â€‰0.1%, and 88.4â€‰Â±â€‰0.1%, respectively (pâ€‰<â€‰0.05). We found that the window size could largely influence the characterization results, and selected the 5â€‰Ã—â€‰5 size as the best condition. We also validated the performance superiority of the proposed method with traditional classification methods. CONCLUSIONS: These experimental results suggest that the proposed method has significant clinical applicability for IVUS-based cardiovascular diagnosis.","Automated classification of dense calcium tissues in gray-scale intravascular ultrasound images using a deep belief network. BACKGROUND: IVUS is widely used to quantitatively assess coronary artery disease. The purpose of this study was to automatically characterize dense calcium (DC) tissue in the gray scale intravascular ultrasound (IVUS) images using the image textural features. METHODS: A total of 316â€‰Gy-scale IVUS and corresponding virtual histology images from 26 patients with acute coronary syndrome who underwent IVUS along with X-ray angiography between October 2009 to September 2014 were retrospectively acquired and analyzed. One expert performed all procedures and assessed their IVUS scans. After image acquisition, the DC candidate and corresponding acoustic shadow regions were automatically determined. Then, nine image-base feature groups were extracted from the DC candidates. In order to reduce the dimensionalities, principal component analysis (PCA) was performed, and selected feature sets were utilized as an input for a deep belief network. Classification results were validated using 10-fold cross validation. RESULTS: The dimensionality of the feature map was efficiently reduced by 50% (from 66 to 33) without any performance decrease using PCA method. Sensitivity, specificity, and accuracy of the proposed method were 92.8â€‰Â±â€‰0.1%, 85.1â€‰Â±â€‰0.1%, and 88.4â€‰Â±â€‰0.1%, respectively (pâ€‰<â€‰0.05). We found that the window size could largely influence the characterization results, and selected the 5â€‰Ã—â€‰5 size as the best condition. We also validated the performance superiority of the proposed method with traditional classification methods. CONCLUSIONS: These experimental results suggest that the proposed method has significant clinical applicability for IVUS-based cardiovascular diagnosis."
1,Chest Radiographs in Congestive Heart Failure: Visualizing Neural Network Learning,"Purpose To examine Generative Visual Rationales (GVRs) as a tool for visualizing neural network learning of chest radiograph features in congestive heart failure (CHF). Materials and Methods A total of 103 489 frontal chest radiographs in 46 712 patients acquired from January 1, 2007, to December 31, 2016, were divided into a labeled data set (with B-type natriuretic peptide [BNP] result as a marker of CHF) and unlabeled data set (without BNP result). A generative model was trained on the unlabeled data set, and a neural network was trained on the encoded representations of the labeled data set to estimate BNP. The model was used to visualize how a radiograph with high estimated BNP would look without disease (a ""healthy"" radiograph). An overfitted model was developed for comparison, and 100 GVRs were blindly assessed by two experts for features of CHF. Area under the receiver operating characteristic curve (AUC), kappa coefficient, and mixed-effects logistic regression were used for statistical analyses. Results At a cutoff BNP of 100 ng/L as a marker of CHF, the correctly trained model achieved an AUC of 0.82. Assessment of GVRs revealed that the correctly trained model highlighted conventional radiographic features of CHF as reasons for an elevated BNP prediction more frequently than the overfitted model, including cardiomegaly (153 [76.5%] of 200 vs 64 [32%] of 200, respectively; P < .001) and pleural effusions (47 [23.5%] of 200 vs 16 [8%] of 200, respectively; P = .003). Conclusion Features of congestive heart failure on chest radiographs learned by neural networks can be identified using Generative Visual Rationales, enabling detection of bias and overfitted models. (c) RSNA, 2018 See also the editorial by Ngo in this issue.","Chest Radiographs in Congestive Heart Failure: Visualizing Neural Network Learning. Purpose To examine Generative Visual Rationales (GVRs) as a tool for visualizing neural network learning of chest radiograph features in congestive heart failure (CHF). Materials and Methods A total of 103 489 frontal chest radiographs in 46 712 patients acquired from January 1, 2007, to December 31, 2016, were divided into a labeled data set (with B-type natriuretic peptide [BNP] result as a marker of CHF) and unlabeled data set (without BNP result). A generative model was trained on the unlabeled data set, and a neural network was trained on the encoded representations of the labeled data set to estimate BNP. The model was used to visualize how a radiograph with high estimated BNP would look without disease (a ""healthy"" radiograph). An overfitted model was developed for comparison, and 100 GVRs were blindly assessed by two experts for features of CHF. Area under the receiver operating characteristic curve (AUC), kappa coefficient, and mixed-effects logistic regression were used for statistical analyses. Results At a cutoff BNP of 100 ng/L as a marker of CHF, the correctly trained model achieved an AUC of 0.82. Assessment of GVRs revealed that the correctly trained model highlighted conventional radiographic features of CHF as reasons for an elevated BNP prediction more frequently than the overfitted model, including cardiomegaly (153 [76.5%] of 200 vs 64 [32%] of 200, respectively; P < .001) and pleural effusions (47 [23.5%] of 200 vs 16 [8%] of 200, respectively; P = .003). Conclusion Features of congestive heart failure on chest radiographs learned by neural networks can be identified using Generative Visual Rationales, enabling detection of bias and overfitted models. (c) RSNA, 2018 See also the editorial by Ngo in this issue."
1,Multi-Institutional Validation of Deep Learning for Pretreatment Identification of Extranodal Extension in Head and Neck Squamous Cell Carcinoma,"PURPOSE: Extranodal extension (ENE) is a well-established poor prognosticator and an indication for adjuvant treatment escalation in patients with head and neck squamous cell carcinoma (HNSCC). Identification of ENE on pretreatment imaging represents a diagnostic challenge that limits its clinical utility. We previously developed a deep learning algorithm that identifies ENE on pretreatment computed tomography (CT) imaging in patients with HNSCC. We sought to validate our algorithm performance for patients from a diverse set of institutions and compare its diagnostic ability to that of expert diagnosticians. METHODS: We obtained preoperative, contrast-enhanced CT scans and corresponding pathology results from two external data sets of patients with HNSCC: an external institution and The Cancer Genome Atlas (TCGA) HNSCC imaging data. Lymph nodes were segmented and annotated as ENE-positive or ENE-negative on the basis of pathologic confirmation. Deep learning algorithm performance was evaluated and compared directly to two board-certified neuroradiologists. RESULTS: A total of 200 lymph nodes were examined in the external validation data sets. For lymph nodes from the external institution, the algorithm achieved an area under the receiver operating characteristic curve (AUC) of 0.84 (83.1% accuracy), outperforming radiologists' AUCs of 0.70 and 0.71 (P = .02 and P = .01). Similarly, for lymph nodes from the TCGA, the algorithm achieved an AUC of 0.90 (88.6% accuracy), outperforming radiologist AUCs of 0.60 and 0.82 (P < .0001 and P = .16). Radiologist diagnostic accuracy improved when receiving deep learning assistance. CONCLUSION: Deep learning successfully identified ENE on pretreatment imaging across multiple institutions, exceeding the diagnostic ability of radiologists with specialized head and neck experience. Our findings suggest that deep learning has utility in the identification of ENE in patients with HNSCC and has the potential to be integrated into clinical decision making.","Multi-Institutional Validation of Deep Learning for Pretreatment Identification of Extranodal Extension in Head and Neck Squamous Cell Carcinoma. PURPOSE: Extranodal extension (ENE) is a well-established poor prognosticator and an indication for adjuvant treatment escalation in patients with head and neck squamous cell carcinoma (HNSCC). Identification of ENE on pretreatment imaging represents a diagnostic challenge that limits its clinical utility. We previously developed a deep learning algorithm that identifies ENE on pretreatment computed tomography (CT) imaging in patients with HNSCC. We sought to validate our algorithm performance for patients from a diverse set of institutions and compare its diagnostic ability to that of expert diagnosticians. METHODS: We obtained preoperative, contrast-enhanced CT scans and corresponding pathology results from two external data sets of patients with HNSCC: an external institution and The Cancer Genome Atlas (TCGA) HNSCC imaging data. Lymph nodes were segmented and annotated as ENE-positive or ENE-negative on the basis of pathologic confirmation. Deep learning algorithm performance was evaluated and compared directly to two board-certified neuroradiologists. RESULTS: A total of 200 lymph nodes were examined in the external validation data sets. For lymph nodes from the external institution, the algorithm achieved an area under the receiver operating characteristic curve (AUC) of 0.84 (83.1% accuracy), outperforming radiologists' AUCs of 0.70 and 0.71 (P = .02 and P = .01). Similarly, for lymph nodes from the TCGA, the algorithm achieved an AUC of 0.90 (88.6% accuracy), outperforming radiologist AUCs of 0.60 and 0.82 (P < .0001 and P = .16). Radiologist diagnostic accuracy improved when receiving deep learning assistance. CONCLUSION: Deep learning successfully identified ENE on pretreatment imaging across multiple institutions, exceeding the diagnostic ability of radiologists with specialized head and neck experience. Our findings suggest that deep learning has utility in the identification of ENE in patients with HNSCC and has the potential to be integrated into clinical decision making."
1,Predicting life expectancy with a long short-term memory recurrent neural network using electronic medical records,"BACKGROUND: Life expectancy is one of the most important factors in end-of-life decision making. Good prognostication for example helps to determine the course of treatment and helps to anticipate the procurement of health care services and facilities, or more broadly: facilitates Advance Care Planning. Advance Care Planning improves the quality of the final phase of life by stimulating doctors to explore the preferences for end-of-life care with their patients, and people close to the patients. Physicians, however, tend to overestimate life expectancy, and miss the window of opportunity to initiate Advance Care Planning. This research tests the potential of using machine learning and natural language processing techniques for predicting life expectancy from electronic medical records. METHODS: We approached the task of predicting life expectancy as a supervised machine learning task. We trained and tested a long short-term memory recurrent neural network on the medical records of deceased patients. We developed the model with a ten-fold cross-validation procedure, and evaluated its performance on a held-out set of test data. We compared the performance of a model which does not use text features (baseline model) to the performance of a model which uses features extracted from the free texts of the medical records (keyword model), and to doctors' performance on a similar task as described in scientific literature. RESULTS: Both doctors and the baseline model were correct in 20% of the cases, taking a margin of 33% around the actual life expectancy as the target. The keyword model, in comparison, attained an accuracy of 29% with its prognoses. While doctors overestimated life expectancy in 63% of the incorrect prognoses, which harms anticipation to appropriate end-of-life care, the keyword model overestimated life expectancy in only 31% of the incorrect prognoses. CONCLUSIONS: Prognostication of life expectancy is difficult for humans. Our research shows that machine learning and natural language processing techniques offer a feasible and promising approach to predicting life expectancy. The research has potential for real-life applications, such as supporting timely recognition of the right moment to start Advance Care Planning.","Predicting life expectancy with a long short-term memory recurrent neural network using electronic medical records. BACKGROUND: Life expectancy is one of the most important factors in end-of-life decision making. Good prognostication for example helps to determine the course of treatment and helps to anticipate the procurement of health care services and facilities, or more broadly: facilitates Advance Care Planning. Advance Care Planning improves the quality of the final phase of life by stimulating doctors to explore the preferences for end-of-life care with their patients, and people close to the patients. Physicians, however, tend to overestimate life expectancy, and miss the window of opportunity to initiate Advance Care Planning. This research tests the potential of using machine learning and natural language processing techniques for predicting life expectancy from electronic medical records. METHODS: We approached the task of predicting life expectancy as a supervised machine learning task. We trained and tested a long short-term memory recurrent neural network on the medical records of deceased patients. We developed the model with a ten-fold cross-validation procedure, and evaluated its performance on a held-out set of test data. We compared the performance of a model which does not use text features (baseline model) to the performance of a model which uses features extracted from the free texts of the medical records (keyword model), and to doctors' performance on a similar task as described in scientific literature. RESULTS: Both doctors and the baseline model were correct in 20% of the cases, taking a margin of 33% around the actual life expectancy as the target. The keyword model, in comparison, attained an accuracy of 29% with its prognoses. While doctors overestimated life expectancy in 63% of the incorrect prognoses, which harms anticipation to appropriate end-of-life care, the keyword model overestimated life expectancy in only 31% of the incorrect prognoses. CONCLUSIONS: Prognostication of life expectancy is difficult for humans. Our research shows that machine learning and natural language processing techniques offer a feasible and promising approach to predicting life expectancy. The research has potential for real-life applications, such as supporting timely recognition of the right moment to start Advance Care Planning."
1,DeepSeeNet: A Deep Learning Model for Automated Classification of Patient-based Age-related Macular Degeneration Severity from Color Fundus Photographs,"PURPOSE: In assessing the severity of age-related macular degeneration (AMD), the Age-Related Eye Disease Study (AREDS) Simplified Severity Scale predicts the risk of progression to late AMD. However, its manual use requires the time-consuming participation of expert practitioners. Although several automated deep learning systems have been developed for classifying color fundus photographs (CFP) of individual eyes by AREDS severity score, none to date has used a patient-based scoring system that uses images from both eyes to assign a severity score. DESIGN: DeepSeeNet, a deep learning model, was developed to classify patients automatically by the AREDS Simplified Severity Scale (score 0-5) using bilateral CFP. PARTICIPANTS: DeepSeeNet was trained on 58 402 and tested on 900 images from the longitudinal follow-up of 4549 participants from AREDS. Gold standard labels were obtained using reading center grades. METHODS: DeepSeeNet simulates the human grading process by first detecting individual AMD risk factors (drusen size, pigmentary abnormalities) for each eye and then calculating a patient-based AMD severity score using the AREDS Simplified Severity Scale. MAIN OUTCOME MEASURES: Overall accuracy, specificity, sensitivity, Cohen's kappa, and area under the curve (AUC). The performance of DeepSeeNet was compared with that of retinal specialists. RESULTS: DeepSeeNet performed better on patient-based classification (accuracy = 0.671; kappa = 0.558) than retinal specialists (accuracy = 0.599; kappa = 0.467) with high AUC in the detection of large drusen (0.94), pigmentary abnormalities (0.93), and late AMD (0.97). DeepSeeNet also outperformed retinal specialists in the detection of large drusen (accuracy 0.742 vs. 0.696; kappa 0.601 vs. 0.517) and pigmentary abnormalities (accuracy 0.890 vs. 0.813; kappa 0.723 vs. 0.535) but showed lower performance in the detection of late AMD (accuracy 0.967 vs. 0.973; kappa 0.663 vs. 0.754). CONCLUSIONS: By simulating the human grading process, DeepSeeNet demonstrated high accuracy with increased transparency in the automated assignment of individual patients to AMD risk categories based on the AREDS Simplified Severity Scale. These results highlight the potential of deep learning to assist and enhance clinical decision-making in patients with AMD, such as early AMD detection and risk prediction for developing late AMD. DeepSeeNet is publicly available on https://github.com/ncbi-nlp/DeepSeeNet.","DeepSeeNet: A Deep Learning Model for Automated Classification of Patient-based Age-related Macular Degeneration Severity from Color Fundus Photographs. PURPOSE: In assessing the severity of age-related macular degeneration (AMD), the Age-Related Eye Disease Study (AREDS) Simplified Severity Scale predicts the risk of progression to late AMD. However, its manual use requires the time-consuming participation of expert practitioners. Although several automated deep learning systems have been developed for classifying color fundus photographs (CFP) of individual eyes by AREDS severity score, none to date has used a patient-based scoring system that uses images from both eyes to assign a severity score. DESIGN: DeepSeeNet, a deep learning model, was developed to classify patients automatically by the AREDS Simplified Severity Scale (score 0-5) using bilateral CFP. PARTICIPANTS: DeepSeeNet was trained on 58 402 and tested on 900 images from the longitudinal follow-up of 4549 participants from AREDS. Gold standard labels were obtained using reading center grades. METHODS: DeepSeeNet simulates the human grading process by first detecting individual AMD risk factors (drusen size, pigmentary abnormalities) for each eye and then calculating a patient-based AMD severity score using the AREDS Simplified Severity Scale. MAIN OUTCOME MEASURES: Overall accuracy, specificity, sensitivity, Cohen's kappa, and area under the curve (AUC). The performance of DeepSeeNet was compared with that of retinal specialists. RESULTS: DeepSeeNet performed better on patient-based classification (accuracy = 0.671; kappa = 0.558) than retinal specialists (accuracy = 0.599; kappa = 0.467) with high AUC in the detection of large drusen (0.94), pigmentary abnormalities (0.93), and late AMD (0.97). DeepSeeNet also outperformed retinal specialists in the detection of large drusen (accuracy 0.742 vs. 0.696; kappa 0.601 vs. 0.517) and pigmentary abnormalities (accuracy 0.890 vs. 0.813; kappa 0.723 vs. 0.535) but showed lower performance in the detection of late AMD (accuracy 0.967 vs. 0.973; kappa 0.663 vs. 0.754). CONCLUSIONS: By simulating the human grading process, DeepSeeNet demonstrated high accuracy with increased transparency in the automated assignment of individual patients to AMD risk categories based on the AREDS Simplified Severity Scale. These results highlight the potential of deep learning to assist and enhance clinical decision-making in patients with AMD, such as early AMD detection and risk prediction for developing late AMD. DeepSeeNet is publicly available on https://github.com/ncbi-nlp/DeepSeeNet."
1,How many models/atlases are needed as priors for capturing anatomic population variations?,,
1,Detection of herpesvirus capsids in transmission electron microscopy images using transfer learning,"The detailed analysis of secondary envelopment of the Human betaherpesvirus 5/human cytomegalovirus (HCMV) from transmission electron microscopy (TEM) images is an important step towards understanding the mechanisms underlying the formation of infectious virions. As a step towards a software-based quantification of different stages of HCMV virion morphogenesis in TEM, we developed a transfer learning approach based on convolutional neural networks (CNNs) that automatically detects HCMV nucleocapsids in TEM images. In contrast to existing image analysis techniques that require time-consuming manual definition of structural features, our method automatically learns discriminative features from raw images without the need for extensive pre-processing. For this a constantly growing TEM image database of HCMV infected cells was available which is unique regarding image quality and size in the terms of virological EM. From the two investigated types of transfer learning approaches, namely feature extraction and fine-tuning, the latter enabled us to successfully detect HCMV nucleocapsids in TEM images. Our detection method has outperformed some of the existing image analysis methods based on discriminative textural indicators and radial density profiles for virus detection in TEM images. In summary, we could show that the method of transfer learning can be used for an automated detection of viral capsids in TEM images with high specificity using standard computers. This method is highly adaptable and in future could be easily extended to automatically detect and classify virions of other viruses and even distinguish different virion maturation stages.","Detection of herpesvirus capsids in transmission electron microscopy images using transfer learning. The detailed analysis of secondary envelopment of the Human betaherpesvirus 5/human cytomegalovirus (HCMV) from transmission electron microscopy (TEM) images is an important step towards understanding the mechanisms underlying the formation of infectious virions. As a step towards a software-based quantification of different stages of HCMV virion morphogenesis in TEM, we developed a transfer learning approach based on convolutional neural networks (CNNs) that automatically detects HCMV nucleocapsids in TEM images. In contrast to existing image analysis techniques that require time-consuming manual definition of structural features, our method automatically learns discriminative features from raw images without the need for extensive pre-processing. For this a constantly growing TEM image database of HCMV infected cells was available which is unique regarding image quality and size in the terms of virological EM. From the two investigated types of transfer learning approaches, namely feature extraction and fine-tuning, the latter enabled us to successfully detect HCMV nucleocapsids in TEM images. Our detection method has outperformed some of the existing image analysis methods based on discriminative textural indicators and radial density profiles for virus detection in TEM images. In summary, we could show that the method of transfer learning can be used for an automated detection of viral capsids in TEM images with high specificity using standard computers. This method is highly adaptable and in future could be easily extended to automatically detect and classify virions of other viruses and even distinguish different virion maturation stages."
1,Diagnostic Accuracy of Community-Based Diabetic Retinopathy Screening With an Offline Artificial Intelligence System on a Smartphone,"Importance: Offline automated analysis of retinal images on a smartphone may be a cost-effective and scalable method of screening for diabetic retinopathy; however, to our knowledge, assessment of such an artificial intelligence (AI) system is lacking. Objective: To evaluate the performance of Medios AI (Remidio), a proprietary, offline, smartphone-based, automated system of analysis of retinal images, to detect referable diabetic retinopathy (RDR) in images taken by a minimally trained health care worker with Remidio Non-Mydriatic Fundus on Phone, a smartphone-based, nonmydriatic retinal camera. Referable diabetic retinopathy is defined as any retinopathy more severe than mild diabetic retinopathy, with or without diabetic macular edema. Design, Setting, and Participants: This prospective, cross-sectional, population-based study took place from August 2018 to September 2018. Patients with diabetes mellitus who visited various dispensaries administered by the Municipal Corporation of Greater Mumbai in Mumbai, India, on a particular day were included. Interventions: Three fields of the fundus (the posterior pole, nasal, and temporal fields) were photographed. The images were analyzed by an ophthalmologist and the AI system. Main Outcomes and Measures: To evaluate the sensitivity and specificity of the offline automated analysis system in detecting referable diabetic retinopathy on images taken on the smartphone-based, nonmydriatic retinal imaging system by a health worker. Results: Of 255 patients seen in the dispensaries, 231 patients (90.6%) consented to diabetic retinopathy screening. The major reasons for not participating were unwillingness to wait for screening and the blurring of vision that would occur after dilation. Images from 18 patients were deemed ungradable by the ophthalmologist and hence were excluded. In the remaining participants (110 female patients [51.6%] and 103 male patients [48.4%]; mean [SD] age, 53.1 [10.3] years), the sensitivity and specificity of the offline AI system in diagnosing referable diabetic retinopathy were 100.0% (95% CI, 78.2%-100.0%) and 88.4% (95% CI, 83.2%-92.5%), respectively, and in diagnosing any diabetic retinopathy were 85.2% (95% CI, 66.3%-95.8%) and 92.0% (95% CI, 97.1%-95.4%), respectively, compared with ophthalmologist grading using the same images. Conclusions and Relevance: These pilot study results show promise in the use of an offline AI system in community screening for referable diabetic retinopathy with a smartphone-based fundus camera. The use of AI would enable screening for referable diabetic retinopathy in remote areas where services of an ophthalmologist are unavailable. This study was done on patients with diabetes who were visiting a dispensary that provides curative services to the population at the primary level. A study with a larger sample size may be needed to extend the results to general population screening, however.","Diagnostic Accuracy of Community-Based Diabetic Retinopathy Screening With an Offline Artificial Intelligence System on a Smartphone. Importance: Offline automated analysis of retinal images on a smartphone may be a cost-effective and scalable method of screening for diabetic retinopathy; however, to our knowledge, assessment of such an artificial intelligence (AI) system is lacking. Objective: To evaluate the performance of Medios AI (Remidio), a proprietary, offline, smartphone-based, automated system of analysis of retinal images, to detect referable diabetic retinopathy (RDR) in images taken by a minimally trained health care worker with Remidio Non-Mydriatic Fundus on Phone, a smartphone-based, nonmydriatic retinal camera. Referable diabetic retinopathy is defined as any retinopathy more severe than mild diabetic retinopathy, with or without diabetic macular edema. Design, Setting, and Participants: This prospective, cross-sectional, population-based study took place from August 2018 to September 2018. Patients with diabetes mellitus who visited various dispensaries administered by the Municipal Corporation of Greater Mumbai in Mumbai, India, on a particular day were included. Interventions: Three fields of the fundus (the posterior pole, nasal, and temporal fields) were photographed. The images were analyzed by an ophthalmologist and the AI system. Main Outcomes and Measures: To evaluate the sensitivity and specificity of the offline automated analysis system in detecting referable diabetic retinopathy on images taken on the smartphone-based, nonmydriatic retinal imaging system by a health worker. Results: Of 255 patients seen in the dispensaries, 231 patients (90.6%) consented to diabetic retinopathy screening. The major reasons for not participating were unwillingness to wait for screening and the blurring of vision that would occur after dilation. Images from 18 patients were deemed ungradable by the ophthalmologist and hence were excluded. In the remaining participants (110 female patients [51.6%] and 103 male patients [48.4%]; mean [SD] age, 53.1 [10.3] years), the sensitivity and specificity of the offline AI system in diagnosing referable diabetic retinopathy were 100.0% (95% CI, 78.2%-100.0%) and 88.4% (95% CI, 83.2%-92.5%), respectively, and in diagnosing any diabetic retinopathy were 85.2% (95% CI, 66.3%-95.8%) and 92.0% (95% CI, 97.1%-95.4%), respectively, compared with ophthalmologist grading using the same images. Conclusions and Relevance: These pilot study results show promise in the use of an offline AI system in community screening for referable diabetic retinopathy with a smartphone-based fundus camera. The use of AI would enable screening for referable diabetic retinopathy in remote areas where services of an ophthalmologist are unavailable. This study was done on patients with diabetes who were visiting a dispensary that provides curative services to the population at the primary level. A study with a larger sample size may be needed to extend the results to general population screening, however."
1,Epigenomic signatures underpin the axonal regenerative ability of dorsal root ganglia sensory neurons,"Axonal injury results in regenerative success or failure, depending on whether the axon lies in the peripheral or the CNS, respectively. The present study addresses whether epigenetic signatures in dorsal root ganglia discriminate between regenerative and non-regenerative axonal injury. Chromatin immunoprecipitation for the histone 3 (H3) post-translational modifications H3K9ac, H3K27ac and H3K27me3; an assay for transposase-accessible chromatin; and RNA sequencing were performed in dorsal root ganglia after sciatic nerve or dorsal column axotomy. Distinct histone acetylation and chromatin accessibility signatures correlated with gene expression after peripheral, but not central, axonal injury. DNA-footprinting analyses revealed new transcriptional regulators associated with regenerative ability. Machine-learning algorithms inferred the direction of most of the gene expression changes. Neuronal conditional deletion of the chromatin remodeler CCCTC-binding factor impaired nerve regeneration, implicating chromatin organization in the regenerative competence. Altogether, the present study offers the first epigenomic map providing insight into the transcriptional response to injury and the differential regenerative ability of sensory neurons.","Epigenomic signatures underpin the axonal regenerative ability of dorsal root ganglia sensory neurons. Axonal injury results in regenerative success or failure, depending on whether the axon lies in the peripheral or the CNS, respectively. The present study addresses whether epigenetic signatures in dorsal root ganglia discriminate between regenerative and non-regenerative axonal injury. Chromatin immunoprecipitation for the histone 3 (H3) post-translational modifications H3K9ac, H3K27ac and H3K27me3; an assay for transposase-accessible chromatin; and RNA sequencing were performed in dorsal root ganglia after sciatic nerve or dorsal column axotomy. Distinct histone acetylation and chromatin accessibility signatures correlated with gene expression after peripheral, but not central, axonal injury. DNA-footprinting analyses revealed new transcriptional regulators associated with regenerative ability. Machine-learning algorithms inferred the direction of most of the gene expression changes. Neuronal conditional deletion of the chromatin remodeler CCCTC-binding factor impaired nerve regeneration, implicating chromatin organization in the regenerative competence. Altogether, the present study offers the first epigenomic map providing insight into the transcriptional response to injury and the differential regenerative ability of sensory neurons."
1,Latent Dirichlet Allocation in predicting clinical trial terminations,"BACKGROUND: This study used natural language processing (NLP) and machine learning (ML) techniques to identify reliable patterns from within research narrative documents to distinguish studies that complete successfully, from the ones that terminate. Recent research findings have reported that at least 10 % of all studies that are funded by major research funding agencies terminate without yielding useful results. Since it is well-known that scientific studies that receive funding from major funding agencies are carefully planned, and rigorously vetted through the peer-review process, it was somewhat daunting to us that study-terminations are this prevalent. Moreover, our review of the literature about study terminations suggested that the reasons for study terminations are not well understood. We therefore aimed to address that knowledge gap, by seeking to identify the factors that contribute to study failures. METHOD: We used data from the clinicialTrials.gov repository, from which we extracted both structured data (study characteristics), and unstructured data (the narrative description of the studies). We applied natural language processing techniques to the unstructured data to quantify the risk of termination by identifying distinctive topics that are more frequently associated with trials that are terminated and trials that are completed. We used the Latent Dirichlet Allocation (LDA) technique to derive 25 ""topics"" with corresponding sets of probabilities, which we then used to predict study-termination by utilizing random forest modeling. We fit two distinct models - one using only structured data as predictors and another model with both structured data and the 25 text topics derived from the unstructured data. RESULTS: In this paper, we demonstrate the interpretive and predictive value of LDA as it relates to predicting clinical trial failure. The results also demonstrate that the combined modeling approach yields robust predictive probabilities in terms of both sensitivity and specificity, relative to a model that utilizes the structured data alone. CONCLUSIONS: Our study demonstrated that the use of topic modeling using LDA significantly raises the utility of unstructured data in better predicating the completion vs. termination of studies. This study sets the direction for future research to evaluate the viability of the designs of health studies.","Latent Dirichlet Allocation in predicting clinical trial terminations. BACKGROUND: This study used natural language processing (NLP) and machine learning (ML) techniques to identify reliable patterns from within research narrative documents to distinguish studies that complete successfully, from the ones that terminate. Recent research findings have reported that at least 10 % of all studies that are funded by major research funding agencies terminate without yielding useful results. Since it is well-known that scientific studies that receive funding from major funding agencies are carefully planned, and rigorously vetted through the peer-review process, it was somewhat daunting to us that study-terminations are this prevalent. Moreover, our review of the literature about study terminations suggested that the reasons for study terminations are not well understood. We therefore aimed to address that knowledge gap, by seeking to identify the factors that contribute to study failures. METHOD: We used data from the clinicialTrials.gov repository, from which we extracted both structured data (study characteristics), and unstructured data (the narrative description of the studies). We applied natural language processing techniques to the unstructured data to quantify the risk of termination by identifying distinctive topics that are more frequently associated with trials that are terminated and trials that are completed. We used the Latent Dirichlet Allocation (LDA) technique to derive 25 ""topics"" with corresponding sets of probabilities, which we then used to predict study-termination by utilizing random forest modeling. We fit two distinct models - one using only structured data as predictors and another model with both structured data and the 25 text topics derived from the unstructured data. RESULTS: In this paper, we demonstrate the interpretive and predictive value of LDA as it relates to predicting clinical trial failure. The results also demonstrate that the combined modeling approach yields robust predictive probabilities in terms of both sensitivity and specificity, relative to a model that utilizes the structured data alone. CONCLUSIONS: Our study demonstrated that the use of topic modeling using LDA significantly raises the utility of unstructured data in better predicating the completion vs. termination of studies. This study sets the direction for future research to evaluate the viability of the designs of health studies."
1,Coronary artery centerline extraction in cardiac CT angiography using a CNN-based orientation classifier,"Coronary artery centerline extraction in cardiac CT angiography (CCTA) images is a prerequisite for evaluation of stenoses and atherosclerotic plaque. In this work, we propose an algorithm that extracts coronary artery centerlines in CCTA using a convolutional neural network (CNN). In the proposed method, a 3D dilated CNN is trained to predict the most likely direction and radius of an artery at any given point in a CCTA image based on a local image patch. Starting from a single seed point placed manually or automatically anywhere in a coronary artery, a tracker follows the vessel centerline in two directions using the predictions of the CNN. Tracking is terminated when no direction can be identified with high certainty. The CNN is trained using manually annotated centerlines in training images. No image preprocessing is required, so that the process is guided solely by the local image values around the tracker's location. The CNN was trained using a training set consisting of 8 CCTA images with a total of 32 manually annotated centerlines provided in the MICCAI 2008 Coronary Artery Tracking Challenge (CAT08). Evaluation was performed within the CAT08 challenge using a test set consisting of 24 CCTA test images in which 96 centerlines were extracted. The extracted centerlines had an average overlap of 93.7% with manually annotated reference centerlines. Extracted centerline points were highly accurate, with an average distance of 0.21mm to reference centerline points. Based on these results the method ranks third among 25 publicly evaluated methods in CAT08. In a second test set consisting of 50 CCTA scans acquired at our institution (UMCU), an expert placed 5448 markers in the coronary arteries, along with radius measurements. Each marker was used as a seed point to extract a single centerline, which was compared to the other markers placed by the expert. This showed strong correspondence between extracted centerlines and manually placed markers. In a third test set containing 36 CCTA scans from the MICCAI 2014 Challenge on Automatic Coronary Calcium Scoring (orCaScore), fully automatic seeding and centerline extraction was evaluated using a segment-wise analysis. This showed that the algorithm is able to fully-automatically extract on average 92% of clinically relevant coronary artery segments. Finally, the limits of agreement between reference and automatic artery radius measurements were found to be below the size of one voxel in both the CAT08 dataset and the UMCU dataset. Extraction of a centerline based on a single seed point required on average 0.4+/-0.1 s and fully automatic coronary tree extraction required around 20 s. The proposed method is able to accurately and efficiently determine the direction and radius of coronary arteries based on information derived directly from the image data. The method can be trained with limited training data, and once trained allows fast automatic or interactive extraction of coronary artery trees from CCTA images.","Coronary artery centerline extraction in cardiac CT angiography using a CNN-based orientation classifier. Coronary artery centerline extraction in cardiac CT angiography (CCTA) images is a prerequisite for evaluation of stenoses and atherosclerotic plaque. In this work, we propose an algorithm that extracts coronary artery centerlines in CCTA using a convolutional neural network (CNN). In the proposed method, a 3D dilated CNN is trained to predict the most likely direction and radius of an artery at any given point in a CCTA image based on a local image patch. Starting from a single seed point placed manually or automatically anywhere in a coronary artery, a tracker follows the vessel centerline in two directions using the predictions of the CNN. Tracking is terminated when no direction can be identified with high certainty. The CNN is trained using manually annotated centerlines in training images. No image preprocessing is required, so that the process is guided solely by the local image values around the tracker's location. The CNN was trained using a training set consisting of 8 CCTA images with a total of 32 manually annotated centerlines provided in the MICCAI 2008 Coronary Artery Tracking Challenge (CAT08). Evaluation was performed within the CAT08 challenge using a test set consisting of 24 CCTA test images in which 96 centerlines were extracted. The extracted centerlines had an average overlap of 93.7% with manually annotated reference centerlines. Extracted centerline points were highly accurate, with an average distance of 0.21mm to reference centerline points. Based on these results the method ranks third among 25 publicly evaluated methods in CAT08. In a second test set consisting of 50 CCTA scans acquired at our institution (UMCU), an expert placed 5448 markers in the coronary arteries, along with radius measurements. Each marker was used as a seed point to extract a single centerline, which was compared to the other markers placed by the expert. This showed strong correspondence between extracted centerlines and manually placed markers. In a third test set containing 36 CCTA scans from the MICCAI 2014 Challenge on Automatic Coronary Calcium Scoring (orCaScore), fully automatic seeding and centerline extraction was evaluated using a segment-wise analysis. This showed that the algorithm is able to fully-automatically extract on average 92% of clinically relevant coronary artery segments. Finally, the limits of agreement between reference and automatic artery radius measurements were found to be below the size of one voxel in both the CAT08 dataset and the UMCU dataset. Extraction of a centerline based on a single seed point required on average 0.4+/-0.1 s and fully automatic coronary tree extraction required around 20 s. The proposed method is able to accurately and efficiently determine the direction and radius of coronary arteries based on information derived directly from the image data. The method can be trained with limited training data, and once trained allows fast automatic or interactive extraction of coronary artery trees from CCTA images."
1,Application of a Neural Network Whole Transcriptome-Based Pan-Cancer Method for Diagnosis of Primary and Metastatic Cancers,"Importance: A molecular diagnostic method that incorporates information about the transcriptional status of all genes across multiple tissue types can strengthen confidence in cancer diagnosis. Objective: To determine the practical use of a whole transcriptome-based pan-cancer method in diagnosing primary and metastatic cancers and resolving complex diagnoses. Design, Setting, and Participants: This cross-sectional diagnostic study assessed Supervised Cancer Origin Prediction Using Expression (SCOPE), a machine learning method using whole-transcriptome RNA sequencing data. Training was performed on publicly available primary cancer data sets, including The Cancer Genome Atlas. Testing was performed retrospectively on untreated primary cancers and treated metastases from volunteer adult patients at BC Cancer in Vancouver, British Columbia, from January 1, 2013, to March 31, 2016, and testing spanned 10â€¯822 samples and 66 output classes representing untreated primary cancers (nâ€‰=â€‰40) and adjacent normal tissues (nâ€‰=â€‰26). SCOPE's performance was demonstrated on 211 untreated primary mesothelioma cancers and 201 treatment-resistant metastatic cancers. Finally, SCOPE was used to identify the putative site of origin in 15 cases with initial presentation as cancers with unknown primary of origin. Results: A total of 10â€¯688 adult patient samples representing 40 untreated primary tumor types and 26 adjacent-normal tissues were used for training. Demographic data were not available for all data sets. Among the training data set, 5157 of 10 244 (50.3%) were male and the mean (SD) age was 58.9 (14.5) years. Testing was performed on 211 patients with untreated primary mesothelioma (173 [82.0%] male; mean [SD] age, 64.5 [11.3] years); 201 patients with treatment-resistant cancers (141 [70.1%] female; mean [SD] age, 55.6 [12.9] years); and 15 patients with cancers of unknown primary of origin; among the treatment-resistant cancers, 168 were metastatic, and 33 were the primary presentation. An accuracy rate of 99% was obtained for primary epithelioid mesotheliomas tested (125 of 126). The remaining 85 mesotheliomas had a mixed etiology (sarcomatoid mesotheliomas) and were correctly identified as a mixture of their primary components, with potential implications in resolving subtypes and incidences of mixed histology. SCOPE achieved an overall mean (SD) accuracy rate of 86% (11%) and F1 score of 0.79 (0.12) on the 201 treatment-resistant cancers and matched 12 of 15 of the putative diagnoses for cancers with indeterminate diagnosis from conventional pathology. Conclusions and Relevance: These results suggest that machine learning approaches incorporating multiple tumor profiles can more accurately identify the cancerous state and discriminate it from normal cells. SCOPE uses the whole transcriptomes from normal and tumor tissues, and results of this study suggest that it performs well for rare cancer types, primary cancers, treatment-resistant metastatic cancers, and cancers of unknown primary of origin. Genes most relevant in SCOPE's decision making were examined, and several are known biological markers of respective cancers. SCOPE may be applied as an orthogonal diagnostic method in cases where the site of origin of a cancer is unknown, or when standard pathology assessment is inconclusive.","Application of a Neural Network Whole Transcriptome-Based Pan-Cancer Method for Diagnosis of Primary and Metastatic Cancers. Importance: A molecular diagnostic method that incorporates information about the transcriptional status of all genes across multiple tissue types can strengthen confidence in cancer diagnosis. Objective: To determine the practical use of a whole transcriptome-based pan-cancer method in diagnosing primary and metastatic cancers and resolving complex diagnoses. Design, Setting, and Participants: This cross-sectional diagnostic study assessed Supervised Cancer Origin Prediction Using Expression (SCOPE), a machine learning method using whole-transcriptome RNA sequencing data. Training was performed on publicly available primary cancer data sets, including The Cancer Genome Atlas. Testing was performed retrospectively on untreated primary cancers and treated metastases from volunteer adult patients at BC Cancer in Vancouver, British Columbia, from January 1, 2013, to March 31, 2016, and testing spanned 10â€¯822 samples and 66 output classes representing untreated primary cancers (nâ€‰=â€‰40) and adjacent normal tissues (nâ€‰=â€‰26). SCOPE's performance was demonstrated on 211 untreated primary mesothelioma cancers and 201 treatment-resistant metastatic cancers. Finally, SCOPE was used to identify the putative site of origin in 15 cases with initial presentation as cancers with unknown primary of origin. Results: A total of 10â€¯688 adult patient samples representing 40 untreated primary tumor types and 26 adjacent-normal tissues were used for training. Demographic data were not available for all data sets. Among the training data set, 5157 of 10 244 (50.3%) were male and the mean (SD) age was 58.9 (14.5) years. Testing was performed on 211 patients with untreated primary mesothelioma (173 [82.0%] male; mean [SD] age, 64.5 [11.3] years); 201 patients with treatment-resistant cancers (141 [70.1%] female; mean [SD] age, 55.6 [12.9] years); and 15 patients with cancers of unknown primary of origin; among the treatment-resistant cancers, 168 were metastatic, and 33 were the primary presentation. An accuracy rate of 99% was obtained for primary epithelioid mesotheliomas tested (125 of 126). The remaining 85 mesotheliomas had a mixed etiology (sarcomatoid mesotheliomas) and were correctly identified as a mixture of their primary components, with potential implications in resolving subtypes and incidences of mixed histology. SCOPE achieved an overall mean (SD) accuracy rate of 86% (11%) and F1 score of 0.79 (0.12) on the 201 treatment-resistant cancers and matched 12 of 15 of the putative diagnoses for cancers with indeterminate diagnosis from conventional pathology. Conclusions and Relevance: These results suggest that machine learning approaches incorporating multiple tumor profiles can more accurately identify the cancerous state and discriminate it from normal cells. SCOPE uses the whole transcriptomes from normal and tumor tissues, and results of this study suggest that it performs well for rare cancer types, primary cancers, treatment-resistant metastatic cancers, and cancers of unknown primary of origin. Genes most relevant in SCOPE's decision making were examined, and several are known biological markers of respective cancers. SCOPE may be applied as an orthogonal diagnostic method in cases where the site of origin of a cancer is unknown, or when standard pathology assessment is inconclusive."
1,Management Thyroid Nodules Seen on US Images: Deep Learning May Match Performance of Radiologists,,
1,Deep Learning for MR Angiography: Automated Detection of Cerebral Aneurysms,"Purpose To develop and evaluate a supportive algorithm using deep learning for detecting cerebral aneurysms at time-of-flight MR angiography to provide a second assessment of images already interpreted by radiologists. Materials and Methods MR images reported by radiologists to contain aneurysms were extracted from four institutions for the period from November 2006 through October 2017. The images were divided into three data sets: training data set, internal test data set, and external test data set. The algorithm was constructed by deep learning with the training data set, and its sensitivity to detect aneurysms in the test data sets was evaluated. To find aneurysms that had been overlooked in the initial reports, two radiologists independently performed a blinded interpretation of aneurysm candidates detected by the algorithm. When there was disagreement, the final diagnosis was made in consensus. The number of newly detected aneurysms was also evaluated. Results The training data set, which provided training and validation data, included 748 aneurysms (mean size, 3.1 mm +/- 2.0 [standard deviation]) from 683 examinations; 318 of these examinations were on male patients (mean age, 63 years +/- 13) and 365 were on female patients (mean age, 64 years +/- 13). Test data were provided by the internal test data set (649 aneurysms [mean size, 4.1 mm +/- 3.2] in 521 examinations, including 177 male patients and 344 female patients with mean age of 66 years +/- 12 and 67 years +/- 13, respectively) and the external test data set (80 aneurysms [mean size, 4.1 mm +/- 2.1] in 67 examinations, including 19 male patients and 48 female patients with mean age of 63 years +/- 12 and 68 years +/- 12, respectively). The sensitivity was 91% (592 of 649) and 93% (74 of 80) for the internal and external test data sets, respectively. The algorithm improved aneurysm detection in the internal and external test data sets by 4.8% (31 of 649) and 13% (10 of 80), respectively, compared with the initial reports. Conclusion A deep learning algorithm detected cerebral aneurysms in radiologic reports with high sensitivity and improved aneurysm detection compared with the initial reports. (c) RSNA, 2018 See also the editorial by Flanders in this issue.","Deep Learning for MR Angiography: Automated Detection of Cerebral Aneurysms. Purpose To develop and evaluate a supportive algorithm using deep learning for detecting cerebral aneurysms at time-of-flight MR angiography to provide a second assessment of images already interpreted by radiologists. Materials and Methods MR images reported by radiologists to contain aneurysms were extracted from four institutions for the period from November 2006 through October 2017. The images were divided into three data sets: training data set, internal test data set, and external test data set. The algorithm was constructed by deep learning with the training data set, and its sensitivity to detect aneurysms in the test data sets was evaluated. To find aneurysms that had been overlooked in the initial reports, two radiologists independently performed a blinded interpretation of aneurysm candidates detected by the algorithm. When there was disagreement, the final diagnosis was made in consensus. The number of newly detected aneurysms was also evaluated. Results The training data set, which provided training and validation data, included 748 aneurysms (mean size, 3.1 mm +/- 2.0 [standard deviation]) from 683 examinations; 318 of these examinations were on male patients (mean age, 63 years +/- 13) and 365 were on female patients (mean age, 64 years +/- 13). Test data were provided by the internal test data set (649 aneurysms [mean size, 4.1 mm +/- 3.2] in 521 examinations, including 177 male patients and 344 female patients with mean age of 66 years +/- 12 and 67 years +/- 13, respectively) and the external test data set (80 aneurysms [mean size, 4.1 mm +/- 2.1] in 67 examinations, including 19 male patients and 48 female patients with mean age of 63 years +/- 12 and 68 years +/- 12, respectively). The sensitivity was 91% (592 of 649) and 93% (74 of 80) for the internal and external test data sets, respectively. The algorithm improved aneurysm detection in the internal and external test data sets by 4.8% (31 of 649) and 13% (10 of 80), respectively, compared with the initial reports. Conclusion A deep learning algorithm detected cerebral aneurysms in radiologic reports with high sensitivity and improved aneurysm detection compared with the initial reports. (c) RSNA, 2018 See also the editorial by Flanders in this issue."
1,Assessment of Deep Generative Models for High-Resolution Synthetic Retinal Image Generation of Age-Related Macular Degeneration,"Importance: Deep learning (DL) used for discriminative tasks in ophthalmology, such as diagnosing diabetic retinopathy or age-related macular degeneration (AMD), requires large image data sets graded by human experts to train deep convolutional neural networks (DCNNs). In contrast, generative DL techniques could synthesize large new data sets of artificial retina images with different stages of AMD. Such images could enhance existing data sets of common and rare ophthalmic diseases without concern for personally identifying information to assist medical education of students, residents, and retinal specialists, as well as for training new DL diagnostic models for which extensive data sets from large clinical trials of expertly graded images may not exist. Objective: To develop DL techniques for synthesizing high-resolution realistic fundus images serving as proxy data sets for use by retinal specialists and DL machines. Design, Setting, and Participants: Generative adversarial networks were trained on 133 821 color fundus images from 4613 study participants from the Age-Related Eye Disease Study (AREDS), generating synthetic fundus images with and without AMD. We compared retinal specialists' ability to diagnose AMD on both real and synthetic images, asking them to assess image gradability and testing their ability to discern real from synthetic images. The performance of AMD diagnostic DCNNs (referable vs not referable AMD) trained on either all-real vs all-synthetic data sets was compared. Main Outcomes and Measures: Accuracy of 2 retinal specialists (T.Y.A.L. and K.D.P.) for diagnosing and distinguishing AMD on real vs synthetic images and diagnostic performance (area under the curve) of DL algorithms trained on synthetic vs real images. Results: The diagnostic accuracy of 2 retinal specialists on real vs synthetic images was similar. The accuracy of diagnosis as referable vs nonreferable AMD compared with certified human graders for retinal specialist 1 was 84.54% (error margin, 4.06%) on real images vs 84.12% (error margin, 4.16%) on synthetic images and for retinal specialist 2 was 89.47% (error margin, 3.45%) on real images vs 89.19% (error margin, 3.54%) on synthetic images. Retinal specialists could not distinguish real from synthetic images, with an accuracy of 59.50% (error margin, 3.93%) for retinal specialist 1 and 53.67% (error margin, 3.99%) for retinal specialist 2. The DCNNs trained on real data showed an area under the curve of 0.9706 (error margin, 0.0029), and those trained on synthetic data showed an area under the curve of 0.9235 (error margin, 0.0045). Conclusions and Relevance: Deep learning-synthesized images appeared to be realistic to retinal specialists, and DCNNs achieved diagnostic performance on synthetic data close to that for real images, suggesting that DL generative techniques hold promise for training humans and machines.","Assessment of Deep Generative Models for High-Resolution Synthetic Retinal Image Generation of Age-Related Macular Degeneration. Importance: Deep learning (DL) used for discriminative tasks in ophthalmology, such as diagnosing diabetic retinopathy or age-related macular degeneration (AMD), requires large image data sets graded by human experts to train deep convolutional neural networks (DCNNs). In contrast, generative DL techniques could synthesize large new data sets of artificial retina images with different stages of AMD. Such images could enhance existing data sets of common and rare ophthalmic diseases without concern for personally identifying information to assist medical education of students, residents, and retinal specialists, as well as for training new DL diagnostic models for which extensive data sets from large clinical trials of expertly graded images may not exist. Objective: To develop DL techniques for synthesizing high-resolution realistic fundus images serving as proxy data sets for use by retinal specialists and DL machines. Design, Setting, and Participants: Generative adversarial networks were trained on 133 821 color fundus images from 4613 study participants from the Age-Related Eye Disease Study (AREDS), generating synthetic fundus images with and without AMD. We compared retinal specialists' ability to diagnose AMD on both real and synthetic images, asking them to assess image gradability and testing their ability to discern real from synthetic images. The performance of AMD diagnostic DCNNs (referable vs not referable AMD) trained on either all-real vs all-synthetic data sets was compared. Main Outcomes and Measures: Accuracy of 2 retinal specialists (T.Y.A.L. and K.D.P.) for diagnosing and distinguishing AMD on real vs synthetic images and diagnostic performance (area under the curve) of DL algorithms trained on synthetic vs real images. Results: The diagnostic accuracy of 2 retinal specialists on real vs synthetic images was similar. The accuracy of diagnosis as referable vs nonreferable AMD compared with certified human graders for retinal specialist 1 was 84.54% (error margin, 4.06%) on real images vs 84.12% (error margin, 4.16%) on synthetic images and for retinal specialist 2 was 89.47% (error margin, 3.45%) on real images vs 89.19% (error margin, 3.54%) on synthetic images. Retinal specialists could not distinguish real from synthetic images, with an accuracy of 59.50% (error margin, 3.93%) for retinal specialist 1 and 53.67% (error margin, 3.99%) for retinal specialist 2. The DCNNs trained on real data showed an area under the curve of 0.9706 (error margin, 0.0029), and those trained on synthetic data showed an area under the curve of 0.9235 (error margin, 0.0045). Conclusions and Relevance: Deep learning-synthesized images appeared to be realistic to retinal specialists, and DCNNs achieved diagnostic performance on synthetic data close to that for real images, suggesting that DL generative techniques hold promise for training humans and machines."
1,Deep Learning-Assisted Diagnosis of Cerebral Aneurysms Using the HeadXNet Model,"Importance: Deep learning has the potential to augment clinician performance in medical imaging interpretation and reduce time to diagnosis through automated segmentation. Few studies to date have explored this topic. Objective: To develop and apply a neural network segmentation model (the HeadXNet model) capable of generating precise voxel-by-voxel predictions of intracranial aneurysms on head computed tomographic angiography (CTA) imaging to augment clinicians' intracranial aneurysm diagnostic performance. Design, Setting, and Participants: In this diagnostic study, a 3-dimensional convolutional neural network architecture was developed using a training set of 611 head CTA examinations to generate aneurysm segmentations. Segmentation outputs from this support model on a test set of 115 examinations were provided to clinicians. Between August 13, 2018, and October 4, 2018, 8 clinicians diagnosed the presence of aneurysm on the test set, both with and without model augmentation, in a crossover design using randomized order and a 14-day washout period. Head and neck examinations performed between January 3, 2003, and May 31, 2017, at a single academic medical center were used to train, validate, and test the model. Examinations positive for aneurysm had at least 1 clinically significant, nonruptured intracranial aneurysm. Examinations with hemorrhage, ruptured aneurysm, posttraumatic or infectious pseudoaneurysm, arteriovenous malformation, surgical clips, coils, catheters, or other surgical hardware were excluded. All other CTA examinations were considered controls. Main Outcomes and Measures: Sensitivity, specificity, accuracy, time, and interrater agreement were measured. Metrics for clinician performance with and without model augmentation were compared. Results: The data set contained 818 examinations from 662 unique patients with 328 CTA examinations (40.1%) containing at least 1 intracranial aneurysm and 490 examinations (59.9%) without intracranial aneurysms. The 8 clinicians reading the test set ranged in experience from 2 to 12 years. Augmenting clinicians with artificial intelligence-produced segmentation predictions resulted in clinicians achieving statistically significant improvements in sensitivity, accuracy, and interrater agreement when compared with no augmentation. The clinicians' mean sensitivity increased by 0.059 (95% CI, 0.028-0.091; adjusted Pâ€‰=â€‰.01), mean accuracy increased by 0.038 (95% CI, 0.014-0.062; adjusted Pâ€‰=â€‰.02), and mean interrater agreement (Fleiss Îº) increased by 0.060, from 0.799 to 0.859 (adjusted Pâ€‰=â€‰.05). There was no statistically significant change in mean specificity (0.016; 95% CI, -0.010 to 0.041; adjusted Pâ€‰=â€‰.16) and time to diagnosis (5.71 seconds; 95% CI, 7.22-18.63 seconds; adjusted Pâ€‰=â€‰.19). Conclusions and Relevance: The deep learning model developed successfully detected clinically significant intracranial aneurysms on CTA. This suggests that integration of an artificial intelligence-assisted diagnostic model may augment clinician performance with dependable and accurate predictions and thereby optimize patient care.","Deep Learning-Assisted Diagnosis of Cerebral Aneurysms Using the HeadXNet Model. Importance: Deep learning has the potential to augment clinician performance in medical imaging interpretation and reduce time to diagnosis through automated segmentation. Few studies to date have explored this topic. Objective: To develop and apply a neural network segmentation model (the HeadXNet model) capable of generating precise voxel-by-voxel predictions of intracranial aneurysms on head computed tomographic angiography (CTA) imaging to augment clinicians' intracranial aneurysm diagnostic performance. Design, Setting, and Participants: In this diagnostic study, a 3-dimensional convolutional neural network architecture was developed using a training set of 611 head CTA examinations to generate aneurysm segmentations. Segmentation outputs from this support model on a test set of 115 examinations were provided to clinicians. Between August 13, 2018, and October 4, 2018, 8 clinicians diagnosed the presence of aneurysm on the test set, both with and without model augmentation, in a crossover design using randomized order and a 14-day washout period. Head and neck examinations performed between January 3, 2003, and May 31, 2017, at a single academic medical center were used to train, validate, and test the model. Examinations positive for aneurysm had at least 1 clinically significant, nonruptured intracranial aneurysm. Examinations with hemorrhage, ruptured aneurysm, posttraumatic or infectious pseudoaneurysm, arteriovenous malformation, surgical clips, coils, catheters, or other surgical hardware were excluded. All other CTA examinations were considered controls. Main Outcomes and Measures: Sensitivity, specificity, accuracy, time, and interrater agreement were measured. Metrics for clinician performance with and without model augmentation were compared. Results: The data set contained 818 examinations from 662 unique patients with 328 CTA examinations (40.1%) containing at least 1 intracranial aneurysm and 490 examinations (59.9%) without intracranial aneurysms. The 8 clinicians reading the test set ranged in experience from 2 to 12 years. Augmenting clinicians with artificial intelligence-produced segmentation predictions resulted in clinicians achieving statistically significant improvements in sensitivity, accuracy, and interrater agreement when compared with no augmentation. The clinicians' mean sensitivity increased by 0.059 (95% CI, 0.028-0.091; adjusted Pâ€‰=â€‰.01), mean accuracy increased by 0.038 (95% CI, 0.014-0.062; adjusted Pâ€‰=â€‰.02), and mean interrater agreement (Fleiss Îº) increased by 0.060, from 0.799 to 0.859 (adjusted Pâ€‰=â€‰.05). There was no statistically significant change in mean specificity (0.016; 95% CI, -0.010 to 0.041; adjusted Pâ€‰=â€‰.16) and time to diagnosis (5.71 seconds; 95% CI, 7.22-18.63 seconds; adjusted Pâ€‰=â€‰.19). Conclusions and Relevance: The deep learning model developed successfully detected clinically significant intracranial aneurysms on CTA. This suggests that integration of an artificial intelligence-assisted diagnostic model may augment clinician performance with dependable and accurate predictions and thereby optimize patient care."
1,Predicting survival from colorectal cancer histology slides using deep learning: A retrospective multicenter study,"BACKGROUND: For virtually every patient with colorectal cancer (CRC), hematoxylin-eosin (HE)-stained tissue slides are available. These images contain quantitative information, which is not routinely used to objectively extract prognostic biomarkers. In the present study, we investigated whether deep convolutional neural networks (CNNs) can extract prognosticators directly from these widely available images. METHODS AND FINDINGS: We hand-delineated single-tissue regions in 86 CRC tissue slides, yielding more than 100,000 HE image patches, and used these to train a CNN by transfer learning, reaching a nine-class accuracy of >94% in an independent data set of 7,180 images from 25 CRC patients. With this tool, we performed automated tissue decomposition of representative multitissue HE images from 862 HE slides in 500 stage I-IV CRC patients in the The Cancer Genome Atlas (TCGA) cohort, a large international multicenter collection of CRC tissue. Based on the output neuron activations in the CNN, we calculated a ""deep stroma score,"" which was an independent prognostic factor for overall survival (OS) in a multivariable Cox proportional hazard model (hazard ratio [HR] with 95% confidence interval [CI]: 1.99 [1.27-3.12], p = 0.0028), while in the same cohort, manual quantification of stromal areas and a gene expression signature of cancer-associated fibroblasts (CAFs) were only prognostic in specific tumor stages. We validated these findings in an independent cohort of 409 stage I-IV CRC patients from the ""Darmkrebs: Chancen der Verhutung durch Screening"" (DACHS) study who were recruited between 2003 and 2007 in multiple institutions in Germany. Again, the score was an independent prognostic factor for OS (HR 1.63 [1.14-2.33], p = 0.008), CRC-specific OS (HR 2.29 [1.5-3.48], p = 0.0004), and relapse-free survival (RFS; HR 1.92 [1.34-2.76], p = 0.0004). A prospective validation is required before this biomarker can be implemented in clinical workflows. CONCLUSIONS: In our retrospective study, we show that a CNN can assess the human tumor microenvironment and predict prognosis directly from histopathological images.","Predicting survival from colorectal cancer histology slides using deep learning: A retrospective multicenter study. BACKGROUND: For virtually every patient with colorectal cancer (CRC), hematoxylin-eosin (HE)-stained tissue slides are available. These images contain quantitative information, which is not routinely used to objectively extract prognostic biomarkers. In the present study, we investigated whether deep convolutional neural networks (CNNs) can extract prognosticators directly from these widely available images. METHODS AND FINDINGS: We hand-delineated single-tissue regions in 86 CRC tissue slides, yielding more than 100,000 HE image patches, and used these to train a CNN by transfer learning, reaching a nine-class accuracy of >94% in an independent data set of 7,180 images from 25 CRC patients. With this tool, we performed automated tissue decomposition of representative multitissue HE images from 862 HE slides in 500 stage I-IV CRC patients in the The Cancer Genome Atlas (TCGA) cohort, a large international multicenter collection of CRC tissue. Based on the output neuron activations in the CNN, we calculated a ""deep stroma score,"" which was an independent prognostic factor for overall survival (OS) in a multivariable Cox proportional hazard model (hazard ratio [HR] with 95% confidence interval [CI]: 1.99 [1.27-3.12], p = 0.0028), while in the same cohort, manual quantification of stromal areas and a gene expression signature of cancer-associated fibroblasts (CAFs) were only prognostic in specific tumor stages. We validated these findings in an independent cohort of 409 stage I-IV CRC patients from the ""Darmkrebs: Chancen der Verhutung durch Screening"" (DACHS) study who were recruited between 2003 and 2007 in multiple institutions in Germany. Again, the score was an independent prognostic factor for OS (HR 1.63 [1.14-2.33], p = 0.008), CRC-specific OS (HR 2.29 [1.5-3.48], p = 0.0004), and relapse-free survival (RFS; HR 1.92 [1.34-2.76], p = 0.0004). A prospective validation is required before this biomarker can be implemented in clinical workflows. CONCLUSIONS: In our retrospective study, we show that a CNN can assess the human tumor microenvironment and predict prognosis directly from histopathological images."
1,Patterns of joint involvement in juvenile idiopathic arthritis and prediction of disease course: A prospective study with multilayer non-negative matrix factorization,,
1,Mammographic Breast Density Assessment Using Deep Learning: Clinical Implementation,"Purpose To develop a deep learning (DL) algorithm to assess mammographic breast density. Materials and Methods In this retrospective study, a deep convolutional neural network was trained to assess Breast Imaging Reporting and Data System (BI-RADS) breast density based on the original interpretation by an experienced radiologist of 41 479 digital screening mammograms obtained in 27 684 women from January 2009 to May 2011. The resulting algorithm was tested on a held-out test set of 8677 mammograms in 5741 women. In addition, five radiologists performed a reader study on 500 mammograms randomly selected from the test set. Finally, the algorithm was implemented in routine clinical practice, where eight radiologists reviewed 10 763 consecutive mammograms assessed with the model. Agreement on BI-RADS category for the DL model and for three sets of readings-(a) radiologists in the test set, (b) radiologists working in consensus in the reader study set, and (c) radiologists in the clinical implementation set-were estimated with linear-weighted kappa statistics and were compared across 5000 bootstrap samples to assess significance. Results The DL model showed good agreement with radiologists in the test set (kappa = 0.67; 95% confidence interval [CI]: 0.66, 0.68) and with radiologists in consensus in the reader study set (kappa = 0.78; 95% CI: 0.73, 0.82). There was very good agreement (kappa = 0.85; 95% CI: 0.84, 0.86) with radiologists in the clinical implementation set; for binary categorization of dense or nondense breasts, 10 149 of 10 763 (94%; 95% CI: 94%, 95%) DL assessments were accepted by the interpreting radiologist. Conclusion This DL model can be used to assess mammographic breast density at the level of an experienced mammographer. (c) RSNA, 2018 Online supplemental material is available for this article . See also the editorial by Chan and Helvie in this issue.","Mammographic Breast Density Assessment Using Deep Learning: Clinical Implementation. Purpose To develop a deep learning (DL) algorithm to assess mammographic breast density. Materials and Methods In this retrospective study, a deep convolutional neural network was trained to assess Breast Imaging Reporting and Data System (BI-RADS) breast density based on the original interpretation by an experienced radiologist of 41 479 digital screening mammograms obtained in 27 684 women from January 2009 to May 2011. The resulting algorithm was tested on a held-out test set of 8677 mammograms in 5741 women. In addition, five radiologists performed a reader study on 500 mammograms randomly selected from the test set. Finally, the algorithm was implemented in routine clinical practice, where eight radiologists reviewed 10 763 consecutive mammograms assessed with the model. Agreement on BI-RADS category for the DL model and for three sets of readings-(a) radiologists in the test set, (b) radiologists working in consensus in the reader study set, and (c) radiologists in the clinical implementation set-were estimated with linear-weighted kappa statistics and were compared across 5000 bootstrap samples to assess significance. Results The DL model showed good agreement with radiologists in the test set (kappa = 0.67; 95% confidence interval [CI]: 0.66, 0.68) and with radiologists in consensus in the reader study set (kappa = 0.78; 95% CI: 0.73, 0.82). There was very good agreement (kappa = 0.85; 95% CI: 0.84, 0.86) with radiologists in the clinical implementation set; for binary categorization of dense or nondense breasts, 10 149 of 10 763 (94%; 95% CI: 94%, 95%) DL assessments were accepted by the interpreting radiologist. Conclusion This DL model can be used to assess mammographic breast density at the level of an experienced mammographer. (c) RSNA, 2018 Online supplemental material is available for this article . See also the editorial by Chan and Helvie in this issue."
1,Motion artifact recognition and quantification in coronary CT angiography using convolutional neural networks,"Excellent image quality is a primary prerequisite for diagnostic non-invasive coronary CT angiography. Artifacts due to cardiac motion may interfere with detection and diagnosis of coronary artery disease and render subsequent treatment decisions more difficult. We propose deep-learning-based measures for coronary motion artifact recognition and quantification in order to assess the diagnostic reliability and image quality of coronary CT angiography images. More specifically, the application, steering and evaluation of motion compensation algorithms can be triggered by these measures. A Coronary Motion Forward Artifact model for CT data (CoMoFACT) is developed and applied to clinical cases with excellent image quality to introduce motion artifacts using simulated motion vector fields. The data required for supervised learning is generated by the CoMoFACT from 17 prospectively ECG-triggered clinical cases with controlled motion levels on a scale of 0-10. Convolutional neural networks achieve an accuracy of 93.3%+/-1.8% for the classification task of separating motion-free from motion-perturbed coronary cross-sectional image patches. The target motion level is predicted by a corresponding regression network with a mean absolute error of 1.12+/-0.07. Transferability and generalization capabilities are demonstrated by motion artifact measurements on eight additional CCTA cases with real motion artifacts.","Motion artifact recognition and quantification in coronary CT angiography using convolutional neural networks. Excellent image quality is a primary prerequisite for diagnostic non-invasive coronary CT angiography. Artifacts due to cardiac motion may interfere with detection and diagnosis of coronary artery disease and render subsequent treatment decisions more difficult. We propose deep-learning-based measures for coronary motion artifact recognition and quantification in order to assess the diagnostic reliability and image quality of coronary CT angiography images. More specifically, the application, steering and evaluation of motion compensation algorithms can be triggered by these measures. A Coronary Motion Forward Artifact model for CT data (CoMoFACT) is developed and applied to clinical cases with excellent image quality to introduce motion artifacts using simulated motion vector fields. The data required for supervised learning is generated by the CoMoFACT from 17 prospectively ECG-triggered clinical cases with controlled motion levels on a scale of 0-10. Convolutional neural networks achieve an accuracy of 93.3%+/-1.8% for the classification task of separating motion-free from motion-perturbed coronary cross-sectional image patches. The target motion level is predicted by a corresponding regression network with a mean absolute error of 1.12+/-0.07. Transferability and generalization capabilities are demonstrated by motion artifact measurements on eight additional CCTA cases with real motion artifacts."
1,MySurgeryRisk: Development and Validation of a Machine-learning Risk Algorithm for Major Complications and Death After Surgery,"OBJECTIVE: To accurately calculate the risk for postoperative complications and death after surgery in the preoperative period using machine-learning modeling of clinical data. BACKGROUND: Postoperative complications cause a 2-fold increase in the 30-day mortality and cost, and are associated with long-term consequences. The ability to precisely forecast the risk for major complications before surgery is limited. METHODS: In a single-center cohort of 51,457 surgical patients undergoing major inpatient surgery, we have developed and validated an automated analytics framework for a preoperative risk algorithm (MySurgeryRisk) that uses existing clinical data in electronic health records to forecast patient-level probabilistic risk scores for 8 major postoperative complications (acute kidney injury, sepsis, venous thromboembolism, intensive care unit admission >48 hours, mechanical ventilation >48 hours, wound, neurologic, and cardiovascular complications) and death up to 24 months after surgery. We used the area under the receiver characteristic curve (AUC) and predictiveness curves to evaluate model performance. RESULTS: MySurgeryRisk calculates probabilistic risk scores for 8 postoperative complications with AUC values ranging between 0.82 and 0.94 [99% confidence intervals (CIs) 0.81-0.94]. The model predicts the risk for death at 1, 3, 6, 12, and 24 months with AUC values ranging between 0.77 and 0.83 (99% CI 0.76-0.85). CONCLUSIONS: We constructed an automated predictive analytics framework for machine-learning algorithm with high discriminatory ability for assessing the risk of surgical complications and death using readily available preoperative electronic health records data. The feasibility of this novel algorithm implemented in real time clinical workflow requires further testing.","MySurgeryRisk: Development and Validation of a Machine-learning Risk Algorithm for Major Complications and Death After Surgery. OBJECTIVE: To accurately calculate the risk for postoperative complications and death after surgery in the preoperative period using machine-learning modeling of clinical data. BACKGROUND: Postoperative complications cause a 2-fold increase in the 30-day mortality and cost, and are associated with long-term consequences. The ability to precisely forecast the risk for major complications before surgery is limited. METHODS: In a single-center cohort of 51,457 surgical patients undergoing major inpatient surgery, we have developed and validated an automated analytics framework for a preoperative risk algorithm (MySurgeryRisk) that uses existing clinical data in electronic health records to forecast patient-level probabilistic risk scores for 8 major postoperative complications (acute kidney injury, sepsis, venous thromboembolism, intensive care unit admission >48 hours, mechanical ventilation >48 hours, wound, neurologic, and cardiovascular complications) and death up to 24 months after surgery. We used the area under the receiver characteristic curve (AUC) and predictiveness curves to evaluate model performance. RESULTS: MySurgeryRisk calculates probabilistic risk scores for 8 postoperative complications with AUC values ranging between 0.82 and 0.94 [99% confidence intervals (CIs) 0.81-0.94]. The model predicts the risk for death at 1, 3, 6, 12, and 24 months with AUC values ranging between 0.77 and 0.83 (99% CI 0.76-0.85). CONCLUSIONS: We constructed an automated predictive analytics framework for machine-learning algorithm with high discriminatory ability for assessing the risk of surgical complications and death using readily available preoperative electronic health records data. The feasibility of this novel algorithm implemented in real time clinical workflow requires further testing."
1,Evaluation of Machine-Learning Algorithms for Predicting Opioid Overdose Risk Among Medicare Beneficiaries With Opioid Prescriptions,"Importance: Current approaches to identifying individuals at high risk for opioid overdose target many patients who are not truly at high risk. Objective: To develop and validate a machine-learning algorithm to predict opioid overdose risk among Medicare beneficiaries with at least 1 opioid prescription. Design, Setting, and Participants: A prognostic study was conducted between September 1, 2017, and December 31, 2018. Participants (nâ€‰=â€‰560â€¯057) included fee-for-service Medicare beneficiaries without cancer who filled 1 or more opioid prescriptions from January 1, 2011, to December 31, 2015. Beneficiaries were randomly and equally divided into training, testing, and validation samples. Exposures: Potential predictors (nâ€‰=â€‰268), including sociodemographics, health status, patterns of opioid use, and practitioner-level and regional-level factors, were measured in 3-month windows, starting 3 months before initiating opioids until loss of follow-up or the end of observation. Main Outcomes and Measures: Opioid overdose episodes from inpatient and emergency department claims were identified. Multivariate logistic regression (MLR), least absolute shrinkage and selection operator-type regression (LASSO), random forest (RF), gradient boosting machine (GBM), and deep neural network (DNN) were applied to predict overdose risk in the subsequent 3 months after initiation of treatment with prescription opioids. Prediction performance was assessed using the C statistic and other metrics (eg, sensitivity, specificity, and number needed to evaluate [NNE] to identify one overdose). The Youden index was used to identify the optimized threshold of predicted score that balanced sensitivity and specificity. Results: Beneficiaries in the training (nâ€‰=â€‰186â€¯686), testing (nâ€‰=â€‰186â€¯685), and validation (nâ€‰=â€‰186â€¯686) samples had similar characteristics (mean [SD] ageâ€‰ofâ€‰68.0â€‰[14.5] years, and approximately 63% were female, 82% were white, 35% had disabilities, 41% were dual eligible, and 0.60% had at least 1 overdose episode). In the validation sample, the DNN (C statisticâ€‰=â€‰0.91; 95% CI, 0.88-0.93) and GBM (C statisticâ€‰=â€‰0.90; 95% CI, 0.87-0.94) algorithms outperformed the LASSO (C statisticâ€‰=â€‰0.84; 95% CI, 0.80-0.89), RF (C statisticâ€‰=â€‰0.80; 95% CI, 0.75-0.84), and MLR (C statisticâ€‰=â€‰0.75; 95% CI, 0.69-0.80) methods for predicting opioid overdose. At the optimized sensitivity and specificity, DNN had a sensitivity of 92.3%, specificity of 75.7%, NNE of 542, positive predictive value of 0.18%, and negative predictive value of 99.9%. The DNN classified patients into low-risk (76.2% [142 180] of the cohort), medium-risk (18.6% [34 579] of the cohort), and high-risk (5.2% [9747] of the cohort) subgroups, with only 1 in 10â€¯000 in the low-risk subgroup having an overdose episode. More than 90% of overdose episodes occurred in the high-risk and medium-risk subgroups, although positive predictive values were low, given the rare overdose outcome. Conclusions and Relevance: Machine-learning algorithms appear to perform well for risk prediction and stratification of opioid overdose, especially in identifying low-risk subgroups that have minimal risk of overdose.","Evaluation of Machine-Learning Algorithms for Predicting Opioid Overdose Risk Among Medicare Beneficiaries With Opioid Prescriptions. Importance: Current approaches to identifying individuals at high risk for opioid overdose target many patients who are not truly at high risk. Objective: To develop and validate a machine-learning algorithm to predict opioid overdose risk among Medicare beneficiaries with at least 1 opioid prescription. Design, Setting, and Participants: A prognostic study was conducted between September 1, 2017, and December 31, 2018. Participants (nâ€‰=â€‰560â€¯057) included fee-for-service Medicare beneficiaries without cancer who filled 1 or more opioid prescriptions from January 1, 2011, to December 31, 2015. Beneficiaries were randomly and equally divided into training, testing, and validation samples. Exposures: Potential predictors (nâ€‰=â€‰268), including sociodemographics, health status, patterns of opioid use, and practitioner-level and regional-level factors, were measured in 3-month windows, starting 3 months before initiating opioids until loss of follow-up or the end of observation. Main Outcomes and Measures: Opioid overdose episodes from inpatient and emergency department claims were identified. Multivariate logistic regression (MLR), least absolute shrinkage and selection operator-type regression (LASSO), random forest (RF), gradient boosting machine (GBM), and deep neural network (DNN) were applied to predict overdose risk in the subsequent 3 months after initiation of treatment with prescription opioids. Prediction performance was assessed using the C statistic and other metrics (eg, sensitivity, specificity, and number needed to evaluate [NNE] to identify one overdose). The Youden index was used to identify the optimized threshold of predicted score that balanced sensitivity and specificity. Results: Beneficiaries in the training (nâ€‰=â€‰186â€¯686), testing (nâ€‰=â€‰186â€¯685), and validation (nâ€‰=â€‰186â€¯686) samples had similar characteristics (mean [SD] ageâ€‰ofâ€‰68.0â€‰[14.5] years, and approximately 63% were female, 82% were white, 35% had disabilities, 41% were dual eligible, and 0.60% had at least 1 overdose episode). In the validation sample, the DNN (C statisticâ€‰=â€‰0.91; 95% CI, 0.88-0.93) and GBM (C statisticâ€‰=â€‰0.90; 95% CI, 0.87-0.94) algorithms outperformed the LASSO (C statisticâ€‰=â€‰0.84; 95% CI, 0.80-0.89), RF (C statisticâ€‰=â€‰0.80; 95% CI, 0.75-0.84), and MLR (C statisticâ€‰=â€‰0.75; 95% CI, 0.69-0.80) methods for predicting opioid overdose. At the optimized sensitivity and specificity, DNN had a sensitivity of 92.3%, specificity of 75.7%, NNE of 542, positive predictive value of 0.18%, and negative predictive value of 99.9%. The DNN classified patients into low-risk (76.2% [142 180] of the cohort), medium-risk (18.6% [34 579] of the cohort), and high-risk (5.2% [9747] of the cohort) subgroups, with only 1 in 10â€¯000 in the low-risk subgroup having an overdose episode. More than 90% of overdose episodes occurred in the high-risk and medium-risk subgroups, although positive predictive values were low, given the rare overdose outcome. Conclusions and Relevance: Machine-learning algorithms appear to perform well for risk prediction and stratification of opioid overdose, especially in identifying low-risk subgroups that have minimal risk of overdose."
1,Automated CT and MRI liver segmentation and biometry using a generalized convolutional neural network,"Purpose: To assess feasibility of training a convolutional neural network (CNN) to automate liver segmentation across different imaging modalities and techniques used in clinical practice and to apply this technique to enable automation of liver biometry. Materials and Methods: A two-dimensional U-Net CNN was trained for liver segmentation in two stages by using 330 abdominal MRI and CT examinations. First, the neural network was trained with unenhanced multiecho spoiled gradient-echo images from 300 MRI examinations to yield multiple signal weightings. Then, transfer learning was used to generalize the CNN with additional images from 30 contrast materialâ€“enhanced MRI and CT examinations. Performance of the CNN was assessed by using a distinct multiinstitutional dataset curated from multiple sources (498 subjects). Segmentation accuracy was evaluated by computing Dice scores. These segmentations were used to compute liver volume from CT and T1-weighted MRI examinations and to estimate hepatic proton density fat fraction (PDFF) from multiecho T2*-weighted MRI examinations. Quantitative volumetry and PDFF estimates were compared between automated and manual segmentation by using Pearson correlation and Bland-Altman statistics. Results: Dice scores were 0.94 Â± 0.06 for CT (n = 230), 0.95 Â± 0.03 (n = 100) for T1-weighted MRI, and 0.92 Â± 0.05 for T2*weighted MRI (n = 168). Liver volume measured with manual and automated segmentation agreed closely for CT (95% limits of agreement: âˆ’298 mL, 180 mL) and T1-weighted MRI (95% limits of agreement: âˆ’358 mL, 180 mL). Hepatic PDFF measured by the two segmentations also agreed closely (95% limits of agreement: âˆ’0.62%, 0.80%). Conclusion: By using a transfer-learning strategy, this study has demonstrated the feasibility of a CNN to be generalized to perform liver segmentation across different imaging techniques and modalities. With further refinement and validation, CNNs may have broad applicability for multimodal liver volumetry and hepatic tissue characterization.","Automated CT and MRI liver segmentation and biometry using a generalized convolutional neural network. Purpose: To assess feasibility of training a convolutional neural network (CNN) to automate liver segmentation across different imaging modalities and techniques used in clinical practice and to apply this technique to enable automation of liver biometry. Materials and Methods: A two-dimensional U-Net CNN was trained for liver segmentation in two stages by using 330 abdominal MRI and CT examinations. First, the neural network was trained with unenhanced multiecho spoiled gradient-echo images from 300 MRI examinations to yield multiple signal weightings. Then, transfer learning was used to generalize the CNN with additional images from 30 contrast materialâ€“enhanced MRI and CT examinations. Performance of the CNN was assessed by using a distinct multiinstitutional dataset curated from multiple sources (498 subjects). Segmentation accuracy was evaluated by computing Dice scores. These segmentations were used to compute liver volume from CT and T1-weighted MRI examinations and to estimate hepatic proton density fat fraction (PDFF) from multiecho T2*-weighted MRI examinations. Quantitative volumetry and PDFF estimates were compared between automated and manual segmentation by using Pearson correlation and Bland-Altman statistics. Results: Dice scores were 0.94 Â± 0.06 for CT (n = 230), 0.95 Â± 0.03 (n = 100) for T1-weighted MRI, and 0.92 Â± 0.05 for T2*weighted MRI (n = 168). Liver volume measured with manual and automated segmentation agreed closely for CT (95% limits of agreement: âˆ’298 mL, 180 mL) and T1-weighted MRI (95% limits of agreement: âˆ’358 mL, 180 mL). Hepatic PDFF measured by the two segmentations also agreed closely (95% limits of agreement: âˆ’0.62%, 0.80%). Conclusion: By using a transfer-learning strategy, this study has demonstrated the feasibility of a CNN to be generalized to perform liver segmentation across different imaging techniques and modalities. With further refinement and validation, CNNs may have broad applicability for multimodal liver volumetry and hepatic tissue characterization."
1,Deep vessel segmentation by learning graphical connectivity,,
1,Training recurrent neural networks robust to incomplete data: Application to Alzheimer's disease progression modeling,"Disease progression modeling (DPM) using longitudinal data is a challenging machine learning task. Existing DPM algorithms neglect temporal dependencies among measurements, make parametric assumptions about biomarker trajectories, do not model multiple biomarkers jointly, and need an alignment of subjects' trajectories. In this paper, recurrent neural networks (RNNs) are utilized to address these issues. However, in many cases, longitudinal cohorts contain incomplete data, which hinders the application of standard RNNs and requires a pre-processing step such as imputation of the missing values. Instead, we propose a generalized training rule for the most widely used RNN architecture, long short-term memory (LSTM) networks, that can handle both missing predictor and target values. The proposed LSTM algorithm is applied to model the progression of Alzheimer's disease (AD) using six volumetric magnetic resonance imaging (MRI) biomarkers, i.e., volumes of ventricles, hippocampus, whole brain, fusiform, middle temporal gyrus, and entorhinal cortex, and it is compared to standard LSTM networks with data imputation and a parametric, regression-based DPM method. The results show that the proposed algorithm achieves a significantly lower mean absolute error (MAE) than the alternatives with p<0.05 using Wilcoxon signed rank test in predicting values of almost all of the MRI biomarkers. Moreover, a linear discriminant analysis (LDA) classifier applied to the predicted biomarker values produces a significantly larger area under the receiver operating characteristic curve (AUC) of 0.90vs. at most 0.84 with p<0.001 using McNemar's test for clinical diagnosis of AD. Inspection of MAE curves as a function of the amount of missing data reveals that the proposed LSTM algorithm achieves the best performance up until more than 74% missing values. Finally, it is illustrated how the method can successfully be applied to data with varying time intervals. This paper shows that built-in handling of missing values in training an LSTM network benefits the application of RNNs in neurodegenerative disease progression modeling in longitudinal cohorts.","Training recurrent neural networks robust to incomplete data: Application to Alzheimer's disease progression modeling. Disease progression modeling (DPM) using longitudinal data is a challenging machine learning task. Existing DPM algorithms neglect temporal dependencies among measurements, make parametric assumptions about biomarker trajectories, do not model multiple biomarkers jointly, and need an alignment of subjects' trajectories. In this paper, recurrent neural networks (RNNs) are utilized to address these issues. However, in many cases, longitudinal cohorts contain incomplete data, which hinders the application of standard RNNs and requires a pre-processing step such as imputation of the missing values. Instead, we propose a generalized training rule for the most widely used RNN architecture, long short-term memory (LSTM) networks, that can handle both missing predictor and target values. The proposed LSTM algorithm is applied to model the progression of Alzheimer's disease (AD) using six volumetric magnetic resonance imaging (MRI) biomarkers, i.e., volumes of ventricles, hippocampus, whole brain, fusiform, middle temporal gyrus, and entorhinal cortex, and it is compared to standard LSTM networks with data imputation and a parametric, regression-based DPM method. The results show that the proposed algorithm achieves a significantly lower mean absolute error (MAE) than the alternatives with p<0.05 using Wilcoxon signed rank test in predicting values of almost all of the MRI biomarkers. Moreover, a linear discriminant analysis (LDA) classifier applied to the predicted biomarker values produces a significantly larger area under the receiver operating characteristic curve (AUC) of 0.90vs. at most 0.84 with p<0.001 using McNemar's test for clinical diagnosis of AD. Inspection of MAE curves as a function of the amount of missing data reveals that the proposed LSTM algorithm achieves the best performance up until more than 74% missing values. Finally, it is illustrated how the method can successfully be applied to data with varying time intervals. This paper shows that built-in handling of missing values in training an LSTM network benefits the application of RNNs in neurodegenerative disease progression modeling in longitudinal cohorts."
1,Artificial intelligence estimates the importance of baseline factors in predicting response to anti-PD1 in metastatic melanoma,"Objective: Prognosis of patients with metastatic melanoma has dramatically improved over recent years because of the advent of antibodies targeting programmed cell death protein-1 (PD1). However, the response rate is 40% and baseline biomarkers for the outcome are yet to be identified. Here, we aimed to determine whether artificial intelligence might be useful in weighting the importance of baseline variables in predicting response to anti-PD1. Methods: This is a retrospective study evaluating 173 patients receiving anti-PD1 for melanoma. Using an artificial neuronal network analysis, the importance of different variables was estimated and used in predicting response rate and overall survival. Results: After a mean follow-up of 12.8 (Â±11.9) months, disease control rate was 51%. Using artificial neuronal network, we observed that 3 factors predicted response to anti-PD1: neutrophil-to-lymphocyte ratio (NLR) (importance: 0.195), presence of â‰¥3 metastatic sites (importance: 0.156), and baseline lactate dehydrogenase (LDH) > upper limit of normal (importance: 0.154). Looking at connections between different covariates and overall survival, the most important variables influencing survival were: presence of â‰¥3 metastatic sites (importance: 0.202), age (importance: 0.189), NLR (importance: 0.164), site of primary melanoma (cutaneous vs. noncutaneous) (importance: 0.112), and LDH > upper limit of normal (importance: 0.108). Conclusions: NLR, presence of â‰¥3 metastatic sites, LDH levels, age, and site of primary melanoma are important baseline factors influencing response and survival. Further studies are warranted to estimate a model to drive the choice to administered anti-PD1 treatments in patients with melanoma.","Artificial intelligence estimates the importance of baseline factors in predicting response to anti-PD1 in metastatic melanoma. Objective: Prognosis of patients with metastatic melanoma has dramatically improved over recent years because of the advent of antibodies targeting programmed cell death protein-1 (PD1). However, the response rate is 40% and baseline biomarkers for the outcome are yet to be identified. Here, we aimed to determine whether artificial intelligence might be useful in weighting the importance of baseline variables in predicting response to anti-PD1. Methods: This is a retrospective study evaluating 173 patients receiving anti-PD1 for melanoma. Using an artificial neuronal network analysis, the importance of different variables was estimated and used in predicting response rate and overall survival. Results: After a mean follow-up of 12.8 (Â±11.9) months, disease control rate was 51%. Using artificial neuronal network, we observed that 3 factors predicted response to anti-PD1: neutrophil-to-lymphocyte ratio (NLR) (importance: 0.195), presence of â‰¥3 metastatic sites (importance: 0.156), and baseline lactate dehydrogenase (LDH) > upper limit of normal (importance: 0.154). Looking at connections between different covariates and overall survival, the most important variables influencing survival were: presence of â‰¥3 metastatic sites (importance: 0.202), age (importance: 0.189), NLR (importance: 0.164), site of primary melanoma (cutaneous vs. noncutaneous) (importance: 0.112), and LDH > upper limit of normal (importance: 0.108). Conclusions: NLR, presence of â‰¥3 metastatic sites, LDH levels, age, and site of primary melanoma are important baseline factors influencing response and survival. Further studies are warranted to estimate a model to drive the choice to administered anti-PD1 treatments in patients with melanoma."
1,A machine-learning approach to predict postprandial hypoglycemia,"BACKGROUND: For an effective artificial pancreas (AP) system and an improved therapeutic intervention with continuous glucose monitoring (CGM), predicting the occurrence of hypoglycemia accurately is very important. While there have been many studies reporting successful algorithms for predicting nocturnal hypoglycemia, predicting postprandial hypoglycemia still remains a challenge due to extreme glucose fluctuations that occur around mealtimes. The goal of this study is to evaluate the feasibility of easy-to-use, computationally efficient machine-learning algorithm to predict postprandial hypoglycemia with a unique feature set. METHODS: We use retrospective CGM datasets of 104 people who had experienced at least one hypoglycemia alert value during a three-day CGM session. The algorithms were developed based on four machine learning models with a unique data-driven feature set: a random forest (RF), a support vector machine using a linear function or a radial basis function, a K-nearest neighbor, and a logistic regression. With 5-fold cross-subject validation, the average performance of each model was calculated to compare and contrast their individual performance. The area under a receiver operating characteristic curve (AUC) and the F1 score were used as the main criterion for evaluating the performance. RESULTS: In predicting a hypoglycemia alert value with a 30-min prediction horizon, the RF model showed the best performance with the average AUC of 0.966, the average sensitivity of 89.6%, the average specificity of 91.3%, and the average F1 score of 0.543. In addition, the RF showed the better predictive performance for postprandial hypoglycemic events than other models. CONCLUSION: In conclusion, we showed that machine-learning algorithms have potential in predicting postprandial hypoglycemia, and the RF model could be a better candidate for the further development of postprandial hypoglycemia prediction algorithm to advance the CGM technology and the AP technology further.","A machine-learning approach to predict postprandial hypoglycemia. BACKGROUND: For an effective artificial pancreas (AP) system and an improved therapeutic intervention with continuous glucose monitoring (CGM), predicting the occurrence of hypoglycemia accurately is very important. While there have been many studies reporting successful algorithms for predicting nocturnal hypoglycemia, predicting postprandial hypoglycemia still remains a challenge due to extreme glucose fluctuations that occur around mealtimes. The goal of this study is to evaluate the feasibility of easy-to-use, computationally efficient machine-learning algorithm to predict postprandial hypoglycemia with a unique feature set. METHODS: We use retrospective CGM datasets of 104 people who had experienced at least one hypoglycemia alert value during a three-day CGM session. The algorithms were developed based on four machine learning models with a unique data-driven feature set: a random forest (RF), a support vector machine using a linear function or a radial basis function, a K-nearest neighbor, and a logistic regression. With 5-fold cross-subject validation, the average performance of each model was calculated to compare and contrast their individual performance. The area under a receiver operating characteristic curve (AUC) and the F1 score were used as the main criterion for evaluating the performance. RESULTS: In predicting a hypoglycemia alert value with a 30-min prediction horizon, the RF model showed the best performance with the average AUC of 0.966, the average sensitivity of 89.6%, the average specificity of 91.3%, and the average F1 score of 0.543. In addition, the RF showed the better predictive performance for postprandial hypoglycemic events than other models. CONCLUSION: In conclusion, we showed that machine-learning algorithms have potential in predicting postprandial hypoglycemia, and the RF model could be a better candidate for the further development of postprandial hypoglycemia prediction algorithm to advance the CGM technology and the AP technology further."
1,Fully automated diagnosis of anterior cruciate ligament tears on knee mr images by using deep learning,"Purpose: To investigate the feasibility of using a deep learningâ€“based approach to detect an anterior cruciate ligament (ACL) tear within the knee joint at MRI by using arthroscopy as the reference standard. Materials and Methods: A fully automated deep learningâ€“based diagnosis system was developed by using two deep convolutional neural networks (CNNs) to isolate the ACL on MR images followed by a classification CNN to detect structural abnormalities within the isolated ligament. With institutional review board approval, sagittal proton densityâ€“weighted and fat-suppressed T2-weighted fast spinecho MR images of the knee in 175 subjects with a full-thickness ACL tear (98 male subjects and 77 female subjects; average age, 27.5 years) and 175 subjects with an intact ACL (100 male subjects and 75 female subjects; average age, 39.4 years) were retrospectively analyzed by using the deep learning approach. Sensitivity and specificity of the ACL tear detection system and five clinical radiologists for detecting an ACL tear were determined by using arthroscopic results as the reference standard. Receiver operating characteristic (ROC) analysis and two-sided exact binomial tests were used to further assess diagnostic performance. Results: The sensitivity and specificity of the ACL tear detection system at the optimal threshold were 0.96 and 0.96, respectively. In comparison, the sensitivity of the clinical radiologists ranged between 0.96 and 0.98, while the specificity ranged between 0.90 and 0.98. There was no statistically significant difference in diagnostic performance between the ACL tear detection system and clinical radiologists at P <.05. The area under the ROC curve for the ACL tear detection system was 0.98, indicating high overall diagnostic accuracy. Conclusion: There was no significant difference between the diagnostic performance of the ACL tear detection system and clinical radiologists for determining the presence or absence of an ACL tear at MRI.","Fully automated diagnosis of anterior cruciate ligament tears on knee mr images by using deep learning. Purpose: To investigate the feasibility of using a deep learningâ€“based approach to detect an anterior cruciate ligament (ACL) tear within the knee joint at MRI by using arthroscopy as the reference standard. Materials and Methods: A fully automated deep learningâ€“based diagnosis system was developed by using two deep convolutional neural networks (CNNs) to isolate the ACL on MR images followed by a classification CNN to detect structural abnormalities within the isolated ligament. With institutional review board approval, sagittal proton densityâ€“weighted and fat-suppressed T2-weighted fast spinecho MR images of the knee in 175 subjects with a full-thickness ACL tear (98 male subjects and 77 female subjects; average age, 27.5 years) and 175 subjects with an intact ACL (100 male subjects and 75 female subjects; average age, 39.4 years) were retrospectively analyzed by using the deep learning approach. Sensitivity and specificity of the ACL tear detection system and five clinical radiologists for detecting an ACL tear were determined by using arthroscopic results as the reference standard. Receiver operating characteristic (ROC) analysis and two-sided exact binomial tests were used to further assess diagnostic performance. Results: The sensitivity and specificity of the ACL tear detection system at the optimal threshold were 0.96 and 0.96, respectively. In comparison, the sensitivity of the clinical radiologists ranged between 0.96 and 0.98, while the specificity ranged between 0.90 and 0.98. There was no statistically significant difference in diagnostic performance between the ACL tear detection system and clinical radiologists at P <.05. The area under the ROC curve for the ACL tear detection system was 0.98, indicating high overall diagnostic accuracy. Conclusion: There was no significant difference between the diagnostic performance of the ACL tear detection system and clinical radiologists for determining the presence or absence of an ACL tear at MRI."
1,A comparison between two semantic deep learning frameworks for the autosomal dominant polycystic kidney disease segmentation based on magnetic resonance images,"BACKGROUND: The automatic segmentation of kidneys in medical images is not a trivial task when the subjects undergoing the medical examination are affected by Autosomal Dominant Polycystic Kidney Disease (ADPKD). Several works dealing with the segmentation of Computed Tomography images from pathological subjects were proposed, showing high invasiveness of the examination or requiring interaction by the user for performing the segmentation of the images. In this work, we propose a fully-automated approach for the segmentation of Magnetic Resonance images, both reducing the invasiveness of the acquisition device and not requiring any interaction by the users for the segmentation of the images. METHODS: Two different approaches are proposed based on Deep Learning architectures using Convolutional Neural Networks (CNN) for the semantic segmentation of images, without needing to extract any hand-crafted features. In details, the first approach performs the automatic segmentation of images without any procedure for pre-processing the input. Conversely, the second approach performs a two-steps classification strategy: a first CNN automatically detects Regions Of Interest (ROIs); a subsequent classifier performs the semantic segmentation on the ROIs previously extracted. RESULTS: Results show that even though the detection of ROIs shows an overall high number of false positives, the subsequent semantic segmentation on the extracted ROIs allows achieving high performance in terms of mean Accuracy. However, the segmentation of the entire images input to the network remains the most accurate and reliable approach showing better performance than the previous approach. CONCLUSION: The obtained results show that both the investigated approaches are reliable for the semantic segmentation of polycystic kidneys since both the strategies reach an Accuracy higher than 85%. Also, both the investigated methodologies show performances comparable and consistent with other approaches found in literature working on images from different sources, reducing both the invasiveness of the analyses and the interaction needed by the users for performing the segmentation task.","A comparison between two semantic deep learning frameworks for the autosomal dominant polycystic kidney disease segmentation based on magnetic resonance images. BACKGROUND: The automatic segmentation of kidneys in medical images is not a trivial task when the subjects undergoing the medical examination are affected by Autosomal Dominant Polycystic Kidney Disease (ADPKD). Several works dealing with the segmentation of Computed Tomography images from pathological subjects were proposed, showing high invasiveness of the examination or requiring interaction by the user for performing the segmentation of the images. In this work, we propose a fully-automated approach for the segmentation of Magnetic Resonance images, both reducing the invasiveness of the acquisition device and not requiring any interaction by the users for the segmentation of the images. METHODS: Two different approaches are proposed based on Deep Learning architectures using Convolutional Neural Networks (CNN) for the semantic segmentation of images, without needing to extract any hand-crafted features. In details, the first approach performs the automatic segmentation of images without any procedure for pre-processing the input. Conversely, the second approach performs a two-steps classification strategy: a first CNN automatically detects Regions Of Interest (ROIs); a subsequent classifier performs the semantic segmentation on the ROIs previously extracted. RESULTS: Results show that even though the detection of ROIs shows an overall high number of false positives, the subsequent semantic segmentation on the extracted ROIs allows achieving high performance in terms of mean Accuracy. However, the segmentation of the entire images input to the network remains the most accurate and reliable approach showing better performance than the previous approach. CONCLUSION: The obtained results show that both the investigated approaches are reliable for the semantic segmentation of polycystic kidneys since both the strategies reach an Accuracy higher than 85%. Also, both the investigated methodologies show performances comparable and consistent with other approaches found in literature working on images from different sources, reducing both the invasiveness of the analyses and the interaction needed by the users for performing the segmentation task."
1,EEG-based image classification via a region-level stacked bi-directional deep learning framework,"BACKGROUND: As a physiological signal, EEG data cannot be subjectively changed or hidden. Compared with other physiological signals, EEG signals are directly related to human cortical activities with excellent temporal resolution. After the rapid development of machine learning and artificial intelligence, the analysis and calculation of EEGs has made great progress, leading to a significant boost in performances for content understanding and pattern recognition of brain activities across the areas of both neural science and computer vision. While such an enormous advance has attracted wide range of interests among relevant research communities, EEG-based classification of brain activities evoked by images still demands efforts for further improvement with respect to its accuracy, generalization, and interpretation, yet some characters of human brains have been relatively unexplored. METHODS: We propose a region-level stacked bi-directional deep learning framework for EEG-based image classification. Inspired by the hemispheric lateralization of human brains, we propose to extract additional information at regional level to strengthen and emphasize the differences between two hemispheres. The stacked bi-directional long short-term memories are used to capture the dynamic correlations hidden from both the past and the future to the current state in EEG sequences. RESULTS: Extensive experiments are carried out and our results demonstrate the effectiveness of our proposed framework. Compared with the existing state-of-the-arts, our framework achieves outstanding performances in EEG-based classification of brain activities evoked by images. In addition, we find that the signals of Gamma band are not only useful for achieving good performances for EEG-based image classification, but also play a significant role in capturing relationships between the neural activations and the specific emotional states. CONCLUSIONS: Our proposed framework provides an improved solution for the problem that, given an image used to stimulate brain activities, we should be able to identify which class the stimuli image comes from by analyzing the EEG signals. The region-level information is extracted to preserve and emphasize the hemispheric lateralization for neural functions or cognitive processes of human brains. Further, stacked bi-directional LSTMs are used to capture the dynamic correlations hidden in EEG data. Extensive experiments on standard EEG-based image classification dataset validate that our framework outperforms the existing state-of-the-arts under various contexts and experimental setups.","EEG-based image classification via a region-level stacked bi-directional deep learning framework. BACKGROUND: As a physiological signal, EEG data cannot be subjectively changed or hidden. Compared with other physiological signals, EEG signals are directly related to human cortical activities with excellent temporal resolution. After the rapid development of machine learning and artificial intelligence, the analysis and calculation of EEGs has made great progress, leading to a significant boost in performances for content understanding and pattern recognition of brain activities across the areas of both neural science and computer vision. While such an enormous advance has attracted wide range of interests among relevant research communities, EEG-based classification of brain activities evoked by images still demands efforts for further improvement with respect to its accuracy, generalization, and interpretation, yet some characters of human brains have been relatively unexplored. METHODS: We propose a region-level stacked bi-directional deep learning framework for EEG-based image classification. Inspired by the hemispheric lateralization of human brains, we propose to extract additional information at regional level to strengthen and emphasize the differences between two hemispheres. The stacked bi-directional long short-term memories are used to capture the dynamic correlations hidden from both the past and the future to the current state in EEG sequences. RESULTS: Extensive experiments are carried out and our results demonstrate the effectiveness of our proposed framework. Compared with the existing state-of-the-arts, our framework achieves outstanding performances in EEG-based classification of brain activities evoked by images. In addition, we find that the signals of Gamma band are not only useful for achieving good performances for EEG-based image classification, but also play a significant role in capturing relationships between the neural activations and the specific emotional states. CONCLUSIONS: Our proposed framework provides an improved solution for the problem that, given an image used to stimulate brain activities, we should be able to identify which class the stimuli image comes from by analyzing the EEG signals. The region-level information is extracted to preserve and emphasize the hemispheric lateralization for neural functions or cognitive processes of human brains. Further, stacked bi-directional LSTMs are used to capture the dynamic correlations hidden in EEG data. Extensive experiments on standard EEG-based image classification dataset validate that our framework outperforms the existing state-of-the-arts under various contexts and experimental setups."
1,Automated Abdominal Segmentation of CT Scans for Body Composition Analysis Using Deep Learning,"Purpose To develop and evaluate a fully automated algorithm for segmenting the abdomen from CT to quantify body composition. Materials and Methods For this retrospective study, a convolutional neural network based on the U-Net architecture was trained to perform abdominal segmentation on a data set of 2430 two-dimensional CT examinations and was tested on 270 CT examinations. It was further tested on a separate data set of 2369 patients with hepatocellular carcinoma (HCC). CT examinations were performed between 1997 and 2015. The mean age of patients was 67 years; for male patients, it was 67 years (range, 29-94 years), and for female patients, it was 66 years (range, 31-97 years). Differences in segmentation performance were assessed by using two-way analysis of variance with Bonferroni correction. Results Compared with reference segmentation, the model for this study achieved Dice scores (mean +/- standard deviation) of 0.98 +/- 0.03, 0.96 +/- 0.02, and 0.97 +/- 0.01 in the test set, and 0.94 +/- 0.05, 0.92 +/- 0.04, and 0.98 +/- 0.02 in the HCC data set, for the subcutaneous, muscle, and visceral adipose tissue compartments, respectively. Performance met or exceeded that of expert manual segmentation. Conclusion Model performance met or exceeded the accuracy of expert manual segmentation of CT examinations for both the test data set and the hepatocellular carcinoma data set. The model generalized well to multiple levels of the abdomen and may be capable of fully automated quantification of body composition metrics in three-dimensional CT examinations. (c) RSNA, 2018 Online supplemental material is available for this article. See also the editorial by Chang in this issue.","Automated Abdominal Segmentation of CT Scans for Body Composition Analysis Using Deep Learning. Purpose To develop and evaluate a fully automated algorithm for segmenting the abdomen from CT to quantify body composition. Materials and Methods For this retrospective study, a convolutional neural network based on the U-Net architecture was trained to perform abdominal segmentation on a data set of 2430 two-dimensional CT examinations and was tested on 270 CT examinations. It was further tested on a separate data set of 2369 patients with hepatocellular carcinoma (HCC). CT examinations were performed between 1997 and 2015. The mean age of patients was 67 years; for male patients, it was 67 years (range, 29-94 years), and for female patients, it was 66 years (range, 31-97 years). Differences in segmentation performance were assessed by using two-way analysis of variance with Bonferroni correction. Results Compared with reference segmentation, the model for this study achieved Dice scores (mean +/- standard deviation) of 0.98 +/- 0.03, 0.96 +/- 0.02, and 0.97 +/- 0.01 in the test set, and 0.94 +/- 0.05, 0.92 +/- 0.04, and 0.98 +/- 0.02 in the HCC data set, for the subcutaneous, muscle, and visceral adipose tissue compartments, respectively. Performance met or exceeded that of expert manual segmentation. Conclusion Model performance met or exceeded the accuracy of expert manual segmentation of CT examinations for both the test data set and the hepatocellular carcinoma data set. The model generalized well to multiple levels of the abdomen and may be capable of fully automated quantification of body composition metrics in three-dimensional CT examinations. (c) RSNA, 2018 Online supplemental material is available for this article. See also the editorial by Chang in this issue."
1,Multivariate graph learning for detecting aberrant connectivity of dynamic brain networks in autism,,
1,Improving reference prioritisation with PICO recognition,"BACKGROUND: Machine learning can assist with multiple tasks during systematic reviews to facilitate the rapid retrieval of relevant references during screening and to identify and extract information relevant to the study characteristics, which include the PICO elements of patient/population, intervention, comparator, and outcomes. The latter requires techniques for identifying and categorising fragments of text, known as named entity recognition. METHODS: A publicly available corpus of PICO annotations on biomedical abstracts is used to train a named entity recognition model, which is implemented as a recurrent neural network. This model is then applied to a separate collection of abstracts for references from systematic reviews within biomedical and health domains. The occurrences of words tagged in the context of specific PICO contexts are used as additional features for a relevancy classification model. Simulations of the machine learning-assisted screening are used to evaluate the work saved by the relevancy model with and without the PICO features. Chi-squared and statistical significance of positive predicted values are used to identify words that are more indicative of relevancy within PICO contexts. RESULTS: Inclusion of PICO features improves the performance metric on 15 of the 20 collections, with substantial gains on certain systematic reviews. Examples of words whose PICO context are more precise can explain this increase. CONCLUSIONS: Words within PICO tagged segments in abstracts are predictive features for determining inclusion. Combining PICO annotation model into the relevancy classification pipeline is a promising approach. The annotations may be useful on their own to aid users in pinpointing necessary information for data extraction, or to facilitate semantic search.","Improving reference prioritisation with PICO recognition. BACKGROUND: Machine learning can assist with multiple tasks during systematic reviews to facilitate the rapid retrieval of relevant references during screening and to identify and extract information relevant to the study characteristics, which include the PICO elements of patient/population, intervention, comparator, and outcomes. The latter requires techniques for identifying and categorising fragments of text, known as named entity recognition. METHODS: A publicly available corpus of PICO annotations on biomedical abstracts is used to train a named entity recognition model, which is implemented as a recurrent neural network. This model is then applied to a separate collection of abstracts for references from systematic reviews within biomedical and health domains. The occurrences of words tagged in the context of specific PICO contexts are used as additional features for a relevancy classification model. Simulations of the machine learning-assisted screening are used to evaluate the work saved by the relevancy model with and without the PICO features. Chi-squared and statistical significance of positive predicted values are used to identify words that are more indicative of relevancy within PICO contexts. RESULTS: Inclusion of PICO features improves the performance metric on 15 of the 20 collections, with substantial gains on certain systematic reviews. Examples of words whose PICO context are more precise can explain this increase. CONCLUSIONS: Words within PICO tagged segments in abstracts are predictive features for determining inclusion. Combining PICO annotation model into the relevancy classification pipeline is a promising approach. The annotations may be useful on their own to aid users in pinpointing necessary information for data extraction, or to facilitate semantic search."
1,Applying a deep learning-based sequence labeling approach to detect attributes of medical concepts in clinical text,"BACKGROUND: To detect attributes of medical concepts in clinical text, a traditional method often consists of two steps: named entity recognition of attributes and then relation classification between medical concepts and attributes. Here we present a novel solution, in which attribute detection of given concepts is converted into a sequence labeling problem, thus attribute entity recognition and relation classification are done simultaneously within one step. METHODS: A neural architecture combining bidirectional Long Short-Term Memory networks and Conditional Random fields (Bi-LSTMs-CRF) was adopted to detect various medical concept-attribute pairs in an efficient way. We then compared our deep learning-based sequence labeling approach with traditional two-step systems for three different attribute detection tasks: disease-modifier, medication-signature, and lab test-value. RESULTS: Our results show that the proposed method achieved higher accuracy than the traditional methods for all three medical concept-attribute detection tasks. CONCLUSIONS: This study demonstrates the efficacy of our sequence labeling approach using Bi-LSTM-CRFs on the attribute detection task, indicating its potential to speed up practical clinical NLP applications.","Applying a deep learning-based sequence labeling approach to detect attributes of medical concepts in clinical text. BACKGROUND: To detect attributes of medical concepts in clinical text, a traditional method often consists of two steps: named entity recognition of attributes and then relation classification between medical concepts and attributes. Here we present a novel solution, in which attribute detection of given concepts is converted into a sequence labeling problem, thus attribute entity recognition and relation classification are done simultaneously within one step. METHODS: A neural architecture combining bidirectional Long Short-Term Memory networks and Conditional Random fields (Bi-LSTMs-CRF) was adopted to detect various medical concept-attribute pairs in an efficient way. We then compared our deep learning-based sequence labeling approach with traditional two-step systems for three different attribute detection tasks: disease-modifier, medication-signature, and lab test-value. RESULTS: Our results show that the proposed method achieved higher accuracy than the traditional methods for all three medical concept-attribute detection tasks. CONCLUSIONS: This study demonstrates the efficacy of our sequence labeling approach using Bi-LSTM-CRFs on the attribute detection task, indicating its potential to speed up practical clinical NLP applications."
1,Natural language processing for populating lung cancer clinical research data,"BACKGROUND: Lung cancer is the second most common cancer for men and women; the wide adoption of electronic health records (EHRs) offers a potential to accelerate cohort-related epidemiological studies using informatics approaches. Since manual extraction from large volumes of text materials is time consuming and labor intensive, some efforts have emerged to automatically extract information from text for lung cancer patients using natural language processing (NLP), an artificial intelligence technique. METHODS: In this study, using an existing cohort of 2311 lung cancer patients with information about stage, histology, tumor grade, and therapies (chemotherapy, radiotherapy and surgery) manually ascertained, we developed and evaluated an NLP system to extract information on these variables automatically for the same patients from clinical narratives including clinical notes, pathology reports and surgery reports. RESULTS: Evaluation showed promising results with the recalls for stage, histology, tumor grade, and therapies achieving 89, 98, 78, and 100% respectively and the precisions were 70, 88, 90, and 100% respectively. CONCLUSION: This study demonstrated the feasibility and accuracy of automatically extracting pre-defined information from clinical narratives for lung cancer research.","Natural language processing for populating lung cancer clinical research data. BACKGROUND: Lung cancer is the second most common cancer for men and women; the wide adoption of electronic health records (EHRs) offers a potential to accelerate cohort-related epidemiological studies using informatics approaches. Since manual extraction from large volumes of text materials is time consuming and labor intensive, some efforts have emerged to automatically extract information from text for lung cancer patients using natural language processing (NLP), an artificial intelligence technique. METHODS: In this study, using an existing cohort of 2311 lung cancer patients with information about stage, histology, tumor grade, and therapies (chemotherapy, radiotherapy and surgery) manually ascertained, we developed and evaluated an NLP system to extract information on these variables automatically for the same patients from clinical narratives including clinical notes, pathology reports and surgery reports. RESULTS: Evaluation showed promising results with the recalls for stage, histology, tumor grade, and therapies achieving 89, 98, 78, and 100% respectively and the precisions were 70, 88, 90, and 100% respectively. CONCLUSION: This study demonstrated the feasibility and accuracy of automatically extracting pre-defined information from clinical narratives for lung cancer research."
1,Machine Learning-Based Prediction of Clinical Outcomes for Children During Emergency Department Triage,"Importance: While machine learning approaches may enhance prediction ability, little is known about their utility in emergency department (ED) triage. Objectives: To examine the performance of machine learning approaches to predict clinical outcomes and disposition in children in the ED and to compare their performance with conventional triage approaches. Design, Setting, and Participants: Prognostic study of ED data from the National Hospital Ambulatory Medical Care Survey from January 1, 2007, through December 31, 2015. A nationally representative sample of 52â€¯037 children aged 18 years or younger who presented to the ED were included. Data analysis was performed in August 2018. Main Outcomes and Measures: The outcomes were critical care (admission to an intensive care unit and/or in-hospital death) and hospitalization (direct hospital admission or transfer). In the training set (70% random sample), using routinely available triage data as predictors (eg, demographic characteristics and vital signs), we derived 4 machine learning-based models: lasso regression, random forest, gradient-boosted decision tree, and deep neural network. In the test set (the remaining 30% of the sample), we measured the models' prediction performance by computing C statistics, prospective prediction results, and decision curves. These machine learning models were built for each outcome and compared with the reference model using the conventional triage classification information. Results: Of 52â€¯037 eligible ED visits by children (median [interquartile range] age, 6 [2-14] years; 24â€¯929 [48.0%] female), 163 (0.3%) had the critical care outcome and 2352 (4.5%) had the hospitalization outcome. For the critical care prediction, all machine learning approaches had higher discriminative ability compared with the reference model, although the difference was not statistically significant (eg, C statistics of 0.85 [95% CI, 0.78-0.92] for the deep neural network vs 0.78 [95% CI, 0.71-0.85] for the reference; Pâ€‰=â€‰.16), and lower number of undertriaged critically ill children in the conventional triage levels 3 to 5 (urgent to nonurgent). For the hospitalization prediction, all machine learning approaches had significantly higher discrimination ability (eg, C statistic, 0.80 [95% CI, 0.78-0.81] for the deep neural network vs 0.73 [95% CI, 0.71-0.75] for the reference; Pâ€‰<â€‰.001) and fewer overtriaged children who did not require inpatient management in the conventional triage levels 1 to 3 (immediate to urgent). The decision curve analysis demonstrated a greater net benefit of machine learning models over ranges of clinical thresholds. Conclusions and Relevance: Machine learning-based triage had better discrimination ability to predict clinical outcomes and disposition, with reduction in undertriaging critically ill children and overtriaging children who are less ill.","Machine Learning-Based Prediction of Clinical Outcomes for Children During Emergency Department Triage. Importance: While machine learning approaches may enhance prediction ability, little is known about their utility in emergency department (ED) triage. Objectives: To examine the performance of machine learning approaches to predict clinical outcomes and disposition in children in the ED and to compare their performance with conventional triage approaches. Design, Setting, and Participants: Prognostic study of ED data from the National Hospital Ambulatory Medical Care Survey from January 1, 2007, through December 31, 2015. A nationally representative sample of 52â€¯037 children aged 18 years or younger who presented to the ED were included. Data analysis was performed in August 2018. Main Outcomes and Measures: The outcomes were critical care (admission to an intensive care unit and/or in-hospital death) and hospitalization (direct hospital admission or transfer). In the training set (70% random sample), using routinely available triage data as predictors (eg, demographic characteristics and vital signs), we derived 4 machine learning-based models: lasso regression, random forest, gradient-boosted decision tree, and deep neural network. In the test set (the remaining 30% of the sample), we measured the models' prediction performance by computing C statistics, prospective prediction results, and decision curves. These machine learning models were built for each outcome and compared with the reference model using the conventional triage classification information. Results: Of 52â€¯037 eligible ED visits by children (median [interquartile range] age, 6 [2-14] years; 24â€¯929 [48.0%] female), 163 (0.3%) had the critical care outcome and 2352 (4.5%) had the hospitalization outcome. For the critical care prediction, all machine learning approaches had higher discriminative ability compared with the reference model, although the difference was not statistically significant (eg, C statistics of 0.85 [95% CI, 0.78-0.92] for the deep neural network vs 0.78 [95% CI, 0.71-0.85] for the reference; Pâ€‰=â€‰.16), and lower number of undertriaged critically ill children in the conventional triage levels 3 to 5 (urgent to nonurgent). For the hospitalization prediction, all machine learning approaches had significantly higher discrimination ability (eg, C statistic, 0.80 [95% CI, 0.78-0.81] for the deep neural network vs 0.73 [95% CI, 0.71-0.75] for the reference; Pâ€‰<â€‰.001) and fewer overtriaged children who did not require inpatient management in the conventional triage levels 1 to 3 (immediate to urgent). The decision curve analysis demonstrated a greater net benefit of machine learning models over ranges of clinical thresholds. Conclusions and Relevance: Machine learning-based triage had better discrimination ability to predict clinical outcomes and disposition, with reduction in undertriaging critically ill children and overtriaging children who are less ill."
1,Convolutional neural networks for automated fracture detection and localization on wrist radiographs,"Purpose: To demonstrate the feasibility and performance of an object detection convolutional neural network (CNN) for fracture detection and localization on wrist radiographs. Materials and Methods: Institutional review board approval was obtained with waiver of consent for this retrospective study. A total of 7356 wrist radiographic studies were extracted from a hospital picture archiving and communication system. Radiologists annotated all radius and ulna fractures with bounding boxes. The dataset was split into training (90%) and validation (10%) sets and used to train fracture localization models for frontal and lateral images. Inception-ResNet Faster R-CNN architecture was implemented as a deep learning model. The models were tested on an unseen test set of 524 consecutive emergency department wrist radiographic studies with two radiologists in consensus as the reference standard. Per-fracture, per-image (ie, per-view), and per-study sensitivity and specificity were determined. Area under the receiver operating characteristic curve (AUC) analysis was performed. Results: The model detected and correctly localized 310 (91.2%) of 340 and 236 (96.3%) of 245 of all radius and ulna fractures on the frontal and lateral views, respectively. The per-image sensitivity, specificity, and AUC were 95.7% (95% confidence interval [CI]: 92.4%, 97.8%), 82.5% (95% CI: 77.4%, 86.8%), and 0.918 (95% CI: 0.894, 0.941), respectively, for the frontal view and 96.7% (95% CI: 93.6%, 98.6%), 86.4% (95% CI: 81.9%, 90.2%), and 0.933 (95% CI: 0.912, 0.954), respectively, for the lateral view. The per-study sensitivity, specificity, and AUC were 98.1% (95% CI: 95.6%, 99.4%), 72.9% (95% CI: 67.1%, 78.2%), and 0.895 (95% CI: 0.870, 0.920), respectively. Conclusion: The ability of an object detection CNN to detect and localize radius and ulna fractures on wrist radiographs with high sensitivity and specificity was demonstrated.","Convolutional neural networks for automated fracture detection and localization on wrist radiographs. Purpose: To demonstrate the feasibility and performance of an object detection convolutional neural network (CNN) for fracture detection and localization on wrist radiographs. Materials and Methods: Institutional review board approval was obtained with waiver of consent for this retrospective study. A total of 7356 wrist radiographic studies were extracted from a hospital picture archiving and communication system. Radiologists annotated all radius and ulna fractures with bounding boxes. The dataset was split into training (90%) and validation (10%) sets and used to train fracture localization models for frontal and lateral images. Inception-ResNet Faster R-CNN architecture was implemented as a deep learning model. The models were tested on an unseen test set of 524 consecutive emergency department wrist radiographic studies with two radiologists in consensus as the reference standard. Per-fracture, per-image (ie, per-view), and per-study sensitivity and specificity were determined. Area under the receiver operating characteristic curve (AUC) analysis was performed. Results: The model detected and correctly localized 310 (91.2%) of 340 and 236 (96.3%) of 245 of all radius and ulna fractures on the frontal and lateral views, respectively. The per-image sensitivity, specificity, and AUC were 95.7% (95% confidence interval [CI]: 92.4%, 97.8%), 82.5% (95% CI: 77.4%, 86.8%), and 0.918 (95% CI: 0.894, 0.941), respectively, for the frontal view and 96.7% (95% CI: 93.6%, 98.6%), 86.4% (95% CI: 81.9%, 90.2%), and 0.933 (95% CI: 0.912, 0.954), respectively, for the lateral view. The per-study sensitivity, specificity, and AUC were 98.1% (95% CI: 95.6%, 99.4%), 72.9% (95% CI: 67.1%, 78.2%), and 0.895 (95% CI: 0.870, 0.920), respectively. Conclusion: The ability of an object detection CNN to detect and localize radius and ulna fractures on wrist radiographs with high sensitivity and specificity was demonstrated."
1,Development and validation of an esophageal squamous cell carcinoma detection model by large-scale microrna profiling,"IMPORTANCE Patients with late-stage esophageal squamous cell carcinoma (ESCC) have a poor prognosis. Noninvasive screening tests using serum microRNAs (miRNAs) to accurately detect earlystage ESCC are needed to improve mortality. OBJECTIVE To establish a model using serum miRNAs to distinguish patients with ESCC from noncancer controls. DESIGN, SETTING, AND PARTICIPANTS In this case-control study, serum miRNA expression profiles of patients with ESCC (n = 566) and control patients without cancer (n = 4965) were retrospectively analyzed to establish a diagnostic model, which was tested in a training set and confirmed in a validation set. Patients histologically diagnosed as having ESCC who did not receive prior therapy or have a past or concurrent cancer other than ESCC were enrolled from the National Cancer Center Hospital in Tokyo, Japan. Between October 2010 and November 2015, control samples were collected from the National Cancer Center Biobank, the Biobank of the National Center for Geriatrics and Gerontology, and the general population undergoing routine health examination. Data analysis was performed between August 2015 and October 2018. Serum samples were randomly divided into discovery and validation sets. MAIN OUTCOMES AND MEASURES The expression of 2565 miRNAs was assessed in each sample. The discriminant model (named the EC index) was evaluated in the training set using Fisher linear discriminant analysis with a greedy algorithm. Receiver operating characteristic curve analysis evaluated the diagnostic ability of the model in the validation set. RESULTS In the training set, 283 patients with esophageal cancer (median age, 67 years [range, 37-90 years]; 83.4%male) were compared with 283 control patients (median age, 54 years [range, 22-100 years]; 43.1%male), and the EC index was constructed using 6 miRNAs (miR-8073, miR-6820-5p, miR-6794-5p, miR-3196, miR-744-5p, and miR-6799-5p). The area under the receiver operating characteristic curvewas 1.00, with sensitivity of 1.00 and specificity of 0.98. The validation set included 283 patients (median age, 66 years [range, 42-87 years]; 83.0%male) and 4682 control patients (median age, 68 years [range, 20-98 years]; 44.7%male), and the area under the receiver operating characteristic curve for the EC index was 1.00, with sensitivity of 0.96 and specificity of 0.98. CONCLUSIONS AND RELEVANCE What appears to be novel serum miRNA discriminant model was developed for the diagnosis of ESCC. A multicenter prospective study is ongoing to confirm the present observations.","Development and validation of an esophageal squamous cell carcinoma detection model by large-scale microrna profiling. IMPORTANCE Patients with late-stage esophageal squamous cell carcinoma (ESCC) have a poor prognosis. Noninvasive screening tests using serum microRNAs (miRNAs) to accurately detect earlystage ESCC are needed to improve mortality. OBJECTIVE To establish a model using serum miRNAs to distinguish patients with ESCC from noncancer controls. DESIGN, SETTING, AND PARTICIPANTS In this case-control study, serum miRNA expression profiles of patients with ESCC (n = 566) and control patients without cancer (n = 4965) were retrospectively analyzed to establish a diagnostic model, which was tested in a training set and confirmed in a validation set. Patients histologically diagnosed as having ESCC who did not receive prior therapy or have a past or concurrent cancer other than ESCC were enrolled from the National Cancer Center Hospital in Tokyo, Japan. Between October 2010 and November 2015, control samples were collected from the National Cancer Center Biobank, the Biobank of the National Center for Geriatrics and Gerontology, and the general population undergoing routine health examination. Data analysis was performed between August 2015 and October 2018. Serum samples were randomly divided into discovery and validation sets. MAIN OUTCOMES AND MEASURES The expression of 2565 miRNAs was assessed in each sample. The discriminant model (named the EC index) was evaluated in the training set using Fisher linear discriminant analysis with a greedy algorithm. Receiver operating characteristic curve analysis evaluated the diagnostic ability of the model in the validation set. RESULTS In the training set, 283 patients with esophageal cancer (median age, 67 years [range, 37-90 years]; 83.4%male) were compared with 283 control patients (median age, 54 years [range, 22-100 years]; 43.1%male), and the EC index was constructed using 6 miRNAs (miR-8073, miR-6820-5p, miR-6794-5p, miR-3196, miR-744-5p, and miR-6799-5p). The area under the receiver operating characteristic curvewas 1.00, with sensitivity of 1.00 and specificity of 0.98. The validation set included 283 patients (median age, 66 years [range, 42-87 years]; 83.0%male) and 4682 control patients (median age, 68 years [range, 20-98 years]; 44.7%male), and the area under the receiver operating characteristic curve for the EC index was 1.00, with sensitivity of 0.96 and specificity of 0.98. CONCLUSIONS AND RELEVANCE What appears to be novel serum miRNA discriminant model was developed for the diagnosis of ESCC. A multicenter prospective study is ongoing to confirm the present observations."
1,Development and Validation of Deep Learning-based Automatic Detection Algorithm for Malignant Pulmonary Nodules on Chest Radiographs,,
1,Application of machine learning methodology to assess the performance of DIABETIMSS program for patients with type 2 diabetes in family medicine clinics in Mexico,"BACKGROUND: The study aimed to assess the performance of a multidisciplinary-team diabetes care program called DIABETIMSS on glycemic control of type 2 diabetes (T2D) patients, by using available observational patient data and machine-learning-based targeted learning methods. METHODS: We analyzed electronic health records and laboratory databases from the year 2012 to 2016 of T2D patients from six family medicine clinics (FMCs) delivering the DIABETIMSS program, and five FMCs providing routine care. All FMCs belong to the Mexican Institute of Social Security and are in Mexico City and the State of Mexico. The primary outcome was glycemic control. The study covariates included: patient sex, age, anthropometric data, history of glycemic control, diabetic complications and comorbidity. We measured the effects of DIABETIMSS program through 1) simple unadjusted mean differences; 2) adjusted via standard logistic regression and 3) adjusted via targeted machine learning. We treated the data as a serial cross-sectional study, conducted a standard principal components analysis to explore the distribution of covariates among clinics, and performed regression tree on data transformed to use the prediction model to identify patient sub-groups in whom the program was most successful. To explore the robustness of the machine learning approaches, we conducted a set of simulations and the sensitivity analysis with process-of-care indicators as possible confounders. RESULTS: The study included 78,894 T2D patients, from which 37,767patients received care through DIABETIMSS. The impact of DIABETIMSS ranged, among clinics, from 2 to 8% improvement in glycemic control, with an overall (pooled) estimate of 5% improvement. T2D patients with fewer complications have more significant benefit from DIABETIMSS than those with more complications. At the FMC's delivering the conventional model the predicted impacts were like what was observed empirically in the DIABETIMSS clinics. The sensitivity analysis did not change the overall estimate average across clinics. CONCLUSIONS: DIABETIMSS program had a small, but significant increase in glycemic control. The use of machine learning methods yields both population-level effects and pinpoints the sub-groups of patients the program benefits the most. These methods exploit the potential of routine observational patient data within complex healthcare systems to inform decision-makers.","Application of machine learning methodology to assess the performance of DIABETIMSS program for patients with type 2 diabetes in family medicine clinics in Mexico. BACKGROUND: The study aimed to assess the performance of a multidisciplinary-team diabetes care program called DIABETIMSS on glycemic control of type 2 diabetes (T2D) patients, by using available observational patient data and machine-learning-based targeted learning methods. METHODS: We analyzed electronic health records and laboratory databases from the year 2012 to 2016 of T2D patients from six family medicine clinics (FMCs) delivering the DIABETIMSS program, and five FMCs providing routine care. All FMCs belong to the Mexican Institute of Social Security and are in Mexico City and the State of Mexico. The primary outcome was glycemic control. The study covariates included: patient sex, age, anthropometric data, history of glycemic control, diabetic complications and comorbidity. We measured the effects of DIABETIMSS program through 1) simple unadjusted mean differences; 2) adjusted via standard logistic regression and 3) adjusted via targeted machine learning. We treated the data as a serial cross-sectional study, conducted a standard principal components analysis to explore the distribution of covariates among clinics, and performed regression tree on data transformed to use the prediction model to identify patient sub-groups in whom the program was most successful. To explore the robustness of the machine learning approaches, we conducted a set of simulations and the sensitivity analysis with process-of-care indicators as possible confounders. RESULTS: The study included 78,894 T2D patients, from which 37,767patients received care through DIABETIMSS. The impact of DIABETIMSS ranged, among clinics, from 2 to 8% improvement in glycemic control, with an overall (pooled) estimate of 5% improvement. T2D patients with fewer complications have more significant benefit from DIABETIMSS than those with more complications. At the FMC's delivering the conventional model the predicted impacts were like what was observed empirically in the DIABETIMSS clinics. The sensitivity analysis did not change the overall estimate average across clinics. CONCLUSIONS: DIABETIMSS program had a small, but significant increase in glycemic control. The use of machine learning methods yields both population-level effects and pinpoints the sub-groups of patients the program benefits the most. These methods exploit the potential of routine observational patient data within complex healthcare systems to inform decision-makers."
1,Deep Learning-based Image Conversion of CT Reconstruction Kernels Improves Radiomics Reproducibility for Pulmonary Nodules or Masses,,
1,Development and Validation of a Deep Learning-Based Automated Detection Algorithm for Major Thoracic Diseases on Chest Radiographs,"Importance: Interpretation of chest radiographs is a challenging task prone to errors, requiring expert readers. An automated system that can accurately classify chest radiographs may help streamline the clinical workflow. Objectives: To develop a deep learning-based algorithm that can classify normal and abnormal results from chest radiographs with major thoracic diseases including pulmonary malignant neoplasm, active tuberculosis, pneumonia, and pneumothorax and to validate the algorithm's performance using independent data sets. Design, Setting, and Participants: This diagnostic study developed a deep learning-based algorithm using single-center data collected between November 1, 2016, and January 31, 2017. The algorithm was externally validated with multicenter data collected between May 1 and July 31, 2018. A total of 54â€¯221 chest radiographs with normal findings from 47â€¯917 individuals (21â€¯556 men and 26â€¯361 women; mean [SD] age, 51 [16] years) and 35â€¯613 chest radiographs with abnormal findings from 14â€¯102 individuals (8373 men and 5729 women; mean [SD] age, 62 [15] years) were used to develop the algorithm. A total of 486 chest radiographs with normal results and 529 with abnormal results (1 from each participant; 628 men and 387 women; mean [SD] age, 53 [18] years) from 5 institutions were used for external validation. Fifteen physicians, including nonradiology physicians, board-certified radiologists, and thoracic radiologists, participated in observer performance testing. Data were analyzed in August 2018. Exposures: Deep learning-based algorithm. Main Outcomes and Measures: Image-wise classification performances measured by area under the receiver operating characteristic curve; lesion-wise localization performances measured by area under the alternative free-response receiver operating characteristic curve. Results: The algorithm demonstrated a median (range) area under the curve of 0.979 (0.973-1.000) for image-wise classification and 0.972 (0.923-0.985) for lesion-wise localization; the algorithm demonstrated significantly higher performance than all 3 physician groups in both image-wise classification (0.983 vs 0.814-0.932; all Pâ€‰<â€‰.005) and lesion-wise localization (0.985 vs 0.781-0.907; all Pâ€‰<â€‰.001). Significant improvements in both image-wise classification (0.814-0.932 to 0.904-0.958; all Pâ€‰<â€‰.005) and lesion-wise localization (0.781-0.907 to 0.873-0.938; all Pâ€‰<â€‰.001) were observed in all 3 physician groups with assistance of the algorithm. Conclusions and Relevance: The algorithm consistently outperformed physicians, including thoracic radiologists, in the discrimination of chest radiographs with major thoracic diseases, demonstrating its potential to improve the quality and efficiency of clinical practice.","Development and Validation of a Deep Learning-Based Automated Detection Algorithm for Major Thoracic Diseases on Chest Radiographs. Importance: Interpretation of chest radiographs is a challenging task prone to errors, requiring expert readers. An automated system that can accurately classify chest radiographs may help streamline the clinical workflow. Objectives: To develop a deep learning-based algorithm that can classify normal and abnormal results from chest radiographs with major thoracic diseases including pulmonary malignant neoplasm, active tuberculosis, pneumonia, and pneumothorax and to validate the algorithm's performance using independent data sets. Design, Setting, and Participants: This diagnostic study developed a deep learning-based algorithm using single-center data collected between November 1, 2016, and January 31, 2017. The algorithm was externally validated with multicenter data collected between May 1 and July 31, 2018. A total of 54â€¯221 chest radiographs with normal findings from 47â€¯917 individuals (21â€¯556 men and 26â€¯361 women; mean [SD] age, 51 [16] years) and 35â€¯613 chest radiographs with abnormal findings from 14â€¯102 individuals (8373 men and 5729 women; mean [SD] age, 62 [15] years) were used to develop the algorithm. A total of 486 chest radiographs with normal results and 529 with abnormal results (1 from each participant; 628 men and 387 women; mean [SD] age, 53 [18] years) from 5 institutions were used for external validation. Fifteen physicians, including nonradiology physicians, board-certified radiologists, and thoracic radiologists, participated in observer performance testing. Data were analyzed in August 2018. Exposures: Deep learning-based algorithm. Main Outcomes and Measures: Image-wise classification performances measured by area under the receiver operating characteristic curve; lesion-wise localization performances measured by area under the alternative free-response receiver operating characteristic curve. Results: The algorithm demonstrated a median (range) area under the curve of 0.979 (0.973-1.000) for image-wise classification and 0.972 (0.923-0.985) for lesion-wise localization; the algorithm demonstrated significantly higher performance than all 3 physician groups in both image-wise classification (0.983 vs 0.814-0.932; all Pâ€‰<â€‰.005) and lesion-wise localization (0.985 vs 0.781-0.907; all Pâ€‰<â€‰.001). Significant improvements in both image-wise classification (0.814-0.932 to 0.904-0.958; all Pâ€‰<â€‰.005) and lesion-wise localization (0.781-0.907 to 0.873-0.938; all Pâ€‰<â€‰.001) were observed in all 3 physician groups with assistance of the algorithm. Conclusions and Relevance: The algorithm consistently outperformed physicians, including thoracic radiologists, in the discrimination of chest radiographs with major thoracic diseases, demonstrating its potential to improve the quality and efficiency of clinical practice."
1,Integrating shortest dependency path and sentence sequence into a deep learning framework for relation extraction in clinical text,"BACKGROUND: Extracting relations between important clinical entities is critical but very challenging for natural language processing (NLP) in the medical domain. Researchers have applied deep learning-based approaches to clinical relation extraction; but most of them consider sentence sequence only, without modeling syntactic structures. The aim of this study was to utilize a deep neural network to capture the syntactic features and further improve the performances of relation extraction in clinical notes. METHODS: We propose a novel neural approach to model shortest dependency path (SDP) between target entities together with the sentence sequence for clinical relation extraction. Our neural network architecture consists of three modules: (1) sentence sequence representation module using bidirectional long short-term memory network (Bi-LSTM) to capture the features in the sentence sequence; (2) SDP representation module implementing the convolutional neural network (CNN) and Bi-LSTM network to capture the syntactic context for target entities using SDP information; and (3) classification module utilizing a fully-connected layer with Softmax function to classify the relation type between target entities. RESULTS: Using the 2010 i2b2/VA relation extraction dataset, we compared our approach with other baseline methods. Our experimental results show that the proposed approach achieved significant improvements over comparable existing methods, demonstrating the effectiveness of utilizing syntactic structures in deep learning-based relation extraction. The F-measure of our method reaches 74.34% which is 2.5% higher than the method without using syntactic features. CONCLUSIONS: We propose a new neural network architecture by modeling SDP along with sentence sequence to extract multi-relations from clinical text. Our experimental results show that the proposed approach significantly improve the performances on clinical notes, demonstrating the effectiveness of syntactic structures in deep learning-based relation extraction.","Integrating shortest dependency path and sentence sequence into a deep learning framework for relation extraction in clinical text. BACKGROUND: Extracting relations between important clinical entities is critical but very challenging for natural language processing (NLP) in the medical domain. Researchers have applied deep learning-based approaches to clinical relation extraction; but most of them consider sentence sequence only, without modeling syntactic structures. The aim of this study was to utilize a deep neural network to capture the syntactic features and further improve the performances of relation extraction in clinical notes. METHODS: We propose a novel neural approach to model shortest dependency path (SDP) between target entities together with the sentence sequence for clinical relation extraction. Our neural network architecture consists of three modules: (1) sentence sequence representation module using bidirectional long short-term memory network (Bi-LSTM) to capture the features in the sentence sequence; (2) SDP representation module implementing the convolutional neural network (CNN) and Bi-LSTM network to capture the syntactic context for target entities using SDP information; and (3) classification module utilizing a fully-connected layer with Softmax function to classify the relation type between target entities. RESULTS: Using the 2010 i2b2/VA relation extraction dataset, we compared our approach with other baseline methods. Our experimental results show that the proposed approach achieved significant improvements over comparable existing methods, demonstrating the effectiveness of utilizing syntactic structures in deep learning-based relation extraction. The F-measure of our method reaches 74.34% which is 2.5% higher than the method without using syntactic features. CONCLUSIONS: We propose a new neural network architecture by modeling SDP along with sentence sequence to extract multi-relations from clinical text. Our experimental results show that the proposed approach significantly improve the performances on clinical notes, demonstrating the effectiveness of syntactic structures in deep learning-based relation extraction."
1,Assessment of Accuracy of an Artificial Intelligence Algorithm to Detect Melanoma in Images of Skin Lesions,"Importance: A high proportion of suspicious pigmented skin lesions referred for investigation are benign. Techniques to improve the accuracy of melanoma diagnoses throughout the patient pathway are needed to reduce the pressure on secondary care and pathology services. Objective: To determine the accuracy of an artificial intelligence algorithm in identifying melanoma in dermoscopic images of lesions taken with smartphone and digital single-lens reflex (DSLR) cameras. Design, Setting, and Participants: This prospective, multicenter, single-arm, masked diagnostic trial took place in dermatology and plastic surgery clinics in 7 UK hospitals. Dermoscopic images of suspicious and control skin lesions from 514 patients with at least 1 suspicious pigmented skin lesion scheduled for biopsy were captured on 3 different cameras. Data were collected from January 2017 to July 2018. Clinicians and the Deep Ensemble for Recognition of Malignancy, a deterministic artificial intelligence algorithm trained to identify melanoma in dermoscopic images of pigmented skin lesions using deep learning techniques, assessed the likelihood of melanoma. Initial data analysis was conducted in September 2018; further analysis was conducted from February 2019 to August 2019. Interventions: Clinician and algorithmic assessment of melanoma. Main Outcomes and Measures: Area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity of the algorithmic and specialist assessment, determined using histopathology diagnosis as the criterion standard. Results: The study population of 514 patients included 279 women (55.7%) and 484 white patients (96.8%), with a mean (SD) age of 52.1 (18.6) years. A total of 1550 images of skin lesions were included in the analysis (551 [35.6%] biopsied lesions; 999 [64.4%] control lesions); 286 images (18.6%) were used to train the algorithm, and a further 849 (54.8%) images were missing or unsuitable for analysis. Of the biopsied lesions that were assessed by the algorithm and specialists, 125 (22.7%) were diagnosed as melanoma. Of these, 77 (16.7%) were used for the primary analysis. The algorithm achieved an AUROC of 90.1% (95% CI, 86.3%-94.0%) for biopsied lesions and 95.8% (95% CI, 94.1%-97.6%) for all lesions using iPhone 6s images; an AUROC of 85.8% (95% CI, 81.0%-90.7%) for biopsied lesions and 93.8% (95% CI, 91.4%-96.2%) for all lesions using Galaxy S6 images; and an AUROC of 86.9% (95% CI, 80.8%-93.0%) for biopsied lesions and 91.8% (95% CI, 87.5%-96.1%) for all lesions using DSLR camera images. At 100% sensitivity, the algorithm achieved a specificity of 64.8% with iPhone 6s images. Specialists achieved an AUROC of 77.8% (95% CI, 72.5%-81.9%) and a specificity of 69.9%. Conclusions and Relevance: In this study, the algorithm demonstrated an ability to identify melanoma from dermoscopic images of selected lesions with an accuracy similar to that of specialists..","Assessment of Accuracy of an Artificial Intelligence Algorithm to Detect Melanoma in Images of Skin Lesions. Importance: A high proportion of suspicious pigmented skin lesions referred for investigation are benign. Techniques to improve the accuracy of melanoma diagnoses throughout the patient pathway are needed to reduce the pressure on secondary care and pathology services. Objective: To determine the accuracy of an artificial intelligence algorithm in identifying melanoma in dermoscopic images of lesions taken with smartphone and digital single-lens reflex (DSLR) cameras. Design, Setting, and Participants: This prospective, multicenter, single-arm, masked diagnostic trial took place in dermatology and plastic surgery clinics in 7 UK hospitals. Dermoscopic images of suspicious and control skin lesions from 514 patients with at least 1 suspicious pigmented skin lesion scheduled for biopsy were captured on 3 different cameras. Data were collected from January 2017 to July 2018. Clinicians and the Deep Ensemble for Recognition of Malignancy, a deterministic artificial intelligence algorithm trained to identify melanoma in dermoscopic images of pigmented skin lesions using deep learning techniques, assessed the likelihood of melanoma. Initial data analysis was conducted in September 2018; further analysis was conducted from February 2019 to August 2019. Interventions: Clinician and algorithmic assessment of melanoma. Main Outcomes and Measures: Area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity of the algorithmic and specialist assessment, determined using histopathology diagnosis as the criterion standard. Results: The study population of 514 patients included 279 women (55.7%) and 484 white patients (96.8%), with a mean (SD) age of 52.1 (18.6) years. A total of 1550 images of skin lesions were included in the analysis (551 [35.6%] biopsied lesions; 999 [64.4%] control lesions); 286 images (18.6%) were used to train the algorithm, and a further 849 (54.8%) images were missing or unsuitable for analysis. Of the biopsied lesions that were assessed by the algorithm and specialists, 125 (22.7%) were diagnosed as melanoma. Of these, 77 (16.7%) were used for the primary analysis. The algorithm achieved an AUROC of 90.1% (95% CI, 86.3%-94.0%) for biopsied lesions and 95.8% (95% CI, 94.1%-97.6%) for all lesions using iPhone 6s images; an AUROC of 85.8% (95% CI, 81.0%-90.7%) for biopsied lesions and 93.8% (95% CI, 91.4%-96.2%) for all lesions using Galaxy S6 images; and an AUROC of 86.9% (95% CI, 80.8%-93.0%) for biopsied lesions and 91.8% (95% CI, 87.5%-96.1%) for all lesions using DSLR camera images. At 100% sensitivity, the algorithm achieved a specificity of 64.8% with iPhone 6s images. Specialists achieved an AUROC of 77.8% (95% CI, 72.5%-81.9%) and a specificity of 69.9%. Conclusions and Relevance: In this study, the algorithm demonstrated an ability to identify melanoma from dermoscopic images of selected lesions with an accuracy similar to that of specialists.."
1,Expert-Level Diagnosis of Nonpigmented Skin Cancer by Combined Convolutional Neural Networks,"Importance: Convolutional neural networks (CNNs) achieve expert-level accuracy in the diagnosis of pigmented melanocytic lesions. However, the most common types of skin cancer are nonpigmented and nonmelanocytic, and are more difficult to diagnose. Objective: To compare the accuracy of a CNN-based classifier with that of physicians with different levels of experience. Design, Setting, and Participants: A CNN-based classification model was trained on 7895 dermoscopic and 5829 close-up images of lesions excised at a primary skin cancer clinic between January 1, 2008, and July 13, 2017, for a combined evaluation of both imaging methods. The combined CNN (cCNN) was tested on a set of 2072 unknown cases and compared with results from 95 human raters who were medical personnel, including 62 board-certified dermatologists, with different experience in dermoscopy. Main Outcomes and Measures: The proportions of correct specific diagnoses and the accuracy to differentiate between benign and malignant lesions measured as an area under the receiver operating characteristic curve served as main outcome measures. Results: Among 95 human raters (51.6% female; mean age, 43.4 years; 95% CI, 41.0-45.7 years), the participants were divided into 3 groups (according to years of experience with dermoscopy): beginner raters (<3 years), intermediate raters (3-10 years), or expert raters (>10 years). The area under the receiver operating characteristic curve of the trained cCNN was higher than human ratings (0.742; 95% CI, 0.729-0.755 vs 0.695; 95% CI, 0.676-0.713; P < .001). The specificity was fixed at the mean level of human raters (51.3%), and therefore the sensitivity of the cCNN (80.5%; 95% CI, 79.0%-82.1%) was higher than that of human raters (77.6%; 95% CI, 74.7%-80.5%). The cCNN achieved a higher percentage of correct specific diagnoses compared with human raters (37.6%; 95% CI, 36.6%-38.4% vs 33.5%; 95% CI, 31.5%-35.6%; P = .001) but not compared with experts (37.3%; 95% CI, 35.7%-38.8% vs 40.0%; 95% CI, 37.0%-43.0%; P = .18). Conclusions and Relevance: Neural networks are able to classify dermoscopic and close-up images of nonpigmented lesions as accurately as human experts in an experimental setting.","Expert-Level Diagnosis of Nonpigmented Skin Cancer by Combined Convolutional Neural Networks. Importance: Convolutional neural networks (CNNs) achieve expert-level accuracy in the diagnosis of pigmented melanocytic lesions. However, the most common types of skin cancer are nonpigmented and nonmelanocytic, and are more difficult to diagnose. Objective: To compare the accuracy of a CNN-based classifier with that of physicians with different levels of experience. Design, Setting, and Participants: A CNN-based classification model was trained on 7895 dermoscopic and 5829 close-up images of lesions excised at a primary skin cancer clinic between January 1, 2008, and July 13, 2017, for a combined evaluation of both imaging methods. The combined CNN (cCNN) was tested on a set of 2072 unknown cases and compared with results from 95 human raters who were medical personnel, including 62 board-certified dermatologists, with different experience in dermoscopy. Main Outcomes and Measures: The proportions of correct specific diagnoses and the accuracy to differentiate between benign and malignant lesions measured as an area under the receiver operating characteristic curve served as main outcome measures. Results: Among 95 human raters (51.6% female; mean age, 43.4 years; 95% CI, 41.0-45.7 years), the participants were divided into 3 groups (according to years of experience with dermoscopy): beginner raters (<3 years), intermediate raters (3-10 years), or expert raters (>10 years). The area under the receiver operating characteristic curve of the trained cCNN was higher than human ratings (0.742; 95% CI, 0.729-0.755 vs 0.695; 95% CI, 0.676-0.713; P < .001). The specificity was fixed at the mean level of human raters (51.3%), and therefore the sensitivity of the cCNN (80.5%; 95% CI, 79.0%-82.1%) was higher than that of human raters (77.6%; 95% CI, 74.7%-80.5%). The cCNN achieved a higher percentage of correct specific diagnoses compared with human raters (37.6%; 95% CI, 36.6%-38.4% vs 33.5%; 95% CI, 31.5%-35.6%; P = .001) but not compared with experts (37.3%; 95% CI, 35.7%-38.8% vs 40.0%; 95% CI, 37.0%-43.0%; P = .18). Conclusions and Relevance: Neural networks are able to classify dermoscopic and close-up images of nonpigmented lesions as accurately as human experts in an experimental setting."
1,Evaluating the performance of convolutional neural networks with direct acyclic graph architectures in automatic segmentation of breast lesion in US images,"BACKGROUND: Outlining lesion contours in Ultra Sound (US) breast images is an important step in breast cancer diagnosis. Malignant lesions infiltrate the surrounding tissue, generating irregular contours, with spiculation and angulated margins, whereas benign lesions produce contours with a smooth outline and elliptical shape. In breast imaging, the majority of the existing publications in the literature focus on using Convolutional Neural Networks (CNNs) for segmentation and classification of lesions in mammographic images. In this study our main objective is to assess the ability of CNNs in detecting contour irregularities in breast lesions in US images. METHODS: In this study we compare the performance of two CNNs with Direct Acyclic Graph (DAG) architecture and one CNN with a series architecture for breast lesion segmentation in US images. DAG and series architectures are both feedforward networks. The difference is that a DAG architecture could have more than one path between the first layer and end layer, whereas a series architecture has only one path from the beginning layer to the end layer. The CNN architectures were evaluated with two datasets. RESULTS: With the more complex DAG architecture, the following mean values were obtained for the metrics used to evaluate the segmented contours: global accuracy: 0.956; IOU: 0.876; F measure: 68.77%; Dice coefficient: 0.892. CONCLUSION: The CNN DAG architecture shows the best metric values used for quantitatively evaluating the segmented contours compared with the gold-standard contours. The segmented contours obtained with this architecture also have more details and irregularities, like the gold-standard contours.","Evaluating the performance of convolutional neural networks with direct acyclic graph architectures in automatic segmentation of breast lesion in US images. BACKGROUND: Outlining lesion contours in Ultra Sound (US) breast images is an important step in breast cancer diagnosis. Malignant lesions infiltrate the surrounding tissue, generating irregular contours, with spiculation and angulated margins, whereas benign lesions produce contours with a smooth outline and elliptical shape. In breast imaging, the majority of the existing publications in the literature focus on using Convolutional Neural Networks (CNNs) for segmentation and classification of lesions in mammographic images. In this study our main objective is to assess the ability of CNNs in detecting contour irregularities in breast lesions in US images. METHODS: In this study we compare the performance of two CNNs with Direct Acyclic Graph (DAG) architecture and one CNN with a series architecture for breast lesion segmentation in US images. DAG and series architectures are both feedforward networks. The difference is that a DAG architecture could have more than one path between the first layer and end layer, whereas a series architecture has only one path from the beginning layer to the end layer. The CNN architectures were evaluated with two datasets. RESULTS: With the more complex DAG architecture, the following mean values were obtained for the metrics used to evaluate the segmented contours: global accuracy: 0.956; IOU: 0.876; F measure: 68.77%; Dice coefficient: 0.892. CONCLUSION: The CNN DAG architecture shows the best metric values used for quantitatively evaluating the segmented contours compared with the gold-standard contours. The segmented contours obtained with this architecture also have more details and irregularities, like the gold-standard contours."
1,A dynamic neural network model for predicting risk of Zika in real time,"BACKGROUND: In 2015, the Zika virus spread from Brazil throughout the Americas, posing an unprecedented challenge to the public health community. During the epidemic, international public health officials lacked reliable predictions of the outbreak's expected geographic scale and prevalence of cases, and were therefore unable to plan and allocate surveillance resources in a timely and effective manner. METHODS: In this work, we present a dynamic neural network model to predict the geographic spread of outbreaks in real time. The modeling framework is flexible in three main dimensions (i) selection of the chosen risk indicator, i.e., case counts or incidence rate; (ii) risk classification scheme, which defines the high-risk group based on a relative or absolute threshold; and (iii) prediction forecast window (1 up to 12 weeks). The proposed model can be applied dynamically throughout the course of an outbreak to identify the regions expected to be at greatest risk in the future. RESULTS: The model is applied to the recent Zika epidemic in the Americas at a weekly temporal resolution and country spatial resolution, using epidemiological data, passenger air travel volumes, and vector habitat suitability, socioeconomic, and population data for all affected countries and territories in the Americas. The model performance is quantitatively evaluated based on the predictive accuracy of the model. We show that the model can accurately predict the geographic expansion of Zika in the Americas with the overall average accuracy remaining above 85% even for prediction windows of up to 12 weeks. CONCLUSIONS: Sensitivity analysis illustrated the model performance to be robust across a range of features. Critically, the model performed consistently well at various stages throughout the course of the outbreak, indicating its potential value at any time during an epidemic. The predictive capability was superior for shorter forecast windows and geographically isolated locations that are predominantly connected via air travel. The highly flexible nature of the proposed modeling framework enables policy makers to develop and plan vector control programs and case surveillance strategies which can be tailored to a range of objectives and resource constraints.","A dynamic neural network model for predicting risk of Zika in real time. BACKGROUND: In 2015, the Zika virus spread from Brazil throughout the Americas, posing an unprecedented challenge to the public health community. During the epidemic, international public health officials lacked reliable predictions of the outbreak's expected geographic scale and prevalence of cases, and were therefore unable to plan and allocate surveillance resources in a timely and effective manner. METHODS: In this work, we present a dynamic neural network model to predict the geographic spread of outbreaks in real time. The modeling framework is flexible in three main dimensions (i) selection of the chosen risk indicator, i.e., case counts or incidence rate; (ii) risk classification scheme, which defines the high-risk group based on a relative or absolute threshold; and (iii) prediction forecast window (1 up to 12 weeks). The proposed model can be applied dynamically throughout the course of an outbreak to identify the regions expected to be at greatest risk in the future. RESULTS: The model is applied to the recent Zika epidemic in the Americas at a weekly temporal resolution and country spatial resolution, using epidemiological data, passenger air travel volumes, and vector habitat suitability, socioeconomic, and population data for all affected countries and territories in the Americas. The model performance is quantitatively evaluated based on the predictive accuracy of the model. We show that the model can accurately predict the geographic expansion of Zika in the Americas with the overall average accuracy remaining above 85% even for prediction windows of up to 12 weeks. CONCLUSIONS: Sensitivity analysis illustrated the model performance to be robust across a range of features. Critically, the model performed consistently well at various stages throughout the course of the outbreak, indicating its potential value at any time during an epidemic. The predictive capability was superior for shorter forecast windows and geographically isolated locations that are predominantly connected via air travel. The highly flexible nature of the proposed modeling framework enables policy makers to develop and plan vector control programs and case surveillance strategies which can be tailored to a range of objectives and resource constraints."
1,Deep learning-based prescription of cardiac MRI planes,"Purpose: To develop and evaluate a system to prescribe imaging planes for cardiac MRI based on deep learning (DL)âˆ’based localization of key anatomic landmarks. Materials and Methods: Annotated landmarks on 892 long-axis (LAX) and 493 short-axis (SAX) cine steady-state free precession series from cardiac MR images were retrospectively collected between February 2012 and June 2017. U-Net-based heatmap regression was used for localization of cardiac landmarks, which were used to compute cardiac MRI planes. Performance was evaluated by comparing localization distances and plane angle differences between DL predictions and ground truth. The plane angulations from DL were compared with those prescribed by the technologist at the original time of acquisition. Data were split into 80% for training and 20% for testing, and results confirmed with fivefold cross-validation. Results: On LAX images, DL localized the apex within mean 12.56 mm Â± 19.11 (standard deviation) and the mitral valve (MV) within 7.68 mm Â± 6.91. On SAX images, DL localized the aortic valve within 5.78 mm Â± 5.68, MV within 5.90 mm Â± 5.24, pulmonary valve within 6.55 mm Â± 6.39, and tricuspid valve within 6.39 mm Â± 5.89. On the basis of these localizations, average angle bias and mean error of DL-predicted imaging planes relative to ground truth annotations were as follows: SAX, âˆ’1.27Â° Â± 6.81 and 4.93Â° Â± 4.86; four chambers, 0.38Â° Â± 6.45 and 5.16Â° Â± 3.80; three chambers, 0.13Â° Â± 12.70 and 9.02Â° Â± 8.83; and two chamber, 0.25Â° Â± 9.08 and 6.53Â° Â± 6.28, respectively. Conclusion: DL-based anatomic localization is a feasible strategy for planning cardiac MRI planes. This approach can produce imaging planes comparable to those defined by ground truth landmarks.","Deep learning-based prescription of cardiac MRI planes. Purpose: To develop and evaluate a system to prescribe imaging planes for cardiac MRI based on deep learning (DL)âˆ’based localization of key anatomic landmarks. Materials and Methods: Annotated landmarks on 892 long-axis (LAX) and 493 short-axis (SAX) cine steady-state free precession series from cardiac MR images were retrospectively collected between February 2012 and June 2017. U-Net-based heatmap regression was used for localization of cardiac landmarks, which were used to compute cardiac MRI planes. Performance was evaluated by comparing localization distances and plane angle differences between DL predictions and ground truth. The plane angulations from DL were compared with those prescribed by the technologist at the original time of acquisition. Data were split into 80% for training and 20% for testing, and results confirmed with fivefold cross-validation. Results: On LAX images, DL localized the apex within mean 12.56 mm Â± 19.11 (standard deviation) and the mitral valve (MV) within 7.68 mm Â± 6.91. On SAX images, DL localized the aortic valve within 5.78 mm Â± 5.68, MV within 5.90 mm Â± 5.24, pulmonary valve within 6.55 mm Â± 6.39, and tricuspid valve within 6.39 mm Â± 5.89. On the basis of these localizations, average angle bias and mean error of DL-predicted imaging planes relative to ground truth annotations were as follows: SAX, âˆ’1.27Â° Â± 6.81 and 4.93Â° Â± 4.86; four chambers, 0.38Â° Â± 6.45 and 5.16Â° Â± 3.80; three chambers, 0.13Â° Â± 12.70 and 9.02Â° Â± 8.83; and two chamber, 0.25Â° Â± 9.08 and 6.53Â° Â± 6.28, respectively. Conclusion: DL-based anatomic localization is a feasible strategy for planning cardiac MRI planes. This approach can produce imaging planes comparable to those defined by ground truth landmarks."
1,Assessment of Machine Learning Detection of Environmental Enteropathy and Celiac Disease in Children,"Importance: Duodenal biopsies from children with enteropathies associated with undernutrition, such as environmental enteropathy (EE) and celiac disease (CD), display significant histopathological overlap. Objective: To develop a convolutional neural network (CNN) to enhance the detection of pathologic morphological features in diseased vs healthy duodenal tissue. Design, Setting, and Participants: In this prospective diagnostic study, a CNN consisting of 4 convolutions, 1 fully connected layer, and 1 softmax layer was trained on duodenal biopsy images. Data were provided by 3 sites: Aga Khan University Hospital, Karachi, Pakistan; University Teaching Hospital, Lusaka, Zambia; and University of Virginia, Charlottesville. Duodenal biopsy slides from 102 children (10 with EE from Aga Khan University Hospital, 16 with EE from University Teaching Hospital, 34 with CD from University of Virginia, and 42 with no disease from University of Virginia) were converted into 3118 images. The CNN was designed and analyzed at the University of Virginia. The data were collected, prepared, and analyzed between November 2017 and February 2018. Main Outcomes and Measures: Classification accuracy of the CNN per image and per case and incorrect classification rate identified by aggregated 10-fold cross-validation confusion/error matrices of CNN models. Results: Overall, 102 children participated in this study, with a median (interquartile range) age of 31.0 (20.3-75.5) months and a roughly equal sex distribution, with 53 boys (51.9%). The model demonstrated 93.4% case-detection accuracy and had a false-negative rate of 2.4%. Confusion metrics indicated most incorrect classifications were between patients with CD and healthy patients. Feature map activations were visualized and learned distinctive patterns, including microlevel features in duodenal tissues, such as alterations in secretory cell populations. Conclusions and Relevance: A machine learning-based histopathological analysis model demonstrating 93.4% classification accuracy was developed for identifying and differentiating between duodenal biopsies from children with EE and CD. The combination of the CNN with a deconvolutional network enabled feature recognition and highlighted secretory cells' role in the model's ability to differentiate between these histologically similar diseases.","Assessment of Machine Learning Detection of Environmental Enteropathy and Celiac Disease in Children. Importance: Duodenal biopsies from children with enteropathies associated with undernutrition, such as environmental enteropathy (EE) and celiac disease (CD), display significant histopathological overlap. Objective: To develop a convolutional neural network (CNN) to enhance the detection of pathologic morphological features in diseased vs healthy duodenal tissue. Design, Setting, and Participants: In this prospective diagnostic study, a CNN consisting of 4 convolutions, 1 fully connected layer, and 1 softmax layer was trained on duodenal biopsy images. Data were provided by 3 sites: Aga Khan University Hospital, Karachi, Pakistan; University Teaching Hospital, Lusaka, Zambia; and University of Virginia, Charlottesville. Duodenal biopsy slides from 102 children (10 with EE from Aga Khan University Hospital, 16 with EE from University Teaching Hospital, 34 with CD from University of Virginia, and 42 with no disease from University of Virginia) were converted into 3118 images. The CNN was designed and analyzed at the University of Virginia. The data were collected, prepared, and analyzed between November 2017 and February 2018. Main Outcomes and Measures: Classification accuracy of the CNN per image and per case and incorrect classification rate identified by aggregated 10-fold cross-validation confusion/error matrices of CNN models. Results: Overall, 102 children participated in this study, with a median (interquartile range) age of 31.0 (20.3-75.5) months and a roughly equal sex distribution, with 53 boys (51.9%). The model demonstrated 93.4% case-detection accuracy and had a false-negative rate of 2.4%. Confusion metrics indicated most incorrect classifications were between patients with CD and healthy patients. Feature map activations were visualized and learned distinctive patterns, including microlevel features in duodenal tissues, such as alterations in secretory cell populations. Conclusions and Relevance: A machine learning-based histopathological analysis model demonstrating 93.4% classification accuracy was developed for identifying and differentiating between duodenal biopsies from children with EE and CD. The combination of the CNN with a deconvolutional network enabled feature recognition and highlighted secretory cells' role in the model's ability to differentiate between these histologically similar diseases."
1,Automatic brain labeling via multi-atlas guided fully convolutional networks,"Multi-atlas-based methods are commonly used for MR brain image labeling, which alleviates the burdening and time-consuming task of manual labeling in neuroimaging analysis studies. Traditionally, multi-atlas-based methods first register multiple atlases to the target image, and then propagate the labels from the labeled atlases to the unlabeled target image. However, the registration step involves non-rigid alignment, which is often time-consuming and might lack high accuracy. Alternatively, patch-based methods have shown promise in relaxing the demand for accurate registration, but they often require the use of hand-crafted features. Recently, deep learning techniques have demonstrated their effectiveness in image labeling, by automatically learning comprehensive appearance features from training images. In this paper, we propose a multi-atlas guided fully convolutional network (MA-FCN) for automatic image labeling, which aims at further improving the labeling performance with the aid of prior knowledge from the training atlases. Specifically, we train our MA-FCN model in a patch-based manner, where the input data consists of not only a training image patch but also a set of its neighboring (i.e., most similar) affine-aligned atlas patches. The guidance information from neighboring atlas patches can help boost the discriminative ability of the learned FCN. Experimental results on different datasets demonstrate the effectiveness of our proposed method, by significantly outperforming the conventional FCN and several state-of-the-art MR brain labeling methods.","Automatic brain labeling via multi-atlas guided fully convolutional networks. Multi-atlas-based methods are commonly used for MR brain image labeling, which alleviates the burdening and time-consuming task of manual labeling in neuroimaging analysis studies. Traditionally, multi-atlas-based methods first register multiple atlases to the target image, and then propagate the labels from the labeled atlases to the unlabeled target image. However, the registration step involves non-rigid alignment, which is often time-consuming and might lack high accuracy. Alternatively, patch-based methods have shown promise in relaxing the demand for accurate registration, but they often require the use of hand-crafted features. Recently, deep learning techniques have demonstrated their effectiveness in image labeling, by automatically learning comprehensive appearance features from training images. In this paper, we propose a multi-atlas guided fully convolutional network (MA-FCN) for automatic image labeling, which aims at further improving the labeling performance with the aid of prior knowledge from the training atlases. Specifically, we train our MA-FCN model in a patch-based manner, where the input data consists of not only a training image patch but also a set of its neighboring (i.e., most similar) affine-aligned atlas patches. The guidance information from neighboring atlas patches can help boost the discriminative ability of the learned FCN. Experimental results on different datasets demonstrate the effectiveness of our proposed method, by significantly outperforming the conventional FCN and several state-of-the-art MR brain labeling methods."
1,Quantifying Sex Bias in Clinical Studies at Scale with Automated Data Extraction,"Importance: Analyses of female representation in clinical studies have been limited in scope and scale. Objective: To perform a large-scale analysis of global enrollment sex bias in clinical studies. Design, Setting, and Participants: In this cross-sectional study, clinical studies from published articles from PubMed from 1966 to 2018 and records from Aggregate Analysis of ClinicalTrials.gov from 1999 to 2018 were identified. Global disease prevalence was determined for male and female patients in 11 disease categories from the Global Burden of Disease database: cardiovascular, diabetes, digestive, hepatitis (types A, B, C, and E), HIV/AIDS, kidney (chronic), mental, musculoskeletal, neoplasms, neurological, and respiratory (chronic). Machine reading algorithms were developed that extracted sex data from tables in articles and records on December 31, 2018, at an artificial intelligence research institute. Male and female participants in 43135 articles (792004915 participants) and 13165 records (12977103 participants) were included. Main Outcomes and Measures: Sex bias was defined as the difference between the fraction of female participants in study participants minus prevalence fraction of female participants for each disease category. A total of 1000 bootstrap estimates of sex bias were computed by resampling individual studies with replacement. Sex bias was reported as mean and 95% bootstrap confidence intervals from articles and records in each disease category over time (before or during 1993 to 2018), with studies or participants as the measurement unit. Results: There were 792004915 participants, including 390470834 female participants (49%), in articles and 12977103 participants, including 6351619 female participants (49%), in records. With studies as measurement unit, substantial female underrepresentation (sex bias â‰¤ -0.05) was observed in 7 of 11 disease categories, especially HIV/AIDS (mean for articles, -0.17 [95% CI, -0.18 to -0.16]), chronic kidney diseases (mean, -0.17 [95% CI, -0.17 to -0.16]), and cardiovascular diseases (mean, -0.14 [95% CI, -0.14 to -0.13]). Sex bias in articles for all categories combined was unchanged over time with studies as measurement unit (range, -0.15 [95% CI, -0.16 to -0.13] to -0.10 [95% CI, -0.14 to -0.06]), but improved from before or during 1993 (mean, -0.11 [95% CI, -0.16 to -0.05]) to 2014 to 2018 (mean, -0.05 [95% CI, -0.09 to -0.02]) with participants as the measurement unit. Larger study size was associated with greater female representation. Conclusions and Relevance: Automated extraction of the number of participants in clinical reports provides an effective alternative to manual analysis of demographic bias. Despite legal and policy initiatives to increase female representation, sex bias against female participants in clinical studies persists. Studies with more participants have greater female representation. Differences between sex bias estimates with studies vs participants as measurement unit, and between articles vs records, suggest that sex bias with both measures and data sources should be reported.","Quantifying Sex Bias in Clinical Studies at Scale with Automated Data Extraction. Importance: Analyses of female representation in clinical studies have been limited in scope and scale. Objective: To perform a large-scale analysis of global enrollment sex bias in clinical studies. Design, Setting, and Participants: In this cross-sectional study, clinical studies from published articles from PubMed from 1966 to 2018 and records from Aggregate Analysis of ClinicalTrials.gov from 1999 to 2018 were identified. Global disease prevalence was determined for male and female patients in 11 disease categories from the Global Burden of Disease database: cardiovascular, diabetes, digestive, hepatitis (types A, B, C, and E), HIV/AIDS, kidney (chronic), mental, musculoskeletal, neoplasms, neurological, and respiratory (chronic). Machine reading algorithms were developed that extracted sex data from tables in articles and records on December 31, 2018, at an artificial intelligence research institute. Male and female participants in 43135 articles (792004915 participants) and 13165 records (12977103 participants) were included. Main Outcomes and Measures: Sex bias was defined as the difference between the fraction of female participants in study participants minus prevalence fraction of female participants for each disease category. A total of 1000 bootstrap estimates of sex bias were computed by resampling individual studies with replacement. Sex bias was reported as mean and 95% bootstrap confidence intervals from articles and records in each disease category over time (before or during 1993 to 2018), with studies or participants as the measurement unit. Results: There were 792004915 participants, including 390470834 female participants (49%), in articles and 12977103 participants, including 6351619 female participants (49%), in records. With studies as measurement unit, substantial female underrepresentation (sex bias â‰¤ -0.05) was observed in 7 of 11 disease categories, especially HIV/AIDS (mean for articles, -0.17 [95% CI, -0.18 to -0.16]), chronic kidney diseases (mean, -0.17 [95% CI, -0.17 to -0.16]), and cardiovascular diseases (mean, -0.14 [95% CI, -0.14 to -0.13]). Sex bias in articles for all categories combined was unchanged over time with studies as measurement unit (range, -0.15 [95% CI, -0.16 to -0.13] to -0.10 [95% CI, -0.14 to -0.06]), but improved from before or during 1993 (mean, -0.11 [95% CI, -0.16 to -0.05]) to 2014 to 2018 (mean, -0.05 [95% CI, -0.09 to -0.02]) with participants as the measurement unit. Larger study size was associated with greater female representation. Conclusions and Relevance: Automated extraction of the number of participants in clinical reports provides an effective alternative to manual analysis of demographic bias. Despite legal and policy initiatives to increase female representation, sex bias against female participants in clinical studies persists. Studies with more participants have greater female representation. Differences between sex bias estimates with studies vs participants as measurement unit, and between articles vs records, suggest that sex bias with both measures and data sources should be reported."
1,Predicting factors for survival of breast cancer patients using machine learning techniques,"BACKGROUND: Breast cancer is one of the most common diseases in women worldwide. Many studies have been conducted to predict the survival indicators, however most of these analyses were predominantly performed using basic statistical methods. As an alternative, this study used machine learning techniques to build models for detecting and visualising significant prognostic indicators of breast cancer survival rate. METHODS: A large hospital-based breast cancer dataset retrieved from the University Malaya Medical Centre, Kuala Lumpur, Malaysia (nâ€‰=â€‰8066) with diagnosis information between 1993 and 2016 was used in this study. The dataset contained 23 predictor variables and one dependent variable, which referred to the survival status of the patients (alive or dead). In determining the significant prognostic factors of breast cancer survival rate, prediction models were built using decision tree, random forest, neural networks, extreme boost, logistic regression, and support vector machine. Next, the dataset was clustered based on the receptor status of breast cancer patients identified via immunohistochemistry to perform advanced modelling using random forest. Subsequently, the important variables were ranked via variable selection methods in random forest. Finally, decision trees were built and validation was performed using survival analysis. RESULTS: In terms of both model accuracy and calibration measure, all algorithms produced close outcomes, with the lowest obtained from decision tree (accuracyâ€‰=â€‰79.8%) and the highest from random forest (accuracyâ€‰=â€‰82.7%). The important variables identified in this study were cancer stage classification, tumour size, number of total axillary lymph nodes removed, number of positive lymph nodes, types of primary treatment, and methods of diagnosis. CONCLUSION: Interestingly the various machine learning algorithms used in this study yielded close accuracy hence these methods could be used as alternative predictive tools in the breast cancer survival studies, particularly in the Asian region. The important prognostic factors influencing survival rate of breast cancer identified in this study, which were validated by survival curves, are useful and could be translated into decision support tools in the medical domain.","Predicting factors for survival of breast cancer patients using machine learning techniques. BACKGROUND: Breast cancer is one of the most common diseases in women worldwide. Many studies have been conducted to predict the survival indicators, however most of these analyses were predominantly performed using basic statistical methods. As an alternative, this study used machine learning techniques to build models for detecting and visualising significant prognostic indicators of breast cancer survival rate. METHODS: A large hospital-based breast cancer dataset retrieved from the University Malaya Medical Centre, Kuala Lumpur, Malaysia (nâ€‰=â€‰8066) with diagnosis information between 1993 and 2016 was used in this study. The dataset contained 23 predictor variables and one dependent variable, which referred to the survival status of the patients (alive or dead). In determining the significant prognostic factors of breast cancer survival rate, prediction models were built using decision tree, random forest, neural networks, extreme boost, logistic regression, and support vector machine. Next, the dataset was clustered based on the receptor status of breast cancer patients identified via immunohistochemistry to perform advanced modelling using random forest. Subsequently, the important variables were ranked via variable selection methods in random forest. Finally, decision trees were built and validation was performed using survival analysis. RESULTS: In terms of both model accuracy and calibration measure, all algorithms produced close outcomes, with the lowest obtained from decision tree (accuracyâ€‰=â€‰79.8%) and the highest from random forest (accuracyâ€‰=â€‰82.7%). The important variables identified in this study were cancer stage classification, tumour size, number of total axillary lymph nodes removed, number of positive lymph nodes, types of primary treatment, and methods of diagnosis. CONCLUSION: Interestingly the various machine learning algorithms used in this study yielded close accuracy hence these methods could be used as alternative predictive tools in the breast cancer survival studies, particularly in the Asian region. The important prognostic factors influencing survival rate of breast cancer identified in this study, which were validated by survival curves, are useful and could be translated into decision support tools in the medical domain."
1,Prediction of recurrent venous thrombosis in all patients with a first venous thrombotic event: The Leiden Thrombosis Recurrence Risk Prediction model (L-TRRiP),"BACKGROUND: Recurrent venous thromboembolism (VTE) is common. Current guidelines suggest that patients with unprovoked VTE should continue anticoagulants unless they have a high bleeding risk, whereas all others can stop. Prediction models may refine this dichotomous distinction, but existing models apply only to patients with unprovoked first thrombosis. We aimed to develop a prediction model for all patients with first VTE, either provoked or unprovoked. METHODS AND FINDINGS: Data were used from two population-based cohorts of patients with first VTE from the Netherlands (Multiple Environment and Genetic Assessment of Risk Factors for Venous Thrombosis [MEGA] follow-up study, performed from 1994 to 2009; model derivation; n = 3,750) and from Norway (Tromso study, performed from 1999 to 2016; model validation; n = 663). Four versions of a VTE prediction model were developed: model A (clinical, laboratory, and genetic variables), model B (clinical variables and fewer laboratory markers), model C (clinical and genetic factors), and model D (clinical variables only). The outcome measure was recurrent VTE. To determine the discriminatory power, Harrell's C-statistic was calculated. A prognostic score was assessed for each patient. Kaplan-Meier plots for the observed recurrence risks were created in quintiles of the prognostic scores. For each patient, the 2-year predicted recurrence risk was calculated. Models C and D were validated in the Tromso study. During 19,201 person-years of follow-up (median duration 5.7 years) in the MEGA study, 507 recurrences occurred. Model A had the highest predictive capability, with a C-statistic of 0.73 (95% CI 0.71-0.76). The discriminative performance was somewhat lower in the other models, with C-statistics of 0.72 for model B, 0.70 for model C, and 0.69 for model D. Internal validation showed a minimal degree of optimism bias. Models C and D were externally validated, with C-statistics of 0.64 (95% CI 0.62-0.66) and 0.65 (95% CI 0.63-0.66), respectively. According to model C, in 2,592 patients with provoked first events, 367 (15%) patients had a predicted 2-year risk of >10%, whereas in 1,082 patients whose first event was unprovoked, 484 (45%) had a predicted 2-year risk of <10%. A limitation of both cohorts is that laboratory measurements were missing in a substantial proportion of patients, which therefore were imputed. CONCLUSIONS: The prediction model we propose applies to patients with provoked or unprovoked first VTE-except for patients with (a history of) cancer-allows refined risk stratification, and is easily usable. For optimal individualized treatment, a management study in which bleeding risks are also taken into account is necessary.","Prediction of recurrent venous thrombosis in all patients with a first venous thrombotic event: The Leiden Thrombosis Recurrence Risk Prediction model (L-TRRiP). BACKGROUND: Recurrent venous thromboembolism (VTE) is common. Current guidelines suggest that patients with unprovoked VTE should continue anticoagulants unless they have a high bleeding risk, whereas all others can stop. Prediction models may refine this dichotomous distinction, but existing models apply only to patients with unprovoked first thrombosis. We aimed to develop a prediction model for all patients with first VTE, either provoked or unprovoked. METHODS AND FINDINGS: Data were used from two population-based cohorts of patients with first VTE from the Netherlands (Multiple Environment and Genetic Assessment of Risk Factors for Venous Thrombosis [MEGA] follow-up study, performed from 1994 to 2009; model derivation; n = 3,750) and from Norway (Tromso study, performed from 1999 to 2016; model validation; n = 663). Four versions of a VTE prediction model were developed: model A (clinical, laboratory, and genetic variables), model B (clinical variables and fewer laboratory markers), model C (clinical and genetic factors), and model D (clinical variables only). The outcome measure was recurrent VTE. To determine the discriminatory power, Harrell's C-statistic was calculated. A prognostic score was assessed for each patient. Kaplan-Meier plots for the observed recurrence risks were created in quintiles of the prognostic scores. For each patient, the 2-year predicted recurrence risk was calculated. Models C and D were validated in the Tromso study. During 19,201 person-years of follow-up (median duration 5.7 years) in the MEGA study, 507 recurrences occurred. Model A had the highest predictive capability, with a C-statistic of 0.73 (95% CI 0.71-0.76). The discriminative performance was somewhat lower in the other models, with C-statistics of 0.72 for model B, 0.70 for model C, and 0.69 for model D. Internal validation showed a minimal degree of optimism bias. Models C and D were externally validated, with C-statistics of 0.64 (95% CI 0.62-0.66) and 0.65 (95% CI 0.63-0.66), respectively. According to model C, in 2,592 patients with provoked first events, 367 (15%) patients had a predicted 2-year risk of >10%, whereas in 1,082 patients whose first event was unprovoked, 484 (45%) had a predicted 2-year risk of <10%. A limitation of both cohorts is that laboratory measurements were missing in a substantial proportion of patients, which therefore were imputed. CONCLUSIONS: The prediction model we propose applies to patients with provoked or unprovoked first VTE-except for patients with (a history of) cancer-allows refined risk stratification, and is easily usable. For optimal individualized treatment, a management study in which bleeding risks are also taken into account is necessary."
1,Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images,,
1,Fully automated CT quantification of Epicardial adipose tissue by deep learning: A multicenter study,"Purpose: To evaluate the performance of deep learning for robust and fully automated quantification of epicardial adipose tissue (EAT) from multicenter cardiac CT data. Materials and Methods: In this multicenter study, a convolutional neural network approach was trained to quantify EAT on non-contrast material-enhanced calcium-scoring CT scans from multiple cohorts, scanners, and protocols (n = 850). Deep learning performance was compared with the performance of three expert readers and with interobserver variability in a subset of 141 scans. The deep learning algorithm was incorporated into research software. Automated EAT progression was compared with expert measurements for 70 patients with baseline and follow-up scans. Results: Automated quantification was performed in a mean (6 standard deviation) time of 1.57 seconds 6 0.49, compared with 15 minutes for experts. Deep learning provided high agreement with expert manual quantification for all scans (R = 0.974; P, 001), with no significant bias (0.53 cm3; P =. 13). Manual EAT volumes measured by two experienced readers were highly correlated (R = 0.984; P, 001) but with a bias of 4.35 cm3 (P, 001). Deep learning quantifications were highly correlated with the measurements of both experts (R = 0.973 and R = 0.979; P, 001), with significant bias for reader 1 (5.11 cm3; P, 001) but not for reader 2 (0.88 cm3; P =. 26). EAT progression by deep learning correlated strongly with manual EAT progression (R = 0.905; P, 001) in 70 patients, with no significant bias (0.64 cm3; P =. 43), and was related to an increased noncalcified plaque burden quantified from coronary CT angiography (5.7% vs 1.8%; P =. 026). Conclusion: Deep learning allows rapid, robust, and fully automated quantification of EAT from calcium scoring CT. It performs as well as an expert reader and can be implemented for routine cardiovascular risk assessment.","Fully automated CT quantification of Epicardial adipose tissue by deep learning: A multicenter study. Purpose: To evaluate the performance of deep learning for robust and fully automated quantification of epicardial adipose tissue (EAT) from multicenter cardiac CT data. Materials and Methods: In this multicenter study, a convolutional neural network approach was trained to quantify EAT on non-contrast material-enhanced calcium-scoring CT scans from multiple cohorts, scanners, and protocols (n = 850). Deep learning performance was compared with the performance of three expert readers and with interobserver variability in a subset of 141 scans. The deep learning algorithm was incorporated into research software. Automated EAT progression was compared with expert measurements for 70 patients with baseline and follow-up scans. Results: Automated quantification was performed in a mean (6 standard deviation) time of 1.57 seconds 6 0.49, compared with 15 minutes for experts. Deep learning provided high agreement with expert manual quantification for all scans (R = 0.974; P, 001), with no significant bias (0.53 cm3; P =. 13). Manual EAT volumes measured by two experienced readers were highly correlated (R = 0.984; P, 001) but with a bias of 4.35 cm3 (P, 001). Deep learning quantifications were highly correlated with the measurements of both experts (R = 0.973 and R = 0.979; P, 001), with significant bias for reader 1 (5.11 cm3; P, 001) but not for reader 2 (0.88 cm3; P =. 26). EAT progression by deep learning correlated strongly with manual EAT progression (R = 0.905; P, 001) in 70 patients, with no significant bias (0.64 cm3; P =. 43), and was related to an increased noncalcified plaque burden quantified from coronary CT angiography (5.7% vs 1.8%; P =. 026). Conclusion: Deep learning allows rapid, robust, and fully automated quantification of EAT from calcium scoring CT. It performs as well as an expert reader and can be implemented for routine cardiovascular risk assessment."
1,Holistic decomposition convolution for effective semantic segmentation of medical volume images,,
1,Fusing learned representations from Riesz Filters and Deep CNN for lung tissue classification,,
1,Novel automated non invasive detection of ocular surface squamous neoplasia using multispectral autofluorescence imaging,,
1,Automatic needle detection and real-time Bi-planar needle visualization during 3D ultrasound scanning of the liver,"2D ultrasound (US) image guidance is used in minimally invasive procedures in the liver to visualize the target and the needle. Needle insertion using 2D ultrasound keeping the transducer position to view needle and reach target is challenging. Dedicated needle holders attached to the US transducer help to target in plane and at a specific angle. A drawback of this is that, the probe is fixed to the needle and cannot be rotated to assess the position of the needle in a perpendicular plane. In this study, we propose an automatic needle detection and tracking method using 3D US imaging to improve image guidance and visualization of the target in the liver with respect to the needle during these interventional procedures. The method utilizes a convolutional neural network for detection of the needle in 3D US images. In a subsequent step, the output of the convolutional neural network is used to detect needle candidates, which are fed into a final tracking step to determine the real needle position. The needle position is used to present two perpendicular cross-sectional planes of the 3D US image containing the needle in both directions. Performance of the method was evaluated in phantoms and in-vivo data by calculating the needle position distance and needle orientation angle between segmented needles and reference ground truth needles, which were manually annotated by an observer. The method successfully detects the needle position and orientation with mean errors of 1mm and 2 degrees , respectively. The proposed method yields a robust automatic needle detection and visualization at a frame rate of 3Hz in 3D ultrasound imaging of the liver.","Automatic needle detection and real-time Bi-planar needle visualization during 3D ultrasound scanning of the liver. 2D ultrasound (US) image guidance is used in minimally invasive procedures in the liver to visualize the target and the needle. Needle insertion using 2D ultrasound keeping the transducer position to view needle and reach target is challenging. Dedicated needle holders attached to the US transducer help to target in plane and at a specific angle. A drawback of this is that, the probe is fixed to the needle and cannot be rotated to assess the position of the needle in a perpendicular plane. In this study, we propose an automatic needle detection and tracking method using 3D US imaging to improve image guidance and visualization of the target in the liver with respect to the needle during these interventional procedures. The method utilizes a convolutional neural network for detection of the needle in 3D US images. In a subsequent step, the output of the convolutional neural network is used to detect needle candidates, which are fed into a final tracking step to determine the real needle position. The needle position is used to present two perpendicular cross-sectional planes of the 3D US image containing the needle in both directions. Performance of the method was evaluated in phantoms and in-vivo data by calculating the needle position distance and needle orientation angle between segmented needles and reference ground truth needles, which were manually annotated by an observer. The method successfully detects the needle position and orientation with mean errors of 1mm and 2 degrees , respectively. The proposed method yields a robust automatic needle detection and visualization at a frame rate of 3Hz in 3D ultrasound imaging of the liver."
1,Using a machine learning algorithm to predict acute graft-versus-host disease following allogeneic transplantation,"Acute graft-versus-host disease (aGVHD) is 1 of the critical complications that often occurs following allogeneic hematopoietic stem cell transplantation (HSCT). Thus far, various types of prediction scores have been created using statistical calculations. The primary objective of this study was to establish and validate the machine learningâ€“dependent index for predicting aGVHD. This was a retrospective cohort study that involved analyzing databases of adult HSCT patients in Japan. The alternating decision tree (ADTree) machine learning algorithm was applied to develop models using the training cohort (70%). The ADTree algorithm was confirmed using the hazard model on data from the validation cohort (30%). Data from 26 695 HSCT patients transplanted from allogeneic donors between 1992 and 2016 were included in this study. The cumulative incidence of aGVHD was 42.8%. Of >40 variables considered, 15 were adapted into a model for aGVHD prediction. The model was tested in the validation cohort, and the incidence of aGVHD was clearly stratified according to the categorized ADTree scores; the cumulative incidence of aGVHD was 29.0% for low risk and 58.7% for high risk (hazard ratio, 2.57). Predicting scores for aGVHD also demonstrated the link between the risk of development aGVHD and overall survival after HSCT. The machine learning algorithms produced clinically reasonable and robust risk stratification scores. The relatively high reproducibility and low impacts from the interactions among the variables indicate that the ADTree algorithm, along with the other data-mining approaches, may provide tools for establishing risk score.","Using a machine learning algorithm to predict acute graft-versus-host disease following allogeneic transplantation. Acute graft-versus-host disease (aGVHD) is 1 of the critical complications that often occurs following allogeneic hematopoietic stem cell transplantation (HSCT). Thus far, various types of prediction scores have been created using statistical calculations. The primary objective of this study was to establish and validate the machine learningâ€“dependent index for predicting aGVHD. This was a retrospective cohort study that involved analyzing databases of adult HSCT patients in Japan. The alternating decision tree (ADTree) machine learning algorithm was applied to develop models using the training cohort (70%). The ADTree algorithm was confirmed using the hazard model on data from the validation cohort (30%). Data from 26 695 HSCT patients transplanted from allogeneic donors between 1992 and 2016 were included in this study. The cumulative incidence of aGVHD was 42.8%. Of >40 variables considered, 15 were adapted into a model for aGVHD prediction. The model was tested in the validation cohort, and the incidence of aGVHD was clearly stratified according to the categorized ADTree scores; the cumulative incidence of aGVHD was 29.0% for low risk and 58.7% for high risk (hazard ratio, 2.57). Predicting scores for aGVHD also demonstrated the link between the risk of development aGVHD and overall survival after HSCT. The machine learning algorithms produced clinically reasonable and robust risk stratification scores. The relatively high reproducibility and low impacts from the interactions among the variables indicate that the ADTree algorithm, along with the other data-mining approaches, may provide tools for establishing risk score."
1,Development and evaluation of a computable phenotype to identify pediatric patients with leukemia and lymphoma treated with chemotherapy using electronic health record data,"Background: Widespread implementation of electronic health records (EHR) has created new opportunities for pediatric oncology observational research. Little attention has been given to using EHR data to identify patients with pediatric hematologic malignancies. Methods: This study used EHR-derived data in a pediatric clinical data research network, PEDSnet, to develop and evaluate a computable phenotype algorithm to identify pediatric patients with leukemia and lymphoma who received treatment with chemotherapy. To guide early development, multiple computable phenotype-defined cohorts were compared to one institution's tumor registry. The most promising algorithm was chosen for formal evaluation and consisted of at least two leukemia/lymphoma diagnoses (Systematized Nomenclature of Medicine codes) within a 90-day period, two chemotherapy exposures, and three hematology-oncology provider encounters. During evaluation, the computable phenotype was executed against EHR data from 2011 to 2016 at three large institutions. Classification accuracy was assessed by masked medical record review with phenotype-identified patients compared to a control group with at least three hematology-oncology encounters. Results: The computable phenotype had sensitivity of 100% (confidence interval [CI] 99%, 100%), specificity of 99% (CI 99%, 100%), positive predictive value (PPV) and negative predictive value (NPV) of 100%, and C-statistic of 1 at the development institution. The computable phenotype performance was similar at the two test institutions with sensitivity of 100% (CI 99%, 100%), specificity of 99% (CI 99%, 100%), PPV of 96%, NPV of 100%, and C-statistic of 0.99. Conclusion: The EHR-based computable phenotype is an accurate cohort identification tool for pediatric patients with leukemia and lymphoma who have been treated with chemotherapy and is ready for use in clinical studies.","Development and evaluation of a computable phenotype to identify pediatric patients with leukemia and lymphoma treated with chemotherapy using electronic health record data. Background: Widespread implementation of electronic health records (EHR) has created new opportunities for pediatric oncology observational research. Little attention has been given to using EHR data to identify patients with pediatric hematologic malignancies. Methods: This study used EHR-derived data in a pediatric clinical data research network, PEDSnet, to develop and evaluate a computable phenotype algorithm to identify pediatric patients with leukemia and lymphoma who received treatment with chemotherapy. To guide early development, multiple computable phenotype-defined cohorts were compared to one institution's tumor registry. The most promising algorithm was chosen for formal evaluation and consisted of at least two leukemia/lymphoma diagnoses (Systematized Nomenclature of Medicine codes) within a 90-day period, two chemotherapy exposures, and three hematology-oncology provider encounters. During evaluation, the computable phenotype was executed against EHR data from 2011 to 2016 at three large institutions. Classification accuracy was assessed by masked medical record review with phenotype-identified patients compared to a control group with at least three hematology-oncology encounters. Results: The computable phenotype had sensitivity of 100% (confidence interval [CI] 99%, 100%), specificity of 99% (CI 99%, 100%), positive predictive value (PPV) and negative predictive value (NPV) of 100%, and C-statistic of 1 at the development institution. The computable phenotype performance was similar at the two test institutions with sensitivity of 100% (CI 99%, 100%), specificity of 99% (CI 99%, 100%), PPV of 96%, NPV of 100%, and C-statistic of 0.99. Conclusion: The EHR-based computable phenotype is an accurate cohort identification tool for pediatric patients with leukemia and lymphoma who have been treated with chemotherapy and is ready for use in clinical studies."
1,An algorithm for learning shape and appearance models without annotations,"This paper presents a framework for automatically learning shape and appearance models for medical (and certain other) images. The algorithm was developed with the aim of eventually enabling distributed privacy-preserving analysis of brain image data, such that shared information (shape and appearance basis functions) may be passed across sites, whereas latent variables that encode individual images remain secure within each site. These latent variables are proposed as features for privacy-preserving data mining applications. The approach is demonstrated qualitatively on the KDEF dataset of 2D face images, showing that it can align images that traditionally require shape and appearance models trained using manually annotated data (manually defined landmarks etc.). It is applied to the MNIST dataset of handwritten digits to show its potential for machine learning applications, particularly when training data is limited. The model is able to handle ""missing data"", which allows it to be cross-validated according to how well it can predict left-out voxels. The suitability of the derived features for classifying individuals into patient groups was assessed by applying it to a dataset of over 1900 segmented T1-weighted MR images, which included images from the COBRE and ABIDE datasets.","An algorithm for learning shape and appearance models without annotations. This paper presents a framework for automatically learning shape and appearance models for medical (and certain other) images. The algorithm was developed with the aim of eventually enabling distributed privacy-preserving analysis of brain image data, such that shared information (shape and appearance basis functions) may be passed across sites, whereas latent variables that encode individual images remain secure within each site. These latent variables are proposed as features for privacy-preserving data mining applications. The approach is demonstrated qualitatively on the KDEF dataset of 2D face images, showing that it can align images that traditionally require shape and appearance models trained using manually annotated data (manually defined landmarks etc.). It is applied to the MNIST dataset of handwritten digits to show its potential for machine learning applications, particularly when training data is limited. The model is able to handle ""missing data"", which allows it to be cross-validated according to how well it can predict left-out voxels. The suitability of the derived features for classifying individuals into patient groups was assessed by applying it to a dataset of over 1900 segmented T1-weighted MR images, which included images from the COBRE and ABIDE datasets."
1,An image interpolation approach for acquisition time reduction in navigator-based 4D MRI,,
1,Accurate quantification of astrocyte and neurotransmitter fluorescence dynamics for single-cell and population-level physiology,"Recent work examining astrocytic physiology centers on fluorescence imaging, due to development of sensitive fluorescent indicators and observation of spatiotemporally complex calcium activity. However, the field remains hindered in characterizing these dynamics, both within single cells and at the population level, because of the insufficiency of current region-of-interest-based approaches to describe activity that is often spatially unfixed, size-varying and propagative. Here we present an analytical framework that releases astrocyte biologists from region-of-interest-based tools. The Astrocyte Quantitative Analysis (AQuA) software takes an event-based perspective to model and accurately quantify complex calcium and neurotransmitter activity in fluorescence imaging datasets. We apply AQuA to a range of ex vivo and in vivo imaging data and use physiologically relevant parameters to comprehensively describe the data. Since AQuA is data-driven and based on machine learning principles, it can be applied across model organisms, fluorescent indicators, experimental modes, and imaging resolutions and speeds, enabling researchers to elucidate fundamental neural physiology.","Accurate quantification of astrocyte and neurotransmitter fluorescence dynamics for single-cell and population-level physiology. Recent work examining astrocytic physiology centers on fluorescence imaging, due to development of sensitive fluorescent indicators and observation of spatiotemporally complex calcium activity. However, the field remains hindered in characterizing these dynamics, both within single cells and at the population level, because of the insufficiency of current region-of-interest-based approaches to describe activity that is often spatially unfixed, size-varying and propagative. Here we present an analytical framework that releases astrocyte biologists from region-of-interest-based tools. The Astrocyte Quantitative Analysis (AQuA) software takes an event-based perspective to model and accurately quantify complex calcium and neurotransmitter activity in fluorescence imaging datasets. We apply AQuA to a range of ex vivo and in vivo imaging data and use physiologically relevant parameters to comprehensively describe the data. Since AQuA is data-driven and based on machine learning principles, it can be applied across model organisms, fluorescent indicators, experimental modes, and imaging resolutions and speeds, enabling researchers to elucidate fundamental neural physiology."
1,Natural language processing of Reddit data to evaluate dermatology patient experiences and therapeutics,"BACKGROUND: There is a lack of research studying patient-generated data on Reddit, one of the world's most popular forums with active users interested in dermatology. Techniques within natural language processing, a field of artificial intelligence, can analyze large amounts of text information and extract insights. OBJECTIVE: To apply natural language processing to Reddit comments about dermatology topics to assess for feasibility and potential for insights and engagement. METHODS: A software pipeline preprocessed Reddit comments from 2005 to 2017 from 7 popular dermatology-related subforums on Reddit, applied latent Dirichlet allocation, and used spectral clustering to establish cohesive themes and the frequency of word representation and grouped terms within these topics. RESULTS: We created a corpus of 176,000 comments and identified trends in patient engagement in spaces such as eczema and acne, among others, with a focus on homeopathic treatments and isotretinoin. LIMITATIONS: Latent Dirichlet allocation is an unsupervised model, meaning there is no ground truth to which the model output can be compared. However, because these forums are anonymous, there seems little incentive for patients to be dishonest. CONCLUSIONS: Reddit data has viability and utility for dermatologic research and engagement with the public, especially for common dermatology topics such as tanning, acne, and psoriasis.","Natural language processing of Reddit data to evaluate dermatology patient experiences and therapeutics. BACKGROUND: There is a lack of research studying patient-generated data on Reddit, one of the world's most popular forums with active users interested in dermatology. Techniques within natural language processing, a field of artificial intelligence, can analyze large amounts of text information and extract insights. OBJECTIVE: To apply natural language processing to Reddit comments about dermatology topics to assess for feasibility and potential for insights and engagement. METHODS: A software pipeline preprocessed Reddit comments from 2005 to 2017 from 7 popular dermatology-related subforums on Reddit, applied latent Dirichlet allocation, and used spectral clustering to establish cohesive themes and the frequency of word representation and grouped terms within these topics. RESULTS: We created a corpus of 176,000 comments and identified trends in patient engagement in spaces such as eczema and acne, among others, with a focus on homeopathic treatments and isotretinoin. LIMITATIONS: Latent Dirichlet allocation is an unsupervised model, meaning there is no ground truth to which the model output can be compared. However, because these forums are anonymous, there seems little incentive for patients to be dishonest. CONCLUSIONS: Reddit data has viability and utility for dermatologic research and engagement with the public, especially for common dermatology topics such as tanning, acne, and psoriasis."
1,Deep Learning and Glaucoma Specialists: The Relative Importance of Optic Disc Features to Predict Glaucoma Referral in Fundus Photographs,"PURPOSE: To develop and validate a deep learning (DL) algorithm that predicts referable glaucomatous optic neuropathy (GON) and optic nerve head (ONH) features from color fundus images, to determine the relative importance of these features in referral decisions by glaucoma specialists (GSs) and the algorithm, and to compare the performance of the algorithm with eye care providers. DESIGN: Development and validation of an algorithm. PARTICIPANTS: Fundus images from screening programs, studies, and a glaucoma clinic. METHODS: A DL algorithm was trained using a retrospective dataset of 86 618 images, assessed for glaucomatous ONH features and referable GON (defined as ONH appearance worrisome enough to justify referral for comprehensive examination) by 43 graders. The algorithm was validated using 3 datasets: dataset A (1205 images, 1 image/patient; 18.1% referable), images adjudicated by panels of GSs; dataset B (9642 images, 1 image/patient; 9.2% referable), images from a diabetic teleretinal screening program; and dataset C (346 images, 1 image/patient; 81.7% referable), images from a glaucoma clinic. MAIN OUTCOME MEASURES: The algorithm was evaluated using the area under the receiver operating characteristic curve (AUC), sensitivity, and specificity for referable GON and glaucomatous ONH features. RESULTS: The algorithm's AUC for referable GON was 0.945 (95% confidence interval [CI], 0.929-0.960) in dataset A, 0.855 (95% CI, 0.841-0.870) in dataset B, and 0.881 (95% CI, 0.838-0.918) in dataset C. Algorithm AUCs ranged between 0.661 and 0.973 for glaucomatous ONH features. The algorithm showed significantly higher sensitivity than 7 of 10 graders not involved in determining the reference standard, including 2 of 3 GSs, and showed higher specificity than 3 graders (including 1 GS), while remaining comparable to others. For both GSs and the algorithm, the most crucial features related to referable GON were: presence of vertical cup-to-disc ratio of 0.7 or more, neuroretinal rim notching, retinal nerve fiber layer defect, and bared circumlinear vessels. CONCLUSIONS: A DL algorithm trained on fundus images alone can detect referable GON with higher sensitivity than and comparable specificity to eye care providers. The algorithm maintained good performance on an independent dataset with diagnoses based on a full glaucoma workup.","Deep Learning and Glaucoma Specialists: The Relative Importance of Optic Disc Features to Predict Glaucoma Referral in Fundus Photographs. PURPOSE: To develop and validate a deep learning (DL) algorithm that predicts referable glaucomatous optic neuropathy (GON) and optic nerve head (ONH) features from color fundus images, to determine the relative importance of these features in referral decisions by glaucoma specialists (GSs) and the algorithm, and to compare the performance of the algorithm with eye care providers. DESIGN: Development and validation of an algorithm. PARTICIPANTS: Fundus images from screening programs, studies, and a glaucoma clinic. METHODS: A DL algorithm was trained using a retrospective dataset of 86 618 images, assessed for glaucomatous ONH features and referable GON (defined as ONH appearance worrisome enough to justify referral for comprehensive examination) by 43 graders. The algorithm was validated using 3 datasets: dataset A (1205 images, 1 image/patient; 18.1% referable), images adjudicated by panels of GSs; dataset B (9642 images, 1 image/patient; 9.2% referable), images from a diabetic teleretinal screening program; and dataset C (346 images, 1 image/patient; 81.7% referable), images from a glaucoma clinic. MAIN OUTCOME MEASURES: The algorithm was evaluated using the area under the receiver operating characteristic curve (AUC), sensitivity, and specificity for referable GON and glaucomatous ONH features. RESULTS: The algorithm's AUC for referable GON was 0.945 (95% confidence interval [CI], 0.929-0.960) in dataset A, 0.855 (95% CI, 0.841-0.870) in dataset B, and 0.881 (95% CI, 0.838-0.918) in dataset C. Algorithm AUCs ranged between 0.661 and 0.973 for glaucomatous ONH features. The algorithm showed significantly higher sensitivity than 7 of 10 graders not involved in determining the reference standard, including 2 of 3 GSs, and showed higher specificity than 3 graders (including 1 GS), while remaining comparable to others. For both GSs and the algorithm, the most crucial features related to referable GON were: presence of vertical cup-to-disc ratio of 0.7 or more, neuroretinal rim notching, retinal nerve fiber layer defect, and bared circumlinear vessels. CONCLUSIONS: A DL algorithm trained on fundus images alone can detect referable GON with higher sensitivity than and comparable specificity to eye care providers. The algorithm maintained good performance on an independent dataset with diagnoses based on a full glaucoma workup."
1,Automated segmentation of macular edema in OCT using deep neural networks,"Macular edema is an eye disease that can affect visual acuity. Typical disease symptoms include subretinal fluid (SRF) and pigment epithelium detachment (PED). Optical coherence tomography (OCT) has been widely used for diagnosing macular edema because of its non-invasive and high resolution properties. Segmentation for macular edema lesions from OCT images plays an important role in clinical diagnosis. Many computer-aided systems have been proposed for the segmentation. Most traditional segmentation methods used in these systems are based on low-level hand-crafted features, which require significant domain knowledge and are sensitive to the variations of lesions. To overcome these shortcomings, this paper proposes to use deep neural networks (DNNs) together with atrous spatial pyramid pooling (ASPP) to automatically segment the SRF and PED lesions. Lesions-related features are first extracted by DNNs, then processed by ASPP which is composed of multiple atrous convolutions with different fields of view to accommodate the various scales of the lesions. Based on ASPP, a novel module called stochastic ASPP (sASPP) is proposed to combat the co-adaptation of multiple atrous convolutions. A large OCT dataset provided by a competition platform called ""AI Challenger"" are used to train and evaluate the proposed model. Experimental results demonstrate that the DNNs together with ASPP achieve higher segmentation accuracy compared with the state-of-the-art method. The stochastic operation added in sASPP is empirically verified as an effective regularization method that can alleviate the overfitting problem and significantly reduce the validation error.","Automated segmentation of macular edema in OCT using deep neural networks. Macular edema is an eye disease that can affect visual acuity. Typical disease symptoms include subretinal fluid (SRF) and pigment epithelium detachment (PED). Optical coherence tomography (OCT) has been widely used for diagnosing macular edema because of its non-invasive and high resolution properties. Segmentation for macular edema lesions from OCT images plays an important role in clinical diagnosis. Many computer-aided systems have been proposed for the segmentation. Most traditional segmentation methods used in these systems are based on low-level hand-crafted features, which require significant domain knowledge and are sensitive to the variations of lesions. To overcome these shortcomings, this paper proposes to use deep neural networks (DNNs) together with atrous spatial pyramid pooling (ASPP) to automatically segment the SRF and PED lesions. Lesions-related features are first extracted by DNNs, then processed by ASPP which is composed of multiple atrous convolutions with different fields of view to accommodate the various scales of the lesions. Based on ASPP, a novel module called stochastic ASPP (sASPP) is proposed to combat the co-adaptation of multiple atrous convolutions. A large OCT dataset provided by a competition platform called ""AI Challenger"" are used to train and evaluate the proposed model. Experimental results demonstrate that the DNNs together with ASPP achieve higher segmentation accuracy compared with the state-of-the-art method. The stochastic operation added in sASPP is empirically verified as an effective regularization method that can alleviate the overfitting problem and significantly reduce the validation error."
1,Using an artificial neural network to map cancer common data elements to the biomedical research integrated domain group model in a semi-automated manner,"BACKGROUND: The medical community uses a variety of data standards for both clinical and research reporting needs. ISO 11179 Common Data Elements (CDEs) represent one such standard that provides robust data point definitions. Another standard is the Biomedical Research Integrated Domain Group (BRIDG) model, which is a domain analysis model that provides a contextual framework for biomedical and clinical research data. Mapping the CDEs to the BRIDG model is important; in particular, it can facilitate mapping the CDEs to other standards. Unfortunately, manual mapping, which is the current method for creating the CDE mappings, is error-prone and time-consuming; this creates a significant barrier for researchers who utilize CDEs. METHODS: In this work, we developed a semi-automated algorithm to map CDEs to likely BRIDG classes. First, we extended and improved our previously developed artificial neural network (ANN) alignment algorithm. We then used a collection of 1284 CDEs with robust mappings to BRIDG classes as the gold standard to train and obtain the appropriate weights of six attributes in CDEs. Afterward, we calculated the similarity between a CDE and each BRIDG class. Finally, the algorithm produces a list of candidate BRIDG classes to which the CDE of interest may belong. RESULTS: For CDEs semantically similar to those used in training, a match rate of over 90% was achieved. For those partially similar, a match rate of 80% was obtained and for those with drastically different semantics, a match rate of up to 70% was achieved. DISCUSSION: Our semi-automated mapping process reduces the burden of domain experts. The weights are all significant in six attributes. Experimental results indicate that the availability of training data is more important than the semantic similarity of the testing data to the training data. We address the overfitting problem by selecting CDEs randomly and adjusting the ratio of training and verification samples. CONCLUSIONS: Experimental results on real-world use cases have proven the effectiveness and efficiency of our proposed methodology in mapping CDEs with BRIDG classes, both those CDEs seen before as well as new, unseen CDEs. In addition, it reduces the mapping burden and improves the mapping quality.","Using an artificial neural network to map cancer common data elements to the biomedical research integrated domain group model in a semi-automated manner. BACKGROUND: The medical community uses a variety of data standards for both clinical and research reporting needs. ISO 11179 Common Data Elements (CDEs) represent one such standard that provides robust data point definitions. Another standard is the Biomedical Research Integrated Domain Group (BRIDG) model, which is a domain analysis model that provides a contextual framework for biomedical and clinical research data. Mapping the CDEs to the BRIDG model is important; in particular, it can facilitate mapping the CDEs to other standards. Unfortunately, manual mapping, which is the current method for creating the CDE mappings, is error-prone and time-consuming; this creates a significant barrier for researchers who utilize CDEs. METHODS: In this work, we developed a semi-automated algorithm to map CDEs to likely BRIDG classes. First, we extended and improved our previously developed artificial neural network (ANN) alignment algorithm. We then used a collection of 1284 CDEs with robust mappings to BRIDG classes as the gold standard to train and obtain the appropriate weights of six attributes in CDEs. Afterward, we calculated the similarity between a CDE and each BRIDG class. Finally, the algorithm produces a list of candidate BRIDG classes to which the CDE of interest may belong. RESULTS: For CDEs semantically similar to those used in training, a match rate of over 90% was achieved. For those partially similar, a match rate of 80% was obtained and for those with drastically different semantics, a match rate of up to 70% was achieved. DISCUSSION: Our semi-automated mapping process reduces the burden of domain experts. The weights are all significant in six attributes. Experimental results indicate that the availability of training data is more important than the semantic similarity of the testing data to the training data. We address the overfitting problem by selecting CDEs randomly and adjusting the ratio of training and verification samples. CONCLUSIONS: Experimental results on real-world use cases have proven the effectiveness and efficiency of our proposed methodology in mapping CDEs with BRIDG classes, both those CDEs seen before as well as new, unseen CDEs. In addition, it reduces the mapping burden and improves the mapping quality."
1,Using Artificial Intelligence to Revise ACR TI-RADS Risk Stratification of Thyroid Nodules: Diagnostic Accuracy and Utility,"Background Risk stratification systems for thyroid nodules are often complicated and affected by low specificity. Continual improvement of these systems is necessary to reduce the number of unnecessary thyroid biopsies. Purpose To use artificial intelligence (AI) to optimize the American College of Radiology (ACR) Thyroid Imaging Reporting and Data System (TI-RADS). Materials and Methods A total of 1425 biopsy-proven thyroid nodules from 1264 consecutive patients (1026 women; mean age, 52.9 years [range, 18-93 years]) were evaluated retrospectively. Expert readers assigned points based on five ACR TI-RADS categories (composition, echogenicity, shape, margin, echogenic foci), and a genetic AI algorithm was applied to a training set (1325 nodules). Point and pathologic data were used to create an optimized scoring system (hereafter, AI TI-RADS). Performance of the systems was compared by using a test set of the final 100 nodules with interpretations from the expert reader, eight nonexpert readers, and an expert panel. Initial performance of AI TI-RADS was calculated by using a test for differences between binomial proportions. Additional comparisons across readers were conducted by using bootstrapping; diagnostic performance was assessed by using area under the receiver operating curve. Results AI TI-RADS assigned new point values for eight ACR TI-RADS features. Six features were assigned zero points, which simplified categorization. By using expert reader data, the diagnostic performance of ACR TI-RADS and AI TI-RADS was area under the receiver operating curve of 0.91 and 0.93, respectively. For the same expert, specificity of AI TI-RADS (65%, 55 of 85) was higher (P < .001) than that of ACR TI-RADS (47%, 40 of 85). For the eight nonexpert radiologists, mean specificity for AI TI-RADS (55%) was also higher (P < .001) than that of ACR TI-RADS (48%). An interactive AI TI-RADS calculator can be viewed at http://deckard.duhs.duke.edu/ approximately ai-ti-rads . Conclusion An artificial intelligence-optimized Thyroid Imaging Reporting and Data System (TI-RADS) validates the American College of Radiology TI-RADS while slightly improving specificity and maintaining sensitivity. Additionally, it simplifies feature assignments, which may improve ease of use. (c) RSNA, 2019 Online supplemental material is available for this article.","Using Artificial Intelligence to Revise ACR TI-RADS Risk Stratification of Thyroid Nodules: Diagnostic Accuracy and Utility. Background Risk stratification systems for thyroid nodules are often complicated and affected by low specificity. Continual improvement of these systems is necessary to reduce the number of unnecessary thyroid biopsies. Purpose To use artificial intelligence (AI) to optimize the American College of Radiology (ACR) Thyroid Imaging Reporting and Data System (TI-RADS). Materials and Methods A total of 1425 biopsy-proven thyroid nodules from 1264 consecutive patients (1026 women; mean age, 52.9 years [range, 18-93 years]) were evaluated retrospectively. Expert readers assigned points based on five ACR TI-RADS categories (composition, echogenicity, shape, margin, echogenic foci), and a genetic AI algorithm was applied to a training set (1325 nodules). Point and pathologic data were used to create an optimized scoring system (hereafter, AI TI-RADS). Performance of the systems was compared by using a test set of the final 100 nodules with interpretations from the expert reader, eight nonexpert readers, and an expert panel. Initial performance of AI TI-RADS was calculated by using a test for differences between binomial proportions. Additional comparisons across readers were conducted by using bootstrapping; diagnostic performance was assessed by using area under the receiver operating curve. Results AI TI-RADS assigned new point values for eight ACR TI-RADS features. Six features were assigned zero points, which simplified categorization. By using expert reader data, the diagnostic performance of ACR TI-RADS and AI TI-RADS was area under the receiver operating curve of 0.91 and 0.93, respectively. For the same expert, specificity of AI TI-RADS (65%, 55 of 85) was higher (P < .001) than that of ACR TI-RADS (47%, 40 of 85). For the eight nonexpert radiologists, mean specificity for AI TI-RADS (55%) was also higher (P < .001) than that of ACR TI-RADS (48%). An interactive AI TI-RADS calculator can be viewed at http://deckard.duhs.duke.edu/ approximately ai-ti-rads . Conclusion An artificial intelligence-optimized Thyroid Imaging Reporting and Data System (TI-RADS) validates the American College of Radiology TI-RADS while slightly improving specificity and maintaining sensitivity. Additionally, it simplifies feature assignments, which may improve ease of use. (c) RSNA, 2019 Online supplemental material is available for this article."
1,DeepPET: A deep encoder-decoder network for directly solving the PET image reconstruction inverse problem,,
1,A deep learning framework for unsupervised affine and deformable image registration,"Image registration, the process of aligning two or more images, is the core technique of many (semi-)automatic medical image analysis tasks. Recent studies have shown that deep learning methods, notably convolutional neural networks (ConvNets), can be used for image registration. Thus far training of ConvNets for registration was supervised using predefined example registrations. However, obtaining example registrations is not trivial. To circumvent the need for predefined examples, and thereby to increase convenience of training ConvNets for image registration, we propose the Deep Learning Image Registration (DLIR) framework for unsupervised affine and deformable image registration. In the DLIR framework ConvNets are trained for image registration by exploiting image similarity analogous to conventional intensity-based image registration. After a ConvNet has been trained with the DLIR framework, it can be used to register pairs of unseen images in one shot. We propose flexible ConvNets designs for affine image registration and for deformable image registration. By stacking multiple of these ConvNets into a larger architecture, we are able to perform coarse-to-fine image registration. We show for registration of cardiac cine MRI and registration of chest CT that performance of the DLIR framework is comparable to conventional image registration while being several orders of magnitude faster.","A deep learning framework for unsupervised affine and deformable image registration. Image registration, the process of aligning two or more images, is the core technique of many (semi-)automatic medical image analysis tasks. Recent studies have shown that deep learning methods, notably convolutional neural networks (ConvNets), can be used for image registration. Thus far training of ConvNets for registration was supervised using predefined example registrations. However, obtaining example registrations is not trivial. To circumvent the need for predefined examples, and thereby to increase convenience of training ConvNets for image registration, we propose the Deep Learning Image Registration (DLIR) framework for unsupervised affine and deformable image registration. In the DLIR framework ConvNets are trained for image registration by exploiting image similarity analogous to conventional intensity-based image registration. After a ConvNet has been trained with the DLIR framework, it can be used to register pairs of unseen images in one shot. We propose flexible ConvNets designs for affine image registration and for deformable image registration. By stacking multiple of these ConvNets into a larger architecture, we are able to perform coarse-to-fine image registration. We show for registration of cardiac cine MRI and registration of chest CT that performance of the DLIR framework is comparable to conventional image registration while being several orders of magnitude faster."
1,Identifying undetected dementia in UK primary care patients: a retrospective case-control study comparing machine-learning and standard epidemiological approaches,"BACKGROUND: Identifying dementia early in time, using real world data, is a public health challenge. As only two-thirds of people with dementia now ultimately receive a formal diagnosis in United Kingdom health systems and many receive it late in the disease process, there is ample room for improvement. The policy of the UK government and National Health Service (NHS) is to increase rates of timely dementia diagnosis. We used data from general practice (GP) patient records to create a machine-learning model to identify patients who have or who are developing dementia, but are currently undetected as having the condition by the GP. METHODS: We used electronic patient records from Clinical Practice Research Datalink (CPRD). Using a case-control design, we selected patients aged >65y with a diagnosis of dementia (cases) and matched them 1:1 by sex and age to patients with no evidence of dementia (controls). We developed a list of 70 clinical entities related to the onset of dementia and recorded in the 5â€‰years before diagnosis. After creating binary features, we trialled machine learning classifiers to discriminate between cases and controls (logistic regression, naÃ¯ve Bayes, support vector machines, random forest and neural networks). We examined the most important features contributing to discrimination. RESULTS: The final analysis included data on 93,120 patients, with a median age of 82.6â€‰years; 64.8% were female. The naÃ¯ve Bayes model performed least well. The logistic regression, support vector machine, neural network and random forest performed very similarly with an AUROC of 0.74. The top features retained in the logistic regression model were disorientation and wandering, behaviour change, schizophrenia, self-neglect, and difficulty managing. CONCLUSIONS: Our model could aid GPs or health service planners with the early detection of dementia. Future work could improve the model by exploring the longitudinal nature of patient data and modelling decline in function over time.","Identifying undetected dementia in UK primary care patients: a retrospective case-control study comparing machine-learning and standard epidemiological approaches. BACKGROUND: Identifying dementia early in time, using real world data, is a public health challenge. As only two-thirds of people with dementia now ultimately receive a formal diagnosis in United Kingdom health systems and many receive it late in the disease process, there is ample room for improvement. The policy of the UK government and National Health Service (NHS) is to increase rates of timely dementia diagnosis. We used data from general practice (GP) patient records to create a machine-learning model to identify patients who have or who are developing dementia, but are currently undetected as having the condition by the GP. METHODS: We used electronic patient records from Clinical Practice Research Datalink (CPRD). Using a case-control design, we selected patients aged >65y with a diagnosis of dementia (cases) and matched them 1:1 by sex and age to patients with no evidence of dementia (controls). We developed a list of 70 clinical entities related to the onset of dementia and recorded in the 5â€‰years before diagnosis. After creating binary features, we trialled machine learning classifiers to discriminate between cases and controls (logistic regression, naÃ¯ve Bayes, support vector machines, random forest and neural networks). We examined the most important features contributing to discrimination. RESULTS: The final analysis included data on 93,120 patients, with a median age of 82.6â€‰years; 64.8% were female. The naÃ¯ve Bayes model performed least well. The logistic regression, support vector machine, neural network and random forest performed very similarly with an AUROC of 0.74. The top features retained in the logistic regression model were disorientation and wandering, behaviour change, schizophrenia, self-neglect, and difficulty managing. CONCLUSIONS: Our model could aid GPs or health service planners with the early detection of dementia. Future work could improve the model by exploring the longitudinal nature of patient data and modelling decline in function over time."
1,Facilitating accurate health provider directories using natural language processing,"BACKGROUND: Accurate information in provider directories are vital in health care including health information exchange, health benefits exchange, quality reporting, and in the reimbursement and delivery of care. Maintaining provider directory data and keeping it up to date is challenging. The objective of this study is to determine the feasibility of using natural language processing (NLP) techniques to combine disparate resources and acquire accurate information on health providers. METHODS: Publically available state licensure lists in Connecticut were obtained along with National Plan and Provider Enumeration System (NPPES) public use files. Connecticut licensure lists textual information of each health professional who is licensed to practice within the state. A NLP-based system was developed based on healthcare provider taxonomy code, location, name and address information to identify textual data within the state and federal records. Qualitative and quantitative evaluation were performed, and the recall and precision were calculated. RESULTS: We identified nurse midwives, nurse practitioners, and dentists in the State of Connecticut. The recall and precision were 0.95 and 0.93 respectively. Using the system, we were able to accurately acquire 6849 of the 7177 records of health provider directory information. CONCLUSIONS: The authors demonstrated that the NLP- based approach was effective at acquiring health provider information. Furthermore, the NLP-based system can always be applied to update information further reducing processing burdens as data changes.","Facilitating accurate health provider directories using natural language processing. BACKGROUND: Accurate information in provider directories are vital in health care including health information exchange, health benefits exchange, quality reporting, and in the reimbursement and delivery of care. Maintaining provider directory data and keeping it up to date is challenging. The objective of this study is to determine the feasibility of using natural language processing (NLP) techniques to combine disparate resources and acquire accurate information on health providers. METHODS: Publically available state licensure lists in Connecticut were obtained along with National Plan and Provider Enumeration System (NPPES) public use files. Connecticut licensure lists textual information of each health professional who is licensed to practice within the state. A NLP-based system was developed based on healthcare provider taxonomy code, location, name and address information to identify textual data within the state and federal records. Qualitative and quantitative evaluation were performed, and the recall and precision were calculated. RESULTS: We identified nurse midwives, nurse practitioners, and dentists in the State of Connecticut. The recall and precision were 0.95 and 0.93 respectively. Using the system, we were able to accurately acquire 6849 of the 7177 records of health provider directory information. CONCLUSIONS: The authors demonstrated that the NLP- based approach was effective at acquiring health provider information. Furthermore, the NLP-based system can always be applied to update information further reducing processing burdens as data changes."
1,Temporal indexing of medical entity in Chinese clinical notes,"BACKGROUND: The goal of temporal indexing is to select an occurred time or time interval for each medical entity in clinical notes, so that all medical entities can be indexed on a united timeline, which could assist the understanding of clinical notes and the further application of medical entities. Some temporal relation shared tasks for the medical entity in English clinical notes have been organized in the past few years, such as the 2012 i2b2 NLP challenge, 2015 and 2016 clinical TempEval challenges. In these tasks, many heuristics rule-based and machine learning-based systems have been developed. In recent years, the deep neural network models have shown great potential on many problems including the relation classification. METHODS: In this paper, we propose a recurrent convolutional neural network (RNN-CNN) model for the temporal indexing task, which consists of four layers: input layer - generates representation for each context word of medical entities or temporal expressions; LSTM (long-short term memory) layer - learns the context information of each word in a sentence and outputs a new word representation sequence; CNN layer - extracts meaningful features from a sentence and outputs a new representation for medical entity or temporal expression; Output layer - takes the representations of medical entity, temporal expression and relation features as input and classifies the temporal relation. Finally, the time or time interval for each medical entity can be directly selected according to the probability of each temporal relation predicted by above model. RESULTS: To investigate the performance of our RNN-CNN model for the temporal indexing task, several baseline methods were also employed, such as the rule-based, support vector machine (SVM), convolutional neural network (CNN) and recurrent neural network (RNN) methods. Experiments conducted on a manually annotated corpus (including 563 clinical notes with 12,611 medical entities and 4006 temporal expressions) show that RNN-CNN model achieves the best F1-score of 75.97% for temporal relation classification and the best accuracy of 71.96% for temporal indexing. CONCLUSIONS: Neural network methods perform much better than the traditional rule-based and SVM-based method, which can capture more semantic information from the context of medical entities and temporal expressions. Besides, all our methods perform much better for the accurate time indexing than the time interval indexing, so how to improve the performance for time interval indexing will be the main focus in our future work.","Temporal indexing of medical entity in Chinese clinical notes. BACKGROUND: The goal of temporal indexing is to select an occurred time or time interval for each medical entity in clinical notes, so that all medical entities can be indexed on a united timeline, which could assist the understanding of clinical notes and the further application of medical entities. Some temporal relation shared tasks for the medical entity in English clinical notes have been organized in the past few years, such as the 2012 i2b2 NLP challenge, 2015 and 2016 clinical TempEval challenges. In these tasks, many heuristics rule-based and machine learning-based systems have been developed. In recent years, the deep neural network models have shown great potential on many problems including the relation classification. METHODS: In this paper, we propose a recurrent convolutional neural network (RNN-CNN) model for the temporal indexing task, which consists of four layers: input layer - generates representation for each context word of medical entities or temporal expressions; LSTM (long-short term memory) layer - learns the context information of each word in a sentence and outputs a new word representation sequence; CNN layer - extracts meaningful features from a sentence and outputs a new representation for medical entity or temporal expression; Output layer - takes the representations of medical entity, temporal expression and relation features as input and classifies the temporal relation. Finally, the time or time interval for each medical entity can be directly selected according to the probability of each temporal relation predicted by above model. RESULTS: To investigate the performance of our RNN-CNN model for the temporal indexing task, several baseline methods were also employed, such as the rule-based, support vector machine (SVM), convolutional neural network (CNN) and recurrent neural network (RNN) methods. Experiments conducted on a manually annotated corpus (including 563 clinical notes with 12,611 medical entities and 4006 temporal expressions) show that RNN-CNN model achieves the best F1-score of 75.97% for temporal relation classification and the best accuracy of 71.96% for temporal indexing. CONCLUSIONS: Neural network methods perform much better than the traditional rule-based and SVM-based method, which can capture more semantic information from the context of medical entities and temporal expressions. Besides, all our methods perform much better for the accurate time indexing than the time interval indexing, so how to improve the performance for time interval indexing will be the main focus in our future work."
1,"Computer-aided detection and visualization of pulmonary embolism using a novel, compact, and discriminative image representation",,
1,Detection and classification of myocardial delayed enhancement patterns on mr images with deep neural networks: A feasibility study,"Purpose: To evaluate whether deep neural networks trained on a similar number of images to that required during physician training in the American College of Cardiology Core Cardiovascular Training Statement can acquire the capability to detect and classify myocardial delayed enhancement (MDE) patterns. Materials and Methods: The authors retrospectively evaluated 1995 MDE images for training and validation of a deep neural network. Images were from 200 consecutive patients who underwent cardiovascular MRI and were obtained from the institutional database. Experienced cardiac MR image readers classified the images as showing the following MDE patterns: No pattern, epicardial enhancement, subendocardial enhancement, midwall enhancement, focal enhancement, transmural enhancement, and nondiagnostic. Data were divided into training and validation datasets by using a fourfold cross-validation method. Three untrained deep neural network architectures using the convolutional neural network (CNN) technique were trained with the training dataset images. The detection and classification accuracies of the trained CNNs were calculated with validation data. Results: The 1995 MDE images were classified by human readers as follows: No pattern, 926; epicardial enhancement, 91; subendocardial enhancement, 458; midwall enhancement, 118; focal enhancement, 141; transmural enhancement, 190; and nondiagnostic, 190. GoogLeNet, AlexNet, and ResNet-152 CNNs demonstrated accuracies of 79.5% (1592 of 1995 images), 78.9% (1574 of 1995 images), and 82.1% (1637 of 1995 images), respectively. Conclusion: Deep learning with CNNs using a limited amount of training data, less than that required during physician training, achieved high diagnostic performance in the detection of MDE on MR images.","Detection and classification of myocardial delayed enhancement patterns on mr images with deep neural networks: A feasibility study. Purpose: To evaluate whether deep neural networks trained on a similar number of images to that required during physician training in the American College of Cardiology Core Cardiovascular Training Statement can acquire the capability to detect and classify myocardial delayed enhancement (MDE) patterns. Materials and Methods: The authors retrospectively evaluated 1995 MDE images for training and validation of a deep neural network. Images were from 200 consecutive patients who underwent cardiovascular MRI and were obtained from the institutional database. Experienced cardiac MR image readers classified the images as showing the following MDE patterns: No pattern, epicardial enhancement, subendocardial enhancement, midwall enhancement, focal enhancement, transmural enhancement, and nondiagnostic. Data were divided into training and validation datasets by using a fourfold cross-validation method. Three untrained deep neural network architectures using the convolutional neural network (CNN) technique were trained with the training dataset images. The detection and classification accuracies of the trained CNNs were calculated with validation data. Results: The 1995 MDE images were classified by human readers as follows: No pattern, 926; epicardial enhancement, 91; subendocardial enhancement, 458; midwall enhancement, 118; focal enhancement, 141; transmural enhancement, 190; and nondiagnostic, 190. GoogLeNet, AlexNet, and ResNet-152 CNNs demonstrated accuracies of 79.5% (1592 of 1995 images), 78.9% (1574 of 1995 images), and 82.1% (1637 of 1995 images), respectively. Conclusion: Deep learning with CNNs using a limited amount of training data, less than that required during physician training, achieved high diagnostic performance in the detection of MDE on MR images."
1,Chronic Obstructive Pulmonary Disease: Thoracic CT Texture Analysis and Machine Learning to Predict Pulmonary Ventilation,"Background Fixed airflow limitation and ventilation heterogeneity are common in chronic obstructive pulmonary disease (COPD). Conventional noncontrast CT provides airway and parenchymal measurements but cannot be used to directly determine lung function. Purpose To develop, train, and test a CT texture analysis and machine-learning algorithm to predict lung ventilation heterogeneity in participants with COPD. Materials and Methods In this prospective study (ClinicalTrials.gov: NCT02723474; conducted from January 2010 to February 2017), participants were randomized to optimization (n = 1), training (n = 67), and testing (n = 27) data sets. Hyperpolarized (HP) helium 3 ((3)He) MRI ventilation maps were co-registered with thoracic CT to provide ground truth labels, and 87 quantitative imaging features were extracted and normalized to lung averages to generate 174 features. The volume-of-interest dimension and the training data sampling method were optimized to maximize the area under the receiver operating characteristic curve (AUC). Forward feature selection was performed to reduce the number of features; logistic regression, linear support vector machine, and quadratic support vector machine classifiers were trained through fivefold cross validation. The highest-performing classification model was applied to the test data set. Pearson coefficients were used to determine the relationships between the model, MRI, and pulmonary function measurements. Results The quadratic support vector machine performed best in training and was applied to the test data set. Model-predicted ventilation maps had an accuracy of 88% (95% confidence interval [CI]: 88%, 88%) and an AUC of 0.82 (95% CI: 0.82, 0.83) when the HP (3)He MRI ventilation maps were used as the reference standard. Model-predicted ventilation defect percentage (VDP) was correlated with VDP at HP (3)He MRI (r = 0.90, P < .001). Both model-predicted and HP (3)He MRI VDP were correlated with forced expiratory volume in 1 second (FEV1) (model: r = -0.65, P < .001; MRI: r = -0.70, P < .001), ratio of FEV1 to forced vital capacity (model: r = -0.73, P < .001; MRI: r = -0.75, P < .001), diffusing capacity (model: r = -0.69, P < .001; MRI: r = -0.65, P < .001), and quality-of-life score (model: r = 0.59, P = .001; MRI: r = 0.65, P < .001). Conclusion Model-predicted ventilation maps generated by using CT textures and machine learning were correlated with MRI ventilation maps (r = 0.90, P < .001). (c) RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Fain in this issue.","Chronic Obstructive Pulmonary Disease: Thoracic CT Texture Analysis and Machine Learning to Predict Pulmonary Ventilation. Background Fixed airflow limitation and ventilation heterogeneity are common in chronic obstructive pulmonary disease (COPD). Conventional noncontrast CT provides airway and parenchymal measurements but cannot be used to directly determine lung function. Purpose To develop, train, and test a CT texture analysis and machine-learning algorithm to predict lung ventilation heterogeneity in participants with COPD. Materials and Methods In this prospective study (ClinicalTrials.gov: NCT02723474; conducted from January 2010 to February 2017), participants were randomized to optimization (n = 1), training (n = 67), and testing (n = 27) data sets. Hyperpolarized (HP) helium 3 ((3)He) MRI ventilation maps were co-registered with thoracic CT to provide ground truth labels, and 87 quantitative imaging features were extracted and normalized to lung averages to generate 174 features. The volume-of-interest dimension and the training data sampling method were optimized to maximize the area under the receiver operating characteristic curve (AUC). Forward feature selection was performed to reduce the number of features; logistic regression, linear support vector machine, and quadratic support vector machine classifiers were trained through fivefold cross validation. The highest-performing classification model was applied to the test data set. Pearson coefficients were used to determine the relationships between the model, MRI, and pulmonary function measurements. Results The quadratic support vector machine performed best in training and was applied to the test data set. Model-predicted ventilation maps had an accuracy of 88% (95% confidence interval [CI]: 88%, 88%) and an AUC of 0.82 (95% CI: 0.82, 0.83) when the HP (3)He MRI ventilation maps were used as the reference standard. Model-predicted ventilation defect percentage (VDP) was correlated with VDP at HP (3)He MRI (r = 0.90, P < .001). Both model-predicted and HP (3)He MRI VDP were correlated with forced expiratory volume in 1 second (FEV1) (model: r = -0.65, P < .001; MRI: r = -0.70, P < .001), ratio of FEV1 to forced vital capacity (model: r = -0.73, P < .001; MRI: r = -0.75, P < .001), diffusing capacity (model: r = -0.69, P < .001; MRI: r = -0.65, P < .001), and quality-of-life score (model: r = 0.59, P = .001; MRI: r = 0.65, P < .001). Conclusion Model-predicted ventilation maps generated by using CT textures and machine learning were correlated with MRI ventilation maps (r = 0.90, P < .001). (c) RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Fain in this issue."
1,Radiomics versus Visual and Histogram-based Assessment to Identify Atheromatous Lesions at Coronary CT Angiography: An ex Vivo Study,"Background Visual and histogram-based assessments of coronary CT angiography have limited accuracy in the identification of advanced lesions. Radiomics-based machine learning (ML) could provide a more accurate tool. Purpose To compare the diagnostic performance of radiomics-based ML with that of visual and histogram-based assessment of ex vivo coronary CT angiography cross sections to identify advanced atherosclerotic lesions defined with histologic examination. Materials and Methods In this prospective study, 21 coronary arteries from seven hearts obtained from male donors (mean age, 52.3 years +/- 5.3) were imaged ex vivo with coronary CT angiography between February 23, 2009, and July 31, 2010. From 95 coronary plaques, 611 histologic cross sections were coregistered with coronary CT cross sections. Lesions were considered advanced if early fibroatheroma, late fibroatheroma, or thin-cap atheroma was present. CT cross sections were classified as showing homogeneous, heterogeneous, or napkin-ring sign plaques on the basis of visual assessment. The area of low attenuation (<30 HU) and the average Hounsfield unit were quantified. Radiomic parameters were extracted and used as inputs to ML algorithms. Eight radiomics-based ML models were trained on randomly selected cross sections (training set, 75% of the cross sections) to identify advanced lesions. Visual assessment, histogram-based assessment, and the best ML model were compared on the remaining 25% of the data (validation set) by using the area under the receiver operating characteristic curve (AUC) to identify advanced lesions. Results After excluding sections with no visible plaque (n = 134) and with heavy calcium (n = 32), 445 cross sections were analyzed. Of those 445 cross sections, 134 (30.1%) were advanced lesions. Visual assessment of the 445 cross sections indicated that 207 (46.5%) were homogeneous, 200 (44.9%) were heterogeneous, and 38 (8.5%) demonstrated the napkin-ring sign. A radiomics-based ML model incorporating 13 parameters outperformed visual assessment (AUC = 0.73 with 95% confidence interval [CI] of 0.63, 0.84 vs 0.65 with 95% CI of 0.56, 0.73, respectively; P = .04), area of low attenuation (AUC = 0.55 with 95% CI of 0.42, 0.68; P = .01), and average Hounsfield unit (AUC = 0.53 with 95% CI of 0.42, 0.65; P = .004) in the identification of advanced atheromatous lesions. Conclusion Radiomics-based machine learning analysis improves the discriminatory power of coronary CT angiography in the identification of advanced atherosclerotic lesions. Published under a CC BY 4.0 license.","Radiomics versus Visual and Histogram-based Assessment to Identify Atheromatous Lesions at Coronary CT Angiography: An ex Vivo Study. Background Visual and histogram-based assessments of coronary CT angiography have limited accuracy in the identification of advanced lesions. Radiomics-based machine learning (ML) could provide a more accurate tool. Purpose To compare the diagnostic performance of radiomics-based ML with that of visual and histogram-based assessment of ex vivo coronary CT angiography cross sections to identify advanced atherosclerotic lesions defined with histologic examination. Materials and Methods In this prospective study, 21 coronary arteries from seven hearts obtained from male donors (mean age, 52.3 years +/- 5.3) were imaged ex vivo with coronary CT angiography between February 23, 2009, and July 31, 2010. From 95 coronary plaques, 611 histologic cross sections were coregistered with coronary CT cross sections. Lesions were considered advanced if early fibroatheroma, late fibroatheroma, or thin-cap atheroma was present. CT cross sections were classified as showing homogeneous, heterogeneous, or napkin-ring sign plaques on the basis of visual assessment. The area of low attenuation (<30 HU) and the average Hounsfield unit were quantified. Radiomic parameters were extracted and used as inputs to ML algorithms. Eight radiomics-based ML models were trained on randomly selected cross sections (training set, 75% of the cross sections) to identify advanced lesions. Visual assessment, histogram-based assessment, and the best ML model were compared on the remaining 25% of the data (validation set) by using the area under the receiver operating characteristic curve (AUC) to identify advanced lesions. Results After excluding sections with no visible plaque (n = 134) and with heavy calcium (n = 32), 445 cross sections were analyzed. Of those 445 cross sections, 134 (30.1%) were advanced lesions. Visual assessment of the 445 cross sections indicated that 207 (46.5%) were homogeneous, 200 (44.9%) were heterogeneous, and 38 (8.5%) demonstrated the napkin-ring sign. A radiomics-based ML model incorporating 13 parameters outperformed visual assessment (AUC = 0.73 with 95% confidence interval [CI] of 0.63, 0.84 vs 0.65 with 95% CI of 0.56, 0.73, respectively; P = .04), area of low attenuation (AUC = 0.55 with 95% CI of 0.42, 0.68; P = .01), and average Hounsfield unit (AUC = 0.53 with 95% CI of 0.42, 0.65; P = .004) in the identification of advanced atheromatous lesions. Conclusion Radiomics-based machine learning analysis improves the discriminatory power of coronary CT angiography in the identification of advanced atherosclerotic lesions. Published under a CC BY 4.0 license."
1,Deep learning can predict microsatellite instability directly from histology in gastrointestinal cancer,"Microsatellite instability determines whether patients with gastrointestinal cancer respond exceptionally well to immunotherapy. However, in clinical practice, not every patient is tested for MSI, because this requires additional genetic or immunohistochemical tests. Here we show that deep residual learning can predict MSI directly from H&E histology, which is ubiquitously available. This approach has the potential to provide immunotherapy to a much broader subset of patients with gastrointestinal cancer.","Deep learning can predict microsatellite instability directly from histology in gastrointestinal cancer. Microsatellite instability determines whether patients with gastrointestinal cancer respond exceptionally well to immunotherapy. However, in clinical practice, not every patient is tested for MSI, because this requires additional genetic or immunohistochemical tests. Here we show that deep residual learning can predict MSI directly from H&E histology, which is ubiquitously available. This approach has the potential to provide immunotherapy to a much broader subset of patients with gastrointestinal cancer."
1,A deep learning model incorporating part of speech and self-matching attention for named entity recognition of Chinese electronic medical records,"BACKGROUND: The Named Entity Recognition (NER) task as a key step in the extraction of health information, has encountered many challenges in Chinese Electronic Medical Records (EMRs). Firstly, the casual use of Chinese abbreviations and doctors' personal style may result in multiple expressions of the same entity, and we lack a common Chinese medical dictionary to perform accurate entity extraction. Secondly, the electronic medical record contains entities from a variety of categories of entities, and the length of those entities in different categories varies greatly, which increases the difficult in the extraction for the Chinese NER. Therefore, the entity boundary detection becomes the key to perform accurate entity extraction of Chinese EMRs, and we need to develop a model that supports multiple length entity recognition without relying on any medical dictionary. METHODS: In this study, we incorporate part-of-speech (POS) information into the deep learning model to improve the accuracy of Chinese entity boundary detection. In order to avoid the wrongly POS tagging of long entities, we proposed a method called reduced POS tagging that reserves the tags of general words but not of the seemingly medical entities. The model proposed in this paper, named SM-LSTM-CRF, consists of three layers: self-matching attention layer - calculating the relevance of each character to the entire sentence; LSTM (Long Short-Term Memory) layer - capturing the context feature of each character; CRF (Conditional Random Field) layer - labeling characters based on their features and transfer rules. RESULTS: The experimental results at a Chinese EMRs dataset show that the F1 value of SM-LSTM-CRF is increased by 2.59% compared to that of the LSTM-CRF. After adding POS feature in the model, we get an improvement of about 7.74% at F1. The reduced POS tagging reduces the false tagging on long entities, thus increases the F1 value by 2.42% and achieves an F1 score of 80.07%. CONCLUSIONS: The POS feature marked by the reduced POS tagging together with self-matching attention mechanism puts a stranglehold on entity boundaries and has a good performance in the recognition of clinical entities.","A deep learning model incorporating part of speech and self-matching attention for named entity recognition of Chinese electronic medical records. BACKGROUND: The Named Entity Recognition (NER) task as a key step in the extraction of health information, has encountered many challenges in Chinese Electronic Medical Records (EMRs). Firstly, the casual use of Chinese abbreviations and doctors' personal style may result in multiple expressions of the same entity, and we lack a common Chinese medical dictionary to perform accurate entity extraction. Secondly, the electronic medical record contains entities from a variety of categories of entities, and the length of those entities in different categories varies greatly, which increases the difficult in the extraction for the Chinese NER. Therefore, the entity boundary detection becomes the key to perform accurate entity extraction of Chinese EMRs, and we need to develop a model that supports multiple length entity recognition without relying on any medical dictionary. METHODS: In this study, we incorporate part-of-speech (POS) information into the deep learning model to improve the accuracy of Chinese entity boundary detection. In order to avoid the wrongly POS tagging of long entities, we proposed a method called reduced POS tagging that reserves the tags of general words but not of the seemingly medical entities. The model proposed in this paper, named SM-LSTM-CRF, consists of three layers: self-matching attention layer - calculating the relevance of each character to the entire sentence; LSTM (Long Short-Term Memory) layer - capturing the context feature of each character; CRF (Conditional Random Field) layer - labeling characters based on their features and transfer rules. RESULTS: The experimental results at a Chinese EMRs dataset show that the F1 value of SM-LSTM-CRF is increased by 2.59% compared to that of the LSTM-CRF. After adding POS feature in the model, we get an improvement of about 7.74% at F1. The reduced POS tagging reduces the false tagging on long entities, thus increases the F1 value by 2.42% and achieves an F1 score of 80.07%. CONCLUSIONS: The POS feature marked by the reduced POS tagging together with self-matching attention mechanism puts a stranglehold on entity boundaries and has a good performance in the recognition of clinical entities."
1,A data-driven approach to predicting diabetes and cardiovascular disease with machine learning,"BACKGROUND: Diabetes and cardiovascular disease are two of the main causes of death in the United States. Identifying and predicting these diseases in patients is the first step towards stopping their progression. We evaluate the capabilities of machine learning models in detecting at-risk patients using survey data (and laboratory results), and identify key variables within the data contributing to these diseases among the patients. METHODS: Our research explores data-driven approaches which utilize supervised machine learning models to identify patients with such diseases. Using the National Health and Nutrition Examination Survey (NHANES) dataset, we conduct an exhaustive search of all available feature variables within the data to develop models for cardiovascular, prediabetes, and diabetes detection. Using different time-frames and feature sets for the data (based on laboratory data), multiple machine learning models (logistic regression, support vector machines, random forest, and gradient boosting) were evaluated on their classification performance. The models were then combined to develop a weighted ensemble model, capable of leveraging the performance of the disparate models to improve detection accuracy. Information gain of tree-based models was used to identify the key variables within the patient data that contributed to the detection of at-risk patients in each of the diseases classes by the data-learned models. RESULTS: The developed ensemble model for cardiovascular disease (based on 131 variables) achieved an Area Under - Receiver Operating Characteristics (AU-ROC) score of 83.1% using no laboratory results, and 83.9% accuracy with laboratory results. In diabetes classification (based on 123 variables), eXtreme Gradient Boost (XGBoost) model achieved an AU-ROC score of 86.2% (without laboratory data) and 95.7% (with laboratory data). For pre-diabetic patients, the ensemble model had the top AU-ROC score of 73.7% (without laboratory data), and for laboratory based data XGBoost performed the best at 84.4%. Top five predictors in diabetes patients were 1) waist size, 2) age, 3) self-reported weight, 4) leg length, and 5) sodium intake. For cardiovascular diseases the models identified 1) age, 2) systolic blood pressure, 3) self-reported weight, 4) occurrence of chest pain, and 5) diastolic blood pressure as key contributors. CONCLUSION: We conclude machine learned models based on survey questionnaire can provide an automated identification mechanism for patients at risk of diabetes and cardiovascular diseases. We also identify key contributors to the prediction, which can be further explored for their implications on electronic health records.","A data-driven approach to predicting diabetes and cardiovascular disease with machine learning. BACKGROUND: Diabetes and cardiovascular disease are two of the main causes of death in the United States. Identifying and predicting these diseases in patients is the first step towards stopping their progression. We evaluate the capabilities of machine learning models in detecting at-risk patients using survey data (and laboratory results), and identify key variables within the data contributing to these diseases among the patients. METHODS: Our research explores data-driven approaches which utilize supervised machine learning models to identify patients with such diseases. Using the National Health and Nutrition Examination Survey (NHANES) dataset, we conduct an exhaustive search of all available feature variables within the data to develop models for cardiovascular, prediabetes, and diabetes detection. Using different time-frames and feature sets for the data (based on laboratory data), multiple machine learning models (logistic regression, support vector machines, random forest, and gradient boosting) were evaluated on their classification performance. The models were then combined to develop a weighted ensemble model, capable of leveraging the performance of the disparate models to improve detection accuracy. Information gain of tree-based models was used to identify the key variables within the patient data that contributed to the detection of at-risk patients in each of the diseases classes by the data-learned models. RESULTS: The developed ensemble model for cardiovascular disease (based on 131 variables) achieved an Area Under - Receiver Operating Characteristics (AU-ROC) score of 83.1% using no laboratory results, and 83.9% accuracy with laboratory results. In diabetes classification (based on 123 variables), eXtreme Gradient Boost (XGBoost) model achieved an AU-ROC score of 86.2% (without laboratory data) and 95.7% (with laboratory data). For pre-diabetic patients, the ensemble model had the top AU-ROC score of 73.7% (without laboratory data), and for laboratory based data XGBoost performed the best at 84.4%. Top five predictors in diabetes patients were 1) waist size, 2) age, 3) self-reported weight, 4) leg length, and 5) sodium intake. For cardiovascular diseases the models identified 1) age, 2) systolic blood pressure, 3) self-reported weight, 4) occurrence of chest pain, and 5) diastolic blood pressure as key contributors. CONCLUSION: We conclude machine learned models based on survey questionnaire can provide an automated identification mechanism for patients at risk of diabetes and cardiovascular diseases. We also identify key contributors to the prediction, which can be further explored for their implications on electronic health records."
1,Predicting patient-reported outcomes following hip and knee replacement surgery using supervised machine learning,"BACKGROUND: Machine-learning classifiers mostly offer good predictive performance and are increasingly used to support shared decision-making in clinical practice. Focusing on performance and practicability, this study evaluates prediction of patient-reported outcomes (PROs) by eight supervised classifiers including a linear model, following hip and knee replacement surgery. METHODS: NHS PRO data (130,945 observations) from April 2015 to April 2017 were used to train and test eight classifiers to predict binary postoperative improvement based on minimal important differences. Area under the receiver operating characteristic, J-statistic and several other metrics were calculated. The dependent outcomes were generic and disease-specific improvement based on the EQ-5D-3L visual analogue scale (VAS) as well as the Oxford Hip and Knee Score (Q score). RESULTS: The area under the receiver operating characteristic of the best training models was around 0.87 (VAS) and 0.78 (Q score) for hip replacement, while it was around 0.86 (VAS) and 0.70 (Q score) for knee replacement surgery. Extreme gradient boosting, random forests, multistep elastic net and linear model provided the highest overall J-statistics. Based on variable importance, the most important predictors for post-operative outcomes were preoperative VAS, Q score and single Q score dimensions. Sensitivity analysis for hip replacement VAS evaluated the influence of minimal important difference, patient selection criteria as well as additional data years. Together with a small benchmark of the NHS prediction model, robustness of our results was confirmed. CONCLUSIONS: Supervised machine-learning implementations, like extreme gradient boosting, can provide better performance than linear models and should be considered, when high predictive performance is needed. Preoperative VAS, Q score and specific dimensions like limping are the most important predictors for postoperative hip and knee PROMs.","Predicting patient-reported outcomes following hip and knee replacement surgery using supervised machine learning. BACKGROUND: Machine-learning classifiers mostly offer good predictive performance and are increasingly used to support shared decision-making in clinical practice. Focusing on performance and practicability, this study evaluates prediction of patient-reported outcomes (PROs) by eight supervised classifiers including a linear model, following hip and knee replacement surgery. METHODS: NHS PRO data (130,945 observations) from April 2015 to April 2017 were used to train and test eight classifiers to predict binary postoperative improvement based on minimal important differences. Area under the receiver operating characteristic, J-statistic and several other metrics were calculated. The dependent outcomes were generic and disease-specific improvement based on the EQ-5D-3L visual analogue scale (VAS) as well as the Oxford Hip and Knee Score (Q score). RESULTS: The area under the receiver operating characteristic of the best training models was around 0.87 (VAS) and 0.78 (Q score) for hip replacement, while it was around 0.86 (VAS) and 0.70 (Q score) for knee replacement surgery. Extreme gradient boosting, random forests, multistep elastic net and linear model provided the highest overall J-statistics. Based on variable importance, the most important predictors for post-operative outcomes were preoperative VAS, Q score and single Q score dimensions. Sensitivity analysis for hip replacement VAS evaluated the influence of minimal important difference, patient selection criteria as well as additional data years. Together with a small benchmark of the NHS prediction model, robustness of our results was confirmed. CONCLUSIONS: Supervised machine-learning implementations, like extreme gradient boosting, can provide better performance than linear models and should be considered, when high predictive performance is needed. Preoperative VAS, Q score and specific dimensions like limping are the most important predictors for postoperative hip and knee PROMs."
1,Deep learning-based CT image reconstruction: Initial evaluation targeting hypovascular hepatic metastases,"Purpose: To evaluate the effect of a deep learning-based reconstruction (DLR) method on the conspicuity of hypovascular hepatic metastases on abdominal CT images. Materials and Methods: This retrospective study with institutional review board approval included 58 patients with hypovascular hepatic metastases. A radiologist recorded the standard deviation of attenuation in the paraspinal muscle as the image noise and the contrast-to-noise ratio (CNR). CNR was calculated as region of interest ([ROI]L âˆ’ ROIT)/N, here ROIL is the mean liver parenchyma attenuation, ROIT, the mean tumor attenuation, and N, the noise. Two other radiologists graded the conspicuity of the liver lesion on a five-point scale where 1 is unidentifiable and 5 is detected without diagnostic compromise. Only the smallest liver lesion in each patient, classified as smaller or larger than 10 mm, was evaluated. The difference between hybrid iterative reconstruction (IR) and DLR images was determined by using a two-sided Wilcoxon signed-rank test. Results: The image noise was significantly lower, and the CNR was significantly higher on DLR images than hybrid IR images (median image noise: 19.2 vs 12.8 HU, P, 001; median CNR: Tumors, 10 mm: 1.9 vs 2.5; tumors. 10 mm: 1.7 vs 2.2, both P, 001). The scores for liver lesions were significantly higher for DLR images than hybrid IR images (P, 01 for both in tumors smaller or larger than 10 mm). Conclusion: DLR improved the quality of abdominal CT images for the evaluation of hypovascular hepatic metastases.","Deep learning-based CT image reconstruction: Initial evaluation targeting hypovascular hepatic metastases. Purpose: To evaluate the effect of a deep learning-based reconstruction (DLR) method on the conspicuity of hypovascular hepatic metastases on abdominal CT images. Materials and Methods: This retrospective study with institutional review board approval included 58 patients with hypovascular hepatic metastases. A radiologist recorded the standard deviation of attenuation in the paraspinal muscle as the image noise and the contrast-to-noise ratio (CNR). CNR was calculated as region of interest ([ROI]L âˆ’ ROIT)/N, here ROIL is the mean liver parenchyma attenuation, ROIT, the mean tumor attenuation, and N, the noise. Two other radiologists graded the conspicuity of the liver lesion on a five-point scale where 1 is unidentifiable and 5 is detected without diagnostic compromise. Only the smallest liver lesion in each patient, classified as smaller or larger than 10 mm, was evaluated. The difference between hybrid iterative reconstruction (IR) and DLR images was determined by using a two-sided Wilcoxon signed-rank test. Results: The image noise was significantly lower, and the CNR was significantly higher on DLR images than hybrid IR images (median image noise: 19.2 vs 12.8 HU, P, 001; median CNR: Tumors, 10 mm: 1.9 vs 2.5; tumors. 10 mm: 1.7 vs 2.2, both P, 001). The scores for liver lesions were significantly higher for DLR images than hybrid IR images (P, 01 for both in tumors smaller or larger than 10 mm). Conclusion: DLR improved the quality of abdominal CT images for the evaluation of hypovascular hepatic metastases."
1,Semi-supervised adversarial model for benign-malignant lung nodule classification on chest CT,,
1,Performance of a deep learning model vs human reviewers in grading endoscopic disease severity of patients with ulcerative colitis,"IMPORTANCE Assessing endoscopic disease severity in ulcerative colitis (UC) is a key element in determining therapeutic response, but its use in clinical practice is limited by the requirement for experienced human reviewers. OBJECTIVE To determine whether deep learning models can grade the endoscopic severity of UC as well as experienced human reviewers. DESIGN, SETTING, AND PARTICIPANTS In this diagnostic study, retrospective grading of endoscopic images using the 4-levelMayo subscorewas performed by 2 independent reviewers with score discrepancies adjudicated by a third reviewer. Using 16 514 images from 3082 patients with UC who underwent colonoscopy at a single tertiary care referral center in the United States between January 1, 2007, and December 31, 2017, a 159-layer convolutional neural network (CNN) was constructed as a deep learning model to train and categorize images into 2 clinically relevant groups: Remission (Mayo subscore 0 or 1) and moderate to severe disease (Mayo subscore, 2 or 3). Ninety percent of the cohort was used to build the model and 10% was used to test it; the process was repeated 10 times. A set of 30 full-motion colonoscopy videos, unseen by the model, was then used for external validation to mimic real-world application. MAIN OUTCOMES AND MEASURES Model performance was assessed using area under the receiver operating curve (AUROC), sensitivity and specificity, positive predictive value (PPV), and negative predictive value (NPV). Kappa statistics (Îº) were used to measure agreement of the CNN relative to adjudicated human reference cores. RESULTS The authors included 16 514 images from 3082 unique patients (median [IQR] age, 41.3 [26.1-61.8] years, 1678 [54.4%] female), with 3980 images (24.1%) classified as moderate-to-severe disease by the adjudicated reference score. The CNN was excellent for distinguishing endoscopic remission from moderate-to-severe disease with an AUROC of 0.966 (95%CI, 0.967-0.972); a PPV of 0.87 (95%CI, 0.85-0.88) with a sensitivity of 83.0% (95%CI, 80.8%-85.4%) and specificty of 96.0%(95%CI, 95.1%-97.1%); and NPV of 0.94 (95%CI, 0.93-0.95).Weighted Îº agreement between the CNN and the adjudicated reference score was also good for identifying exactMayo subscores (Îº = 0.84; 95%CI, 0.83-0.86) and was similar to the agreement between experienced reviewers (Îº = 0.86; 95%CI, 0.85-0.87). Applying the CNN to entire colonoscopy videos had similar accuracy for identifying moderate to severe disease (AUROC, 0.97; 95%CI, 0.963-0.969). CONCLUSIONS AND RELEVANCE This study found that deep learning model performance was similar to experienced human reviewers in grading endoscopic severity of UC. Given its scalability, this approach could improve the use of colonoscopy for UC in both research and routine practice.","Performance of a deep learning model vs human reviewers in grading endoscopic disease severity of patients with ulcerative colitis. IMPORTANCE Assessing endoscopic disease severity in ulcerative colitis (UC) is a key element in determining therapeutic response, but its use in clinical practice is limited by the requirement for experienced human reviewers. OBJECTIVE To determine whether deep learning models can grade the endoscopic severity of UC as well as experienced human reviewers. DESIGN, SETTING, AND PARTICIPANTS In this diagnostic study, retrospective grading of endoscopic images using the 4-levelMayo subscorewas performed by 2 independent reviewers with score discrepancies adjudicated by a third reviewer. Using 16 514 images from 3082 patients with UC who underwent colonoscopy at a single tertiary care referral center in the United States between January 1, 2007, and December 31, 2017, a 159-layer convolutional neural network (CNN) was constructed as a deep learning model to train and categorize images into 2 clinically relevant groups: Remission (Mayo subscore 0 or 1) and moderate to severe disease (Mayo subscore, 2 or 3). Ninety percent of the cohort was used to build the model and 10% was used to test it; the process was repeated 10 times. A set of 30 full-motion colonoscopy videos, unseen by the model, was then used for external validation to mimic real-world application. MAIN OUTCOMES AND MEASURES Model performance was assessed using area under the receiver operating curve (AUROC), sensitivity and specificity, positive predictive value (PPV), and negative predictive value (NPV). Kappa statistics (Îº) were used to measure agreement of the CNN relative to adjudicated human reference cores. RESULTS The authors included 16 514 images from 3082 unique patients (median [IQR] age, 41.3 [26.1-61.8] years, 1678 [54.4%] female), with 3980 images (24.1%) classified as moderate-to-severe disease by the adjudicated reference score. The CNN was excellent for distinguishing endoscopic remission from moderate-to-severe disease with an AUROC of 0.966 (95%CI, 0.967-0.972); a PPV of 0.87 (95%CI, 0.85-0.88) with a sensitivity of 83.0% (95%CI, 80.8%-85.4%) and specificty of 96.0%(95%CI, 95.1%-97.1%); and NPV of 0.94 (95%CI, 0.93-0.95).Weighted Îº agreement between the CNN and the adjudicated reference score was also good for identifying exactMayo subscores (Îº = 0.84; 95%CI, 0.83-0.86) and was similar to the agreement between experienced reviewers (Îº = 0.86; 95%CI, 0.85-0.87). Applying the CNN to entire colonoscopy videos had similar accuracy for identifying moderate to severe disease (AUROC, 0.97; 95%CI, 0.963-0.969). CONCLUSIONS AND RELEVANCE This study found that deep learning model performance was similar to experienced human reviewers in grading endoscopic severity of UC. Given its scalability, this approach could improve the use of colonoscopy for UC in both research and routine practice."
1,Development and Evaluation of a Machine Learning Model for the Early Identification of Patients at Risk for Sepsis,"STUDY OBJECTIVE: The Third International Consensus Definitions (Sepsis-3) Task Force recommended the use of the quick Sequential [Sepsis-related] Organ Failure Assessment (qSOFA) score to screen patients for sepsis outside of the ICU. However, subsequent studies raise concerns about the sensitivity of qSOFA as a screening tool. We aim to use machine learning to develop a new sepsis screening tool, the Risk of Sepsis (RoS) score, and compare it with a slate of benchmark sepsis-screening tools, including the Systemic Inflammatory Response Syndrome, Sequential Organ Failure Assessment (SOFA), qSOFA, Modified Early Warning Score, and National Early Warning Score. METHODS: We used retrospective electronic health record data from adult patients who presented to 49 urban community hospital emergency departments during a 22-month period (N=2,759,529). We used the Rhee clinical surveillance criteria as our standard definition of sepsis and as the primary target for developing our model. The data were randomly split into training and test cohorts to derive and then evaluate the model. A feature selection process was carried out in 3 stages: first, we reviewed existing models for sepsis screening; second, we consulted with local subject matter experts; and third, we used a supervised machine learning called gradient boosting. Key metrics of performance included alert rate, area under the receiver operating characteristic curve, sensitivity, specificity, and precision. Performance was assessed at 1, 3, 6, 12, and 24 hours after an index time. RESULTS: The RoS score was the most discriminant screening tool at all time thresholds (area under the receiver operating characteristic curve 0.93 to 0.97). Compared with the next most discriminant benchmark (Sequential Organ Failure Assessment), RoS was significantly more sensitive (67.7% versus 49.2% at 1 hour and 84.6% versus 80.4% at 24 hours) and precise (27.6% versus 12.2% at 1 hour and 28.8% versus 11.4% at 24 hours). The sensitivity of qSOFA was relatively low (3.7% at 1 hour and 23.5% at 24 hours). CONCLUSION: In this retrospective study, RoS was more timely and discriminant than benchmark screening tools, including those recommend by the Sepsis-3 Task Force. Further study is needed to validate the RoS score at independent sites.","Development and Evaluation of a Machine Learning Model for the Early Identification of Patients at Risk for Sepsis. STUDY OBJECTIVE: The Third International Consensus Definitions (Sepsis-3) Task Force recommended the use of the quick Sequential [Sepsis-related] Organ Failure Assessment (qSOFA) score to screen patients for sepsis outside of the ICU. However, subsequent studies raise concerns about the sensitivity of qSOFA as a screening tool. We aim to use machine learning to develop a new sepsis screening tool, the Risk of Sepsis (RoS) score, and compare it with a slate of benchmark sepsis-screening tools, including the Systemic Inflammatory Response Syndrome, Sequential Organ Failure Assessment (SOFA), qSOFA, Modified Early Warning Score, and National Early Warning Score. METHODS: We used retrospective electronic health record data from adult patients who presented to 49 urban community hospital emergency departments during a 22-month period (N=2,759,529). We used the Rhee clinical surveillance criteria as our standard definition of sepsis and as the primary target for developing our model. The data were randomly split into training and test cohorts to derive and then evaluate the model. A feature selection process was carried out in 3 stages: first, we reviewed existing models for sepsis screening; second, we consulted with local subject matter experts; and third, we used a supervised machine learning called gradient boosting. Key metrics of performance included alert rate, area under the receiver operating characteristic curve, sensitivity, specificity, and precision. Performance was assessed at 1, 3, 6, 12, and 24 hours after an index time. RESULTS: The RoS score was the most discriminant screening tool at all time thresholds (area under the receiver operating characteristic curve 0.93 to 0.97). Compared with the next most discriminant benchmark (Sequential Organ Failure Assessment), RoS was significantly more sensitive (67.7% versus 49.2% at 1 hour and 84.6% versus 80.4% at 24 hours) and precise (27.6% versus 12.2% at 1 hour and 28.8% versus 11.4% at 24 hours). The sensitivity of qSOFA was relatively low (3.7% at 1 hour and 23.5% at 24 hours). CONCLUSION: In this retrospective study, RoS was more timely and discriminant than benchmark screening tools, including those recommend by the Sepsis-3 Task Force. Further study is needed to validate the RoS score at independent sites."
1,Recursive neural networks in hospital bed occupancy forecasting,"BACKGROUND: Efficient planning of hospital bed usage is a necessary condition to minimize the hospital costs. In the presented work we deal with the problem of occupancy forecasting in the scale of several months, with a focus on personnel's holiday planning. METHODS: We construct a model based on a set of recursive neural networks, which performs an occupancy prediction using historical admission and release data combined with external factors such as public and school holidays. The model requires no personal information on patients or staff. It is optimized for a 60â€‰days forecast during the summer season (May-September). RESULTS: An average mean absolute percentage error (MAPE) of 6.24% was computed on 8 validation sets. CONCLUSIONS: The proposed machine learning model has shown to be competitive to standard time-series forecasting models and can be recommended for incorporation in medium-size hospitals automatized scheduling and decision making.","Recursive neural networks in hospital bed occupancy forecasting. BACKGROUND: Efficient planning of hospital bed usage is a necessary condition to minimize the hospital costs. In the presented work we deal with the problem of occupancy forecasting in the scale of several months, with a focus on personnel's holiday planning. METHODS: We construct a model based on a set of recursive neural networks, which performs an occupancy prediction using historical admission and release data combined with external factors such as public and school holidays. The model requires no personal information on patients or staff. It is optimized for a 60â€‰days forecast during the summer season (May-September). RESULTS: An average mean absolute percentage error (MAPE) of 6.24% was computed on 8 validation sets. CONCLUSIONS: The proposed machine learning model has shown to be competitive to standard time-series forecasting models and can be recommended for incorporation in medium-size hospitals automatized scheduling and decision making."
1,Automated segmentation of cardiomyocyte Z-disks from high-throughput scanning electron microscopy data,"BACKGROUND: With the advent of new high-throughput electron microscopy techniques such as serial block-face scanning electron microscopy (SBF-SEM) and focused ion-beam scanning electron microscopy (FIB-SEM) biomedical scientists can study sub-cellular structural mechanisms of heart disease at high resolution and high volume. Among several key components that determine healthy contractile function in cardiomyocytes are Z-disks or Z-lines, which are located at the lateral borders of the sarcomere, the fundamental unit of striated muscle. Z-disks play the important role of anchoring contractile proteins within the cell that make the heartbeat. Changes to their organization can affect the force with which the cardiomyocyte contracts and may also affect signaling pathways that regulate cardiomyocyte health and function. Compared to other components in the cell, such as mitochondria, Z-disks appear as very thin linear structures in microscopy data with limited difference in contrast to the remaining components of the cell. METHODS: In this paper, we propose to generate a 3D model of Z-disks within single adult cardiac cells from an automated segmentation of a large serial-block-face scanning electron microscopy (SBF-SEM) dataset. The proposed fully automated segmentation scheme is comprised of three main modules including ""pre-processing"", ""segmentation"" and ""refinement"". We represent a simple, yet effective model to perform segmentation and refinement steps. Contrast stretching, and Gaussian kernels are used to pre-process the dataset, and well-known ""Sobel operators"" are used in the segmentation module. RESULTS: We have validated our model by comparing segmentation results with ground-truth annotated Z-disks in terms of pixel-wise accuracy. The results show that our model correctly detects Z-disks with 90.56% accuracy. We also compare and contrast the accuracy of the proposed algorithm in segmenting a FIB-SEM dataset against the accuracy of segmentations from a machine learning program called Ilastik and discuss the advantages and disadvantages that these two approaches have. CONCLUSIONS: Our validation results demonstrate the robustness and reliability of our algorithm and model both in terms of validation metrics and in terms of a comparison with a 3D visualisation of Z-disks obtained using immunofluorescence based confocal imaging.","Automated segmentation of cardiomyocyte Z-disks from high-throughput scanning electron microscopy data. BACKGROUND: With the advent of new high-throughput electron microscopy techniques such as serial block-face scanning electron microscopy (SBF-SEM) and focused ion-beam scanning electron microscopy (FIB-SEM) biomedical scientists can study sub-cellular structural mechanisms of heart disease at high resolution and high volume. Among several key components that determine healthy contractile function in cardiomyocytes are Z-disks or Z-lines, which are located at the lateral borders of the sarcomere, the fundamental unit of striated muscle. Z-disks play the important role of anchoring contractile proteins within the cell that make the heartbeat. Changes to their organization can affect the force with which the cardiomyocyte contracts and may also affect signaling pathways that regulate cardiomyocyte health and function. Compared to other components in the cell, such as mitochondria, Z-disks appear as very thin linear structures in microscopy data with limited difference in contrast to the remaining components of the cell. METHODS: In this paper, we propose to generate a 3D model of Z-disks within single adult cardiac cells from an automated segmentation of a large serial-block-face scanning electron microscopy (SBF-SEM) dataset. The proposed fully automated segmentation scheme is comprised of three main modules including ""pre-processing"", ""segmentation"" and ""refinement"". We represent a simple, yet effective model to perform segmentation and refinement steps. Contrast stretching, and Gaussian kernels are used to pre-process the dataset, and well-known ""Sobel operators"" are used in the segmentation module. RESULTS: We have validated our model by comparing segmentation results with ground-truth annotated Z-disks in terms of pixel-wise accuracy. The results show that our model correctly detects Z-disks with 90.56% accuracy. We also compare and contrast the accuracy of the proposed algorithm in segmenting a FIB-SEM dataset against the accuracy of segmentations from a machine learning program called Ilastik and discuss the advantages and disadvantages that these two approaches have. CONCLUSIONS: Our validation results demonstrate the robustness and reliability of our algorithm and model both in terms of validation metrics and in terms of a comparison with a 3D visualisation of Z-disks obtained using immunofluorescence based confocal imaging."
1,"AAR-RT - A system for auto-contouring organs at risk on CT images for radiation therapy planning: Principles, design, and large-scale evaluation on head-and-neck and thoracic cancer cases",,
1,Comparison of Machine Learning Methods with National Cardiovascular Data Registry Models for Prediction of Risk of Bleeding after Percutaneous Coronary Intervention,"Importance: Better prediction of major bleeding after percutaneous coronary intervention (PCI) may improve clinical decisions aimed to reduce bleeding risk. Machine learning techniques, bolstered by better selection of variables, hold promise for enhancing prediction. Objective: To determine whether machine learning techniques better predict post-PCI major bleeding compared with the existing National Cardiovascular Data Registry (NCDR) models. Design, Setting, and Participants: This comparative effectiveness study used the NCDR CathPCI Registry data version 4.4 (July 1, 2009, to April 1, 2015), machine learning techniques were used (logistic regression with lasso regularization and gradient descent boosting [XGBoost, version 0.71.2]), and output was then compared with the existing simplified risk score and full NCDR models. The existing models were recreated, and then performance was evaluated through additional techniques and variables in a 5-fold cross-validation in analysis conducted from October 1, 2015, to October 27, 2017. The setting was retrospective modeling of a nationwide clinical registry of PCI. Participants were all patients undergoing PCI. Percutaneous coronary intervention procedures were excluded if they were not the index PCI of admission, if the hospital site had missing outcomes measures, or if the patient underwent subsequent coronary artery bypass grafting. Exposures: Clinical variables available at admission and diagnostic coronary angiography data were used to determine the severity and complexity of presentation. Main Outcomes and Measures: The main outcome was in-hospital major bleeding within 72 hours after PCI. Results were evaluated by comparing C statistics, calibration, and decision threshold-based metrics, including the F score (harmonic mean of positive predictive value and sensitivity) and the false discovery rate. Results: The post-PCI major bleeding rate among 3316465 procedures (patients' median age, 65 years; interquartile range, 56-73 years; 68.1% male) was 4.5%. The existing full model achieved a mean C statistic of 0.78 (95% CI, 0.78-0.78). The use of XGBoost and full range of selected variables achieved a C statistic of 0.82 (95% CI, 0.82-0.82), with an F score of 0.31 (95% CI, 0.30-0.31). XGBoost correctly identified an additional 3.7% of cases identified as high risk who experienced a bleeding event and an overall improvement of 1.0% of cases identified as low risk who did not experience a bleeding event. The data-driven decision threshold helped improve the false discovery rate of the existing techniques. The existing simplified risk score model improved the false discovery rate from more than 90% to 78.7%. Modifying the model and the data decision threshold improved this rate from 78.7% to 73.4%. Conclusions and Relevance: Machine learning techniques improved the prediction of major bleeding after PCI. These techniques may help to better identify patients who would benefit most from strategies to reduce bleeding risk.","Comparison of Machine Learning Methods with National Cardiovascular Data Registry Models for Prediction of Risk of Bleeding after Percutaneous Coronary Intervention. Importance: Better prediction of major bleeding after percutaneous coronary intervention (PCI) may improve clinical decisions aimed to reduce bleeding risk. Machine learning techniques, bolstered by better selection of variables, hold promise for enhancing prediction. Objective: To determine whether machine learning techniques better predict post-PCI major bleeding compared with the existing National Cardiovascular Data Registry (NCDR) models. Design, Setting, and Participants: This comparative effectiveness study used the NCDR CathPCI Registry data version 4.4 (July 1, 2009, to April 1, 2015), machine learning techniques were used (logistic regression with lasso regularization and gradient descent boosting [XGBoost, version 0.71.2]), and output was then compared with the existing simplified risk score and full NCDR models. The existing models were recreated, and then performance was evaluated through additional techniques and variables in a 5-fold cross-validation in analysis conducted from October 1, 2015, to October 27, 2017. The setting was retrospective modeling of a nationwide clinical registry of PCI. Participants were all patients undergoing PCI. Percutaneous coronary intervention procedures were excluded if they were not the index PCI of admission, if the hospital site had missing outcomes measures, or if the patient underwent subsequent coronary artery bypass grafting. Exposures: Clinical variables available at admission and diagnostic coronary angiography data were used to determine the severity and complexity of presentation. Main Outcomes and Measures: The main outcome was in-hospital major bleeding within 72 hours after PCI. Results were evaluated by comparing C statistics, calibration, and decision threshold-based metrics, including the F score (harmonic mean of positive predictive value and sensitivity) and the false discovery rate. Results: The post-PCI major bleeding rate among 3316465 procedures (patients' median age, 65 years; interquartile range, 56-73 years; 68.1% male) was 4.5%. The existing full model achieved a mean C statistic of 0.78 (95% CI, 0.78-0.78). The use of XGBoost and full range of selected variables achieved a C statistic of 0.82 (95% CI, 0.82-0.82), with an F score of 0.31 (95% CI, 0.30-0.31). XGBoost correctly identified an additional 3.7% of cases identified as high risk who experienced a bleeding event and an overall improvement of 1.0% of cases identified as low risk who did not experience a bleeding event. The data-driven decision threshold helped improve the false discovery rate of the existing techniques. The existing simplified risk score model improved the false discovery rate from more than 90% to 78.7%. Modifying the model and the data decision threshold improved this rate from 78.7% to 73.4%. Conclusions and Relevance: Machine learning techniques improved the prediction of major bleeding after PCI. These techniques may help to better identify patients who would benefit most from strategies to reduce bleeding risk."
1,BIRNet: Brain image registration using dual-supervised fully convolutional networks,,
1,Combining entity co-occurrence with specialized word embeddings to measure entity relation in Alzheimer's disease,"BACKGROUND: Extracting useful information from biomedical literature plays an important role in the development of modern medicine. In natural language processing, there have been rigorous attempts to find meaningful relationships between entities automatically by co-occurrence-based methods. It has been increasingly important to understand whether relationships exist, and if so how strong, between any two entities extracted from a large number of texts. One of the defining methods is to measure semantic similarity and relatedness between two entities. METHODS: We propose a hybrid ranking method that combines a co-occurrence approach considering both direct and indirect entity pair relationship with specialized word embeddings for measuring the relatedness of two entities. RESULTS: We evaluate the proposed ranking method comparatively with other well-known methods such as co-occurrence, Word2Vec, COALS (Correlated Occurrence Analog to Lexical Semantics), and random indexing by calculating top-ranked entities related to Alzheimer's disease. In addition, we analyze gene, pathway, and gene-phenotype relationships. Overall, the proposed method tends to find more hidden relationships than the other methods. CONCLUSION: Our proposed method is able to select more useful related entities that not only highly co-occur but also have more indirect relations for the target entity. In pathway analysis, our proposed method shows superior performance at identifying (functional) cross clustering and higher-level pathways. Our proposed method, resulting from phenotype analysis, has an advantage in identifying the common genotype relating to phenotypes from biological literature.","Combining entity co-occurrence with specialized word embeddings to measure entity relation in Alzheimer's disease. BACKGROUND: Extracting useful information from biomedical literature plays an important role in the development of modern medicine. In natural language processing, there have been rigorous attempts to find meaningful relationships between entities automatically by co-occurrence-based methods. It has been increasingly important to understand whether relationships exist, and if so how strong, between any two entities extracted from a large number of texts. One of the defining methods is to measure semantic similarity and relatedness between two entities. METHODS: We propose a hybrid ranking method that combines a co-occurrence approach considering both direct and indirect entity pair relationship with specialized word embeddings for measuring the relatedness of two entities. RESULTS: We evaluate the proposed ranking method comparatively with other well-known methods such as co-occurrence, Word2Vec, COALS (Correlated Occurrence Analog to Lexical Semantics), and random indexing by calculating top-ranked entities related to Alzheimer's disease. In addition, we analyze gene, pathway, and gene-phenotype relationships. Overall, the proposed method tends to find more hidden relationships than the other methods. CONCLUSION: Our proposed method is able to select more useful related entities that not only highly co-occur but also have more indirect relations for the target entity. In pathway analysis, our proposed method shows superior performance at identifying (functional) cross clustering and higher-level pathways. Our proposed method, resulting from phenotype analysis, has an advantage in identifying the common genotype relating to phenotypes from biological literature."
1,DeepFHR: intelligent prediction of fetal Acidemia using fetal heart rate signals based on convolutional neural network,"BACKGROUND: Fetal heart rate (FHR) monitoring is a screening tool used by obstetricians to evaluate the fetal state. Because of the complexity and non-linearity, a visual interpretation of FHR signals using common guidelines usually results in significant subjective inter-observer and intra-observer variability. OBJECTIVE: Therefore, computer aided diagnosis (CAD) systems based on advanced artificial intelligence (AI) technology have recently been developed to assist obstetricians in making objective medical decisions. METHODS: In this work, we present an 8-layer deep convolutional neural network (CNN) framework to automatically predict fetal acidemia. After signal preprocessing, the input 2-dimensional (2D) images are obtained using the continuous wavelet transform (CWT), which provides a better way to observe and capture the hidden characteristic information of the FHR signals in both the time and frequency domains. Unlike the conventional machine learning (ML) approaches, this work does not require the execution of complex feature engineering, i.e., feature extraction and selection. In fact, 2D CNN model can self-learn useful features from the input data with the prerequisite of not losing informative features, representing the tremendous advantage of deep learning (DL) over ML. RESULTS: Based on the test open-access database (CTU-UHB), after comprehensive experimentation, we achieved better classification performance using the optimal CNN configuration compared to other state-of-the-art methods: the averaged ten-fold cross-validation of the accuracy, sensitivity, specificity, quality index defined as the geometric mean of the sensitivity and specificity, and the area under the curve yielded results of 98.34, 98.22, 94.87, 96.53 and 97.82%, respectively CONCLUSIONS: Once the proposed CNN model is successfully trained, the corresponding CAD system can be served as an effective tool to predict fetal asphyxia objectively and accurately.","DeepFHR: intelligent prediction of fetal Acidemia using fetal heart rate signals based on convolutional neural network. BACKGROUND: Fetal heart rate (FHR) monitoring is a screening tool used by obstetricians to evaluate the fetal state. Because of the complexity and non-linearity, a visual interpretation of FHR signals using common guidelines usually results in significant subjective inter-observer and intra-observer variability. OBJECTIVE: Therefore, computer aided diagnosis (CAD) systems based on advanced artificial intelligence (AI) technology have recently been developed to assist obstetricians in making objective medical decisions. METHODS: In this work, we present an 8-layer deep convolutional neural network (CNN) framework to automatically predict fetal acidemia. After signal preprocessing, the input 2-dimensional (2D) images are obtained using the continuous wavelet transform (CWT), which provides a better way to observe and capture the hidden characteristic information of the FHR signals in both the time and frequency domains. Unlike the conventional machine learning (ML) approaches, this work does not require the execution of complex feature engineering, i.e., feature extraction and selection. In fact, 2D CNN model can self-learn useful features from the input data with the prerequisite of not losing informative features, representing the tremendous advantage of deep learning (DL) over ML. RESULTS: Based on the test open-access database (CTU-UHB), after comprehensive experimentation, we achieved better classification performance using the optimal CNN configuration compared to other state-of-the-art methods: the averaged ten-fold cross-validation of the accuracy, sensitivity, specificity, quality index defined as the geometric mean of the sensitivity and specificity, and the area under the curve yielded results of 98.34, 98.22, 94.87, 96.53 and 97.82%, respectively CONCLUSIONS: Once the proposed CNN model is successfully trained, the corresponding CAD system can be served as an effective tool to predict fetal asphyxia objectively and accurately."
1,Rare disease knowledge enrichment through a data-driven approach,"BACKGROUND: Existing resources to assist the diagnosis of rare diseases are usually curated from the literature that can be limited for clinical use. It often takes substantial effort before the suspicion of a rare disease is even raised to utilize those resources. The primary goal of this study was to apply a data-driven approach to enrich existing rare disease resources by mining phenotype-disease associations from electronic medical record (EMR). METHODS: We first applied association rule mining algorithms on EMR to extract significant phenotype-disease associations and enriched existing rare disease resources (Human Phenotype Ontology and Orphanet (HPO-Orphanet)). We generated phenotype-disease bipartite graphs for HPO-Orphanet, EMR, and enriched knowledge base HPO-Orphanetâ€‰+â€‰and conducted a case study on Hodgkin lymphoma to compare performance on differential diagnosis among these three graphs. RESULTS: We used disease-disease similarity generated by the eRAM, an existing rare disease encyclopedia, as a gold standard to compare the three graphs with sensitivity and specificity as (0.17, 0.36, 0.46) and (0.52, 0.47, 0.51) for three graphs respectively. We also compared the top 15 diseases generated by the HPO-Orphanetâ€‰+â€‰graph with eRAM and another clinical diagnostic tool, the Phenomizer. CONCLUSIONS: Per our evaluation results, our approach was able to enrich existing rare disease knowledge resources with phenotype-disease associations from EMR and thus support rare disease differential diagnosis.","Rare disease knowledge enrichment through a data-driven approach. BACKGROUND: Existing resources to assist the diagnosis of rare diseases are usually curated from the literature that can be limited for clinical use. It often takes substantial effort before the suspicion of a rare disease is even raised to utilize those resources. The primary goal of this study was to apply a data-driven approach to enrich existing rare disease resources by mining phenotype-disease associations from electronic medical record (EMR). METHODS: We first applied association rule mining algorithms on EMR to extract significant phenotype-disease associations and enriched existing rare disease resources (Human Phenotype Ontology and Orphanet (HPO-Orphanet)). We generated phenotype-disease bipartite graphs for HPO-Orphanet, EMR, and enriched knowledge base HPO-Orphanetâ€‰+â€‰and conducted a case study on Hodgkin lymphoma to compare performance on differential diagnosis among these three graphs. RESULTS: We used disease-disease similarity generated by the eRAM, an existing rare disease encyclopedia, as a gold standard to compare the three graphs with sensitivity and specificity as (0.17, 0.36, 0.46) and (0.52, 0.47, 0.51) for three graphs respectively. We also compared the top 15 diseases generated by the HPO-Orphanetâ€‰+â€‰graph with eRAM and another clinical diagnostic tool, the Phenomizer. CONCLUSIONS: Per our evaluation results, our approach was able to enrich existing rare disease knowledge resources with phenotype-disease associations from EMR and thus support rare disease differential diagnosis."
1,Supervised learning for bone shape and cortical thickness estimation from CT images for finite element analysis,"Knowledge about the thickness of the cortical bone is of high interest for fracture risk assessment. Most finite element model solutions overlook this information because of the coarse resolution of the CT images. To circumvent this limitation, a three-steps approach is proposed. 1) Two initial surface meshes approximating the outer and inner cortical surfaces are generated via a shape regression based on morphometric features and statistical shape model parameters. 2) The meshes are then corrected locally using a supervised learning model build from image features extracted from pairs of QCT (0.3-1 mm resolution) and HRpQCT images (82 microm resolution). As the resulting meshes better follow the cortical surfaces, the cortical thickness can be estimated at sub-voxel precision. 3) The meshes are finally regularized by a Gaussian process model featuring a two-kernel model, which seamlessly enables smoothness and shape-awareness priors during regularization. The resulting meshes yield high-quality mesh element properties, suitable for construction of tetrahedral meshes and finite element simulations. This pipeline was applied to 36 pairs of proximal femurs (17 males, 19 females, 76+/-12 years) scanned under QCT and HRpQCT modalities. On a set of leave-one-out experiments, we quantified accuracy (root mean square error = 0.36+/-0.29 mm) and robustness (Hausdorff distance = 3.90+/-1.57 mm) of the outer surface meshes. The error in the estimated cortical thickness (0.05+/-0.40 mm), and the tetrahedral mesh quality (aspect ratio = 1.4+/-0.02) are also reported. The proposed pipeline produces finite element meshes with patient-specific bone shape and sub-voxel cortical thickness directly from CT scans. It also ensures that the nodes and elements numbering remains consistent and independent of the morphology, which is a distinct advantage in population studies.","Supervised learning for bone shape and cortical thickness estimation from CT images for finite element analysis. Knowledge about the thickness of the cortical bone is of high interest for fracture risk assessment. Most finite element model solutions overlook this information because of the coarse resolution of the CT images. To circumvent this limitation, a three-steps approach is proposed. 1) Two initial surface meshes approximating the outer and inner cortical surfaces are generated via a shape regression based on morphometric features and statistical shape model parameters. 2) The meshes are then corrected locally using a supervised learning model build from image features extracted from pairs of QCT (0.3-1 mm resolution) and HRpQCT images (82 microm resolution). As the resulting meshes better follow the cortical surfaces, the cortical thickness can be estimated at sub-voxel precision. 3) The meshes are finally regularized by a Gaussian process model featuring a two-kernel model, which seamlessly enables smoothness and shape-awareness priors during regularization. The resulting meshes yield high-quality mesh element properties, suitable for construction of tetrahedral meshes and finite element simulations. This pipeline was applied to 36 pairs of proximal femurs (17 males, 19 females, 76+/-12 years) scanned under QCT and HRpQCT modalities. On a set of leave-one-out experiments, we quantified accuracy (root mean square error = 0.36+/-0.29 mm) and robustness (Hausdorff distance = 3.90+/-1.57 mm) of the outer surface meshes. The error in the estimated cortical thickness (0.05+/-0.40 mm), and the tetrahedral mesh quality (aspect ratio = 1.4+/-0.02) are also reported. The proposed pipeline produces finite element meshes with patient-specific bone shape and sub-voxel cortical thickness directly from CT scans. It also ensures that the nodes and elements numbering remains consistent and independent of the morphology, which is a distinct advantage in population studies."
1,Ultra-Low-Dose (18)F-Florbetaben Amyloid PET Imaging Using Deep Learning with Multi-Contrast MRI Inputs,"Purpose To reduce radiotracer requirements for amyloid PET/MRI without sacrificing diagnostic quality by using deep learning methods. Materials and Methods Forty data sets from 39 patients (mean age +/- standard deviation [SD], 67 years +/- 8), including 16 male patients and 23 female patients (mean age, 66 years +/- 6 and 68 years +/- 9, respectively), who underwent simultaneous amyloid (fluorine 18 [(18)F]-florbetaben) PET/MRI examinations were acquired from March 2016 through October 2017 and retrospectively analyzed. One hundredth of the raw list-mode PET data were randomly chosen to simulate a low-dose (1%) acquisition. Convolutional neural networks were implemented with low-dose PET and multiple MR images (PET-plus-MR model) or with low-dose PET alone (PET-only) as inputs to predict full-dose PET images. Quality of the synthesized images was evaluated while Bland-Altman plots assessed the agreement of regional standard uptake value ratios (SUVRs) between image types. Two readers scored image quality on a five-point scale (5 = excellent) and determined amyloid status (positive or negative). Statistical analyses were carried out to assess the difference of image quality metrics and reader agreement and to determine confidence intervals (CIs) for reading results. Results The synthesized images (especially from the PET-plus-MR model) showed marked improvement on all quality metrics compared with the low-dose image. All PET-plus-MR images scored 3 or higher, with proportions of images rated greater than 3 similar to those for the full-dose images (-10% difference [eight of 80 readings], 95% CI: -15%, -5%). Accuracy for amyloid status was high (71 of 80 readings [89%]) and similar to intrareader reproducibility of full-dose images (73 of 80 [91%]). The PET-plus-MR model also had the smallest mean and variance for SUVR difference to full-dose images. Conclusion Simultaneously acquired MRI and ultra-low-dose PET data can be used to synthesize full-dose-like amyloid PET images. (c) RSNA, 2018 Online supplemental material is available for this article. See also the editorial by Catana in this issue.","Ultra-Low-Dose (18)F-Florbetaben Amyloid PET Imaging Using Deep Learning with Multi-Contrast MRI Inputs. Purpose To reduce radiotracer requirements for amyloid PET/MRI without sacrificing diagnostic quality by using deep learning methods. Materials and Methods Forty data sets from 39 patients (mean age +/- standard deviation [SD], 67 years +/- 8), including 16 male patients and 23 female patients (mean age, 66 years +/- 6 and 68 years +/- 9, respectively), who underwent simultaneous amyloid (fluorine 18 [(18)F]-florbetaben) PET/MRI examinations were acquired from March 2016 through October 2017 and retrospectively analyzed. One hundredth of the raw list-mode PET data were randomly chosen to simulate a low-dose (1%) acquisition. Convolutional neural networks were implemented with low-dose PET and multiple MR images (PET-plus-MR model) or with low-dose PET alone (PET-only) as inputs to predict full-dose PET images. Quality of the synthesized images was evaluated while Bland-Altman plots assessed the agreement of regional standard uptake value ratios (SUVRs) between image types. Two readers scored image quality on a five-point scale (5 = excellent) and determined amyloid status (positive or negative). Statistical analyses were carried out to assess the difference of image quality metrics and reader agreement and to determine confidence intervals (CIs) for reading results. Results The synthesized images (especially from the PET-plus-MR model) showed marked improvement on all quality metrics compared with the low-dose image. All PET-plus-MR images scored 3 or higher, with proportions of images rated greater than 3 similar to those for the full-dose images (-10% difference [eight of 80 readings], 95% CI: -15%, -5%). Accuracy for amyloid status was high (71 of 80 readings [89%]) and similar to intrareader reproducibility of full-dose images (73 of 80 [91%]). The PET-plus-MR model also had the smallest mean and variance for SUVR difference to full-dose images. Conclusion Simultaneously acquired MRI and ultra-low-dose PET data can be used to synthesize full-dose-like amyloid PET images. (c) RSNA, 2018 Online supplemental material is available for this article. See also the editorial by Catana in this issue."
1,Multi-institutional Clinical Tool for Predicting High-risk Lesions on 3 Tesla Multiparametric Prostate Magnetic Resonance Imaging,"Background: Multiparametric magnetic resonance imaging (mpMRI) for prostate cancer detection without careful patient selection may lead to excessive resource utilization and costs. Objective: To develop and validate a clinical tool for predicting the presence of high-risk lesions on mpMRI. Design, setting, and participants: Four tertiary care centers were included in this retrospective and prospective study (BiRCH Study Collaborative). Statistical models were generated using 1269 biopsy-naive, prior negative biopsy, and active surveillance patients who underwent mpMRI. Using age, prostate-specific antigen, and prostate volume, a support vector machine model was developed for predicting the probability of harboring Prostate Imaging Reporting and Data System 4 or 5 lesions. The accuracy of future predictions was then prospectively assessed in 214 consecutive patients. Outcome measurements and statistical analysis: Receiver operating characteristic, calibration, and decision curves were generated to assess model performance. Results and limitations: For biopsy-naÃ¯ve and prior negative biopsy patients (n = 811), the area under the curve (AUC) was 0.730 on internal validation. Excellent calibration and high net clinical benefit were observed. On prospective external validation at two separate institutions (n = 88 and n = 126), the machine learning model discriminated with AUCs of 0.740 and 0.744, respectively. The final model was developed on the Microsoft Azure Machine Learning platform (birch.azurewebsites.net). This model requires a prostate volume measurement as input. Conclusions: In patients who are naÃ¯ve to biopsy or those with a prior negative biopsy, BiRCH models can be used to select patients for mpMRI. Patient summary: In this multicenter study, we developed and prospectively validated a calculator that can be used to predict prostate magnetic resonance imaging (MRI) results using patient age, prostate-specific antigen, and prostate volume as input. This tool can aid health care professionals and patients to make an informed decision regarding whether to get an MRI. Previously, there were no clinical tools to predict Prostate Imaging Reporting and Data System 4â€“5 before magnetic resonance imaging (MRI). BiRCH models were developed to predict the MRI result using logistic regression and machine learning, and can be used to select patients for multiparametric MRI.","Multi-institutional Clinical Tool for Predicting High-risk Lesions on 3 Tesla Multiparametric Prostate Magnetic Resonance Imaging. Background: Multiparametric magnetic resonance imaging (mpMRI) for prostate cancer detection without careful patient selection may lead to excessive resource utilization and costs. Objective: To develop and validate a clinical tool for predicting the presence of high-risk lesions on mpMRI. Design, setting, and participants: Four tertiary care centers were included in this retrospective and prospective study (BiRCH Study Collaborative). Statistical models were generated using 1269 biopsy-naive, prior negative biopsy, and active surveillance patients who underwent mpMRI. Using age, prostate-specific antigen, and prostate volume, a support vector machine model was developed for predicting the probability of harboring Prostate Imaging Reporting and Data System 4 or 5 lesions. The accuracy of future predictions was then prospectively assessed in 214 consecutive patients. Outcome measurements and statistical analysis: Receiver operating characteristic, calibration, and decision curves were generated to assess model performance. Results and limitations: For biopsy-naÃ¯ve and prior negative biopsy patients (n = 811), the area under the curve (AUC) was 0.730 on internal validation. Excellent calibration and high net clinical benefit were observed. On prospective external validation at two separate institutions (n = 88 and n = 126), the machine learning model discriminated with AUCs of 0.740 and 0.744, respectively. The final model was developed on the Microsoft Azure Machine Learning platform (birch.azurewebsites.net). This model requires a prostate volume measurement as input. Conclusions: In patients who are naÃ¯ve to biopsy or those with a prior negative biopsy, BiRCH models can be used to select patients for mpMRI. Patient summary: In this multicenter study, we developed and prospectively validated a calculator that can be used to predict prostate magnetic resonance imaging (MRI) results using patient age, prostate-specific antigen, and prostate volume as input. This tool can aid health care professionals and patients to make an informed decision regarding whether to get an MRI. Previously, there were no clinical tools to predict Prostate Imaging Reporting and Data System 4â€“5 before magnetic resonance imaging (MRI). BiRCH models were developed to predict the MRI result using logistic regression and machine learning, and can be used to select patients for multiparametric MRI."
1,"Comparison of the accuracy of human readers versus machine-learning algorithms for pigmented skin lesion classification: an open, web-based, international, diagnostic study","BACKGROUND: Whether machine-learning algorithms can diagnose all pigmented skin lesions as accurately as human experts is unclear. The aim of this study was to compare the diagnostic accuracy of state-of-the-art machine-learning algorithms with human readers for all clinically relevant types of benign and malignant pigmented skin lesions. METHODS: For this open, web-based, international, diagnostic study, human readers were asked to diagnose dermatoscopic images selected randomly in 30-image batches from a test set of 1511 images. The diagnoses from human readers were compared with those of 139 algorithms created by 77 machine-learning labs, who participated in the International Skin Imaging Collaboration 2018 challenge and received a training set of 10 015 images in advance. The ground truth of each lesion fell into one of seven predefined disease categories: intraepithelial carcinoma including actinic keratoses and Bowen's disease; basal cell carcinoma; benign keratinocytic lesions including solar lentigo, seborrheic keratosis and lichen planus-like keratosis; dermatofibroma; melanoma; melanocytic nevus; and vascular lesions. The two main outcomes were the differences in the number of correct specific diagnoses per batch between all human readers and the top three algorithms, and between human experts and the top three algorithms. FINDINGS: Between Aug 4, 2018, and Sept 30, 2018, 511 human readers from 63 countries had at least one attempt in the reader study. 283 (55.4%) of 511 human readers were board-certified dermatologists, 118 (23.1%) were dermatology residents, and 83 (16.2%) were general practitioners. When comparing all human readers with all machine-learning algorithms, the algorithms achieved a mean of 2.01 (95% CI 1.97 to 2.04; p<0.0001) more correct diagnoses (17.91 [SD 3.42] vs 19.92 [4.27]). 27 human experts with more than 10 years of experience achieved a mean of 18.78 (SD 3.15) correct answers, compared with 25.43 (1.95) correct answers for the top three machine algorithms (mean difference 6.65, 95% CI 6.06-7.25; p<0.0001). The difference between human experts and the top three algorithms was significantly lower for images in the test set that were collected from sources not included in the training set (human underperformance of 11.4%, 95% CI 9.9-12.9 vs 3.6%, 0.8-6.3; p<0.0001). INTERPRETATION: State-of-the-art machine-learning classifiers outperformed human experts in the diagnosis of pigmented skin lesions and should have a more important role in clinical practice. However, a possible limitation of these algorithms is their decreased performance for out-of-distribution images, which should be addressed in future research. FUNDING: None.","Comparison of the accuracy of human readers versus machine-learning algorithms for pigmented skin lesion classification: an open, web-based, international, diagnostic study. BACKGROUND: Whether machine-learning algorithms can diagnose all pigmented skin lesions as accurately as human experts is unclear. The aim of this study was to compare the diagnostic accuracy of state-of-the-art machine-learning algorithms with human readers for all clinically relevant types of benign and malignant pigmented skin lesions. METHODS: For this open, web-based, international, diagnostic study, human readers were asked to diagnose dermatoscopic images selected randomly in 30-image batches from a test set of 1511 images. The diagnoses from human readers were compared with those of 139 algorithms created by 77 machine-learning labs, who participated in the International Skin Imaging Collaboration 2018 challenge and received a training set of 10 015 images in advance. The ground truth of each lesion fell into one of seven predefined disease categories: intraepithelial carcinoma including actinic keratoses and Bowen's disease; basal cell carcinoma; benign keratinocytic lesions including solar lentigo, seborrheic keratosis and lichen planus-like keratosis; dermatofibroma; melanoma; melanocytic nevus; and vascular lesions. The two main outcomes were the differences in the number of correct specific diagnoses per batch between all human readers and the top three algorithms, and between human experts and the top three algorithms. FINDINGS: Between Aug 4, 2018, and Sept 30, 2018, 511 human readers from 63 countries had at least one attempt in the reader study. 283 (55.4%) of 511 human readers were board-certified dermatologists, 118 (23.1%) were dermatology residents, and 83 (16.2%) were general practitioners. When comparing all human readers with all machine-learning algorithms, the algorithms achieved a mean of 2.01 (95% CI 1.97 to 2.04; p<0.0001) more correct diagnoses (17.91 [SD 3.42] vs 19.92 [4.27]). 27 human experts with more than 10 years of experience achieved a mean of 18.78 (SD 3.15) correct answers, compared with 25.43 (1.95) correct answers for the top three machine algorithms (mean difference 6.65, 95% CI 6.06-7.25; p<0.0001). The difference between human experts and the top three algorithms was significantly lower for images in the test set that were collected from sources not included in the training set (human underperformance of 11.4%, 95% CI 9.9-12.9 vs 3.6%, 0.8-6.3; p<0.0001). INTERPRETATION: State-of-the-art machine-learning classifiers outperformed human experts in the diagnosis of pigmented skin lesions and should have a more important role in clinical practice. However, a possible limitation of these algorithms is their decreased performance for out-of-distribution images, which should be addressed in future research. FUNDING: None."
1,Self-supervised learning for medical image analysis using image context restoration,,
1,Assessment of Automated Identification of Phases in Videos of Cataract Surgery Using Machine Learning and Deep Learning Techniques,"Importance: Competence in cataract surgery is a public health necessity, and videos of cataract surgery are routinely available to educators and trainees but currently are of limited use in training. Machine learning and deep learning techniques can yield tools that efficiently segment videos of cataract surgery into constituent phases for subsequent automated skill assessment and feedback. Objective: To evaluate machine learning and deep learning algorithms for automated phase classification of manually presegmented phases in videos of cataract surgery. Design, Setting, and Participants: This was a cross-sectional study using a data set of videos from a convenience sample of 100 cataract procedures performed by faculty and trainee surgeons in an ophthalmology residency program from July 2011 to December 2017. Demographic characteristics for surgeons and patients were not captured. Ten standard labels in the procedure and 14 instruments used during surgery were manually annotated, which served as the ground truth. Exposures: Five algorithms with different input data: (1) a support vector machine input with cross-sectional instrument label data; (2) a recurrent neural network (RNN) input with a time series of instrument labels; (3) a convolutional neural network (CNN) input with cross-sectional image data; (4) a CNN-RNN input with a time series of images; and (5) a CNN-RNN input with time series of images and instrument labels. Each algorithm was evaluated with 5-fold cross-validation. Main Outcomes and Measures: Accuracy, area under the receiver operating characteristic curve, sensitivity, specificity, and precision. Results: Unweighted accuracy for the 5 algorithms ranged between 0.915 and 0.959. Area under the receiver operating characteristic curve for the 5 algorithms ranged between 0.712 and 0.773, with small differences among them. The area under the receiver operating characteristic curve for the image-only CNN-RNN (0.752) was significantly greater than that of the CNN with cross-sectional image data (0.712) (difference, -0.040; 95% CI, -0.049 to -0.033) and the CNN-RNN with images and instrument labels (0.737) (difference, 0.016; 95% CI, 0.014 to 0.018). While specificity was uniformly high for all phases with all 5 algorithms (range, 0.877 to 0.999), sensitivity ranged between 0.005 (95% CI, 0.000 to 0.015) for the support vector machine for wound closure (corneal hydration) and 0.974 (95% CI, 0.957 to 0.991) for the RNN for main incision. Precision ranged between 0.283 and 0.963. Conclusions and Relevance: Time series modeling of instrument labels and video images using deep learning techniques may yield potentially useful tools for the automated detection of phases in cataract surgery procedures.","Assessment of Automated Identification of Phases in Videos of Cataract Surgery Using Machine Learning and Deep Learning Techniques. Importance: Competence in cataract surgery is a public health necessity, and videos of cataract surgery are routinely available to educators and trainees but currently are of limited use in training. Machine learning and deep learning techniques can yield tools that efficiently segment videos of cataract surgery into constituent phases for subsequent automated skill assessment and feedback. Objective: To evaluate machine learning and deep learning algorithms for automated phase classification of manually presegmented phases in videos of cataract surgery. Design, Setting, and Participants: This was a cross-sectional study using a data set of videos from a convenience sample of 100 cataract procedures performed by faculty and trainee surgeons in an ophthalmology residency program from July 2011 to December 2017. Demographic characteristics for surgeons and patients were not captured. Ten standard labels in the procedure and 14 instruments used during surgery were manually annotated, which served as the ground truth. Exposures: Five algorithms with different input data: (1) a support vector machine input with cross-sectional instrument label data; (2) a recurrent neural network (RNN) input with a time series of instrument labels; (3) a convolutional neural network (CNN) input with cross-sectional image data; (4) a CNN-RNN input with a time series of images; and (5) a CNN-RNN input with time series of images and instrument labels. Each algorithm was evaluated with 5-fold cross-validation. Main Outcomes and Measures: Accuracy, area under the receiver operating characteristic curve, sensitivity, specificity, and precision. Results: Unweighted accuracy for the 5 algorithms ranged between 0.915 and 0.959. Area under the receiver operating characteristic curve for the 5 algorithms ranged between 0.712 and 0.773, with small differences among them. The area under the receiver operating characteristic curve for the image-only CNN-RNN (0.752) was significantly greater than that of the CNN with cross-sectional image data (0.712) (difference, -0.040; 95% CI, -0.049 to -0.033) and the CNN-RNN with images and instrument labels (0.737) (difference, 0.016; 95% CI, 0.014 to 0.018). While specificity was uniformly high for all phases with all 5 algorithms (range, 0.877 to 0.999), sensitivity ranged between 0.005 (95% CI, 0.000 to 0.015) for the support vector machine for wound closure (corneal hydration) and 0.974 (95% CI, 0.957 to 0.991) for the RNN for main incision. Precision ranged between 0.283 and 0.963. Conclusions and Relevance: Time series modeling of instrument labels and video images using deep learning techniques may yield potentially useful tools for the automated detection of phases in cataract surgery procedures."
1,Using machine learning models to improve stroke risk level classification methods of China national stroke screening,"BACKGROUND: With the character of high incidence, high prevalence and high mortality, stroke has brought a heavy burden to families and society in China. In 2009, the Ministry of Health of China launched the China national stroke screening and intervention program, which screens stroke and its risk factors and conducts high-risk population interventions for people aged above 40â€‰years old all over China. In this program, stroke risk factors include hypertension, diabetes, dyslipidemia, smoking, lack of exercise, apparently overweight and family history of stroke. People with more than two risk factors or history of stroke or transient ischemic attack (TIA) are considered as high-risk. However, it is impossible for this criterion to classify stroke risk levels for people with unknown values in fields of risk factors. The missing of stroke risk levels results in reduced efficiency of stroke interventions and inaccuracies in statistical results at the national level. In this paper, we use 2017 national stroke screening data to develop stroke risk classification models based on machine learning algorithms to improve the classification efficiency. METHOD: Firstly, we construct training set and test sets and process the imbalance training set based on oversampling and undersampling method. Then, we develop logistic regression model, NaÃ¯ve Bayesian model, Bayesian network model, decision tree model, neural network model, random forest model, bagged decision tree model, voting model and boosting model with decision trees to classify stroke risk levels. RESULT: The recall of the boosting model with decision trees is the highest (99.94%), and the precision of the model based on the random forest is highest (97.33%). Using the random forest model (recall: 98.44%), the recall will be increased by about 2.8% compared with the method currently used, and several thousands more people with high risk of stroke can be identified each year. CONCLUSION: Models developed in this paper can improve the current screening method in the way that it can avoid the impact of unknown values, and avoid unnecessary rescreening and intervention expenditures. The national stroke screening program can choose classification models according to the practice need.","Using machine learning models to improve stroke risk level classification methods of China national stroke screening. BACKGROUND: With the character of high incidence, high prevalence and high mortality, stroke has brought a heavy burden to families and society in China. In 2009, the Ministry of Health of China launched the China national stroke screening and intervention program, which screens stroke and its risk factors and conducts high-risk population interventions for people aged above 40â€‰years old all over China. In this program, stroke risk factors include hypertension, diabetes, dyslipidemia, smoking, lack of exercise, apparently overweight and family history of stroke. People with more than two risk factors or history of stroke or transient ischemic attack (TIA) are considered as high-risk. However, it is impossible for this criterion to classify stroke risk levels for people with unknown values in fields of risk factors. The missing of stroke risk levels results in reduced efficiency of stroke interventions and inaccuracies in statistical results at the national level. In this paper, we use 2017 national stroke screening data to develop stroke risk classification models based on machine learning algorithms to improve the classification efficiency. METHOD: Firstly, we construct training set and test sets and process the imbalance training set based on oversampling and undersampling method. Then, we develop logistic regression model, NaÃ¯ve Bayesian model, Bayesian network model, decision tree model, neural network model, random forest model, bagged decision tree model, voting model and boosting model with decision trees to classify stroke risk levels. RESULT: The recall of the boosting model with decision trees is the highest (99.94%), and the precision of the model based on the random forest is highest (97.33%). Using the random forest model (recall: 98.44%), the recall will be increased by about 2.8% compared with the method currently used, and several thousands more people with high risk of stroke can be identified each year. CONCLUSION: Models developed in this paper can improve the current screening method in the way that it can avoid the impact of unknown values, and avoid unnecessary rescreening and intervention expenditures. The national stroke screening program can choose classification models according to the practice need."
1,A parsimonious 3-gene signature predicts clinical outcomes in an acute myeloid leukemia multicohort study,"Acute myeloid leukemia (AML) is a genetically heterogeneous hematological malignancy with variable responses to chemotherapy. Although recurring cytogenetic abnormalities and gene mutations are important predictors of outcome, 50% to 70% of AMLs harbor normal or risk-indeterminate karyotypes. Therefore, identifying more effective biomarkers predictive of treatment success and failure is essential for informing tailored therapeutic decisions. We applied an artificial neural network (ANN)-based machine learning approach to a publicly available data set for a discovery cohort of 593 adults with nonpromyelocytic AML. ANN analysis identified a parsimonious 3-gene expression signature comprising CALCRL, CD109, and LSP1, which was predictive of event-free survival (EFS) and overall survival (OS). We computed a prognostic index (PI) using normalized gene-expression levels and b-values from subsequently created Cox proportional hazards models, coupled with clinically established prognosticators. Our 3-gene PI separated the adult patients in each European LeukemiaNet cytogenetic risk category into subgroups with different survival probabilities and identified patients with very high-risk features, such as those with a high PI and either FLT3 internal tandem duplication or nonmutated nucleophosmin 1. The PI remained significantly associated with poor EFS and OS after adjusting for established prognosticators, and its ability to stratify survival was validated in 3 independent adult cohorts (n = 905 subjects) and 1 cohort of childhood AML (n = 145 subjects). Further in silico analyses established that AML was the only tumor type among 39 distinct malignancies for which the concomitant upregulation of CALCRL, CD109, and LSP1 predicted survival. Therefore, our ANN-derived 3-gene signature refines the accuracy of patient stratification and the potential to significantly improve outcome prediction.","A parsimonious 3-gene signature predicts clinical outcomes in an acute myeloid leukemia multicohort study. Acute myeloid leukemia (AML) is a genetically heterogeneous hematological malignancy with variable responses to chemotherapy. Although recurring cytogenetic abnormalities and gene mutations are important predictors of outcome, 50% to 70% of AMLs harbor normal or risk-indeterminate karyotypes. Therefore, identifying more effective biomarkers predictive of treatment success and failure is essential for informing tailored therapeutic decisions. We applied an artificial neural network (ANN)-based machine learning approach to a publicly available data set for a discovery cohort of 593 adults with nonpromyelocytic AML. ANN analysis identified a parsimonious 3-gene expression signature comprising CALCRL, CD109, and LSP1, which was predictive of event-free survival (EFS) and overall survival (OS). We computed a prognostic index (PI) using normalized gene-expression levels and b-values from subsequently created Cox proportional hazards models, coupled with clinically established prognosticators. Our 3-gene PI separated the adult patients in each European LeukemiaNet cytogenetic risk category into subgroups with different survival probabilities and identified patients with very high-risk features, such as those with a high PI and either FLT3 internal tandem duplication or nonmutated nucleophosmin 1. The PI remained significantly associated with poor EFS and OS after adjusting for established prognosticators, and its ability to stratify survival was validated in 3 independent adult cohorts (n = 905 subjects) and 1 cohort of childhood AML (n = 145 subjects). Further in silico analyses established that AML was the only tumor type among 39 distinct malignancies for which the concomitant upregulation of CALCRL, CD109, and LSP1 predicted survival. Therefore, our ANN-derived 3-gene signature refines the accuracy of patient stratification and the potential to significantly improve outcome prediction."
1,Multi-task learning for quality assessment of fetal head ultrasound images,,
1,Utilizing Precision Medicine to Estimate Timing for Surgical Closure of Traumatic Extremity Wounds,,
1,Forecasting one-day-forward wellness conditions for community-dwelling elderly with single lead short electrocardiogram signals,"BACKGROUND: The accelerated growth of elderly population is creating a heavy burden to the healthcare system in many developed countries and regions. Electrocardiogram (ECG) analysis has been recognized as effective approach to cardiovascular disease diagnosis and widely utilized for monitoring personalized health conditions. METHOD: In this study, we present a novel approach to forecasting one-day-forward wellness conditions for community-dwelling elderly by analyzing single lead short ECG signals acquired from a station-based monitoring device. More specifically, exponentially weighted moving-average (EWMA) method is employed to eliminate the high-frequency noise from original signals at first. Then, Fisher-Yates normalization approach is used to adjust the self-evaluated wellness score distribution since the scores among different individuals are skewed. Finally, both deep learning-based and traditional machine learning-based methods are utilized for building wellness forecasting models. RESULTS: The experiment results show that the deep learning-based methods achieve the best fitted forecasting performance, where the forecasting accuracy and F value are 93.21% and 91.98% respectively. The deep learning-based methods, with the merit of non-hand-crafted engineering, have superior wellness forecasting performance towards the competitive traditional machine learning-based methods. CONCLUSION: The developed approach in this paper is effective in wellness forecasting for community-dwelling elderly, which can provide insights in terms of implementing a cost-effective approach to informing healthcare provider about health conditions of elderly in advance and taking timely interventions to reduce the risk of malignant events.","Forecasting one-day-forward wellness conditions for community-dwelling elderly with single lead short electrocardiogram signals. BACKGROUND: The accelerated growth of elderly population is creating a heavy burden to the healthcare system in many developed countries and regions. Electrocardiogram (ECG) analysis has been recognized as effective approach to cardiovascular disease diagnosis and widely utilized for monitoring personalized health conditions. METHOD: In this study, we present a novel approach to forecasting one-day-forward wellness conditions for community-dwelling elderly by analyzing single lead short ECG signals acquired from a station-based monitoring device. More specifically, exponentially weighted moving-average (EWMA) method is employed to eliminate the high-frequency noise from original signals at first. Then, Fisher-Yates normalization approach is used to adjust the self-evaluated wellness score distribution since the scores among different individuals are skewed. Finally, both deep learning-based and traditional machine learning-based methods are utilized for building wellness forecasting models. RESULTS: The experiment results show that the deep learning-based methods achieve the best fitted forecasting performance, where the forecasting accuracy and F value are 93.21% and 91.98% respectively. The deep learning-based methods, with the merit of non-hand-crafted engineering, have superior wellness forecasting performance towards the competitive traditional machine learning-based methods. CONCLUSION: The developed approach in this paper is effective in wellness forecasting for community-dwelling elderly, which can provide insights in terms of implementing a cost-effective approach to informing healthcare provider about health conditions of elderly in advance and taking timely interventions to reduce the risk of malignant events."
1,Assessment of Machine Learning vs Standard Prediction Rules for Predicting Hospital Readmissions,"Importance: Hospital readmissions are associated with patient harm and expense. Ways to prevent hospital readmissions have focused on identifying patients at greatest risk using prediction scores. Objective: To identify the type of score that best predicts hospital readmissions. Design, Setting, and Participants: This prognostic study included 14â€¯062 consecutive adult hospital patients with 16â€¯649 discharges from a tertiary care center, suburban community hospital, and urban critical access hospital in Maryland from September 1, 2016, through December 31, 2016. Patients not included as eligible discharges by the Centers for Medicare & Medicaid Services or the Chesapeake Regional Information System for Our Patients were excluded. A machine learning rank score, the Baltimore score (B score) developed using a machine learning technique, for each individual hospital using data from the 2 years before September 1, 2016, was compared with standard readmission risk assessment scores to predict 30-day unplanned readmissions. Main Outcomes and Measures: The 30-day readmission rate evaluated using various readmission scores: B score, HOSPITAL score, modified LACE score, and Maxim/RightCare score. Results: Of the 10â€¯732 patients (5605 [52.2%] male; mean [SD] age, 54.56 [22.42] years) deemed to be eligible for the study, 1422 were readmitted. The area under the receiver operating characteristic curve (AUROC) for individual rules was 0.63 (95% CI, 0.61-0.65) for the HOSPITAL score, which was significantly lower than the 0.66 for modified LACE score (95% CI, 0.64-0.68; Pâ€‰<â€‰.001). The B score machine learning score was significantly better than all other scores; 48 hours after admission, the AUROC of the B score was 0.72 (95% CI, 0.70-0.73), which increased to 0.78 (95% CI, 0.77-0.79) at discharge (all Pâ€‰<â€‰.001). At the hospital using Maxim/RightCare score, the AUROC was 0.63 (95% CI, 0.59-0.69) for HOSPITAL, 0.64 (95% CI, 0.61-0.68) for Maxim/RightCare, and 0.66 (95% CI, 0.62-0.69) for modified LACE score. The B score was 0.72 (95% CI, 0.69-0.75) 48 hours after admission and 0.81 (95% CI, 0.79-0.84) at discharge. In directly comparing the B score with the sensitivity at cutoff values for modified LACE, HOSPITAL, and Maxim/RightCare scores, the B score was able to identify the same number of readmitted patients while flagging 25.5% to 54.9% fewer patients. Conclusions and Relevance: Among 3 hospitals in different settings, an automated machine learning score better predicted readmissions than commonly used readmission scores. More efficiently targeting patients at higher risk of readmission may be the first step toward potentially preventing readmissions.","Assessment of Machine Learning vs Standard Prediction Rules for Predicting Hospital Readmissions. Importance: Hospital readmissions are associated with patient harm and expense. Ways to prevent hospital readmissions have focused on identifying patients at greatest risk using prediction scores. Objective: To identify the type of score that best predicts hospital readmissions. Design, Setting, and Participants: This prognostic study included 14â€¯062 consecutive adult hospital patients with 16â€¯649 discharges from a tertiary care center, suburban community hospital, and urban critical access hospital in Maryland from September 1, 2016, through December 31, 2016. Patients not included as eligible discharges by the Centers for Medicare & Medicaid Services or the Chesapeake Regional Information System for Our Patients were excluded. A machine learning rank score, the Baltimore score (B score) developed using a machine learning technique, for each individual hospital using data from the 2 years before September 1, 2016, was compared with standard readmission risk assessment scores to predict 30-day unplanned readmissions. Main Outcomes and Measures: The 30-day readmission rate evaluated using various readmission scores: B score, HOSPITAL score, modified LACE score, and Maxim/RightCare score. Results: Of the 10â€¯732 patients (5605 [52.2%] male; mean [SD] age, 54.56 [22.42] years) deemed to be eligible for the study, 1422 were readmitted. The area under the receiver operating characteristic curve (AUROC) for individual rules was 0.63 (95% CI, 0.61-0.65) for the HOSPITAL score, which was significantly lower than the 0.66 for modified LACE score (95% CI, 0.64-0.68; Pâ€‰<â€‰.001). The B score machine learning score was significantly better than all other scores; 48 hours after admission, the AUROC of the B score was 0.72 (95% CI, 0.70-0.73), which increased to 0.78 (95% CI, 0.77-0.79) at discharge (all Pâ€‰<â€‰.001). At the hospital using Maxim/RightCare score, the AUROC was 0.63 (95% CI, 0.59-0.69) for HOSPITAL, 0.64 (95% CI, 0.61-0.68) for Maxim/RightCare, and 0.66 (95% CI, 0.62-0.69) for modified LACE score. The B score was 0.72 (95% CI, 0.69-0.75) 48 hours after admission and 0.81 (95% CI, 0.79-0.84) at discharge. In directly comparing the B score with the sensitivity at cutoff values for modified LACE, HOSPITAL, and Maxim/RightCare scores, the B score was able to identify the same number of readmitted patients while flagging 25.5% to 54.9% fewer patients. Conclusions and Relevance: Among 3 hospitals in different settings, an automated machine learning score better predicted readmissions than commonly used readmission scores. More efficiently targeting patients at higher risk of readmission may be the first step toward potentially preventing readmissions."
1,Clinical Implications of Transcriptomic Changes After Neoadjuvant Chemotherapy in Patients with Triple-Negative Breast Cancer,"Background: Pathological response to neoadjuvant chemotherapy (NAC) is critical in prognosis and selection of systemic treatments for patients with triple-negative breast cancer (TNBC). The aim of this study is to identify gene expression-based markers to predict response to NAC. Patients and Methods: A survey of 43 publicly available gene expression datasets was performed. We identified a cohort of TNBC patients treated with NAC (n = 708). Gene expression data from different studies were renormalized, and the differences between pretreatment (pre-NAC), on-treatment (post-C1), and surgical (Sx) specimens were evaluated. Euclidean statistical distances were calculated to estimate changes in gene expression patterns induced by NAC. Hierarchical clustering and pathway enrichment analyses were used to characterize relationships between differentially expressed genes and affected gene pathways. Machine learning was employed to refine a gene expression signature with the potential to predict response to NAC. Results: Forty nine genes consistently affected by NAC were involved in enhanced regulation of wound response, chemokine release, cell division, and decreased programmed cell death in residual invasive disease. The statistical distances between pre-NAC and post-C1 significantly predicted pathological complete response [area under the curve (AUC) = 0.75; p = 0.003; 95% confidence interval (CI) 0.58â€“0.92]. Finally, the expression of CCND1, a cyclin that forms complexes with CDK4/6 to promote the cell cycle, was the most informative feature in pre-NAC biopsies to predict response to NAC. Conclusions: The results of this study reveal significant transcriptomic changes induced by NAC and suggest that chemotherapy-induced gene expression changes observed early in therapy may be good predictors of response to NAC.","Clinical Implications of Transcriptomic Changes After Neoadjuvant Chemotherapy in Patients with Triple-Negative Breast Cancer. Background: Pathological response to neoadjuvant chemotherapy (NAC) is critical in prognosis and selection of systemic treatments for patients with triple-negative breast cancer (TNBC). The aim of this study is to identify gene expression-based markers to predict response to NAC. Patients and Methods: A survey of 43 publicly available gene expression datasets was performed. We identified a cohort of TNBC patients treated with NAC (n = 708). Gene expression data from different studies were renormalized, and the differences between pretreatment (pre-NAC), on-treatment (post-C1), and surgical (Sx) specimens were evaluated. Euclidean statistical distances were calculated to estimate changes in gene expression patterns induced by NAC. Hierarchical clustering and pathway enrichment analyses were used to characterize relationships between differentially expressed genes and affected gene pathways. Machine learning was employed to refine a gene expression signature with the potential to predict response to NAC. Results: Forty nine genes consistently affected by NAC were involved in enhanced regulation of wound response, chemokine release, cell division, and decreased programmed cell death in residual invasive disease. The statistical distances between pre-NAC and post-C1 significantly predicted pathological complete response [area under the curve (AUC) = 0.75; p = 0.003; 95% confidence interval (CI) 0.58â€“0.92]. Finally, the expression of CCND1, a cyclin that forms complexes with CDK4/6 to promote the cell cycle, was the most informative feature in pre-NAC biopsies to predict response to NAC. Conclusions: The results of this study reveal significant transcriptomic changes induced by NAC and suggest that chemotherapy-induced gene expression changes observed early in therapy may be good predictors of response to NAC."
1,Fast and accurate tumor segmentation of histology images using persistent homology and deep convolutional features,,
1,Beyond Performance Metrics: Automatic Deep Learning Retinal OCT Analysis Reproduces Clinical Trial Outcome,"PURPOSE: To validate the efficacy of a fully automatic, deep learning-based segmentation algorithm beyond conventional performance metrics by measuring the primary outcome of a clinical trial for macular telangiectasia type 2 (MacTel2). DESIGN: Evaluation of diagnostic test or technology. PARTICIPANTS: A total of 92 eyes from 62 participants with MacTel2 from a phase 2 clinical trial (NCT01949324) randomized to 1 of 2 treatment groups METHODS: The ellipsoid zone (EZ) defect areas were measured on spectral domain OCT images of each eye at 2 time points (baseline and month 24) by a fully automatic, deep learning-based segmentation algorithm. The change in EZ defect area from baseline to month 24 was calculated and analyzed according to the clinical trial protocol. MAIN OUTCOME MEASURE: Difference in the change in EZ defect area from baseline to month 24 between the 2 treatment groups. RESULTS: The difference in the change in EZ defect area from baseline to month 24 between the 2 treatment groups measured by the fully automatic segmentation algorithm was 0.072+/-0.035 mm(2) (P = 0.021). This was comparable to the outcome of the clinical trial using semiautomatic measurements by expert readers, 0.065+/-0.033 mm(2) (P = 0.025). CONCLUSIONS: The fully automatic segmentation algorithm was as accurate as semiautomatic expert segmentation to assess EZ defect areas and was able to reliably reproduce the statistically significant primary outcome measure of the clinical trial. This approach, to validate the performance of an automatic segmentation algorithm on the primary clinical trial end point, provides a robust gauge of its clinical applicability.","Beyond Performance Metrics: Automatic Deep Learning Retinal OCT Analysis Reproduces Clinical Trial Outcome. PURPOSE: To validate the efficacy of a fully automatic, deep learning-based segmentation algorithm beyond conventional performance metrics by measuring the primary outcome of a clinical trial for macular telangiectasia type 2 (MacTel2). DESIGN: Evaluation of diagnostic test or technology. PARTICIPANTS: A total of 92 eyes from 62 participants with MacTel2 from a phase 2 clinical trial (NCT01949324) randomized to 1 of 2 treatment groups METHODS: The ellipsoid zone (EZ) defect areas were measured on spectral domain OCT images of each eye at 2 time points (baseline and month 24) by a fully automatic, deep learning-based segmentation algorithm. The change in EZ defect area from baseline to month 24 was calculated and analyzed according to the clinical trial protocol. MAIN OUTCOME MEASURE: Difference in the change in EZ defect area from baseline to month 24 between the 2 treatment groups. RESULTS: The difference in the change in EZ defect area from baseline to month 24 between the 2 treatment groups measured by the fully automatic segmentation algorithm was 0.072+/-0.035 mm(2) (P = 0.021). This was comparable to the outcome of the clinical trial using semiautomatic measurements by expert readers, 0.065+/-0.033 mm(2) (P = 0.025). CONCLUSIONS: The fully automatic segmentation algorithm was as accurate as semiautomatic expert segmentation to assess EZ defect areas and was able to reliably reproduce the statistically significant primary outcome measure of the clinical trial. This approach, to validate the performance of an automatic segmentation algorithm on the primary clinical trial end point, provides a robust gauge of its clinical applicability."
1,Natural Language Processing to Quantify Microbial Keratitis Measurements,,
1,Constrained-CNN losses for weakly supervised segmentation,,
1,Detecting middle ear fluid using smartphones,,
1,Personal clinical history predicts antibiotic resistance of urinary tract infections,"Antibiotic resistance is prevalent among the bacterial pathogens causing urinary tract infections. However, antimicrobial treatment is often prescribed 'empirically', in the absence of antibiotic susceptibility testing, risking mismatched and therefore ineffective treatment. Here, linking a 10-year longitudinal data set of over 700,000 community-acquired urinary tract infections with over 5,000,000 individually resolved records of antibiotic purchases, we identify strong associations of antibiotic resistance with the demographics, records of past urine cultures and history of drug purchases of the patients. When combined together, these associations allow for machine-learning-based personalized drug-specific predictions of antibiotic resistance, thereby enabling drug-prescribing algorithms that match an antibiotic treatment recommendation to the expected resistance of each sample. Applying these algorithms retrospectively, over a 1-year test period, we find that they greatly reduce the risk of mismatched treatment compared with the current standard of care. The clinical application of such algorithms may help improve the effectiveness of antimicrobial treatments.","Personal clinical history predicts antibiotic resistance of urinary tract infections. Antibiotic resistance is prevalent among the bacterial pathogens causing urinary tract infections. However, antimicrobial treatment is often prescribed 'empirically', in the absence of antibiotic susceptibility testing, risking mismatched and therefore ineffective treatment. Here, linking a 10-year longitudinal data set of over 700,000 community-acquired urinary tract infections with over 5,000,000 individually resolved records of antibiotic purchases, we identify strong associations of antibiotic resistance with the demographics, records of past urine cultures and history of drug purchases of the patients. When combined together, these associations allow for machine-learning-based personalized drug-specific predictions of antibiotic resistance, thereby enabling drug-prescribing algorithms that match an antibiotic treatment recommendation to the expected resistance of each sample. Applying these algorithms retrospectively, over a 1-year test period, we find that they greatly reduce the risk of mismatched treatment compared with the current standard of care. The clinical application of such algorithms may help improve the effectiveness of antimicrobial treatments."
1,Machine learning algorithms estimating prognosis and guiding therapy in adult congenital heart disease: data from a single tertiary centre including 10 019 patients,"AIMS: To assess the utility of machine learning algorithms on estimating prognosis and guiding therapy in a large cohort of patients with adult congenital heart disease (ACHD) or pulmonary hypertension at a single, tertiary centre. METHODS AND RESULTS: We included 10 019 adult patients (age 36.3 +/- 17.3 years) under follow-up at our institution between 2000 and 2018. Clinical and demographic data, ECG parameters, cardiopulmonary exercise testing, and selected laboratory markers where collected and included in deep learning (DL) algorithms. Specific DL-models were built based on raw data to categorize diagnostic group, disease complexity, and New York Heart Association (NYHA) class. In addition, models were developed to estimate need for discussion at multidisciplinary team (MDT) meetings and to gauge prognosis of individual patients. Overall, the DL-algorithms-based on over 44 000 medical records-categorized diagnosis, disease complexity, and NYHA class with an accuracy of 91.1%, 97.0%, and 90.6%, respectively in the test sample. Similarly, patient presentation at MDT-meetings was predicted with a test sample accuracy of 90.2%. During a median follow-up time of 8 years, 785 patients died. The automatically derived disease severity-score derived from clinical information was related to survival on Cox analysis independently of demographic, exercise, laboratory, and ECG parameters. CONCLUSION: We present herewith the utility of machine learning algorithms trained on large datasets to estimate prognosis and potentially to guide therapy in ACHD. Due to the largely automated process involved, these DL-algorithms can easily be scaled to multi-institutional datasets to further improve accuracy and ultimately serve as online based decision-making tools.","Machine learning algorithms estimating prognosis and guiding therapy in adult congenital heart disease: data from a single tertiary centre including 10 019 patients. AIMS: To assess the utility of machine learning algorithms on estimating prognosis and guiding therapy in a large cohort of patients with adult congenital heart disease (ACHD) or pulmonary hypertension at a single, tertiary centre. METHODS AND RESULTS: We included 10 019 adult patients (age 36.3 +/- 17.3 years) under follow-up at our institution between 2000 and 2018. Clinical and demographic data, ECG parameters, cardiopulmonary exercise testing, and selected laboratory markers where collected and included in deep learning (DL) algorithms. Specific DL-models were built based on raw data to categorize diagnostic group, disease complexity, and New York Heart Association (NYHA) class. In addition, models were developed to estimate need for discussion at multidisciplinary team (MDT) meetings and to gauge prognosis of individual patients. Overall, the DL-algorithms-based on over 44 000 medical records-categorized diagnosis, disease complexity, and NYHA class with an accuracy of 91.1%, 97.0%, and 90.6%, respectively in the test sample. Similarly, patient presentation at MDT-meetings was predicted with a test sample accuracy of 90.2%. During a median follow-up time of 8 years, 785 patients died. The automatically derived disease severity-score derived from clinical information was related to survival on Cox analysis independently of demographic, exercise, laboratory, and ECG parameters. CONCLUSION: We present herewith the utility of machine learning algorithms trained on large datasets to estimate prognosis and potentially to guide therapy in ACHD. Due to the largely automated process involved, these DL-algorithms can easily be scaled to multi-institutional datasets to further improve accuracy and ultimately serve as online based decision-making tools."
1,Radiomic versus Convolutional Neural Networks Analysis for Classification of Contrast-enhancing Lesions at Multiparametric Breast MRI,"Purpose To compare the diagnostic performance of radiomic analysis (RA) and a convolutional neural network (CNN) to radiologists for classification of contrast agent-enhancing lesions as benign or malignant at multiparametric breast MRI. Materials and Methods Between August 2011 and August 2015, 447 patients with 1294 enhancing lesions (787 malignant, 507 benign; median size, 15 mm +/- 20) were evaluated. Lesions were manually segmented by one breast radiologist. RA was performed by using L1 regularization and principal component analysis. CNN used a deep residual neural network with 34 layers. All algorithms were also retrained on half the number of lesions (n = 647). Machine interpretations were compared with prospective interpretations by three breast radiologists. Standard of reference was histologic analysis or follow-up. Areas under the receiver operating curve (AUCs) were used to compare diagnostic performance. Results CNN trained on the full cohort was superior to training on the half-size cohort (AUC, 0.88 vs 0.83, respectively; P = .01), but there was no difference for RA and L1 regularization (AUC, 0.81 vs 0.80, respectively; P = .76) or RA and principal component analysis (AUC, 0.78 vs 0.78, respectively; P = .93). By using the full cohort, CNN performance (AUC, 0.88; 95% confidence interval: 0.86, 0.89) was better than RA and L1 regularization (AUC, 0.81; 95% confidence interval: 0.79, 0.83; P < .001) and RA and principal component analysis (AUC, 0.78; 95% confidence interval: 0.76, 0.80; P < .001). However, CNN was inferior to breast radiologist interpretation (AUC, 0.98; 95% confidence interval: 0.96, 0.99; P < .001). Conclusion A convolutional neural network was superior to radiomic analysis for classification of enhancing lesions as benign or malignant at multiparametric breast MRI. Both approaches were inferior to radiologists' performance; however, more training data will further improve performance of convolutional neural network, but not that of radiomics algorithms. (c) RSNA, 2018 Online supplemental material is available for this article.","Radiomic versus Convolutional Neural Networks Analysis for Classification of Contrast-enhancing Lesions at Multiparametric Breast MRI. Purpose To compare the diagnostic performance of radiomic analysis (RA) and a convolutional neural network (CNN) to radiologists for classification of contrast agent-enhancing lesions as benign or malignant at multiparametric breast MRI. Materials and Methods Between August 2011 and August 2015, 447 patients with 1294 enhancing lesions (787 malignant, 507 benign; median size, 15 mm +/- 20) were evaluated. Lesions were manually segmented by one breast radiologist. RA was performed by using L1 regularization and principal component analysis. CNN used a deep residual neural network with 34 layers. All algorithms were also retrained on half the number of lesions (n = 647). Machine interpretations were compared with prospective interpretations by three breast radiologists. Standard of reference was histologic analysis or follow-up. Areas under the receiver operating curve (AUCs) were used to compare diagnostic performance. Results CNN trained on the full cohort was superior to training on the half-size cohort (AUC, 0.88 vs 0.83, respectively; P = .01), but there was no difference for RA and L1 regularization (AUC, 0.81 vs 0.80, respectively; P = .76) or RA and principal component analysis (AUC, 0.78 vs 0.78, respectively; P = .93). By using the full cohort, CNN performance (AUC, 0.88; 95% confidence interval: 0.86, 0.89) was better than RA and L1 regularization (AUC, 0.81; 95% confidence interval: 0.79, 0.83; P < .001) and RA and principal component analysis (AUC, 0.78; 95% confidence interval: 0.76, 0.80; P < .001). However, CNN was inferior to breast radiologist interpretation (AUC, 0.98; 95% confidence interval: 0.96, 0.99; P < .001). Conclusion A convolutional neural network was superior to radiomic analysis for classification of enhancing lesions as benign or malignant at multiparametric breast MRI. Both approaches were inferior to radiologists' performance; however, more training data will further improve performance of convolutional neural network, but not that of radiomics algorithms. (c) RSNA, 2018 Online supplemental material is available for this article."
1,Convolutional sparse kernel network for unsupervised medical image analysis,,
1,Estimating Retinal Sensitivity Using Optical Coherence Tomography With Deep-Learning Algorithms in Macular Telangiectasia Type 2,"Importance: As currently used, microperimetry is a burdensome clinical testing modality for testing retinal sensitivity requiring long testing times and trained technicians. Objective: To create a deep-learning network that could directly estimate function from structure de novo to provide an en face high-resolution map of estimated retinal sensitivity. Design, Setting, and Participants: A cross-sectional imaging study using data collected between January 1, 2016, and November 30, 2017, from the Natural History Observation and Registry of macular telangiectasia type 2 (MacTel) evaluated 38 participants with confirmed MacTel from 2 centers. Main Outcomes and Measures: Mean absolute error of estimated compared with observed retinal sensitivity. Observed retinal sensitivity was obtained with fundus-controlled perimetry (microperimetry). Estimates of retinal sensitivity were made with deep-learning models that learned on superpositions of high-resolution optical coherence tomography (OCT) scans and microperimetry results. Those predictions were used to create high-density en face sensitivity maps of the macula. Training, validation, and test sets were segregated at the patient level. Results: A total of 2499 microperimetry sensitivities were mapped onto 1708 OCT B-scans from 63 eyes of 38 patients (mean [SD] age, 74.3 [9.7] years; 15 men [39.5%]). The numbers of examples for our algorithm were 67â€¯899 (103â€¯053 after data augmentation) for training, 1695 for validation, and 1212 for testing. Mean absolute error results were 4.51 dB (95% CI, 4.36-4.65 dB) when using linear regression and 3.66 dB (95% CI, 3.53-3.78 dB) when using the LeNet model. Using a 49.9 million-variable deep-learning model, a mean absolute error of 3.36 dB (95% CI, 3.25-3.48 dB) of retinal sensitivity for validation and test was achieved. Correlation showed a high degree of agreement (Pearson correlation râ€‰=â€‰0.78). By paired Wilcoxon rank sum test, our model significantly outperformed these 2 baseline models (Pâ€‰<â€‰.001). Conclusions and Relevance: High-resolution en face maps of estimated retinal sensitivities were created in eyes with MacTel. The maps were of unequalled resolution compared with microperimetry and were able to correctly delineate functionally healthy and impaired retina. This model may be useful to monitor structural and functional disease progression and has potential as an objective surrogate outcome measure in investigational trials.","Estimating Retinal Sensitivity Using Optical Coherence Tomography With Deep-Learning Algorithms in Macular Telangiectasia Type 2. Importance: As currently used, microperimetry is a burdensome clinical testing modality for testing retinal sensitivity requiring long testing times and trained technicians. Objective: To create a deep-learning network that could directly estimate function from structure de novo to provide an en face high-resolution map of estimated retinal sensitivity. Design, Setting, and Participants: A cross-sectional imaging study using data collected between January 1, 2016, and November 30, 2017, from the Natural History Observation and Registry of macular telangiectasia type 2 (MacTel) evaluated 38 participants with confirmed MacTel from 2 centers. Main Outcomes and Measures: Mean absolute error of estimated compared with observed retinal sensitivity. Observed retinal sensitivity was obtained with fundus-controlled perimetry (microperimetry). Estimates of retinal sensitivity were made with deep-learning models that learned on superpositions of high-resolution optical coherence tomography (OCT) scans and microperimetry results. Those predictions were used to create high-density en face sensitivity maps of the macula. Training, validation, and test sets were segregated at the patient level. Results: A total of 2499 microperimetry sensitivities were mapped onto 1708 OCT B-scans from 63 eyes of 38 patients (mean [SD] age, 74.3 [9.7] years; 15 men [39.5%]). The numbers of examples for our algorithm were 67â€¯899 (103â€¯053 after data augmentation) for training, 1695 for validation, and 1212 for testing. Mean absolute error results were 4.51 dB (95% CI, 4.36-4.65 dB) when using linear regression and 3.66 dB (95% CI, 3.53-3.78 dB) when using the LeNet model. Using a 49.9 million-variable deep-learning model, a mean absolute error of 3.36 dB (95% CI, 3.25-3.48 dB) of retinal sensitivity for validation and test was achieved. Correlation showed a high degree of agreement (Pearson correlation râ€‰=â€‰0.78). By paired Wilcoxon rank sum test, our model significantly outperformed these 2 baseline models (Pâ€‰<â€‰.001). Conclusions and Relevance: High-resolution en face maps of estimated retinal sensitivities were created in eyes with MacTel. The maps were of unequalled resolution compared with microperimetry and were able to correctly delineate functionally healthy and impaired retina. This model may be useful to monitor structural and functional disease progression and has potential as an objective surrogate outcome measure in investigational trials."
1,"Real-time artificial intelligence for detection of upper gastrointestinal cancer by endoscopy: a multicentre, case-control, diagnostic study","BACKGROUND: Upper gastrointestinal cancers (including oesophageal cancer and gastric cancer) are the most common cancers worldwide. Artificial intelligence platforms using deep learning algorithms have made remarkable progress in medical imaging but their application in upper gastrointestinal cancers has been limited. We aimed to develop and validate the Gastrointestinal Artificial Intelligence Diagnostic System (GRAIDS) for the diagnosis of upper gastrointestinal cancers through analysis of imaging data from clinical endoscopies. METHODS: This multicentre, case-control, diagnostic study was done in six hospitals of different tiers (ie, municipal, provincial, and national) in China. The images of consecutive participants, aged 18 years or older, who had not had a previous endoscopy were retrieved from all participating hospitals. All patients with upper gastrointestinal cancer lesions (including oesophageal cancer and gastric cancer) that were histologically proven malignancies were eligible for this study. Only images with standard white light were deemed eligible. The images from Sun Yat-sen University Cancer Center were randomly assigned (8:1:1) to the training and intrinsic verification datasets for developing GRAIDS, and the internal validation dataset for evaluating the performance of GRAIDS. Its diagnostic performance was evaluated using an internal and prospective validation set from Sun Yat-sen University Cancer Center (a national hospital) and additional external validation sets from five primary care hospitals. The performance of GRAIDS was also compared with endoscopists with three degrees of expertise: expert, competent, and trainee. The diagnostic accuracy, sensitivity, specificity, positive predictive value, and negative predictive value of GRAIDS and endoscopists for the identification of cancerous lesions were evaluated by calculating the 95% CIs using the Clopper-Pearson method. FINDINGS: 1 036 496 endoscopy images from 84 424 individuals were used to develop and test GRAIDS. The diagnostic accuracy in identifying upper gastrointestinal cancers was 0.955 (95% CI 0.952-0.957) in the internal validation set, 0.927 (0.925-0.929) in the prospective set, and ranged from 0.915 (0.913-0.917) to 0.977 (0.977-0.978) in the five external validation sets. GRAIDS achieved diagnostic sensitivity similar to that of the expert endoscopist (0.942 [95% CI 0.924-0.957] vs 0.945 [0.927-0.959]; p=0.692) and superior sensitivity compared with competent (0.858 [0.832-0.880], p<0.0001) and trainee (0.722 [0.691-0.752], p<0.0001) endoscopists. The positive predictive value was 0.814 (95% CI 0.788-0.838) for GRAIDS, 0.932 (0.913-0.948) for the expert endoscopist, 0.974 (0.960-0.984) for the competent endoscopist, and 0.824 (0.795-0.850) for the trainee endoscopist. The negative predictive value was 0.978 (95% CI 0.971-0.984) for GRAIDS, 0.980 (0.974-0.985) for the expert endoscopist, 0.951 (0.942-0.959) for the competent endoscopist, and 0.904 (0.893-0.916) for the trainee endoscopist. INTERPRETATION: GRAIDS achieved high diagnostic accuracy in detecting upper gastrointestinal cancers, with sensitivity similar to that of expert endoscopists and was superior to that of non-expert endoscopists. This system could assist community-based hospitals in improving their effectiveness in upper gastrointestinal cancer diagnoses. FUNDING: The National Key R&D Program of China, the Natural Science Foundation of Guangdong Province, the Science and Technology Program of Guangdong, the Science and Technology Program of Guangzhou, and the Fundamental Research Funds for the Central Universities.","Real-time artificial intelligence for detection of upper gastrointestinal cancer by endoscopy: a multicentre, case-control, diagnostic study. BACKGROUND: Upper gastrointestinal cancers (including oesophageal cancer and gastric cancer) are the most common cancers worldwide. Artificial intelligence platforms using deep learning algorithms have made remarkable progress in medical imaging but their application in upper gastrointestinal cancers has been limited. We aimed to develop and validate the Gastrointestinal Artificial Intelligence Diagnostic System (GRAIDS) for the diagnosis of upper gastrointestinal cancers through analysis of imaging data from clinical endoscopies. METHODS: This multicentre, case-control, diagnostic study was done in six hospitals of different tiers (ie, municipal, provincial, and national) in China. The images of consecutive participants, aged 18 years or older, who had not had a previous endoscopy were retrieved from all participating hospitals. All patients with upper gastrointestinal cancer lesions (including oesophageal cancer and gastric cancer) that were histologically proven malignancies were eligible for this study. Only images with standard white light were deemed eligible. The images from Sun Yat-sen University Cancer Center were randomly assigned (8:1:1) to the training and intrinsic verification datasets for developing GRAIDS, and the internal validation dataset for evaluating the performance of GRAIDS. Its diagnostic performance was evaluated using an internal and prospective validation set from Sun Yat-sen University Cancer Center (a national hospital) and additional external validation sets from five primary care hospitals. The performance of GRAIDS was also compared with endoscopists with three degrees of expertise: expert, competent, and trainee. The diagnostic accuracy, sensitivity, specificity, positive predictive value, and negative predictive value of GRAIDS and endoscopists for the identification of cancerous lesions were evaluated by calculating the 95% CIs using the Clopper-Pearson method. FINDINGS: 1 036 496 endoscopy images from 84 424 individuals were used to develop and test GRAIDS. The diagnostic accuracy in identifying upper gastrointestinal cancers was 0.955 (95% CI 0.952-0.957) in the internal validation set, 0.927 (0.925-0.929) in the prospective set, and ranged from 0.915 (0.913-0.917) to 0.977 (0.977-0.978) in the five external validation sets. GRAIDS achieved diagnostic sensitivity similar to that of the expert endoscopist (0.942 [95% CI 0.924-0.957] vs 0.945 [0.927-0.959]; p=0.692) and superior sensitivity compared with competent (0.858 [0.832-0.880], p<0.0001) and trainee (0.722 [0.691-0.752], p<0.0001) endoscopists. The positive predictive value was 0.814 (95% CI 0.788-0.838) for GRAIDS, 0.932 (0.913-0.948) for the expert endoscopist, 0.974 (0.960-0.984) for the competent endoscopist, and 0.824 (0.795-0.850) for the trainee endoscopist. The negative predictive value was 0.978 (95% CI 0.971-0.984) for GRAIDS, 0.980 (0.974-0.985) for the expert endoscopist, 0.951 (0.942-0.959) for the competent endoscopist, and 0.904 (0.893-0.916) for the trainee endoscopist. INTERPRETATION: GRAIDS achieved high diagnostic accuracy in detecting upper gastrointestinal cancers, with sensitivity similar to that of expert endoscopists and was superior to that of non-expert endoscopists. This system could assist community-based hospitals in improving their effectiveness in upper gastrointestinal cancer diagnoses. FUNDING: The National Key R&D Program of China, the Natural Science Foundation of Guangdong Province, the Science and Technology Program of Guangdong, the Science and Technology Program of Guangzhou, and the Fundamental Research Funds for the Central Universities."
1,Fully automatic 3D reconstruction of the placenta and its peripheral vasculature in intrauterine fetal MRI,,
1,An automatic restoration framework based on GPU-accelerated collateral filtering in brain MR images,"BACKGROUND: Image restoration is one of the fundamental and essential tasks within image processing. In medical imaging, developing an effective algorithm that can automatically remove random noise in brain magnetic resonance (MR) images is challenging. The collateral filter has been shown a more powerful algorithm than many existing methods. However, the computation of the collateral filter is more time-consuming and the selection of the filter parameters is also laborious. This paper proposes an automatic noise removal system based on the accelerated collateral filter for brain MR images. METHODS: To solve these problems, we first accelerated the collateral filter with parallel computing using the graphics processing unit (GPU) architecture. We adopted the compute unified device architecture (CUDA), an application programming interface for the GPU by NVIDIA, to hasten the computation. Subsequently, the optimal filter parameters were selected and the automation was achieved by artificial neural networks. Specifically, an artificial neural network system associated with image feature analysis was adopted to establish the automatic image restoration framework. The best feature combination was selected by the paired t-test and the sequential forward floating selection (SFFS) methods. RESULTS: Experimental results indicated that not only did the proposed automatic image restoration algorithm perform dramatically faster than the traditional collateral filter, but it also effectively removed the noise in a wide variety of brain MR images. A speed up gain of 34 was attained to process an MR image, which completed within 0.1â€‰s. Representative illustrations of brain tumor images demonstrated the capability of identifying lesion boundaries, which outperformed many existing methods. CONCLUSIONS: We believe that our accelerated and automated restoration framework is promising for achieving robust filtering in many brain MR image restoration applications.","An automatic restoration framework based on GPU-accelerated collateral filtering in brain MR images. BACKGROUND: Image restoration is one of the fundamental and essential tasks within image processing. In medical imaging, developing an effective algorithm that can automatically remove random noise in brain magnetic resonance (MR) images is challenging. The collateral filter has been shown a more powerful algorithm than many existing methods. However, the computation of the collateral filter is more time-consuming and the selection of the filter parameters is also laborious. This paper proposes an automatic noise removal system based on the accelerated collateral filter for brain MR images. METHODS: To solve these problems, we first accelerated the collateral filter with parallel computing using the graphics processing unit (GPU) architecture. We adopted the compute unified device architecture (CUDA), an application programming interface for the GPU by NVIDIA, to hasten the computation. Subsequently, the optimal filter parameters were selected and the automation was achieved by artificial neural networks. Specifically, an artificial neural network system associated with image feature analysis was adopted to establish the automatic image restoration framework. The best feature combination was selected by the paired t-test and the sequential forward floating selection (SFFS) methods. RESULTS: Experimental results indicated that not only did the proposed automatic image restoration algorithm perform dramatically faster than the traditional collateral filter, but it also effectively removed the noise in a wide variety of brain MR images. A speed up gain of 34 was attained to process an MR image, which completed within 0.1â€‰s. Representative illustrations of brain tumor images demonstrated the capability of identifying lesion boundaries, which outperformed many existing methods. CONCLUSIONS: We believe that our accelerated and automated restoration framework is promising for achieving robust filtering in many brain MR image restoration applications."
1,Predicting breast tumor proliferation from whole-slide images: The TUPAC16 challenge,,
1,Circuit mechanisms for the maintenance and manipulation of information in working memory,"Recently it has been proposed that information in working memory (WM) may not always be stored in persistent neuronal activity but can be maintained in 'activity-silent' hidden states, such as synaptic efficacies endowed with short-term synaptic plasticity. To test this idea computationally, we investigated recurrent neural network models trained to perform several WM-dependent tasks, in which WM representation emerges from learning and is not a priori assumed to depend on self-sustained persistent activity. We found that short-term synaptic plasticity can support the short-term maintenance of information, provided that the memory delay period is sufficiently short. However, in tasks that require actively manipulating information, persistent activity naturally emerges from learning, and the amount of persistent activity scales with the degree of manipulation required. These results shed insight into the current debate on WM encoding and suggest that persistent activity can vary markedly between short-term memory tasks with different cognitive demands.","Circuit mechanisms for the maintenance and manipulation of information in working memory. Recently it has been proposed that information in working memory (WM) may not always be stored in persistent neuronal activity but can be maintained in 'activity-silent' hidden states, such as synaptic efficacies endowed with short-term synaptic plasticity. To test this idea computationally, we investigated recurrent neural network models trained to perform several WM-dependent tasks, in which WM representation emerges from learning and is not a priori assumed to depend on self-sustained persistent activity. We found that short-term synaptic plasticity can support the short-term maintenance of information, provided that the memory delay period is sufficiently short. However, in tasks that require actively manipulating information, persistent activity naturally emerges from learning, and the amount of persistent activity scales with the degree of manipulation required. These results shed insight into the current debate on WM encoding and suggest that persistent activity can vary markedly between short-term memory tasks with different cognitive demands."
1,Evidential MACE prediction of acute coronary syndrome using electronic health records,"BACKGROUND: Major adverse cardiac event (MACE) prediction plays a key role in providing efficient and effective treatment strategies for patients with acute coronary syndrome (ACS) during their hospitalizations. Existing prediction models have limitations to cope with imprecise and ambiguous clinical information such that clinicians cannot reach to reliable MACE prediction results for individuals. METHODS: To remedy it, this study proposes a hybrid method using Rough Set Theory (RST) and Dempster-Shafer Theory (DST) of evidence. In details, four state-of-the-art models, including one traditional ACS risk scoring model, i.e., GRACE, and three machine learning based models, i.e., Support Vector Machine, L1-Logistic Regression, and Classification and Regression Tree, are employed to generate initial MACE prediction results, and then RST is applied to determine the weights of the four single models. After that, the acquired prediction results are assumed as basic beliefs for the problem propositions and in this way, an evidential prediction result is generated based on DST in an integrative manner. RESULTS: Having applied the proposed method on a clinical dataset consisting of 2930 ACS patient samples, our model achieves 0.715 AUC value with competitive standard deviation, which is the best prediction results comparing with the four single base models and two baseline ensemble models. CONCLUSIONS: Facing with the limitations in traditional ACS risk scoring models, machine learning models and the uncertainties of EHR data, we present an ensemble approach via RST and DST to alleviate this problem. The experimental results reveal that our proposed method achieves better performance for the problem of MACE prediction when compared with the single models.","Evidential MACE prediction of acute coronary syndrome using electronic health records. BACKGROUND: Major adverse cardiac event (MACE) prediction plays a key role in providing efficient and effective treatment strategies for patients with acute coronary syndrome (ACS) during their hospitalizations. Existing prediction models have limitations to cope with imprecise and ambiguous clinical information such that clinicians cannot reach to reliable MACE prediction results for individuals. METHODS: To remedy it, this study proposes a hybrid method using Rough Set Theory (RST) and Dempster-Shafer Theory (DST) of evidence. In details, four state-of-the-art models, including one traditional ACS risk scoring model, i.e., GRACE, and three machine learning based models, i.e., Support Vector Machine, L1-Logistic Regression, and Classification and Regression Tree, are employed to generate initial MACE prediction results, and then RST is applied to determine the weights of the four single models. After that, the acquired prediction results are assumed as basic beliefs for the problem propositions and in this way, an evidential prediction result is generated based on DST in an integrative manner. RESULTS: Having applied the proposed method on a clinical dataset consisting of 2930 ACS patient samples, our model achieves 0.715 AUC value with competitive standard deviation, which is the best prediction results comparing with the four single base models and two baseline ensemble models. CONCLUSIONS: Facing with the limitations in traditional ACS risk scoring models, machine learning models and the uncertainties of EHR data, we present an ensemble approach via RST and DST to alleviate this problem. The experimental results reveal that our proposed method achieves better performance for the problem of MACE prediction when compared with the single models."
1,Development and validation of an automated HIV prediction algorithm to identify candidates for pre-exposure prophylaxis: a modelling study,"BACKGROUND: HIV pre-exposure prophylaxis (PrEP) is effective but underused, in part because clinicians do not have the tools to identify PrEP candidates. We developed and validated an automated prediction algorithm that uses electronic health record (EHR) data to identify individuals at increased risk for HIV acquisition. METHODS: We used machine learning algorithms to predict incident HIV infections with 180 potential predictors of HIV risk drawn from EHR data from 2007-15 at Atrius Health, an ambulatory group practice in Massachusetts, USA. We included EHRs of all patients aged 15 years or older with at least one clinical encounter during 2007-15. We used ten-fold cross-validated area under the receiver operating characteristic curve (cv-AUC) with 95% CIs to assess the model's performance at identifying individuals with incident HIV and patients independently prescribed PrEP by clinicians. The best-performing model was validated prospectively with 2016 data from Atrius Health and externally with 2011-16 data from Fenway Health, a community health centre specialising in sexual health care in Boston (MA, USA). We calculated HIV risk scores (ie, probability of an incident HIV diagnosis) for every HIV-uninfected patient not on PrEP during 2007-15 at Atrius Health and assessed the distribution of scores for thresholds to determine possible candidates for PrEP in the three study cohorts. FINDINGS: We included 1 155 966 Atrius Health patients from 2007-15 (150 [<0.1%] patients with incident HIV) in our development cohort, 537 257 Atrius Health patients in 2016 (16 [<0.1%] with incident HIV) in our prospective validation cohort, and 33 404 Fenway Health patients from 2011-16 (423 [1.3%] with incident HIV) in our external validation cohort. The best-performing algorithm was obtained with least absolute shrinkage and selection operator (LASSO) and had a cv-AUC of 0.86 (95% CI 0.82-0.90) for identification of incident HIV infections in the development cohort, 0.91 (0.81-1.00) on prospective validation, and 0.77 (0.74-0.79) on external validation. The LASSO model successfully identified patients independently prescribed PrEP by clinicians at Atrius Health in 2016 (cv-AUC 0.93, 95% CI 0.90-0.96) or Fenway Health (0.79, 0.78-0.80). HIV risk scores increased steeply at the 98th percentile. Using this score as a threshold, we prospectively identified 9515 (1.8%) of 536 384 patients at Atrius Health in 2016 and 4385 (15.3%) of 28 702 Fenway Health patients as potential PrEP candidates. INTERPRETATION: Automated algorithms can efficiently identify patients at increased risk for HIV acquisition. Integrating these models into EHRs to alert providers about patients who might benefit from PrEP could improve prescribing and prevent new HIV infections. FUNDING: Harvard University Center for AIDS Research, Providence/Boston Center for AIDS Research, Rhode Island IDeA-CTR, the National Institute of Mental Health, and the US Centers for Disease Control and Prevention.","Development and validation of an automated HIV prediction algorithm to identify candidates for pre-exposure prophylaxis: a modelling study. BACKGROUND: HIV pre-exposure prophylaxis (PrEP) is effective but underused, in part because clinicians do not have the tools to identify PrEP candidates. We developed and validated an automated prediction algorithm that uses electronic health record (EHR) data to identify individuals at increased risk for HIV acquisition. METHODS: We used machine learning algorithms to predict incident HIV infections with 180 potential predictors of HIV risk drawn from EHR data from 2007-15 at Atrius Health, an ambulatory group practice in Massachusetts, USA. We included EHRs of all patients aged 15 years or older with at least one clinical encounter during 2007-15. We used ten-fold cross-validated area under the receiver operating characteristic curve (cv-AUC) with 95% CIs to assess the model's performance at identifying individuals with incident HIV and patients independently prescribed PrEP by clinicians. The best-performing model was validated prospectively with 2016 data from Atrius Health and externally with 2011-16 data from Fenway Health, a community health centre specialising in sexual health care in Boston (MA, USA). We calculated HIV risk scores (ie, probability of an incident HIV diagnosis) for every HIV-uninfected patient not on PrEP during 2007-15 at Atrius Health and assessed the distribution of scores for thresholds to determine possible candidates for PrEP in the three study cohorts. FINDINGS: We included 1 155 966 Atrius Health patients from 2007-15 (150 [<0.1%] patients with incident HIV) in our development cohort, 537 257 Atrius Health patients in 2016 (16 [<0.1%] with incident HIV) in our prospective validation cohort, and 33 404 Fenway Health patients from 2011-16 (423 [1.3%] with incident HIV) in our external validation cohort. The best-performing algorithm was obtained with least absolute shrinkage and selection operator (LASSO) and had a cv-AUC of 0.86 (95% CI 0.82-0.90) for identification of incident HIV infections in the development cohort, 0.91 (0.81-1.00) on prospective validation, and 0.77 (0.74-0.79) on external validation. The LASSO model successfully identified patients independently prescribed PrEP by clinicians at Atrius Health in 2016 (cv-AUC 0.93, 95% CI 0.90-0.96) or Fenway Health (0.79, 0.78-0.80). HIV risk scores increased steeply at the 98th percentile. Using this score as a threshold, we prospectively identified 9515 (1.8%) of 536 384 patients at Atrius Health in 2016 and 4385 (15.3%) of 28 702 Fenway Health patients as potential PrEP candidates. INTERPRETATION: Automated algorithms can efficiently identify patients at increased risk for HIV acquisition. Integrating these models into EHRs to alert providers about patients who might benefit from PrEP could improve prescribing and prevent new HIV infections. FUNDING: Harvard University Center for AIDS Research, Providence/Boston Center for AIDS Research, Rhode Island IDeA-CTR, the National Institute of Mental Health, and the US Centers for Disease Control and Prevention."
1,Opioid overdose detection using smartphones,"Early detection and rapid intervention can prevent death from opioid overdose. At high doses, opioids (particularly fentanyl) can cause rapid cessation of breathing (apnea), hypoxemic/hypercarbic respiratory failure, and death, the physiologic sequence by which people commonly succumb from unintentional opioid overdose. We present algorithms that run on smartphones and unobtrusively detect opioid overdose events and their precursors. Our proof-of- concept contactless system converts the phone into a short-range active sonar using frequency shifts to identify respiratory depression, apnea, and gross motor movements associated with acute opioid toxicity. We develop algorithms and perform testing in two environments: (i) an approved supervised injection facility (SIF), where people self-inject illicit opioids, and (ii) the operating room (OR), where we simulate rapid, opioid-induced overdose events using routine induction of general anesthesia. In the SIF (n = 209), our system identified postinjection, opioid-induced central apnea with 96% sensitivity and 98% specificity and identified respiratory depression with 87% sensitivity and 89% specificity. These two key events commonly precede fatal opioid overdose. In the OR, our algorithm identified 19 of 20 simulated overdose events. Given the reliable reversibility of acute opioid toxicity, smartphone-enabled overdose detection coupled with the ability to alert naloxone-equipped friends and family or emergency medical services (EMS) could hold potential as a low-barrier, harm reduction intervention.","Opioid overdose detection using smartphones. Early detection and rapid intervention can prevent death from opioid overdose. At high doses, opioids (particularly fentanyl) can cause rapid cessation of breathing (apnea), hypoxemic/hypercarbic respiratory failure, and death, the physiologic sequence by which people commonly succumb from unintentional opioid overdose. We present algorithms that run on smartphones and unobtrusively detect opioid overdose events and their precursors. Our proof-of- concept contactless system converts the phone into a short-range active sonar using frequency shifts to identify respiratory depression, apnea, and gross motor movements associated with acute opioid toxicity. We develop algorithms and perform testing in two environments: (i) an approved supervised injection facility (SIF), where people self-inject illicit opioids, and (ii) the operating room (OR), where we simulate rapid, opioid-induced overdose events using routine induction of general anesthesia. In the SIF (n = 209), our system identified postinjection, opioid-induced central apnea with 96% sensitivity and 98% specificity and identified respiratory depression with 87% sensitivity and 89% specificity. These two key events commonly precede fatal opioid overdose. In the OR, our algorithm identified 19 of 20 simulated overdose events. Given the reliable reversibility of acute opioid toxicity, smartphone-enabled overdose detection coupled with the ability to alert naloxone-equipped friends and family or emergency medical services (EMS) could hold potential as a low-barrier, harm reduction intervention."
1,Heterogeneous effects of alveolar recruitment in acute respiratory distress syndrome: a machine learning reanalysis of the Alveolar Recruitment for Acute Respiratory Distress Syndrome Trial,"BACKGROUND: Despite a robust physiological rationale, recruitment manoeuvres with PEEP titration were associated with harm in the Alveolar Recruitment for Acute Respiratory Distress Syndrome Trial (ART). We sought to investigate the potential heterogeneity in treatment effects in patients enrolled in the ART, using a machine learning approach. METHODS: The primary outcome was hospital mortality. Patients were clustered using baseline clinical and physiological data using the k-means for mixed large data method. The heterogeneity in treatment effect between clusters was investigated using Bayesian methods. We further investigated whether baseline driving pressure could modulate the association between treatment arm, cluster, and mortality. RESULTS: Data from all 1010 patients enrolled in the ART were analysed. Partitioning suggested that three clusters were present in the ART population. The largest cluster (Cluster 1) was characterised by patients with pneumonia and requiring vasopressor support. Recruitment manoeuvres with PEEP titration were associated with higher mortality in Cluster 1 (probability of harm of >98%), but this association was absent in Clusters 2 and 3 (probability of harm of 45% and 68%, respectively). Higher baseline driving pressure was associated with a progressive reduction in the association between alveolar recruitment with PEEP titration and mortality. CONCLUSIONS: Recruitment manoeuvre with PEEP titration may be harmful in acute respiratory distress syndrome (ARDS) patients with pneumonia or requiring vasopressor support. Driving pressure appears to modulate the association between the ART study intervention, aetiology of ARDS, and mortality. This machine learning approach may help tailor future RCTs. CLINICAL TRIAL REGISTRATION: NCT01374022.","Heterogeneous effects of alveolar recruitment in acute respiratory distress syndrome: a machine learning reanalysis of the Alveolar Recruitment for Acute Respiratory Distress Syndrome Trial. BACKGROUND: Despite a robust physiological rationale, recruitment manoeuvres with PEEP titration were associated with harm in the Alveolar Recruitment for Acute Respiratory Distress Syndrome Trial (ART). We sought to investigate the potential heterogeneity in treatment effects in patients enrolled in the ART, using a machine learning approach. METHODS: The primary outcome was hospital mortality. Patients were clustered using baseline clinical and physiological data using the k-means for mixed large data method. The heterogeneity in treatment effect between clusters was investigated using Bayesian methods. We further investigated whether baseline driving pressure could modulate the association between treatment arm, cluster, and mortality. RESULTS: Data from all 1010 patients enrolled in the ART were analysed. Partitioning suggested that three clusters were present in the ART population. The largest cluster (Cluster 1) was characterised by patients with pneumonia and requiring vasopressor support. Recruitment manoeuvres with PEEP titration were associated with higher mortality in Cluster 1 (probability of harm of >98%), but this association was absent in Clusters 2 and 3 (probability of harm of 45% and 68%, respectively). Higher baseline driving pressure was associated with a progressive reduction in the association between alveolar recruitment with PEEP titration and mortality. CONCLUSIONS: Recruitment manoeuvre with PEEP titration may be harmful in acute respiratory distress syndrome (ARDS) patients with pneumonia or requiring vasopressor support. Driving pressure appears to modulate the association between the ART study intervention, aetiology of ARDS, and mortality. This machine learning approach may help tailor future RCTs. CLINICAL TRIAL REGISTRATION: NCT01374022."
1,Relative Prognostic Importance and Optimal Levels of Risk Factors for Mortality and Cardiovascular Outcomes in Type 1 Diabetes Mellitus,,
1,Computer Vision Analysis of Intraoperative Video: Automated Recognition of Operative Steps in Laparoscopic Sleeve Gastrectomy,"OBJECTIVE(S): To develop and assess AI algorithms to identify operative steps in laparoscopic sleeve gastrectomy (LSG). BACKGROUND: Computer vision, a form of artificial intelligence (AI), allows for quantitative analysis of video by computers for identification of objects and patterns, such as in autonomous driving. METHODS: Intraoperative video from LSG from an academic institution was annotated by 2 fellowship-trained, board-certified bariatric surgeons. Videos were segmented into the following steps: 1) port placement, 2) liver retraction, 3) liver biopsy, 4) gastrocolic ligament dissection, 5) stapling of the stomach, 6) bagging specimen, and 7) final inspection of staple line. Deep neural networks were used to analyze videos. Accuracy of operative step identification by the AI was determined by comparing to surgeon annotations. RESULTS: Eighty-eight cases of LSG were analyzed. A random 70% sample of these clips was used to train the AI and 30% to test the AI's performance. Mean concordance correlation coefficient for human annotators was 0.862, suggesting excellent agreement. Mean (+/-SD) accuracy of the AI in identifying operative steps in the test set was 82% +/- 4% with a maximum of 85.6%. CONCLUSIONS: AI can extract quantitative surgical data from video with 85.6% accuracy. This suggests operative video could be used as a quantitative data source for research in intraoperative clinical decision support, risk prediction, or outcomes studies.","Computer Vision Analysis of Intraoperative Video: Automated Recognition of Operative Steps in Laparoscopic Sleeve Gastrectomy. OBJECTIVE(S): To develop and assess AI algorithms to identify operative steps in laparoscopic sleeve gastrectomy (LSG). BACKGROUND: Computer vision, a form of artificial intelligence (AI), allows for quantitative analysis of video by computers for identification of objects and patterns, such as in autonomous driving. METHODS: Intraoperative video from LSG from an academic institution was annotated by 2 fellowship-trained, board-certified bariatric surgeons. Videos were segmented into the following steps: 1) port placement, 2) liver retraction, 3) liver biopsy, 4) gastrocolic ligament dissection, 5) stapling of the stomach, 6) bagging specimen, and 7) final inspection of staple line. Deep neural networks were used to analyze videos. Accuracy of operative step identification by the AI was determined by comparing to surgeon annotations. RESULTS: Eighty-eight cases of LSG were analyzed. A random 70% sample of these clips was used to train the AI and 30% to test the AI's performance. Mean concordance correlation coefficient for human annotators was 0.862, suggesting excellent agreement. Mean (+/-SD) accuracy of the AI in identifying operative steps in the test set was 82% +/- 4% with a maximum of 85.6%. CONCLUSIONS: AI can extract quantitative surgical data from video with 85.6% accuracy. This suggests operative video could be used as a quantitative data source for research in intraoperative clinical decision support, risk prediction, or outcomes studies."
1,Novel Machine Learning Approach to Identify Preoperative Risk Factors Associated With Super-Utilization of Medicare Expenditure Following Surgery,"Importance: Typically defined as the top 5% of health care users, super-utilizers are responsible for an estimated 40% to 55% of all health care costs. Little is known about which factors may be associated with increased risk of long-term postoperative super-utilization. Objective: To identify clusters of patients with distinct constellations of clinical and comorbid patterns who may be associated with an elevated risk of super-utilization in the year following elective surgery. Design, Setting, and Participants: A retrospective longitudinal cohort study of 1049160 patients who underwent abdominal aortic aneurysm repair, coronary artery bypass graft, colectomy, total hip arthroplasty, total knee arthroplasty, or lung resection were identified from the 100% Medicare inpatient and outpatient Standard Analytic Files at all inpatient facilities performing 1 or more of the evaluated surgical procedures from 2013 to 2015. Data from 2012 to 2016 were used to evaluate expenditures in the year preceding and following surgery. Using a machine learning approach known as Logic Forest, comorbidities and interactions of comorbidities that put patients at an increased chance of becoming a super-utilizer were identified. All comorbidities, as defined by the Charlson (range, 0-24) and Elixhauser (range, 0-29) comorbidity indices, were used in the analysis. Higher scores indicated higher comorbidity burden. Data analysis was completed on November 16, 2018. Main Outcome and Measures: Super-utilization of health care in the year following surgery. Results: In total, 1049160 patients met inclusion criteria and were included in the analytic cohort. Their median (interquartile range) age was 73 (69-78) years, and approximately 40% were male. Super-utilizers comprised 4.8% of the overall cohort (n = 79746) yet incurred 31.7% of the expenditures. Although the difference in overall expenditures per person between super-utilizers ($4049) and low users ($2148) was relatively modest prior to surgery, the difference in expenditures between super-utilizers ($79698) vs low users ($2977) was marked in the year following surgery. Risk factors associated with super-utilization of health care included hemiplegia/paraplegia (odds ratio, 5.2; 95% CI, 4.4-6.2), weight loss (odds ratio, 3.5; 95% CI, 2.9-4.2), and congestive heart failure with chronic kidney disease stages I to IV (odds ratio, 3.4; 95% CI, 3.0-3.9). Conclusions and Relevance: Super-utilizers comprised only a small fraction of the surgical population yet were responsible for a disproportionate amount of Medicare expenditure. Certain subpopulations were associated with super-utilization of health care following surgical intervention despite having lower overall use in the preoperative period.","Novel Machine Learning Approach to Identify Preoperative Risk Factors Associated With Super-Utilization of Medicare Expenditure Following Surgery. Importance: Typically defined as the top 5% of health care users, super-utilizers are responsible for an estimated 40% to 55% of all health care costs. Little is known about which factors may be associated with increased risk of long-term postoperative super-utilization. Objective: To identify clusters of patients with distinct constellations of clinical and comorbid patterns who may be associated with an elevated risk of super-utilization in the year following elective surgery. Design, Setting, and Participants: A retrospective longitudinal cohort study of 1049160 patients who underwent abdominal aortic aneurysm repair, coronary artery bypass graft, colectomy, total hip arthroplasty, total knee arthroplasty, or lung resection were identified from the 100% Medicare inpatient and outpatient Standard Analytic Files at all inpatient facilities performing 1 or more of the evaluated surgical procedures from 2013 to 2015. Data from 2012 to 2016 were used to evaluate expenditures in the year preceding and following surgery. Using a machine learning approach known as Logic Forest, comorbidities and interactions of comorbidities that put patients at an increased chance of becoming a super-utilizer were identified. All comorbidities, as defined by the Charlson (range, 0-24) and Elixhauser (range, 0-29) comorbidity indices, were used in the analysis. Higher scores indicated higher comorbidity burden. Data analysis was completed on November 16, 2018. Main Outcome and Measures: Super-utilization of health care in the year following surgery. Results: In total, 1049160 patients met inclusion criteria and were included in the analytic cohort. Their median (interquartile range) age was 73 (69-78) years, and approximately 40% were male. Super-utilizers comprised 4.8% of the overall cohort (n = 79746) yet incurred 31.7% of the expenditures. Although the difference in overall expenditures per person between super-utilizers ($4049) and low users ($2148) was relatively modest prior to surgery, the difference in expenditures between super-utilizers ($79698) vs low users ($2977) was marked in the year following surgery. Risk factors associated with super-utilization of health care included hemiplegia/paraplegia (odds ratio, 5.2; 95% CI, 4.4-6.2), weight loss (odds ratio, 3.5; 95% CI, 2.9-4.2), and congestive heart failure with chronic kidney disease stages I to IV (odds ratio, 3.4; 95% CI, 3.0-3.9). Conclusions and Relevance: Super-utilizers comprised only a small fraction of the surgical population yet were responsible for a disproportionate amount of Medicare expenditure. Certain subpopulations were associated with super-utilization of health care following surgical intervention despite having lower overall use in the preoperative period."
1,Interpreting patient-Specific risk prediction using contextual decomposition of BiLSTMs: application to children with asthma,"BACKGROUND: Predictive modeling with longitudinal electronic health record (EHR) data offers great promise for accelerating personalized medicine and better informs clinical decision-making. Recently, deep learning models have achieved state-of-the-art performance for many healthcare prediction tasks. However, deep models lack interpretability, which is integral to successful decision-making and can lead to better patient care. In this paper, we build upon the contextual decomposition (CD) method, an algorithm for producing importance scores from long short-term memory networks (LSTMs). We extend the method to bidirectional LSTMs (BiLSTMs) and use it in the context of predicting future clinical outcomes using patients' EHR historical visits. METHODS: We use a real EHR dataset comprising 11071 patients, to evaluate and compare CD interpretations from LSTM and BiLSTM models. First, we train LSTM and BiLSTM models for the task of predicting which pre-school children with respiratory system-related complications will have asthma at school-age. After that, we conduct quantitative and qualitative analysis to evaluate the CD interpretations produced by the contextual decomposition of the trained models. In addition, we develop an interactive visualization to demonstrate the utility of CD scores in explaining predicted outcomes. RESULTS: Our experimental evaluation demonstrate that whenever a clear visit-level pattern exists, the models learn that pattern and the contextual decomposition can appropriately attribute the prediction to the correct pattern. In addition, the results confirm that the CD scores agree to a large extent with the importance scores generated using logistic regression coefficients. Our main insight was that rather than interpreting the attribution of individual visits to the predicted outcome, we could instead attribute a model's prediction to a group of visits. CONCLUSION: We presented a quantitative and qualitative evidence that CD interpretations can explain patient-specific predictions using CD attributions of individual visits or a group of visits.","Interpreting patient-Specific risk prediction using contextual decomposition of BiLSTMs: application to children with asthma. BACKGROUND: Predictive modeling with longitudinal electronic health record (EHR) data offers great promise for accelerating personalized medicine and better informs clinical decision-making. Recently, deep learning models have achieved state-of-the-art performance for many healthcare prediction tasks. However, deep models lack interpretability, which is integral to successful decision-making and can lead to better patient care. In this paper, we build upon the contextual decomposition (CD) method, an algorithm for producing importance scores from long short-term memory networks (LSTMs). We extend the method to bidirectional LSTMs (BiLSTMs) and use it in the context of predicting future clinical outcomes using patients' EHR historical visits. METHODS: We use a real EHR dataset comprising 11071 patients, to evaluate and compare CD interpretations from LSTM and BiLSTM models. First, we train LSTM and BiLSTM models for the task of predicting which pre-school children with respiratory system-related complications will have asthma at school-age. After that, we conduct quantitative and qualitative analysis to evaluate the CD interpretations produced by the contextual decomposition of the trained models. In addition, we develop an interactive visualization to demonstrate the utility of CD scores in explaining predicted outcomes. RESULTS: Our experimental evaluation demonstrate that whenever a clear visit-level pattern exists, the models learn that pattern and the contextual decomposition can appropriately attribute the prediction to the correct pattern. In addition, the results confirm that the CD scores agree to a large extent with the importance scores generated using logistic regression coefficients. Our main insight was that rather than interpreting the attribution of individual visits to the predicted outcome, we could instead attribute a model's prediction to a group of visits. CONCLUSION: We presented a quantitative and qualitative evidence that CD interpretations can explain patient-specific predictions using CD attributions of individual visits or a group of visits."
1,A geometric framework for ensemble average propagator reconstruction from diffusion MRI,,
1,Developing a portable natural language processing based phenotyping system,"BACKGROUND: This paper presents a portable phenotyping system that is capable of integrating both rule-based and statistical machine learning based approaches. METHODS: Our system utilizes UMLS to extract clinically relevant features from the unstructured text and then facilitates portability across different institutions and data systems by incorporating OHDSI's OMOP Common Data Model (CDM) to standardize necessary data elements. Our system can also store the key components of rule-based systems (e.g., regular expression matches) in the format of OMOP CDM, thus enabling the reuse, adaptation and extension of many existing rule-based clinical NLP systems. We experimented with our system on the corpus from i2b2's Obesity Challenge as a pilot study. RESULTS: Our system facilitates portable phenotyping of obesity and its 15 comorbidities based on the unstructured patient discharge summaries, while achieving a performance that often ranked among the top 10 of the challenge participants. CONCLUSION: Our system of standardization enables a consistent application of numerous rule-based and machine learning based classification techniques downstream across disparate datasets which may originate across different institutions and data systems.","Developing a portable natural language processing based phenotyping system. BACKGROUND: This paper presents a portable phenotyping system that is capable of integrating both rule-based and statistical machine learning based approaches. METHODS: Our system utilizes UMLS to extract clinically relevant features from the unstructured text and then facilitates portability across different institutions and data systems by incorporating OHDSI's OMOP Common Data Model (CDM) to standardize necessary data elements. Our system can also store the key components of rule-based systems (e.g., regular expression matches) in the format of OMOP CDM, thus enabling the reuse, adaptation and extension of many existing rule-based clinical NLP systems. We experimented with our system on the corpus from i2b2's Obesity Challenge as a pilot study. RESULTS: Our system facilitates portable phenotyping of obesity and its 15 comorbidities based on the unstructured patient discharge summaries, while achieving a performance that often ranked among the top 10 of the challenge participants. CONCLUSION: Our system of standardization enables a consistent application of numerous rule-based and machine learning based classification techniques downstream across disparate datasets which may originate across different institutions and data systems."
1,Urinary stone detection on ct images using deep convolutional neural networks: Evaluation of model performance and generalization,"Purpose: To investigate the diagnostic accuracy of cascading convolutional neural network (CNN) for urinary stone detection on unenhanced CT images and to evaluate the performance of pretrained models enriched with labeled CT images across different scanners. Materials and Methods: This HIPAA-compliant, institutional review boardâ€“approved, retrospective clinical study used unenhanced abdominopelvic CT scans from 535 adults suspected of having urolithiasis. The scans were obtained on two scanners (scanner 1 [hereafter S1] and scanner 2 [hereafter S2]). A radiologist reviewed clinical reports and labeled cases for determination of reference standard. Stones were present on 279 (S1, 131; S2, 148) and absent on 256 (S1, 158; S2, 98) scans. One hundred scans (50 from each scanner) were randomly reserved as the test dataset, and the rest were used for developing a cascade of two CNNs: The first CNN identified the extent of the urinary tract, and the second CNN detected presence of stone. Nine variations of models were developed through the combination of different training data sources (S1, S2, or both [hereafter SB]) with (ImageNet, GrayNet) and without (Random) pretrained CNNs. First, models were compared for generalizability at the section level. Second, models were assessed by using area under the receiver operating characteristic curve (AUC) and accuracy at the patient level with test dataset from both scanners (n = 100). Results: The GrayNet-pretrained model showed higher classifier exactness than did ImageNet-pretrained or Random-initialized models when tested by using data from the same or different scanners at section level. At the patient level, the AUC for stone detection was 0.92â€“0.95, depending on the model. Accuracy of GrayNet-SB (95%) was higher than that of ImageNet-SB (91%) and Random-SB (88%). For stones larger than 4 mm, all models showed similar performance (false-negative results: Two of 34). For stones smaller than 4 mm, the number of false-negative results for GrayNet-SB, ImageNet-SB, and Random-SB were one of 16, three of 16, and five of 16, respectively. GrayNet-SB identified stones in all 22 test cases that had obstructive uropathy. Conclusion: A cascading model of CNNs can detect urinary tract stones on unenhanced CT scans with a high accuracy (AUC, 0.954). Performance and generalization of CNNs across scanners can be enhanced by using transfer learning with datasets enriched with labeled medical images.","Urinary stone detection on ct images using deep convolutional neural networks: Evaluation of model performance and generalization. Purpose: To investigate the diagnostic accuracy of cascading convolutional neural network (CNN) for urinary stone detection on unenhanced CT images and to evaluate the performance of pretrained models enriched with labeled CT images across different scanners. Materials and Methods: This HIPAA-compliant, institutional review boardâ€“approved, retrospective clinical study used unenhanced abdominopelvic CT scans from 535 adults suspected of having urolithiasis. The scans were obtained on two scanners (scanner 1 [hereafter S1] and scanner 2 [hereafter S2]). A radiologist reviewed clinical reports and labeled cases for determination of reference standard. Stones were present on 279 (S1, 131; S2, 148) and absent on 256 (S1, 158; S2, 98) scans. One hundred scans (50 from each scanner) were randomly reserved as the test dataset, and the rest were used for developing a cascade of two CNNs: The first CNN identified the extent of the urinary tract, and the second CNN detected presence of stone. Nine variations of models were developed through the combination of different training data sources (S1, S2, or both [hereafter SB]) with (ImageNet, GrayNet) and without (Random) pretrained CNNs. First, models were compared for generalizability at the section level. Second, models were assessed by using area under the receiver operating characteristic curve (AUC) and accuracy at the patient level with test dataset from both scanners (n = 100). Results: The GrayNet-pretrained model showed higher classifier exactness than did ImageNet-pretrained or Random-initialized models when tested by using data from the same or different scanners at section level. At the patient level, the AUC for stone detection was 0.92â€“0.95, depending on the model. Accuracy of GrayNet-SB (95%) was higher than that of ImageNet-SB (91%) and Random-SB (88%). For stones larger than 4 mm, all models showed similar performance (false-negative results: Two of 34). For stones smaller than 4 mm, the number of false-negative results for GrayNet-SB, ImageNet-SB, and Random-SB were one of 16, three of 16, and five of 16, respectively. GrayNet-SB identified stones in all 22 test cases that had obstructive uropathy. Conclusion: A cascading model of CNNs can detect urinary tract stones on unenhanced CT scans with a high accuracy (AUC, 0.954). Performance and generalization of CNNs across scanners can be enhanced by using transfer learning with datasets enriched with labeled medical images."
1,RMDL: Recalibrated multi-instance deep learning for whole slide gastric image classification,"The whole slide histopathology images (WSIs) play a critical role in gastric cancer diagnosis. However, due to the large scale of WSIs and various sizes of the abnormal area, how to select informative regions and analyze them are quite challenging during the automatic diagnosis process. The multi-instance learning based on the most discriminative instances can be of great benefit for whole slide gastric image diagnosis. In this paper, we design a recalibrated multi-instance deep learning method (RMDL) to address this challenging problem. We first select the discriminative instances, and then utilize these instances to diagnose diseases based on the proposed RMDL approach. The designed RMDL network is capable of capturing instance-wise dependencies and recalibrating instance features according to the importance coefficient learned from the fused features. Furthermore, we build a large whole-slide gastric histopathology image dataset with detailed pixel-level annotations. Experimental results on the constructed gastric dataset demonstrate the significant improvement on the accuracy of our proposed framework compared with other state-of-the-art multi-instance learning methods. Moreover, our method is general and can be extended to other diagnosis tasks of different cancer types based on WSIs.","RMDL: Recalibrated multi-instance deep learning for whole slide gastric image classification. The whole slide histopathology images (WSIs) play a critical role in gastric cancer diagnosis. However, due to the large scale of WSIs and various sizes of the abnormal area, how to select informative regions and analyze them are quite challenging during the automatic diagnosis process. The multi-instance learning based on the most discriminative instances can be of great benefit for whole slide gastric image diagnosis. In this paper, we design a recalibrated multi-instance deep learning method (RMDL) to address this challenging problem. We first select the discriminative instances, and then utilize these instances to diagnose diseases based on the proposed RMDL approach. The designed RMDL network is capable of capturing instance-wise dependencies and recalibrating instance features according to the importance coefficient learned from the fused features. Furthermore, we build a large whole-slide gastric histopathology image dataset with detailed pixel-level annotations. Experimental results on the constructed gastric dataset demonstrate the significant improvement on the accuracy of our proposed framework compared with other state-of-the-art multi-instance learning methods. Moreover, our method is general and can be extended to other diagnosis tasks of different cancer types based on WSIs."
1,Ontology-based venous thromboembolism risk assessment model developing from medical records,"BACKGROUND: Padua linear model is widely used for the risk assessment of venous thromboembolism (VTE), a common but preventable complication for inpatients. However, genetic and environmental differences between Western and Chinese population limit the validity of Padua model in Chinese patients. Medical records which contain rich information about disease progression, are useful in mining new risk factors related to Chinese VTE patients. Furthermore, machine learning (ML) methods provide new opportunities to build precise risk prediction model by automatic selection of risk factors based on original medical records. METHODS: Medical records of 3,106 inpatients including 224 VTE patients were collected and various types of ontologies were integrated to parse unstructured text. A workflow of ontology-based VTE risk prediction model, that combines natural language processing (NLP) and machine learning (ML) technologies, was proposed. Firstly ontology terms were extracted from medical records, then sorted according to their calculated weights. Next importance of each term in the unit of section was evaluated and finally a ML model was built based on a subset of terms. Four ML methods were tested, and the best model was decided by comparing area under the receiver operating characteristic curve (AUROC). RESULTS: Medical records were first split into different sections and subsequently, terms from each section were sorted by their weights calculated by multiple types of information. Greedy selection algorithm was used to obtain significant sections and terms. Top terms in each section were selected to construct patients' distributed representations by word embedding technique. Using top 300 terms of two important sections, namely the 'Progress Note' section and 'Admitting Diagnosis' section, the model showed relatively better predictive performance. Then ML model which utilizes a subset of terms from two sections, about 110 terms, achieved the best AUC score, of 0.973â€‰Â±â€‰0.006, which was significantly better compared to the Padua's performance of 0.791â€‰Â±â€‰0.022. Terms found by the model showed their potential to help clinicians explore new risk factors. CONCLUSIONS: In this study, a new VTE risk assessment model based on ontologies extraction from raw medical records is developed and its performance is verified on real clinical dataset. Results of selected terms can help clinicians to discover meaningful risk factors.","Ontology-based venous thromboembolism risk assessment model developing from medical records. BACKGROUND: Padua linear model is widely used for the risk assessment of venous thromboembolism (VTE), a common but preventable complication for inpatients. However, genetic and environmental differences between Western and Chinese population limit the validity of Padua model in Chinese patients. Medical records which contain rich information about disease progression, are useful in mining new risk factors related to Chinese VTE patients. Furthermore, machine learning (ML) methods provide new opportunities to build precise risk prediction model by automatic selection of risk factors based on original medical records. METHODS: Medical records of 3,106 inpatients including 224 VTE patients were collected and various types of ontologies were integrated to parse unstructured text. A workflow of ontology-based VTE risk prediction model, that combines natural language processing (NLP) and machine learning (ML) technologies, was proposed. Firstly ontology terms were extracted from medical records, then sorted according to their calculated weights. Next importance of each term in the unit of section was evaluated and finally a ML model was built based on a subset of terms. Four ML methods were tested, and the best model was decided by comparing area under the receiver operating characteristic curve (AUROC). RESULTS: Medical records were first split into different sections and subsequently, terms from each section were sorted by their weights calculated by multiple types of information. Greedy selection algorithm was used to obtain significant sections and terms. Top terms in each section were selected to construct patients' distributed representations by word embedding technique. Using top 300 terms of two important sections, namely the 'Progress Note' section and 'Admitting Diagnosis' section, the model showed relatively better predictive performance. Then ML model which utilizes a subset of terms from two sections, about 110 terms, achieved the best AUC score, of 0.973â€‰Â±â€‰0.006, which was significantly better compared to the Padua's performance of 0.791â€‰Â±â€‰0.022. Terms found by the model showed their potential to help clinicians explore new risk factors. CONCLUSIONS: In this study, a new VTE risk assessment model based on ontologies extraction from raw medical records is developed and its performance is verified on real clinical dataset. Results of selected terms can help clinicians to discover meaningful risk factors."
1,Predicting Breast Cancer by Applying Deep Learning to Linked Health Records and Mammograms,,
1,A multimodality test to guide the management of patients with a pancreatic cyst,,
1,Applying deep matching networks to Chinese medical question answering: a study and a dataset,"BACKGROUND: Medical and clinical question answering (QA) is highly concerned by researchers recently. Though there are remarkable advances in this field, the development in Chinese medical domain is relatively backward. It can be attributed to the difficulty of Chinese text processing and the lack of large-scale datasets. To bridge the gap, this paper introduces a Chinese medical QA dataset and proposes effective methods for the task. METHODS: We first construct a large scale Chinese medical QA dataset. Then we leverage deep matching neural networks to capture semantic interaction between words in questions and answers. Considering that Chinese Word Segmentation (CWS) tools may fail to identify clinical terms, we design a module to merge the word segments and produce a new representation. It learns the common compositions of words or segments by using convolutional kernels and selects the strongest signals by windowed pooling. RESULTS: The best performer among popular CWS tools on our dataset is found. In our experiments, deep matching models substantially outperform existing methods. Results also show that our proposed semantic clustered representation module improves the performance of models by up to 5.5% Precision at 1 and 4.9% Mean Average Precision. CONCLUSIONS: In this paper, we introduce a large scale Chinese medical QA dataset and cast the task into a semantic matching problem. We also compare different CWS tools and input units. Among the two state-of-the-art deep matching neural networks, MatchPyramid performs better. Results also show the effectiveness of the proposed semantic clustered representation module.","Applying deep matching networks to Chinese medical question answering: a study and a dataset. BACKGROUND: Medical and clinical question answering (QA) is highly concerned by researchers recently. Though there are remarkable advances in this field, the development in Chinese medical domain is relatively backward. It can be attributed to the difficulty of Chinese text processing and the lack of large-scale datasets. To bridge the gap, this paper introduces a Chinese medical QA dataset and proposes effective methods for the task. METHODS: We first construct a large scale Chinese medical QA dataset. Then we leverage deep matching neural networks to capture semantic interaction between words in questions and answers. Considering that Chinese Word Segmentation (CWS) tools may fail to identify clinical terms, we design a module to merge the word segments and produce a new representation. It learns the common compositions of words or segments by using convolutional kernels and selects the strongest signals by windowed pooling. RESULTS: The best performer among popular CWS tools on our dataset is found. In our experiments, deep matching models substantially outperform existing methods. Results also show that our proposed semantic clustered representation module improves the performance of models by up to 5.5% Precision at 1 and 4.9% Mean Average Precision. CONCLUSIONS: In this paper, we introduce a large scale Chinese medical QA dataset and cast the task into a semantic matching problem. We also compare different CWS tools and input units. Among the two state-of-the-art deep matching neural networks, MatchPyramid performs better. Results also show the effectiveness of the proposed semantic clustered representation module."
1,Adversarial learning for mono- or multi-modal registration,,
1,Evaluating a fully automated pulmonary nodule detection approach and its impact on radiologist performance,"Purpose: To compare sensitivity in the detection of lung nodules between the deep learning (DL) model and radiologists using various patient population and scanning parameters and to assess whether the radiologistsâ€™ detection performance could be enhanced when using the DL model for assistance. Materials and Methods: A total of 12 754 thin-section chest CT scans from January 2012 to June 2017 were retrospectively collected for DL model training, validation, and testing. Pulmonary nodules from these scans were categorized into four types: Solid, subsolid, calcified, and pleural. The testing dataset was divided into three cohorts based on radiation dose, patient age, and CT manufacturer. Detection performance of the DL model was analyzed by using a free-response receiver operating characteristic curve. Sensitivities of the DL model and radiologists were compared by using exploratory data analysis. False-positive detection rates of the DL model were compared within each cohort. Detection performance of the same radiologist with and without the DL model were compared by using nodule-level sensitivity and patient-level localization receiver operating characteristic curves. Results: The DL model showed elevated overall sensitivity compared with manual review of pulmonary nodules. No significant dependence regarding radiation dose, patient age range, or CT manufacturer was observed. The sensitivity of the junior radiologist was significantly dependent on patient age. When radiologists used the DL model for assistance, their performance improved and reading time was reduced. Conclusion: DL shows promise to enhance the identification of pulmonary nodules and benefit nodule management.","Evaluating a fully automated pulmonary nodule detection approach and its impact on radiologist performance. Purpose: To compare sensitivity in the detection of lung nodules between the deep learning (DL) model and radiologists using various patient population and scanning parameters and to assess whether the radiologistsâ€™ detection performance could be enhanced when using the DL model for assistance. Materials and Methods: A total of 12 754 thin-section chest CT scans from January 2012 to June 2017 were retrospectively collected for DL model training, validation, and testing. Pulmonary nodules from these scans were categorized into four types: Solid, subsolid, calcified, and pleural. The testing dataset was divided into three cohorts based on radiation dose, patient age, and CT manufacturer. Detection performance of the DL model was analyzed by using a free-response receiver operating characteristic curve. Sensitivities of the DL model and radiologists were compared by using exploratory data analysis. False-positive detection rates of the DL model were compared within each cohort. Detection performance of the same radiologist with and without the DL model were compared by using nodule-level sensitivity and patient-level localization receiver operating characteristic curves. Results: The DL model showed elevated overall sensitivity compared with manual review of pulmonary nodules. No significant dependence regarding radiation dose, patient age range, or CT manufacturer was observed. The sensitivity of the junior radiologist was significantly dependent on patient age. When radiologists used the DL model for assistance, their performance improved and reading time was reduced. Conclusion: DL shows promise to enhance the identification of pulmonary nodules and benefit nodule management."
1,Modeling Surgical Technical Skill Using Expert Assessment for Automated Computer Rating,"OBJECTIVE: Computer vision was used to predict expert performance ratings from surgeon hand motions for tying and suturing tasks. SUMMARY BACKGROUND DATA: Existing methods, including the objective structured assessment of technical skills (OSATS), have proven reliable, but do not readily discriminate at the task level. Computer vision may be used for evaluating distinct task performance throughout an operation. METHODS: Open surgeries was videoed and surgeon hands were tracked without using sensors or markers. An expert panel of 3 attending surgeons rated tying and suturing video clips on continuous scales from 0 to 10 along 3 task measures adapted from the broader OSATS: motion economy, fluidity of motion, and tissue handling. Empirical models were developed to predict the expert consensus ratings based on the hand kinematic data records. RESULTS: The predicted versus panel ratings for suturing had slopes from 0.73 to 1, and intercepts from 0.36 to 1.54 (Average R2 = 0.81). Predicted versus panel ratings for tying had slopes from 0.39 to 0.88, and intercepts from 0.79 to 4.36 (Average R2 = 0.57). The mean square error among predicted and expert ratings was consistently less than the mean squared difference among individual expert ratings and the eventual consensus ratings. CONCLUSIONS: The computer algorithm consistently predicted the panel ratings of individual tasks, and were more objective and reliable than individual assessment by surgical experts.","Modeling Surgical Technical Skill Using Expert Assessment for Automated Computer Rating. OBJECTIVE: Computer vision was used to predict expert performance ratings from surgeon hand motions for tying and suturing tasks. SUMMARY BACKGROUND DATA: Existing methods, including the objective structured assessment of technical skills (OSATS), have proven reliable, but do not readily discriminate at the task level. Computer vision may be used for evaluating distinct task performance throughout an operation. METHODS: Open surgeries was videoed and surgeon hands were tracked without using sensors or markers. An expert panel of 3 attending surgeons rated tying and suturing video clips on continuous scales from 0 to 10 along 3 task measures adapted from the broader OSATS: motion economy, fluidity of motion, and tissue handling. Empirical models were developed to predict the expert consensus ratings based on the hand kinematic data records. RESULTS: The predicted versus panel ratings for suturing had slopes from 0.73 to 1, and intercepts from 0.36 to 1.54 (Average R2 = 0.81). Predicted versus panel ratings for tying had slopes from 0.39 to 0.88, and intercepts from 0.79 to 4.36 (Average R2 = 0.57). The mean square error among predicted and expert ratings was consistently less than the mean squared difference among individual expert ratings and the eventual consensus ratings. CONCLUSIONS: The computer algorithm consistently predicted the panel ratings of individual tasks, and were more objective and reliable than individual assessment by surgical experts."
1,Digital Mammography in Breast Cancer: Additive Value of Radiomics of Breast Parenchyma,,
1,Reproducibility of CT Radiomic Features within the Same Patient: Influence of Radiation Dose and CT Reconstruction Settings,"Background Results of recent phantom studies show that variation in CT acquisition parameters and reconstruction techniques may make radiomic features largely nonreproduceable and of limited use for prognostic clinical studies. Purpose To investigate the effect of CT radiation dose and reconstruction settings on the reproducibility of radiomic features, as well as to identify correction factors for mitigating these sources of variability. Materials and Methods This was a secondary analysis of a prospective study of metastatic liver lesions in patients who underwent staging with single-energy dual-source contrast material-enhanced staging CT between September 2011 and April 2012. Technique parameters were altered, resulting in 28 CT data sets per patient that included different dose levels, section thicknesses, kernels, and reconstruction algorithm settings. By using a training data set (n = 76), reproducible intensity, shape, and texture radiomic features (reproducibility threshold, R(2) >/= 0.95) were selected and correction factors were calculated by using a linear model to convert each radiomic feature to its estimated value in a reference technique. By using a test data set (n = 75), the reproducibility of hierarchical clustering based on 106 radiomic features measured with different CT techniques was assessed. Results Data in 78 patients (mean age, 60 years +/- 10; 33 women) with 151 liver lesions were included. The percentage of radiomic features deemed reproducible for any variation of the different technical parameters was 11% (12 of 106). Of all technical parameters, reconstructed section thickness had the largest impact on the reproducibility of radiomic features (12.3% [13 of 106]) if only one technical parameter was changed while all other technical parameters were kept constant. The results of the hierarchical cluster analysis showed improved clustering reproducibility when reproducible radiomic features with dedicated correction factors were used (rho = 0.39-0.71 vs rho = 0.14-0.47). Conclusion Most radiomic features are highly affected by CT acquisition and reconstruction settings, to the point of being nonreproducible. Selecting reproducible radiomic features along with study-specific correction factors offers improved clustering reproducibility. (c) RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Sosna in this issue.","Reproducibility of CT Radiomic Features within the Same Patient: Influence of Radiation Dose and CT Reconstruction Settings. Background Results of recent phantom studies show that variation in CT acquisition parameters and reconstruction techniques may make radiomic features largely nonreproduceable and of limited use for prognostic clinical studies. Purpose To investigate the effect of CT radiation dose and reconstruction settings on the reproducibility of radiomic features, as well as to identify correction factors for mitigating these sources of variability. Materials and Methods This was a secondary analysis of a prospective study of metastatic liver lesions in patients who underwent staging with single-energy dual-source contrast material-enhanced staging CT between September 2011 and April 2012. Technique parameters were altered, resulting in 28 CT data sets per patient that included different dose levels, section thicknesses, kernels, and reconstruction algorithm settings. By using a training data set (n = 76), reproducible intensity, shape, and texture radiomic features (reproducibility threshold, R(2) >/= 0.95) were selected and correction factors were calculated by using a linear model to convert each radiomic feature to its estimated value in a reference technique. By using a test data set (n = 75), the reproducibility of hierarchical clustering based on 106 radiomic features measured with different CT techniques was assessed. Results Data in 78 patients (mean age, 60 years +/- 10; 33 women) with 151 liver lesions were included. The percentage of radiomic features deemed reproducible for any variation of the different technical parameters was 11% (12 of 106). Of all technical parameters, reconstructed section thickness had the largest impact on the reproducibility of radiomic features (12.3% [13 of 106]) if only one technical parameter was changed while all other technical parameters were kept constant. The results of the hierarchical cluster analysis showed improved clustering reproducibility when reproducible radiomic features with dedicated correction factors were used (rho = 0.39-0.71 vs rho = 0.14-0.47). Conclusion Most radiomic features are highly affected by CT acquisition and reconstruction settings, to the point of being nonreproducible. Selecting reproducible radiomic features along with study-specific correction factors offers improved clustering reproducibility. (c) RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Sosna in this issue."
1,Utilizing dynamic treatment information for MACE prediction of acute coronary syndrome,"BACKGROUND: Main adverse cardiac events (MACE) are essentially composite endpoints for assessing safety and efficacy of treatment processes of acute coronary syndrome (ACS) patients. Timely prediction of MACE is highly valuable for improving the effects of ACS treatments. Most existing tools are specific to predict MACE by mainly using static patient features and neglecting dynamic treatment information during learning. METHODS: We address this challenge by developing a deep learning-based approach to utilize a large volume of heterogeneous electronic health record (EHR) for predicting MACE after ACS. Specifically, we obtain the deep representation of dynamic treatment features from EHR data, using the bidirectional recurrent neural network. And then, the extracted latent representation of treatment features can be utilized to predict whether a patient occurs MACE in his or her hospitalization. RESULTS: We validate the effectiveness of our approach on a clinical dataset containing 2930 ACS patient samples with 232 static feature types and 2194 dynamic feature types. The performance of our best model for predicting MACE after ACS remains robust and reaches 0.713 and 0.764 in terms of AUC and Accuracy, respectively, and has over 11.9% (1.2%) and 1.9% (7.5%) performance gain of AUC (Accuracy) in comparison with both logistic regression and a boosted resampling model presented in our previous work, respectively. The results are statistically significant. CONCLUSIONS: We hypothesize that our proposed model adapted to leverage dynamic treatment information in EHR data appears to boost the performance of MACE prediction for ACS, and can readily meet the demand clinical prediction of other diseases, from a large volume of EHR in an open-ended fashion.","Utilizing dynamic treatment information for MACE prediction of acute coronary syndrome. BACKGROUND: Main adverse cardiac events (MACE) are essentially composite endpoints for assessing safety and efficacy of treatment processes of acute coronary syndrome (ACS) patients. Timely prediction of MACE is highly valuable for improving the effects of ACS treatments. Most existing tools are specific to predict MACE by mainly using static patient features and neglecting dynamic treatment information during learning. METHODS: We address this challenge by developing a deep learning-based approach to utilize a large volume of heterogeneous electronic health record (EHR) for predicting MACE after ACS. Specifically, we obtain the deep representation of dynamic treatment features from EHR data, using the bidirectional recurrent neural network. And then, the extracted latent representation of treatment features can be utilized to predict whether a patient occurs MACE in his or her hospitalization. RESULTS: We validate the effectiveness of our approach on a clinical dataset containing 2930 ACS patient samples with 232 static feature types and 2194 dynamic feature types. The performance of our best model for predicting MACE after ACS remains robust and reaches 0.713 and 0.764 in terms of AUC and Accuracy, respectively, and has over 11.9% (1.2%) and 1.9% (7.5%) performance gain of AUC (Accuracy) in comparison with both logistic regression and a boosted resampling model presented in our previous work, respectively. The results are statistically significant. CONCLUSIONS: We hypothesize that our proposed model adapted to leverage dynamic treatment information in EHR data appears to boost the performance of MACE prediction for ACS, and can readily meet the demand clinical prediction of other diseases, from a large volume of EHR in an open-ended fashion."
1,Implementation and validation of a three-dimensional cardiac motion estimation network,"Purpose: To describe an unsupervised three-dimensional cardiac motion estimation network (CarMEN) for deformable motion estimation from two-dimensional cine MR images. Materials and Methods: A function was implemented using CarMEN, a convolutional neural network that takes two three-dimensional input volumes and outputs a motion field. A smoothness constraint was imposed on the field by regularizing the Frobenius norm of its Jacobian matrix. CarMEN was trained and tested with data from 150 cardiac patients who underwent MRI examinations and was validated on synthetic (n = 100) and pediatric (n = 33) datasets. CarMEN was compared to five state-of-the-art nonrigid body registration methods by using several performance metrics, including Dice similarity coefficient (DSC) and end-point error. Results: On the synthetic dataset, CarMEN achieved a median DSC of 0.85, which was higher than all five methods (minimumâ€“ maximum median [or MMM], 0.67â€“0.84; P,.001), and a median end-point error of 1.7, which was lower than (MMM, 2.1â€“2.7; P,.001) or similar to (MMM, 1.6â€“1.7; P..05) all other techniques. On the real datasets, CarMEN achieved a median DSC of 0.73 for Automated Cardiac Diagnosis Challenge data, which was higher than (MMM, 0.33; P,.0001) or similar to (MMM, 0.72â€“0.75; P..05) all other methods, and a median DSC of 0.77 for pediatric data, which was higher than (MMM, 0.71â€“0.76; P,.0001) or similar to (MMM, 0.77â€“0.78; P..05) all other methods. All P values were derived from pairwise testing. For all other metrics, CarMEN achieved better accuracy on all datasets than all other techniques except for one, which had the worst motion estimation accuracy. Conclusion: The proposed deep learningâ€“based approach for three-dimensional cardiac motion estimation allowed the derivation of a motion model that balances motion characterization and image registration accuracy and achieved motion estimation accuracy comparable to or better than that of several state-of-the-art image registration algorithms.","Implementation and validation of a three-dimensional cardiac motion estimation network. Purpose: To describe an unsupervised three-dimensional cardiac motion estimation network (CarMEN) for deformable motion estimation from two-dimensional cine MR images. Materials and Methods: A function was implemented using CarMEN, a convolutional neural network that takes two three-dimensional input volumes and outputs a motion field. A smoothness constraint was imposed on the field by regularizing the Frobenius norm of its Jacobian matrix. CarMEN was trained and tested with data from 150 cardiac patients who underwent MRI examinations and was validated on synthetic (n = 100) and pediatric (n = 33) datasets. CarMEN was compared to five state-of-the-art nonrigid body registration methods by using several performance metrics, including Dice similarity coefficient (DSC) and end-point error. Results: On the synthetic dataset, CarMEN achieved a median DSC of 0.85, which was higher than all five methods (minimumâ€“ maximum median [or MMM], 0.67â€“0.84; P,.001), and a median end-point error of 1.7, which was lower than (MMM, 2.1â€“2.7; P,.001) or similar to (MMM, 1.6â€“1.7; P..05) all other techniques. On the real datasets, CarMEN achieved a median DSC of 0.73 for Automated Cardiac Diagnosis Challenge data, which was higher than (MMM, 0.33; P,.0001) or similar to (MMM, 0.72â€“0.75; P..05) all other methods, and a median DSC of 0.77 for pediatric data, which was higher than (MMM, 0.71â€“0.76; P,.0001) or similar to (MMM, 0.77â€“0.78; P..05) all other methods. All P values were derived from pairwise testing. For all other metrics, CarMEN achieved better accuracy on all datasets than all other techniques except for one, which had the worst motion estimation accuracy. Conclusion: The proposed deep learningâ€“based approach for three-dimensional cardiac motion estimation allowed the derivation of a motion model that balances motion characterization and image registration accuracy and achieved motion estimation accuracy comparable to or better than that of several state-of-the-art image registration algorithms."
1,Development and validation of machine learning models in prediction of remission in patients with moderate to severe Crohn disease,"IMPORTANCE Biological therapies have revolutionized inflammatory bowel disease management, but many patients do not respond to biological monotherapy. Identification of likely responders could reduce costs and delays in remission. OBJECTIVE To identify patients with Crohn disease likely to be durable responders to ustekinumab before committing to long-term treatment. DESIGN, SETTING, AND PARTICIPANTS This cohort study analyzed data from 3 phase 3 randomized clinical trials (UNITI-1, UNITI-2, and IM-UNITI) conducted from 2011 to 2015. Participants (n = 401) were individuals with active (C-reactive protein [CRP] measurement of_5mg/L at enrollment) Crohn disease who received ustekinumab therapy. Data analysis was performed from November 1, 2017, to June 1, 2018. EXPOSURES All included patients were exposed to 1 or more dose of ustekinumab for 8 weeks or more. MAIN OUTCOMES AND MEASURES Random forest methods were used in building 2 models for predicting Crohn disease remission, with a CRP level lower than 5mg/dL as a proxy for biological remission, beyond week 42 of ustekinumab treatment. The first model used only baseline data, and the second used data through week 8. RESULTS In total, 401 participants, with a mean (SD) age of 36.3 (12.6) years and 170 male (42.4%), were included. Theweek-8 model had a mean area under the receiver operating characteristic curve (AUROC) of 0.78 (95%CI, 0.69-0.87). In the testing data set, 27 of 55 participants (49.1%) classified as likely to have treatment success achieved success with a CRP level lower than 5mg/L after week 42, and 7 of 65 participants (10.8%) classified as likely to have treatment failure achieved this outcome. In the full cohort, 87 patients (21.7%) attained remission after week 42. A prediction model using the week-6 albumin to CRP ratio had an AUROC of 0.76 (95%CI, 0.71-0.82). Baseline ustekinumab serum levels did not improve the model's prediction performance. CONCLUSIONS AND RELEVANCE In patients with active Crohn disease, demographic and laboratory data before week 8 of treatment appeared to allow the prompt identification of likely nonresponders to ustekinumab without the need for costly drug-level monitoring.","Development and validation of machine learning models in prediction of remission in patients with moderate to severe Crohn disease. IMPORTANCE Biological therapies have revolutionized inflammatory bowel disease management, but many patients do not respond to biological monotherapy. Identification of likely responders could reduce costs and delays in remission. OBJECTIVE To identify patients with Crohn disease likely to be durable responders to ustekinumab before committing to long-term treatment. DESIGN, SETTING, AND PARTICIPANTS This cohort study analyzed data from 3 phase 3 randomized clinical trials (UNITI-1, UNITI-2, and IM-UNITI) conducted from 2011 to 2015. Participants (n = 401) were individuals with active (C-reactive protein [CRP] measurement of_5mg/L at enrollment) Crohn disease who received ustekinumab therapy. Data analysis was performed from November 1, 2017, to June 1, 2018. EXPOSURES All included patients were exposed to 1 or more dose of ustekinumab for 8 weeks or more. MAIN OUTCOMES AND MEASURES Random forest methods were used in building 2 models for predicting Crohn disease remission, with a CRP level lower than 5mg/dL as a proxy for biological remission, beyond week 42 of ustekinumab treatment. The first model used only baseline data, and the second used data through week 8. RESULTS In total, 401 participants, with a mean (SD) age of 36.3 (12.6) years and 170 male (42.4%), were included. Theweek-8 model had a mean area under the receiver operating characteristic curve (AUROC) of 0.78 (95%CI, 0.69-0.87). In the testing data set, 27 of 55 participants (49.1%) classified as likely to have treatment success achieved success with a CRP level lower than 5mg/L after week 42, and 7 of 65 participants (10.8%) classified as likely to have treatment failure achieved this outcome. In the full cohort, 87 patients (21.7%) attained remission after week 42. A prediction model using the week-6 albumin to CRP ratio had an AUROC of 0.76 (95%CI, 0.71-0.82). Baseline ustekinumab serum levels did not improve the model's prediction performance. CONCLUSIONS AND RELEVANCE In patients with active Crohn disease, demographic and laboratory data before week 8 of treatment appeared to allow the prompt identification of likely nonresponders to ustekinumab without the need for costly drug-level monitoring."
1,Factors affecting sex-related reporting in medical research: a cross-disciplinary bibliometric analysis,,
1,A novel machine learning-derived radiotranscriptomic signature of perivascular fat improves cardiac risk prediction using coronary CT angiography,"BACKGROUND: Coronary inflammation induces dynamic changes in the balance between water and lipid content in perivascular adipose tissue (PVAT), as captured by perivascular Fat Attenuation Index (FAI) in standard coronary CT angiography (CCTA). However, inflammation is not the only process involved in atherogenesis and we hypothesized that additional radiomic signatures of adverse fibrotic and microvascular PVAT remodelling, may further improve cardiac risk prediction. METHODS AND RESULTS: We present a new artificial intelligence-powered method to predict cardiac risk by analysing the radiomic profile of coronary PVAT, developed and validated in patient cohorts acquired in three different studies. In Study 1, adipose tissue biopsies were obtained from 167 patients undergoing cardiac surgery, and the expression of genes representing inflammation, fibrosis and vascularity was linked with the radiomic features extracted from tissue CT images. Adipose tissue wavelet-transformed mean attenuation (captured by FAI) was the most sensitive radiomic feature in describing tissue inflammation (TNFA expression), while features of radiomic texture were related to adipose tissue fibrosis (COL1A1 expression) and vascularity (CD31 expression). In Study 2, we analysed 1391 coronary PVAT radiomic features in 101 patients who experienced major adverse cardiac events (MACE) within 5 years of having a CCTA and 101 matched controls, training and validating a machine learning (random forest) algorithm (fat radiomic profile, FRP) to discriminate cases from controls (C-statistic 0.77 [95%CI: 0.62-0.93] in the external validation set). The coronary FRP signature was then tested in 1575 consecutive eligible participants in the SCOT-HEART trial, where it significantly improved MACE prediction beyond traditional risk stratification that included risk factors, coronary calcium score, coronary stenosis, and high-risk plaque features on CCTA (Delta[C-statistic] = 0.126, P < 0.001). In Study 3, FRP was significantly higher in 44 patients presenting with acute myocardial infarction compared with 44 matched controls, but unlike FAI, remained unchanged 6 months after the index event, confirming that FRP detects persistent PVAT changes not captured by FAI. CONCLUSION: The CCTA-based radiomic profiling of coronary artery PVAT detects perivascular structural remodelling associated with coronary artery disease, beyond inflammation. A new artificial intelligence (AI)-powered imaging biomarker (FRP) leads to a striking improvement of cardiac risk prediction over and above the current state-of-the-art.","A novel machine learning-derived radiotranscriptomic signature of perivascular fat improves cardiac risk prediction using coronary CT angiography. BACKGROUND: Coronary inflammation induces dynamic changes in the balance between water and lipid content in perivascular adipose tissue (PVAT), as captured by perivascular Fat Attenuation Index (FAI) in standard coronary CT angiography (CCTA). However, inflammation is not the only process involved in atherogenesis and we hypothesized that additional radiomic signatures of adverse fibrotic and microvascular PVAT remodelling, may further improve cardiac risk prediction. METHODS AND RESULTS: We present a new artificial intelligence-powered method to predict cardiac risk by analysing the radiomic profile of coronary PVAT, developed and validated in patient cohorts acquired in three different studies. In Study 1, adipose tissue biopsies were obtained from 167 patients undergoing cardiac surgery, and the expression of genes representing inflammation, fibrosis and vascularity was linked with the radiomic features extracted from tissue CT images. Adipose tissue wavelet-transformed mean attenuation (captured by FAI) was the most sensitive radiomic feature in describing tissue inflammation (TNFA expression), while features of radiomic texture were related to adipose tissue fibrosis (COL1A1 expression) and vascularity (CD31 expression). In Study 2, we analysed 1391 coronary PVAT radiomic features in 101 patients who experienced major adverse cardiac events (MACE) within 5 years of having a CCTA and 101 matched controls, training and validating a machine learning (random forest) algorithm (fat radiomic profile, FRP) to discriminate cases from controls (C-statistic 0.77 [95%CI: 0.62-0.93] in the external validation set). The coronary FRP signature was then tested in 1575 consecutive eligible participants in the SCOT-HEART trial, where it significantly improved MACE prediction beyond traditional risk stratification that included risk factors, coronary calcium score, coronary stenosis, and high-risk plaque features on CCTA (Delta[C-statistic] = 0.126, P < 0.001). In Study 3, FRP was significantly higher in 44 patients presenting with acute myocardial infarction compared with 44 matched controls, but unlike FAI, remained unchanged 6 months after the index event, confirming that FRP detects persistent PVAT changes not captured by FAI. CONCLUSION: The CCTA-based radiomic profiling of coronary artery PVAT detects perivascular structural remodelling associated with coronary artery disease, beyond inflammation. A new artificial intelligence (AI)-powered imaging biomarker (FRP) leads to a striking improvement of cardiac risk prediction over and above the current state-of-the-art."
1,Defining HLA-II Ligand Processing and Binding Rules with Mass Spectrometry Enhances Cancer Epitope Prediction,"Increasing evidence indicates CD4(+) T cells can recognize cancer-specific antigens and control tumor growth. However, it remains difficult to predict the antigens that will be presented by human leukocyte antigen class II molecules (HLA-II), hindering efforts to optimally target them therapeutically. Obstacles include inaccurate peptide-binding prediction and unsolved complexities of the HLA-II pathway. To address these challenges, we developed an improved technology for discovering HLA-II binding motifs and conducted a comprehensive analysis of tumor ligandomes to learn processing rules relevant in the tumor microenvironment. We profiled >40 HLA-II alleles and showed that binding motifs were highly sensitive to HLA-DM, a peptide-loading chaperone. We also revealed that intratumoral HLA-II presentation was dominated by professional antigen-presenting cells (APCs) rather than cancer cells. Integrating these observations, we developed algorithms that accurately predicted APC ligandomes, including peptides from phagocytosed cancer cells. These tools and biological insights will enable improved HLA-II-directed cancer therapies.","Defining HLA-II Ligand Processing and Binding Rules with Mass Spectrometry Enhances Cancer Epitope Prediction. Increasing evidence indicates CD4(+) T cells can recognize cancer-specific antigens and control tumor growth. However, it remains difficult to predict the antigens that will be presented by human leukocyte antigen class II molecules (HLA-II), hindering efforts to optimally target them therapeutically. Obstacles include inaccurate peptide-binding prediction and unsolved complexities of the HLA-II pathway. To address these challenges, we developed an improved technology for discovering HLA-II binding motifs and conducted a comprehensive analysis of tumor ligandomes to learn processing rules relevant in the tumor microenvironment. We profiled >40 HLA-II alleles and showed that binding motifs were highly sensitive to HLA-DM, a peptide-loading chaperone. We also revealed that intratumoral HLA-II presentation was dominated by professional antigen-presenting cells (APCs) rather than cancer cells. Integrating these observations, we developed algorithms that accurately predicted APC ligandomes, including peptides from phagocytosed cancer cells. These tools and biological insights will enable improved HLA-II-directed cancer therapies."
1,A Deep Learning Model to Predict a Diagnosis of Alzheimer Disease by Using F-18-FDG PET of the Brain,,
1,"Urinary Metabolomic Markers of Protein Glycation, Oxidation, and Nitration in Early-Stage Decline in Metabolic, Vascular, and Renal Health","Glycation, oxidation, nitration, and crosslinking of proteins are implicated in the pathogenic mechanisms of type 2 diabetes, cardiovascular disease, and chronic kidney disease. Related modified amino acids formed by proteolysis are excreted in urine. We quantified urinary levels of these metabolites and branched-chain amino acids (BCAAs) in healthy subjects and assessed changes in early-stage decline in metabolic, vascular, and renal health and explored their diagnostic utility for a noninvasive health screen. We recruited 200 human subjects with early-stage health decline and healthy controls. Urinary amino acid metabolites were determined by stable isotopic dilution analysis liquid chromatography-tandem mass spectrometry. Machine learning was applied to optimise and validate algorithms to discriminate between study groups for potential diagnostic utility. Urinary analyte changes were as follows: impaired metabolic health - increased NÏµ-carboxymethyl-lysine, glucosepane, glutamic semialdehyde, and pyrraline; impaired vascular health - increased glucosepane; and impaired renal health - increased BCAAs and decreased NÏµ-(Î³-glutamyl)lysine. Algorithms combining subject age, BMI, and BCAAs discriminated between healthy controls and impaired metabolic, vascular, and renal health study groups with accuracy of 84%, 72%, and 90%, respectively. In 2-step analysis, algorithms combining subject age, BMI, and urinary NÏµ-fructosyl-lysine and valine discriminated between healthy controls and impaired health (any type), accuracy of 78%, and then between types of health impairment with accuracy of 69%-78% (cf. random selection 33%). From likelihood ratios, this provided small, moderate, and conclusive evidence of early-stage cardiovascular, metabolic, and renal disease with diagnostic odds ratios of 6 - 7, 26 - 28, and 34 - 79, respectively. We conclude that measurement of urinary glycated, oxidized, crosslinked, and branched-chain amino acids provides the basis for a noninvasive health screen for early-stage health decline in metabolic, vascular, and renal health.","Urinary Metabolomic Markers of Protein Glycation, Oxidation, and Nitration in Early-Stage Decline in Metabolic, Vascular, and Renal Health. Glycation, oxidation, nitration, and crosslinking of proteins are implicated in the pathogenic mechanisms of type 2 diabetes, cardiovascular disease, and chronic kidney disease. Related modified amino acids formed by proteolysis are excreted in urine. We quantified urinary levels of these metabolites and branched-chain amino acids (BCAAs) in healthy subjects and assessed changes in early-stage decline in metabolic, vascular, and renal health and explored their diagnostic utility for a noninvasive health screen. We recruited 200 human subjects with early-stage health decline and healthy controls. Urinary amino acid metabolites were determined by stable isotopic dilution analysis liquid chromatography-tandem mass spectrometry. Machine learning was applied to optimise and validate algorithms to discriminate between study groups for potential diagnostic utility. Urinary analyte changes were as follows: impaired metabolic health - increased NÏµ-carboxymethyl-lysine, glucosepane, glutamic semialdehyde, and pyrraline; impaired vascular health - increased glucosepane; and impaired renal health - increased BCAAs and decreased NÏµ-(Î³-glutamyl)lysine. Algorithms combining subject age, BMI, and BCAAs discriminated between healthy controls and impaired metabolic, vascular, and renal health study groups with accuracy of 84%, 72%, and 90%, respectively. In 2-step analysis, algorithms combining subject age, BMI, and urinary NÏµ-fructosyl-lysine and valine discriminated between healthy controls and impaired health (any type), accuracy of 78%, and then between types of health impairment with accuracy of 69%-78% (cf. random selection 33%). From likelihood ratios, this provided small, moderate, and conclusive evidence of early-stage cardiovascular, metabolic, and renal disease with diagnostic odds ratios of 6 - 7, 26 - 28, and 34 - 79, respectively. We conclude that measurement of urinary glycated, oxidized, crosslinked, and branched-chain amino acids provides the basis for a noninvasive health screen for early-stage health decline in metabolic, vascular, and renal health."
1,ICU staffing feature phenotypes and their relationship with patients' outcomes: an unsupervised machine learning analysis,"PURPOSE: To study whether ICU staffing features are associated with improved hospital mortality, ICU length of stay (LOS) and duration of mechanical ventilation (MV) using cluster analysis directed by machine learning. METHODS: The following variables were included in the analysis: average bed to nurse, physiotherapist and physician ratios, presence of 24/7 board-certified intensivists and dedicated pharmacists in the ICU, and nurse and physiotherapist autonomy scores. Clusters were defined using the partition around medoids method. We assessed the association between clusters and hospital mortality using logistic regression and with ICU LOS and MV duration using competing risk regression. RESULTS: Analysis included data from 129,680 patients admitted to 93 ICUs (2014-2015). Three clusters were identified. The features distinguishing between the clusters were: the presence of board-certified intensivists in the ICU 24/7 (present in Cluster 3), dedicated pharmacists (present in Clusters 2 and 3) and the extent of nurse autonomy (which increased from Clusters 1 to 3). The patients in Cluster 3 exhibited the best outcomes, with lower adjusted hospital mortality [odds ratio 0.92 (95% confidence interval (CI), 0.87-0.98)], shorter ICU LOS [subhazard ratio (SHR) for patients surviving to ICU discharge 1.24 (95% CI 1.22-1.26)] and shorter durations of MV [SHR for undergoing extubation 1.61(95% CI 1.54-1.69)]. Cluster 1 had the worst outcomes. CONCLUSION: Patients treated in ICUs combining 24/7 expert intensivist coverage, a dedicated pharmacist and nurses with greater autonomy had the best outcomes. All of these features represent achievable targets that should be considered by policy makers with an interest in promoting equal and optimal ICU care.","ICU staffing feature phenotypes and their relationship with patients' outcomes: an unsupervised machine learning analysis. PURPOSE: To study whether ICU staffing features are associated with improved hospital mortality, ICU length of stay (LOS) and duration of mechanical ventilation (MV) using cluster analysis directed by machine learning. METHODS: The following variables were included in the analysis: average bed to nurse, physiotherapist and physician ratios, presence of 24/7 board-certified intensivists and dedicated pharmacists in the ICU, and nurse and physiotherapist autonomy scores. Clusters were defined using the partition around medoids method. We assessed the association between clusters and hospital mortality using logistic regression and with ICU LOS and MV duration using competing risk regression. RESULTS: Analysis included data from 129,680 patients admitted to 93 ICUs (2014-2015). Three clusters were identified. The features distinguishing between the clusters were: the presence of board-certified intensivists in the ICU 24/7 (present in Cluster 3), dedicated pharmacists (present in Clusters 2 and 3) and the extent of nurse autonomy (which increased from Clusters 1 to 3). The patients in Cluster 3 exhibited the best outcomes, with lower adjusted hospital mortality [odds ratio 0.92 (95% confidence interval (CI), 0.87-0.98)], shorter ICU LOS [subhazard ratio (SHR) for patients surviving to ICU discharge 1.24 (95% CI 1.22-1.26)] and shorter durations of MV [SHR for undergoing extubation 1.61(95% CI 1.54-1.69)]. Cluster 1 had the worst outcomes. CONCLUSION: Patients treated in ICUs combining 24/7 expert intensivist coverage, a dedicated pharmacist and nurses with greater autonomy had the best outcomes. All of these features represent achievable targets that should be considered by policy makers with an interest in promoting equal and optimal ICU care."
1,Assessment of Deep Learning Using Nonimaging Information and Sequential Medical Records to Develop a Prediction Model for Nonmelanoma Skin Cancer,"Importance: A prediction model for new-onset nonmelanoma skin cancer could enhance prevention measures, but few patient data-driven tools exist for more accurate prediction. Objective: To use machine learning to develop a prediction model for incident nonmelanoma skin cancer based on large-scale, multidimensional, nonimaging medical information. Design, Setting, and Participants: This study used a database comprising 2 million randomly sampled patients from the Taiwan National Health Insurance Research Database from January 1, 1999, to December 31, 2013. A total of 1829 patients with nonmelanoma skin cancer as their first diagnosed cancer and 7665 random controls without cancer were included in the analysis. A convolutional neural network, a deep learning approach, was used to develop a risk prediction model. This risk prediction model used 3-year clinical diagnostic information, medical records, and temporal-sequential information to predict the skin cancer risk of a given patient within the next year. Stepwise feature selection was also performed to investigate important and determining factors of the model. Statistical analysis was performed from November 1, 2016, to October 31, 2018. Main Outcomes and Measures: Sensitivity, specificity, and area under the receiver operating characteristic (AUROC) curve were used to evaluate the performance of the models. Results: A total of 1829 patients (923 women [50.5%] and 906 men [49.5%]; mean [SD] age, 65.3 [15.7] years) with nonmelanoma skin cancer and 7665 random controls without cancer (3951 women [51.5%] and 3714 men [48.4%]; mean [SD] age, 47.5 [17.3] years) were included in the analysis. The 1-year incident nonmelanoma skin cancer risk prediction model using sequential diagnostic information and drug prescription information as a time-incorporated feature matrix could attain an AUROC of 0.89 (95% CI, 0.87-0.91), with a mean (SD) sensitivity of 83.1% (3.5%) and mean (SD) specificity of 82.3% (4.1%). Carcinoma in situ of skin (AUROC, 0.867; -2.80% loss) and other chronic comorbidities (eg, degenerative osteopathy [AUROC, 0.872; -2.32% loss], hypertension [AUROC, 0.879; -1.53% loss], and chronic kidney insufficiency [AUROC, 0.879; -1.52% loss]) served as more discriminative factors for the prediction. Medications such as trazodone, acarbose, systemic antifungal agents, statins, nonsteroidal anti-inflammatory drugs, and thiazide diuretics were the top-ranking discriminative features in the model; each led to more than a 1% decrease of the AUROC when eliminated individually (eg, trazodone AUROC, 0.868; -2.67% reduction; acarbose AUROC, 0.870; -2.50 reduction; and systemic antifungal agents AUROC, 0.875; -1.99 reduction). Conclusions and Relevance: The findings of this study suggest that a risk prediction model may have potential predictive factors for nonmelanoma skin cancer. This model may help health care professionals target high-risk populations for more intensive skin cancer preventive methods.","Assessment of Deep Learning Using Nonimaging Information and Sequential Medical Records to Develop a Prediction Model for Nonmelanoma Skin Cancer. Importance: A prediction model for new-onset nonmelanoma skin cancer could enhance prevention measures, but few patient data-driven tools exist for more accurate prediction. Objective: To use machine learning to develop a prediction model for incident nonmelanoma skin cancer based on large-scale, multidimensional, nonimaging medical information. Design, Setting, and Participants: This study used a database comprising 2 million randomly sampled patients from the Taiwan National Health Insurance Research Database from January 1, 1999, to December 31, 2013. A total of 1829 patients with nonmelanoma skin cancer as their first diagnosed cancer and 7665 random controls without cancer were included in the analysis. A convolutional neural network, a deep learning approach, was used to develop a risk prediction model. This risk prediction model used 3-year clinical diagnostic information, medical records, and temporal-sequential information to predict the skin cancer risk of a given patient within the next year. Stepwise feature selection was also performed to investigate important and determining factors of the model. Statistical analysis was performed from November 1, 2016, to October 31, 2018. Main Outcomes and Measures: Sensitivity, specificity, and area under the receiver operating characteristic (AUROC) curve were used to evaluate the performance of the models. Results: A total of 1829 patients (923 women [50.5%] and 906 men [49.5%]; mean [SD] age, 65.3 [15.7] years) with nonmelanoma skin cancer and 7665 random controls without cancer (3951 women [51.5%] and 3714 men [48.4%]; mean [SD] age, 47.5 [17.3] years) were included in the analysis. The 1-year incident nonmelanoma skin cancer risk prediction model using sequential diagnostic information and drug prescription information as a time-incorporated feature matrix could attain an AUROC of 0.89 (95% CI, 0.87-0.91), with a mean (SD) sensitivity of 83.1% (3.5%) and mean (SD) specificity of 82.3% (4.1%). Carcinoma in situ of skin (AUROC, 0.867; -2.80% loss) and other chronic comorbidities (eg, degenerative osteopathy [AUROC, 0.872; -2.32% loss], hypertension [AUROC, 0.879; -1.53% loss], and chronic kidney insufficiency [AUROC, 0.879; -1.52% loss]) served as more discriminative factors for the prediction. Medications such as trazodone, acarbose, systemic antifungal agents, statins, nonsteroidal anti-inflammatory drugs, and thiazide diuretics were the top-ranking discriminative features in the model; each led to more than a 1% decrease of the AUROC when eliminated individually (eg, trazodone AUROC, 0.868; -2.67% reduction; acarbose AUROC, 0.870; -2.50 reduction; and systemic antifungal agents AUROC, 0.875; -1.99 reduction). Conclusions and Relevance: The findings of this study suggest that a risk prediction model may have potential predictive factors for nonmelanoma skin cancer. This model may help health care professionals target high-risk populations for more intensive skin cancer preventive methods."
1,Accurate and robust deep learning-based segmentation of the prostate clinical target volume in ultrasound images,"The goal of this work was to develop a method for accurate and robust automatic segmentation of the prostate clinical target volume in transrectal ultrasound (TRUS) images for brachytherapy. These images can be difficult to segment because of weak or insufficient landmarks or strong artifacts. We devise a method, based on convolutional neural networks (CNNs), that produces accurate segmentations on easy and difficult images alike. We propose two strategies to achieve improved segmentation accuracy on difficult images. First, for CNN training we adopt an adaptive sampling strategy, whereby the training process is encouraged to pay more attention to images that are difficult to segment. Secondly, we train a CNN ensemble and use the disagreement among this ensemble to identify uncertain segmentations and to estimate a segmentation uncertainty map. We improve uncertain segmentations by utilizing the prior shape information in the form of a statistical shape model. Our method achieves Hausdorff distance of 2.7+/-2.3 mm and Dice score of 93.9+/-3.5%. Comparisons with several competing methods show that our method achieves significantly better results and reduces the likelihood of committing large segmentation errors. Furthermore, our experiments show that our approach to estimating segmentation uncertainty is better than or on par with recent methods for estimation of prediction uncertainty in deep learning models. Our study demonstrates that estimation of model uncertainty and use of prior shape information can significantly improve the performance of CNN-based medical image segmentation methods, especially on difficult images.","Accurate and robust deep learning-based segmentation of the prostate clinical target volume in ultrasound images. The goal of this work was to develop a method for accurate and robust automatic segmentation of the prostate clinical target volume in transrectal ultrasound (TRUS) images for brachytherapy. These images can be difficult to segment because of weak or insufficient landmarks or strong artifacts. We devise a method, based on convolutional neural networks (CNNs), that produces accurate segmentations on easy and difficult images alike. We propose two strategies to achieve improved segmentation accuracy on difficult images. First, for CNN training we adopt an adaptive sampling strategy, whereby the training process is encouraged to pay more attention to images that are difficult to segment. Secondly, we train a CNN ensemble and use the disagreement among this ensemble to identify uncertain segmentations and to estimate a segmentation uncertainty map. We improve uncertain segmentations by utilizing the prior shape information in the form of a statistical shape model. Our method achieves Hausdorff distance of 2.7+/-2.3 mm and Dice score of 93.9+/-3.5%. Comparisons with several competing methods show that our method achieves significantly better results and reduces the likelihood of committing large segmentation errors. Furthermore, our experiments show that our approach to estimating segmentation uncertainty is better than or on par with recent methods for estimation of prediction uncertainty in deep learning models. Our study demonstrates that estimation of model uncertainty and use of prior shape information can significantly improve the performance of CNN-based medical image segmentation methods, especially on difficult images."
1,Radiomics of Brain MRI: Utility in Prediction of Metastatic Tumor Type,,
1,The implementation of natural language processing to extract index lesions from breast magnetic resonance imaging reports,"BACKGROUND: There are often multiple lesions in breast magnetic resonance imaging (MRI) reports and radiologists usually focus on describing the index lesion that is most crucial to clinicians in determining the management and prognosis of patients. Natural language processing (NLP) has been used for information extraction from mammography reports. However, few studies have investigated NLP in breast MRI data based on free-form text. The objective of the current study was to assess the validity of our NLP program to accurately extract index lesions and their corresponding imaging features from free-form text of breast MRI reports. METHODS: This cross-sectional study examined 1633 free-form text reports of breast MRIs from 2014 to 2017. First, the NLP system was used to extract 9 features from all the lesions in the reports according to the Breast Imaging Reporting and Data System (BI-RADS) descriptors. Second, the index lesion was defined as the lesion with the largest number of imaging features. Third, we extracted the values of each imaging feature and the BI-RADS category from each index lesion. To evaluate the accuracy of our system, 478 reports were manually reviewed by two individuals. The time taken to extract data by NLP was compared with that by reviewers. RESULTS: The NLP system extracted 889 lesions from 478 reports. The mean number of imaging features per lesion was 6.5Â Â±Â 2.1 (range: 3-9; 95% CI: 6.362-6.638). The mean number of imaging features per index lesion was 8.0Â Â±Â 1.1 (range: 5-9; 95% CI: 7.901-8.099). The NLP system demonstrated a recall of 100.0% and a precision of 99.6% for correct identification of the index lesion. The recall and precision of NLP to correctly extract the value of imaging features from the index lesions were 91.0 and 92.6%, respectively. The recall and precision for the correct identification of the BI-RADS categories were 96.6 and 94.8%, respectively. NLP generated the total results in less than 1â€‰s, whereas the manual reviewers averaged 4.47â€‰min and 4.56â€‰min per report. CONCLUSIONS: Our NLP method successfully extracted the index lesion and its corresponding information from free-form text.","The implementation of natural language processing to extract index lesions from breast magnetic resonance imaging reports. BACKGROUND: There are often multiple lesions in breast magnetic resonance imaging (MRI) reports and radiologists usually focus on describing the index lesion that is most crucial to clinicians in determining the management and prognosis of patients. Natural language processing (NLP) has been used for information extraction from mammography reports. However, few studies have investigated NLP in breast MRI data based on free-form text. The objective of the current study was to assess the validity of our NLP program to accurately extract index lesions and their corresponding imaging features from free-form text of breast MRI reports. METHODS: This cross-sectional study examined 1633 free-form text reports of breast MRIs from 2014 to 2017. First, the NLP system was used to extract 9 features from all the lesions in the reports according to the Breast Imaging Reporting and Data System (BI-RADS) descriptors. Second, the index lesion was defined as the lesion with the largest number of imaging features. Third, we extracted the values of each imaging feature and the BI-RADS category from each index lesion. To evaluate the accuracy of our system, 478 reports were manually reviewed by two individuals. The time taken to extract data by NLP was compared with that by reviewers. RESULTS: The NLP system extracted 889 lesions from 478 reports. The mean number of imaging features per lesion was 6.5Â Â±Â 2.1 (range: 3-9; 95% CI: 6.362-6.638). The mean number of imaging features per index lesion was 8.0Â Â±Â 1.1 (range: 5-9; 95% CI: 7.901-8.099). The NLP system demonstrated a recall of 100.0% and a precision of 99.6% for correct identification of the index lesion. The recall and precision of NLP to correctly extract the value of imaging features from the index lesions were 91.0 and 92.6%, respectively. The recall and precision for the correct identification of the BI-RADS categories were 96.6 and 94.8%, respectively. NLP generated the total results in less than 1â€‰s, whereas the manual reviewers averaged 4.47â€‰min and 4.56â€‰min per report. CONCLUSIONS: Our NLP method successfully extracted the index lesion and its corresponding information from free-form text."
1,askMUSIC: Leveraging a Clinical Registry to Develop a New Machine Learning Model to Inform Patients of Prostate Cancer Treatments Chosen by Similar Men,"BACKGROUND: Clinical registries provide physicians with a means for making data-driven decisions but few opportunities exist for patients to interact with registry data to help make decisions. OBJECTIVE: We sought to develop a web-based system that uses a prostate cancer (CaP) registry to provide newly diagnosed men with a platform to view predicted treatment decisions based on patients with similar characteristics. DESIGN, SETTING, AND PARTICIPANTS: The Michigan Urological Surgery Improvement Collaborative (MUSIC) is a quality improvement consortium of urology practices that maintains a prospective registry of men with CaP. We used registry data from 45 MUSIC urology practices from 2015 to 2017 to develop and validate a random forest machine learning model. After fitting the random forest model to a derivation cohort consisting of a random two-thirds sample of patients after stratifying by practice location, we evaluated the model performance in a validation cohort consisting of the remaining one-third of patients using a multiclass area under the curve (AUC) measure and calibration plots. RESULTS AND LIMITATIONS: We identified 7543 men diagnosed with CaP, of whom 45% underwent radical prostatectomy, 30% surveillance, 17% radiation therapy, 5.6% androgen deprivation, and 1.8% watchful waiting. The personalized prediction for patients in the validation cohort was highly accurate (AUC 0.81). CONCLUSIONS: Using clinical registry data and machine learning methods, we created a web-based platform for patients that generates accurate predictions for most CaP treatments. PATIENT SUMMARY: We have developed and tested a tool to help men newly diagnosed with prostate cancer to view predicted treatment decisions based on similar patients from our registry. We have made this tool available online for patients to use.","askMUSIC: Leveraging a Clinical Registry to Develop a New Machine Learning Model to Inform Patients of Prostate Cancer Treatments Chosen by Similar Men. BACKGROUND: Clinical registries provide physicians with a means for making data-driven decisions but few opportunities exist for patients to interact with registry data to help make decisions. OBJECTIVE: We sought to develop a web-based system that uses a prostate cancer (CaP) registry to provide newly diagnosed men with a platform to view predicted treatment decisions based on patients with similar characteristics. DESIGN, SETTING, AND PARTICIPANTS: The Michigan Urological Surgery Improvement Collaborative (MUSIC) is a quality improvement consortium of urology practices that maintains a prospective registry of men with CaP. We used registry data from 45 MUSIC urology practices from 2015 to 2017 to develop and validate a random forest machine learning model. After fitting the random forest model to a derivation cohort consisting of a random two-thirds sample of patients after stratifying by practice location, we evaluated the model performance in a validation cohort consisting of the remaining one-third of patients using a multiclass area under the curve (AUC) measure and calibration plots. RESULTS AND LIMITATIONS: We identified 7543 men diagnosed with CaP, of whom 45% underwent radical prostatectomy, 30% surveillance, 17% radiation therapy, 5.6% androgen deprivation, and 1.8% watchful waiting. The personalized prediction for patients in the validation cohort was highly accurate (AUC 0.81). CONCLUSIONS: Using clinical registry data and machine learning methods, we created a web-based platform for patients that generates accurate predictions for most CaP treatments. PATIENT SUMMARY: We have developed and tested a tool to help men newly diagnosed with prostate cancer to view predicted treatment decisions based on similar patients from our registry. We have made this tool available online for patients to use."
1,Automatic segmentation of prostate MRI using convolutional neural networks: Investigating the impact of network architecture on the accuracy of volume measurement and MRI-ultrasound registration,"Convolutional neural networks (CNNs) have recently led to significant advances in automatic segmentations of anatomical structures in medical images, and a wide variety of network architectures are now available to the research community. For applications such as segmentation of the prostate in magnetic resonance images (MRI), the results of the PROMISE12 online algorithm evaluation platform have demonstrated differences between the best-performing segmentation algorithms in terms of numerical accuracy using standard metrics such as the Dice score and boundary distance. These small differences in the segmented regions/boundaries outputted by different algorithms may potentially have an unsubstantial impact on the results of downstream image analysis tasks, such as estimating organ volume and multimodal image registration, which inform clinical decisions. This impact has not been previously investigated. In this work, we quantified the accuracy of six different CNNs in segmenting the prostate in 3D patient T2-weighted MRI scans and compared the accuracy of organ volume estimation and MRI-ultrasound (US) registration errors using the prostate segmentations produced by different networks. Networks were trained and tested using a set of 232 patient MRIs with labels provided by experienced clinicians. A statistically significant difference was found among the Dice scores and boundary distances produced by these networks in a non-parametric analysis of variance (p < 0.001 and p < 0.001, respectively), where the following multiple comparison tests revealed that the statistically significant difference in segmentation errors were caused by at least one tested network. Gland volume errors (GVEs) and target registration errors (TREs) were then estimated using the CNN-generated segmentations. Interestingly, there was no statistical difference found in either GVEs or TREs among different networks, (p=0.34 and p=0.26, respectively). This result provides a real-world example that these networks with different segmentation performances may potentially provide indistinguishably adequate registration accuracies to assist prostate cancer imaging applications. We conclude by recommending that the differences in the accuracy of downstream image analysis tasks that make use of data output by automatic segmentation methods, such as CNNs, within a clinical pipeline should be taken into account when selecting between different network architectures, in addition to reporting the segmentation accuracy.","Automatic segmentation of prostate MRI using convolutional neural networks: Investigating the impact of network architecture on the accuracy of volume measurement and MRI-ultrasound registration. Convolutional neural networks (CNNs) have recently led to significant advances in automatic segmentations of anatomical structures in medical images, and a wide variety of network architectures are now available to the research community. For applications such as segmentation of the prostate in magnetic resonance images (MRI), the results of the PROMISE12 online algorithm evaluation platform have demonstrated differences between the best-performing segmentation algorithms in terms of numerical accuracy using standard metrics such as the Dice score and boundary distance. These small differences in the segmented regions/boundaries outputted by different algorithms may potentially have an unsubstantial impact on the results of downstream image analysis tasks, such as estimating organ volume and multimodal image registration, which inform clinical decisions. This impact has not been previously investigated. In this work, we quantified the accuracy of six different CNNs in segmenting the prostate in 3D patient T2-weighted MRI scans and compared the accuracy of organ volume estimation and MRI-ultrasound (US) registration errors using the prostate segmentations produced by different networks. Networks were trained and tested using a set of 232 patient MRIs with labels provided by experienced clinicians. A statistically significant difference was found among the Dice scores and boundary distances produced by these networks in a non-parametric analysis of variance (p < 0.001 and p < 0.001, respectively), where the following multiple comparison tests revealed that the statistically significant difference in segmentation errors were caused by at least one tested network. Gland volume errors (GVEs) and target registration errors (TREs) were then estimated using the CNN-generated segmentations. Interestingly, there was no statistical difference found in either GVEs or TREs among different networks, (p=0.34 and p=0.26, respectively). This result provides a real-world example that these networks with different segmentation performances may potentially provide indistinguishably adequate registration accuracies to assist prostate cancer imaging applications. We conclude by recommending that the differences in the accuracy of downstream image analysis tasks that make use of data output by automatic segmentation methods, such as CNNs, within a clinical pipeline should be taken into account when selecting between different network architectures, in addition to reporting the segmentation accuracy."
1,Automated Liver Fat Quantification at Nonenhanced Abdominal CT for Population-based Steatosis Assessment,"Background Nonalcoholic fatty liver disease and its consequences are a growing public health concern requiring cross-sectional imaging for noninvasive diagnosis and quantification of liver fat. Purpose To investigate a deep learning-based automated liver fat quantification tool at nonenhanced CT for establishing the prevalence of steatosis in a large screening cohort. Materials and Methods In this retrospective study, a fully automated liver segmentation algorithm was applied to noncontrast abdominal CT examinations from consecutive asymptomatic adults by using three-dimensional convolutional neural networks, including a subcohort with follow-up scans. Automated volume-based liver attenuation was analyzed, including conversion to CT fat fraction, and compared with manual measurement in a large subset of scans. Results A total of 11 669 CT scans in 9552 adults (mean age +/- standard deviation, 57.2 years +/- 7.9; 5314 women and 4238 men; median body mass index [BMI], 27.8 kg/m(2)) were evaluated, including 2117 follow-up scans in 1862 adults (mean age, 59.2 years; 971 women and 891 men; mean interval, 5.5 years). Algorithm failure occurred in seven scans. Mean CT liver attenuation was 55 HU +/- 10, corresponding to CT fat fraction of 6.4% (slightly fattier in men than in women [7.4% +/- 6.0 vs 5.8% +/- 5.7%; P < .001]). Mean liver Hounsfield unit varied little by age (<4 HU difference among all age groups) and only weak correlation was seen with BMI (r(2) = 0.14). By category, 47.9% (5584 of 11 669) had negligible or no liver fat (CT fat fraction <5%), 42.4% (4948 of 11 669) had mild steatosis (CT fat fraction of 5%-14%), 8.8% (1025 of 11 669) had moderate steatosis (CT fat fraction of 14%-28%), and 1% (112 of 11 669) had severe steatosis (CT fat fraction >28%). Excellent agreement was seen between automated and manual measurements, with a mean difference of 2.7 HU (median, 3 HU) and r(2) of 0.92. Among the subcohort with longitudinal follow-up, mean change was only -3 HU +/- 9, but 43.3% (806 of 1861) of patients changed steatosis category between first and last scans. Conclusion This fully automated CT-based liver fat quantification tool allows for population-based assessment of hepatic steatosis and nonalcoholic fatty liver disease, with objective data that match well with manual measurement. The prevalence of at least mild steatosis was greater than 50% in this asymptomatic screening cohort. (c) RSNA, 2019.","Automated Liver Fat Quantification at Nonenhanced Abdominal CT for Population-based Steatosis Assessment. Background Nonalcoholic fatty liver disease and its consequences are a growing public health concern requiring cross-sectional imaging for noninvasive diagnosis and quantification of liver fat. Purpose To investigate a deep learning-based automated liver fat quantification tool at nonenhanced CT for establishing the prevalence of steatosis in a large screening cohort. Materials and Methods In this retrospective study, a fully automated liver segmentation algorithm was applied to noncontrast abdominal CT examinations from consecutive asymptomatic adults by using three-dimensional convolutional neural networks, including a subcohort with follow-up scans. Automated volume-based liver attenuation was analyzed, including conversion to CT fat fraction, and compared with manual measurement in a large subset of scans. Results A total of 11 669 CT scans in 9552 adults (mean age +/- standard deviation, 57.2 years +/- 7.9; 5314 women and 4238 men; median body mass index [BMI], 27.8 kg/m(2)) were evaluated, including 2117 follow-up scans in 1862 adults (mean age, 59.2 years; 971 women and 891 men; mean interval, 5.5 years). Algorithm failure occurred in seven scans. Mean CT liver attenuation was 55 HU +/- 10, corresponding to CT fat fraction of 6.4% (slightly fattier in men than in women [7.4% +/- 6.0 vs 5.8% +/- 5.7%; P < .001]). Mean liver Hounsfield unit varied little by age (<4 HU difference among all age groups) and only weak correlation was seen with BMI (r(2) = 0.14). By category, 47.9% (5584 of 11 669) had negligible or no liver fat (CT fat fraction <5%), 42.4% (4948 of 11 669) had mild steatosis (CT fat fraction of 5%-14%), 8.8% (1025 of 11 669) had moderate steatosis (CT fat fraction of 14%-28%), and 1% (112 of 11 669) had severe steatosis (CT fat fraction >28%). Excellent agreement was seen between automated and manual measurements, with a mean difference of 2.7 HU (median, 3 HU) and r(2) of 0.92. Among the subcohort with longitudinal follow-up, mean change was only -3 HU +/- 9, but 43.3% (806 of 1861) of patients changed steatosis category between first and last scans. Conclusion This fully automated CT-based liver fat quantification tool allows for population-based assessment of hepatic steatosis and nonalcoholic fatty liver disease, with objective data that match well with manual measurement. The prevalence of at least mild steatosis was greater than 50% in this asymptomatic screening cohort. (c) RSNA, 2019."
1,Artificial Intelligence Algorithms to Assess Hormonal Status from Tissue Microarrays in Patients with Breast Cancer,"Importance: Immunohistochemistry (IHC) is the most widely used assay for identification of molecular biomarkers. However, IHC is time consuming and costly, depends on tissue-handling protocols, and relies on pathologists' subjective interpretation. Image analysis by machine learning is gaining ground for various applications in pathology but has not been proposed to replace chemical-based assays for molecular detection. Objective: To assess the prediction feasibility of molecular expression of biomarkers in cancer tissues, relying only on tissue architecture as seen in digitized hematoxylin-eosin (H&E)-stained specimens. Design, Setting, and Participants: This single-institution retrospective diagnostic study assessed the breast cancer tissue microarrays library of patients from Vancouver General Hospital, British Columbia, Canada. The study and analysis were conducted from July 1, 2015, through July 1, 2018. A machine learning method, termed morphological-based molecular profiling (MBMP), was developed. Logistic regression was used to explore correlations between histomorphology and biomarker expression, and a deep convolutional neural network was used to predict the biomarker expression in examined tissues. Main Outcomes and Measures: Positive predictive value (PPV), negative predictive value (NPV), and area under the receiver operating characteristics curve measures of MBMP for assessment of molecular biomarkers. Results: The database consisted of 20600 digitized, publicly available H&E-stained sections of 5356 patients with breast cancer from 2 cohorts. The median age at diagnosis was 61 years for cohort 1 (412 patients) and 62 years for cohort 2 (4944 patients), and the median follow-up was 12.0 years and 12.4 years, respectively. Tissue histomorphology was significantly correlated with the molecular expression of all 19 biomarkers assayed, including estrogen receptor (ER), progesterone receptor (PR), and ERBB2 (formerly HER2). Expression of ER was predicted for 105 of 207 validation patients in cohort 1 (50.7%) and 1059 of 2046 validation patients in cohort 2 (51.8%), with PPVs of 97% and 98%, respectively, NPVs of 68% and 76%, respectively, and accuracy of 91% and 92%, respectively, which were noninferior to traditional IHC (PPV, 91%-98%; NPV, 51%-78%; and accuracy, 81%-90%). Diagnostic accuracy improved given more data. Morphological analysis of patients with ER-negative/PR-positive status by IHC revealed resemblance to patients with ER-positive status (Bhattacharyya distance, 0.03) and not those with ER-negative/PR-negative status (Bhattacharyya distance, 0.25). This suggests a false-negative IHC finding and warrants antihormonal therapy for these patients. Conclusions and Relevance: For at least half of the patients in this study, MBMP appeared to predict biomarker expression with noninferiority to IHC. Results suggest that prediction accuracy is likely to improve as data used for training expand. Morphological-based molecular profiling could be used as a general approach for mass-scale molecular profiling based on digitized H&E-stained images, allowing quick, accurate, and inexpensive methods for simultaneous profiling of multiple biomarkers in cancer tissues..","Artificial Intelligence Algorithms to Assess Hormonal Status from Tissue Microarrays in Patients with Breast Cancer. Importance: Immunohistochemistry (IHC) is the most widely used assay for identification of molecular biomarkers. However, IHC is time consuming and costly, depends on tissue-handling protocols, and relies on pathologists' subjective interpretation. Image analysis by machine learning is gaining ground for various applications in pathology but has not been proposed to replace chemical-based assays for molecular detection. Objective: To assess the prediction feasibility of molecular expression of biomarkers in cancer tissues, relying only on tissue architecture as seen in digitized hematoxylin-eosin (H&E)-stained specimens. Design, Setting, and Participants: This single-institution retrospective diagnostic study assessed the breast cancer tissue microarrays library of patients from Vancouver General Hospital, British Columbia, Canada. The study and analysis were conducted from July 1, 2015, through July 1, 2018. A machine learning method, termed morphological-based molecular profiling (MBMP), was developed. Logistic regression was used to explore correlations between histomorphology and biomarker expression, and a deep convolutional neural network was used to predict the biomarker expression in examined tissues. Main Outcomes and Measures: Positive predictive value (PPV), negative predictive value (NPV), and area under the receiver operating characteristics curve measures of MBMP for assessment of molecular biomarkers. Results: The database consisted of 20600 digitized, publicly available H&E-stained sections of 5356 patients with breast cancer from 2 cohorts. The median age at diagnosis was 61 years for cohort 1 (412 patients) and 62 years for cohort 2 (4944 patients), and the median follow-up was 12.0 years and 12.4 years, respectively. Tissue histomorphology was significantly correlated with the molecular expression of all 19 biomarkers assayed, including estrogen receptor (ER), progesterone receptor (PR), and ERBB2 (formerly HER2). Expression of ER was predicted for 105 of 207 validation patients in cohort 1 (50.7%) and 1059 of 2046 validation patients in cohort 2 (51.8%), with PPVs of 97% and 98%, respectively, NPVs of 68% and 76%, respectively, and accuracy of 91% and 92%, respectively, which were noninferior to traditional IHC (PPV, 91%-98%; NPV, 51%-78%; and accuracy, 81%-90%). Diagnostic accuracy improved given more data. Morphological analysis of patients with ER-negative/PR-positive status by IHC revealed resemblance to patients with ER-positive status (Bhattacharyya distance, 0.03) and not those with ER-negative/PR-negative status (Bhattacharyya distance, 0.25). This suggests a false-negative IHC finding and warrants antihormonal therapy for these patients. Conclusions and Relevance: For at least half of the patients in this study, MBMP appeared to predict biomarker expression with noninferiority to IHC. Results suggest that prediction accuracy is likely to improve as data used for training expand. Morphological-based molecular profiling could be used as a general approach for mass-scale molecular profiling based on digitized H&E-stained images, allowing quick, accurate, and inexpensive methods for simultaneous profiling of multiple biomarkers in cancer tissues.."
1,Deep learning-based classification of mesothelioma improves prediction of patient outcome,"Malignant mesothelioma (MM) is an aggressive cancer primarily diagnosed on the basis of histological criteria(1). The 2015 World Health Organization classification subdivides mesothelioma tumors into three histological types: epithelioid, biphasic and sarcomatoid MM. MM is a highly complex and heterogeneous disease, rendering its diagnosis and histological typing difficult and leading to suboptimal patient care and decisions regarding treatment modalities(2). Here we have developed a new approach-based on deep convolutional neural networks-called MesoNet to accurately predict the overall survival of mesothelioma patients from whole-slide digitized images, without any pathologist-provided locally annotated regions. We validated MesoNet on both an internal validation cohort from the French MESOBANK and an independent cohort from The Cancer Genome Atlas (TCGA). We also demonstrated that the model was more accurate in predicting patient survival than using current pathology practices. Furthermore, unlike classical black-box deep learning methods, MesoNet identified regions contributing to patient outcome prediction. Strikingly, we found that these regions are mainly located in the stroma and are histological features associated with inflammation, cellular diversity and vacuolization. These findings suggest that deep learning models can identify new features predictive of patient survival and potentially lead to new biomarker discoveries.","Deep learning-based classification of mesothelioma improves prediction of patient outcome. Malignant mesothelioma (MM) is an aggressive cancer primarily diagnosed on the basis of histological criteria(1). The 2015 World Health Organization classification subdivides mesothelioma tumors into three histological types: epithelioid, biphasic and sarcomatoid MM. MM is a highly complex and heterogeneous disease, rendering its diagnosis and histological typing difficult and leading to suboptimal patient care and decisions regarding treatment modalities(2). Here we have developed a new approach-based on deep convolutional neural networks-called MesoNet to accurately predict the overall survival of mesothelioma patients from whole-slide digitized images, without any pathologist-provided locally annotated regions. We validated MesoNet on both an internal validation cohort from the French MESOBANK and an independent cohort from The Cancer Genome Atlas (TCGA). We also demonstrated that the model was more accurate in predicting patient survival than using current pathology practices. Furthermore, unlike classical black-box deep learning methods, MesoNet identified regions contributing to patient outcome prediction. Strikingly, we found that these regions are mainly located in the stroma and are histological features associated with inflammation, cellular diversity and vacuolization. These findings suggest that deep learning models can identify new features predictive of patient survival and potentially lead to new biomarker discoveries."
1,Optimal surface segmentation with convex priors in irregularly sampled space,,
1,3D regression neural network for the quantification of enlarged perivascular spaces in brain MRI,"Enlarged perivascular spaces (EPVS) in the brain are an emerging imaging marker for cerebral small vessel disease, and have been shown to be related to increased risk of various neurological diseases, including stroke and dementia. Automated quantification of EPVS would greatly help to advance research into its etiology and its potential as a risk indicator of disease. We propose a convolutional network regression method to quantify the extent of EPVS in the basal ganglia from 3D brain MRI. We first segment the basal ganglia and subsequently apply a 3D convolutional regression network designed for small object detection within this region of interest. The network takes an image as input, and outputs a quantification score of EPVS. The network has significantly more convolution operations than pooling ones and no final activation, allowing it to span the space of real numbers. We validated our approach using a dataset of 2000 brain MRI scans scored visually. Experiments with varying sizes of training and test sets showed that a good performance can be achieved with a training set of only 200 scans. With a training set of 1000 scans, the intraclass correlation coefficient (ICC) between our scoring method and the expert's visual score was 0.74. Our method outperforms by a large margin - more than 0.10 - four more conventional automated approaches based on intensities, scale-invariant feature transform, and random forest. We show that the network learns the structures of interest and investigate the influence of hyper-parameters on the performance. We also evaluate the reproducibility of our network using a set of 60 subjects scanned twice (scan-rescan reproducibility). On this set our network achieves an ICC of 0.93, while the intrarater agreement reaches 0.80. Furthermore, the automated EPVS scoring correlates similarly to age as visual scoring.","3D regression neural network for the quantification of enlarged perivascular spaces in brain MRI. Enlarged perivascular spaces (EPVS) in the brain are an emerging imaging marker for cerebral small vessel disease, and have been shown to be related to increased risk of various neurological diseases, including stroke and dementia. Automated quantification of EPVS would greatly help to advance research into its etiology and its potential as a risk indicator of disease. We propose a convolutional network regression method to quantify the extent of EPVS in the basal ganglia from 3D brain MRI. We first segment the basal ganglia and subsequently apply a 3D convolutional regression network designed for small object detection within this region of interest. The network takes an image as input, and outputs a quantification score of EPVS. The network has significantly more convolution operations than pooling ones and no final activation, allowing it to span the space of real numbers. We validated our approach using a dataset of 2000 brain MRI scans scored visually. Experiments with varying sizes of training and test sets showed that a good performance can be achieved with a training set of only 200 scans. With a training set of 1000 scans, the intraclass correlation coefficient (ICC) between our scoring method and the expert's visual score was 0.74. Our method outperforms by a large margin - more than 0.10 - four more conventional automated approaches based on intensities, scale-invariant feature transform, and random forest. We show that the network learns the structures of interest and investigate the influence of hyper-parameters on the performance. We also evaluate the reproducibility of our network using a set of 60 subjects scanned twice (scan-rescan reproducibility). On this set our network achieves an ICC of 0.93, while the intrarater agreement reaches 0.80. Furthermore, the automated EPVS scoring correlates similarly to age as visual scoring."
1,"Quantitative CMR population imaging on 20,000 subjects of the UK Biobank imaging study: LV/RV quantification pipeline and its evaluation",,
1,Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces,,
1,Pre and post-hoc diagnosis and interpretation of malignancy from breast DCE-MRI,"We propose a new method for breast cancer screening from DCE-MRI based on a post-hoc approach that is trained using weakly annotated data (i.e., labels are available only at the image level without any lesion delineation). Our proposed post-hoc method automatically diagnosis the whole volume and, for positive cases, it localizes the malignant lesions that led to such diagnosis. Conversely, traditional approaches follow a pre-hoc approach that initially localises suspicious areas that are subsequently classified to establish the breast malignancy â€“ this approach is trained using strongly annotated data (i.e., it needs a delineation and classification of all lesions in an image). We also aim to establish the advantages and disadvantages of both approaches when applied to breast screening from DCE-MRI. Relying on experiments on a breast DCE-MRI dataset that contains scans of 117 patients, our results show that the post-hoc method is more accurate for diagnosing the whole volume per patient, achieving an AUC of 0.91, while the pre-hoc method achieves an AUC of 0.81. However, the performance for localising the malignant lesions remains challenging for the post-hoc method due to the weakly labelled dataset employed during training.","Pre and post-hoc diagnosis and interpretation of malignancy from breast DCE-MRI. We propose a new method for breast cancer screening from DCE-MRI based on a post-hoc approach that is trained using weakly annotated data (i.e., labels are available only at the image level without any lesion delineation). Our proposed post-hoc method automatically diagnosis the whole volume and, for positive cases, it localizes the malignant lesions that led to such diagnosis. Conversely, traditional approaches follow a pre-hoc approach that initially localises suspicious areas that are subsequently classified to establish the breast malignancy â€“ this approach is trained using strongly annotated data (i.e., it needs a delineation and classification of all lesions in an image). We also aim to establish the advantages and disadvantages of both approaches when applied to breast screening from DCE-MRI. Relying on experiments on a breast DCE-MRI dataset that contains scans of 117 patients, our results show that the post-hoc method is more accurate for diagnosing the whole volume per patient, achieving an AUC of 0.91, while the pre-hoc method achieves an AUC of 0.81. However, the performance for localising the malignant lesions remains challenging for the post-hoc method due to the weakly labelled dataset employed during training."
1,Combined tract segmentation and orientation mapping for bundle-specific tractography,,
1,"A collaborative computer aided diagnosis (C-CAD) system with eye-tracking, sparse attentional model, and deep learning","Computer aided diagnosis (CAD) tools help radiologists to reduce diagnostic errors such as missing tumors and misdiagnosis. Vision researchers have been analyzing behaviors of radiologists during screening to understand how and why they miss tumors or misdiagnose. In this regard, eye-trackers have been instrumental in understanding visual search processes of radiologists. However, most relevant studies in this aspect are not compatible with realistic radiology reading rooms. In this study, we aim to develop a paradigm shifting CAD system, called collaborative CAD (C-CAD), that unifies CAD and eye-tracking systems in realistic radiology room settings. We first developed an eye-tracking interface providing radiologists with a real radiology reading room experience. Second, we propose a novel algorithm that unifies eye-tracking data and a CAD system. Specifically, we present a new graph based clustering and sparsification algorithm to transform eye-tracking data (gaze) into a graph model to interpret gaze patterns quantitatively and qualitatively. The proposed C-CAD collaborates with radiologists via eye-tracking technology and helps them to improve their diagnostic decisions. The C-CAD uses radiologists' search efficiency by processing their gaze patterns. Furthermore, the C-CAD incorporates a deep learning algorithm in a newly designed multi-task learning platform to segment and diagnose suspicious areas simultaneously. The proposed C-CAD system has been tested in a lung cancer screening experiment with multiple radiologists, reading low dose chest CTs. Promising results support the efficiency, accuracy and applicability of the proposed C-CAD system in a real radiology room setting. We have also shown that our framework is generalizable to more complex applications such as prostate cancer screening with multi-parametric magnetic resonance imaging (mp-MRI).","A collaborative computer aided diagnosis (C-CAD) system with eye-tracking, sparse attentional model, and deep learning. Computer aided diagnosis (CAD) tools help radiologists to reduce diagnostic errors such as missing tumors and misdiagnosis. Vision researchers have been analyzing behaviors of radiologists during screening to understand how and why they miss tumors or misdiagnose. In this regard, eye-trackers have been instrumental in understanding visual search processes of radiologists. However, most relevant studies in this aspect are not compatible with realistic radiology reading rooms. In this study, we aim to develop a paradigm shifting CAD system, called collaborative CAD (C-CAD), that unifies CAD and eye-tracking systems in realistic radiology room settings. We first developed an eye-tracking interface providing radiologists with a real radiology reading room experience. Second, we propose a novel algorithm that unifies eye-tracking data and a CAD system. Specifically, we present a new graph based clustering and sparsification algorithm to transform eye-tracking data (gaze) into a graph model to interpret gaze patterns quantitatively and qualitatively. The proposed C-CAD collaborates with radiologists via eye-tracking technology and helps them to improve their diagnostic decisions. The C-CAD uses radiologists' search efficiency by processing their gaze patterns. Furthermore, the C-CAD incorporates a deep learning algorithm in a newly designed multi-task learning platform to segment and diagnose suspicious areas simultaneously. The proposed C-CAD system has been tested in a lung cancer screening experiment with multiple radiologists, reading low dose chest CTs. Promising results support the efficiency, accuracy and applicability of the proposed C-CAD system in a real radiology room setting. We have also shown that our framework is generalizable to more complex applications such as prostate cancer screening with multi-parametric magnetic resonance imaging (mp-MRI)."
1,A hybrid neural network model for predicting kidney disease in hypertension patients based on electronic health records,"BACKGROUND: Disease prediction based on Electronic Health Records (EHR) has become one hot research topic in biomedical community. Existing work mainly focuses on the prediction of one target disease, and little work is proposed for multiple associated diseases prediction. Meanwhile, a piece of EHR usually contains two main information: the textual description and physical indicators. However, existing work largely adopts statistical models with discrete features from numerical physical indicators in EHR, and fails to make full use of textual description information. METHODS: In this paper, we study the problem of kidney disease prediction in hypertension patients by using neural network model. Specifically, we first model the prediction problem as a binary classification task. Then we propose a hybrid neural network which incorporates Bidirectional Long Short-Term Memory (BiLSTM) and Autoencoder networks to fully capture the information in EHR. RESULTS: We construct a dataset based on a large number of raw EHR data. The dataset consists of totally 35,332 records from hypertension patients. Experimental results show that the proposed neural model achieves 89.7% accuracy for the task. CONCLUSIONS: A hybrid neural network model was presented. Based on the constructed dataset, the comparison results of different models demonstrated the effectiveness of the proposed neural model. The proposed model outperformed traditional statistical models with discrete features and neural baseline systems.","A hybrid neural network model for predicting kidney disease in hypertension patients based on electronic health records. BACKGROUND: Disease prediction based on Electronic Health Records (EHR) has become one hot research topic in biomedical community. Existing work mainly focuses on the prediction of one target disease, and little work is proposed for multiple associated diseases prediction. Meanwhile, a piece of EHR usually contains two main information: the textual description and physical indicators. However, existing work largely adopts statistical models with discrete features from numerical physical indicators in EHR, and fails to make full use of textual description information. METHODS: In this paper, we study the problem of kidney disease prediction in hypertension patients by using neural network model. Specifically, we first model the prediction problem as a binary classification task. Then we propose a hybrid neural network which incorporates Bidirectional Long Short-Term Memory (BiLSTM) and Autoencoder networks to fully capture the information in EHR. RESULTS: We construct a dataset based on a large number of raw EHR data. The dataset consists of totally 35,332 records from hypertension patients. Experimental results show that the proposed neural model achieves 89.7% accuracy for the task. CONCLUSIONS: A hybrid neural network model was presented. Based on the constructed dataset, the comparison results of different models demonstrated the effectiveness of the proposed neural model. The proposed model outperformed traditional statistical models with discrete features and neural baseline systems."
1,Improving Accuracy and Efficiency with Concurrent Use of Artificial Intelligence for Digital Breast Tomosynthesis,,
1,Evidence that recurrent circuits are critical to the ventral stream's execution of core object recognition behavior,"Non-recurrent deep convolutional neural networks (CNNs) are currently the best at modeling core object recognition, a behavior that is supported by the densely recurrent primate ventral stream, culminating in the inferior temporal (IT) cortex. If recurrence is critical to this behavior, then primates should outperform feedforward-only deep CNNs for images that require additional recurrent processing beyond the feedforward IT response. Here we first used behavioral methods to discover hundreds of these 'challenge' images. Second, using large-scale electrophysiology, we observed that behaviorally sufficient object identity solutions emerged ~30 ms later in the IT cortex for challenge images compared with primate performance-matched 'control' images. Third, these behaviorally critical late-phase IT response patterns were poorly predicted by feedforward deep CNN activations. Notably, very-deep CNNs and shallower recurrent CNNs better predicted these late IT responses, suggesting that there is a functional equivalence between additional nonlinear transformations and recurrence. Beyond arguing that recurrent circuits are critical for rapid object identification, our results provide strong constraints for future recurrent model development.","Evidence that recurrent circuits are critical to the ventral stream's execution of core object recognition behavior. Non-recurrent deep convolutional neural networks (CNNs) are currently the best at modeling core object recognition, a behavior that is supported by the densely recurrent primate ventral stream, culminating in the inferior temporal (IT) cortex. If recurrence is critical to this behavior, then primates should outperform feedforward-only deep CNNs for images that require additional recurrent processing beyond the feedforward IT response. Here we first used behavioral methods to discover hundreds of these 'challenge' images. Second, using large-scale electrophysiology, we observed that behaviorally sufficient object identity solutions emerged ~30 ms later in the IT cortex for challenge images compared with primate performance-matched 'control' images. Third, these behaviorally critical late-phase IT response patterns were poorly predicted by feedforward deep CNN activations. Notably, very-deep CNNs and shallower recurrent CNNs better predicted these late IT responses, suggesting that there is a functional equivalence between additional nonlinear transformations and recurrence. Beyond arguing that recurrent circuits are critical for rapid object identification, our results provide strong constraints for future recurrent model development."
1,A Deep Learning Model to Triage Screening Mammograms: A Simulation Study,"Background Recent deep learning (DL) approaches have shown promise in improving sensitivity but have not addressed limitations in radiologist specificity or efficiency. Purpose To develop a DL model to triage a portion of mammograms as cancer free, improving performance and workflow efficiency. Materials and Methods In this retrospective study, 223 109 consecutive screening mammograms performed in 66 661 women from January 2009 to December 2016 were collected with cancer outcomes obtained through linkage to a regional tumor registry. This cohort was split by patient into 212 272, 25 999, and 26 540 mammograms from 56 831, 7021, and 7176 patients for training, validation, and testing, respectively. A DL model was developed to triage mammograms as cancer free and evaluated on the test set. A DL-triage workflow was simulated in which radiologists skipped mammograms triaged as cancer free (interpreting them as negative for cancer) and read mammograms not triaged as cancer free by using the original interpreting radiologists' assessments. Sensitivities, specificities, and percentage of mammograms read were calculated, with and without the DL-triage-simulated workflow. Statistics were computed across 5000 bootstrap samples to assess confidence intervals (CIs). Specificities were compared by using a two-tailed t test (P < .05) and sensitivities were compared by using a one-sided t test with a noninferiority margin of 5% (P < .05). Results The test set included 7176 women (mean age, 57.8 years +/- 10.9 [standard deviation]). When reading all mammograms, radiologists obtained a sensitivity and specificity of 90.6% (173 of 191; 95% CI: 86.6%, 94.7%) and 93.5% (24 625 of 26 349; 95% CI: 93.3%, 93.9%). In the DL-simulated workflow, the radiologists obtained a sensitivity and specificity of 90.1% (172 of 191; 95% CI: 86.0%, 94.3%) and 94.2% (24 814 of 26 349; 95% CI: 94.0%, 94.6%) while reading 80.7% (21 420 of 26 540) of the mammograms. The simulated workflow improved specificity (P = .002) and obtained a noninferior sensitivity with a margin of 5% (P < .001). Conclusion This deep learning model has the potential to reduce radiologist workload and significantly improve specificity without harming sensitivity. (c) RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Kontos and Conant in this issue.","A Deep Learning Model to Triage Screening Mammograms: A Simulation Study. Background Recent deep learning (DL) approaches have shown promise in improving sensitivity but have not addressed limitations in radiologist specificity or efficiency. Purpose To develop a DL model to triage a portion of mammograms as cancer free, improving performance and workflow efficiency. Materials and Methods In this retrospective study, 223 109 consecutive screening mammograms performed in 66 661 women from January 2009 to December 2016 were collected with cancer outcomes obtained through linkage to a regional tumor registry. This cohort was split by patient into 212 272, 25 999, and 26 540 mammograms from 56 831, 7021, and 7176 patients for training, validation, and testing, respectively. A DL model was developed to triage mammograms as cancer free and evaluated on the test set. A DL-triage workflow was simulated in which radiologists skipped mammograms triaged as cancer free (interpreting them as negative for cancer) and read mammograms not triaged as cancer free by using the original interpreting radiologists' assessments. Sensitivities, specificities, and percentage of mammograms read were calculated, with and without the DL-triage-simulated workflow. Statistics were computed across 5000 bootstrap samples to assess confidence intervals (CIs). Specificities were compared by using a two-tailed t test (P < .05) and sensitivities were compared by using a one-sided t test with a noninferiority margin of 5% (P < .05). Results The test set included 7176 women (mean age, 57.8 years +/- 10.9 [standard deviation]). When reading all mammograms, radiologists obtained a sensitivity and specificity of 90.6% (173 of 191; 95% CI: 86.6%, 94.7%) and 93.5% (24 625 of 26 349; 95% CI: 93.3%, 93.9%). In the DL-simulated workflow, the radiologists obtained a sensitivity and specificity of 90.1% (172 of 191; 95% CI: 86.0%, 94.3%) and 94.2% (24 814 of 26 349; 95% CI: 94.0%, 94.6%) while reading 80.7% (21 420 of 26 540) of the mammograms. The simulated workflow improved specificity (P = .002) and obtained a noninferior sensitivity with a margin of 5% (P < .001). Conclusion This deep learning model has the potential to reduce radiologist workload and significantly improve specificity without harming sensitivity. (c) RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Kontos and Conant in this issue."
1,Identifying clinically important COPD sub-types using data-driven approaches in primary care population based electronic health records,"BACKGROUND: COPD is a highly heterogeneous disease composed of different phenotypes with different aetiological and prognostic profiles and current classification systems do not fully capture this heterogeneity. In this study we sought to discover, describe and validate COPD subtypes using cluster analysis on data derived from electronic health records. METHODS: We applied two unsupervised learning algorithms (k-means and hierarchical clustering) in 30,961 current and former smokers diagnosed with COPD, using linked national structured electronic health records in England available through the CALIBER resource. We used 15 clinical features, including risk factors and comorbidities and performed dimensionality reduction using multiple correspondence analysis. We compared the association between cluster membership and COPD exacerbations and respiratory and cardiovascular death with 10,736 deaths recorded over 146,466 person-years of follow-up. We also implemented and tested a process to assign unseen patients into clusters using a decision tree classifier. RESULTS: We identified and characterized five COPD patient clusters with distinct patient characteristics with respect to demographics, comorbidities, risk of death and exacerbations. The four subgroups were associated with 1) anxiety/depression; 2) severe airflow obstruction and frailty; 3) cardiovascular disease and diabetes and 4) obesity/atopy. A fifth cluster was associated with low prevalence of most comorbid conditions. CONCLUSIONS: COPD patients can be sub-classified into groups with differing risk factors, comorbidities, and prognosis, based on data included in their primary care records. The identified clusters confirm findings of previous clustering studies and draw attention to anxiety and depression as important drivers of the disease in young, female patients.","Identifying clinically important COPD sub-types using data-driven approaches in primary care population based electronic health records. BACKGROUND: COPD is a highly heterogeneous disease composed of different phenotypes with different aetiological and prognostic profiles and current classification systems do not fully capture this heterogeneity. In this study we sought to discover, describe and validate COPD subtypes using cluster analysis on data derived from electronic health records. METHODS: We applied two unsupervised learning algorithms (k-means and hierarchical clustering) in 30,961 current and former smokers diagnosed with COPD, using linked national structured electronic health records in England available through the CALIBER resource. We used 15 clinical features, including risk factors and comorbidities and performed dimensionality reduction using multiple correspondence analysis. We compared the association between cluster membership and COPD exacerbations and respiratory and cardiovascular death with 10,736 deaths recorded over 146,466 person-years of follow-up. We also implemented and tested a process to assign unseen patients into clusters using a decision tree classifier. RESULTS: We identified and characterized five COPD patient clusters with distinct patient characteristics with respect to demographics, comorbidities, risk of death and exacerbations. The four subgroups were associated with 1) anxiety/depression; 2) severe airflow obstruction and frailty; 3) cardiovascular disease and diabetes and 4) obesity/atopy. A fifth cluster was associated with low prevalence of most comorbid conditions. CONCLUSIONS: COPD patients can be sub-classified into groups with differing risk factors, comorbidities, and prognosis, based on data included in their primary care records. The identified clusters confirm findings of previous clustering studies and draw attention to anxiety and depression as important drivers of the disease in young, female patients."
1,Learning the implicit strain reconstruction in ultrasound elastography using privileged information,"Quasi-static ultrasound elastography is an importance imaging technology to assess the conditions of various diseases through reconstructing the tissue strain from radio frequency data. State-of-the-art strain reconstruction techniques suffer from the inexperienced user unfriendliness, high model bias, and low effectiveness-to-efficiency ratio. The three challenges result from the explicitness characteristic (i.e. explicit formulation of the reconstruction model) in these techniques. For these challenges, we are the first to develop an implicit strain reconstruction framework by a deep neural network architecture. However, the classic neural network methods are unsuitable to the strain reconstruction task because they are difficult to impose any direct influence on the intermediate state of the learning process. This may lead the map learned by the neural network to be biased with the desired map. In order to correct the intermediate state of the learning process, our framework proposes the learning-using-privileged-information (LUPI) paradigm with causality in the network. It provides the causal privileged information besides the training examples to help the network learning, while makes these privileged information unavailable at the test stage. This improvement can narrow the search region of the map learned by the network, and thus prompts the network to evolve towards the actual ultrasound elastography process. Moreover, in order to ensure the causality in LUPI, our framework proposes a physically-based data generation strategy to produce the triplets of privileged information, training examples and labels. This data generation process can approximately describes the actual ultrasound elastography process by the numerical simulation based on the tissue biomechanics and ultrasound physics. It thus can build the causal relationship between the privileged information and training examples/labels. It can also address the medical data insufficiency problem. The performance of our framework has been validated on 100 simulation data, 42 phantom data and 4 real clinical data by comparing with the ground truth performed by an ultrasound simulation system and four state-of-the-art methods. The experimental results show that our framework is well agreed (average bias is 0.065 for strain reconstruction) with the ground truth, as well as superior to these state-of-the-art methods. These results can demonstrate the effectiveness of our framework in the strain reconstruction.","Learning the implicit strain reconstruction in ultrasound elastography using privileged information. Quasi-static ultrasound elastography is an importance imaging technology to assess the conditions of various diseases through reconstructing the tissue strain from radio frequency data. State-of-the-art strain reconstruction techniques suffer from the inexperienced user unfriendliness, high model bias, and low effectiveness-to-efficiency ratio. The three challenges result from the explicitness characteristic (i.e. explicit formulation of the reconstruction model) in these techniques. For these challenges, we are the first to develop an implicit strain reconstruction framework by a deep neural network architecture. However, the classic neural network methods are unsuitable to the strain reconstruction task because they are difficult to impose any direct influence on the intermediate state of the learning process. This may lead the map learned by the neural network to be biased with the desired map. In order to correct the intermediate state of the learning process, our framework proposes the learning-using-privileged-information (LUPI) paradigm with causality in the network. It provides the causal privileged information besides the training examples to help the network learning, while makes these privileged information unavailable at the test stage. This improvement can narrow the search region of the map learned by the network, and thus prompts the network to evolve towards the actual ultrasound elastography process. Moreover, in order to ensure the causality in LUPI, our framework proposes a physically-based data generation strategy to produce the triplets of privileged information, training examples and labels. This data generation process can approximately describes the actual ultrasound elastography process by the numerical simulation based on the tissue biomechanics and ultrasound physics. It thus can build the causal relationship between the privileged information and training examples/labels. It can also address the medical data insufficiency problem. The performance of our framework has been validated on 100 simulation data, 42 phantom data and 4 real clinical data by comparing with the ground truth performed by an ultrasound simulation system and four state-of-the-art methods. The experimental results show that our framework is well agreed (average bias is 0.065 for strain reconstruction) with the ground truth, as well as superior to these state-of-the-art methods. These results can demonstrate the effectiveness of our framework in the strain reconstruction."
1,Application of machine learning to determine top predictors of noncalcified coronary burden in psoriasis: An observational cohort study,"BACKGROUND: Psoriasis is associated with elevated risk of heart attack and increased accumulation of subclinical noncalcified coronary burden by coronary computed tomography angiography (CCTA). Machine learning algorithms have been shown to effectively analyze well-characterized data sets. OBJECTIVE: In this study, we used machine learning algorithms to determine the top predictors of noncalcified coronary burden by CCTA in psoriasis. METHODS: The analysis included 263 consecutive patients with 63 available variables from the Psoriasis Atherosclerosis Cardiometabolic Initiative. The random forest algorithm was used to determine the top predictors of noncalcified coronary burden by CCTA. We evaluated our results using linear regression models. RESULTS: Using the random forest algorithm, we found that the top 10 predictors of noncalcified coronary burden were body mass index, visceral adiposity, total adiposity, apolipoprotein A1, high-density lipoprotein, erythrocyte sedimentation rate, subcutaneous adiposity, small low-density lipoprotein particle, cholesterol efflux capacity and the absolute granulocyte count. Linear regression of noncalcified coronary burden yielded results consistent with our machine learning output. LIMITATION: We were unable to provide external validation and did not study cardiovascular events. CONCLUSION: Machine learning methods identified the top predictors of noncalcified coronary burden in psoriasis. These factors were related to obesity, dyslipidemia, and inflammation, showing that these are important targets when treating comorbidities in psoriasis.","Application of machine learning to determine top predictors of noncalcified coronary burden in psoriasis: An observational cohort study. BACKGROUND: Psoriasis is associated with elevated risk of heart attack and increased accumulation of subclinical noncalcified coronary burden by coronary computed tomography angiography (CCTA). Machine learning algorithms have been shown to effectively analyze well-characterized data sets. OBJECTIVE: In this study, we used machine learning algorithms to determine the top predictors of noncalcified coronary burden by CCTA in psoriasis. METHODS: The analysis included 263 consecutive patients with 63 available variables from the Psoriasis Atherosclerosis Cardiometabolic Initiative. The random forest algorithm was used to determine the top predictors of noncalcified coronary burden by CCTA. We evaluated our results using linear regression models. RESULTS: Using the random forest algorithm, we found that the top 10 predictors of noncalcified coronary burden were body mass index, visceral adiposity, total adiposity, apolipoprotein A1, high-density lipoprotein, erythrocyte sedimentation rate, subcutaneous adiposity, small low-density lipoprotein particle, cholesterol efflux capacity and the absolute granulocyte count. Linear regression of noncalcified coronary burden yielded results consistent with our machine learning output. LIMITATION: We were unable to provide external validation and did not study cardiovascular events. CONCLUSION: Machine learning methods identified the top predictors of noncalcified coronary burden in psoriasis. These factors were related to obesity, dyslipidemia, and inflammation, showing that these are important targets when treating comorbidities in psoriasis."
1,Deep Learning for Diagnosis of Chronic Myocardial Infarction on Nonenhanced Cardiac Cine MRI,"Background Renal impairment is common in patients with coronary artery disease and, if severe, late gadolinium enhancement (LGE) imaging for myocardial infarction (MI) evaluation cannot be performed. Purpose To develop a fully automatic framework for chronic MI delineation via deep learning on non-contrast material-enhanced cardiac cine MRI. Materials and Methods In this retrospective single-center study, a deep learning model was developed to extract motion features from the left ventricle and delineate MI regions on nonenhanced cardiac cine MRI collected between October 2015 and March 2017. Patients with chronic MI, as well as healthy control patients, had both nonenhanced cardiac cine (25 phases per cardiac cycle) and LGE MRI examinations. Eighty percent of MRI examinations were used for the training data set and 20% for the independent testing data set. Chronic MI regions on LGE MRI were defined as ground truth. Diagnostic performance was assessed by analysis of the area under the receiver operating characteristic curve (AUC). MI area and MI area percentage from nonenhanced cardiac cine and LGE MRI were compared by using the Pearson correlation, paired t test, and Bland-Altman analysis. Results Study participants included 212 patients with chronic MI (men, 171; age, 57.2 years +/- 12.5) and 87 healthy control patients (men, 42; age, 43.3 years +/- 15.5). Using the full cardiac cine MRI, the per-segment sensitivity and specificity for detecting chronic MI in the independent test set was 89.8% and 99.1%, respectively, with an AUC of 0.94. There were no differences between nonenhanced cardiac cine and LGE MRI analyses in number of MI segments (114 vs 127, respectively; P = .38), per-patient MI area (6.2 cm(2) +/- 2.8 vs 5.5 cm(2) +/- 2.3, respectively; P = .27; correlation coefficient, r = 0.88), and MI area percentage (21.5% +/- 17.3 vs 18.5% +/- 15.4; P = .17; correlation coefficient, r = 0.89). Conclusion The proposed deep learning framework on nonenhanced cardiac cine MRI enables the confirmation (presence), detection (position), and delineation (transmurality and size) of chronic myocardial infarction. However, future larger-scale multicenter studies are required for a full validation. Published under a CC BY 4.0 license. Online supplemental material is available for this article. See also the editorial by Leiner in this issue.","Deep Learning for Diagnosis of Chronic Myocardial Infarction on Nonenhanced Cardiac Cine MRI. Background Renal impairment is common in patients with coronary artery disease and, if severe, late gadolinium enhancement (LGE) imaging for myocardial infarction (MI) evaluation cannot be performed. Purpose To develop a fully automatic framework for chronic MI delineation via deep learning on non-contrast material-enhanced cardiac cine MRI. Materials and Methods In this retrospective single-center study, a deep learning model was developed to extract motion features from the left ventricle and delineate MI regions on nonenhanced cardiac cine MRI collected between October 2015 and March 2017. Patients with chronic MI, as well as healthy control patients, had both nonenhanced cardiac cine (25 phases per cardiac cycle) and LGE MRI examinations. Eighty percent of MRI examinations were used for the training data set and 20% for the independent testing data set. Chronic MI regions on LGE MRI were defined as ground truth. Diagnostic performance was assessed by analysis of the area under the receiver operating characteristic curve (AUC). MI area and MI area percentage from nonenhanced cardiac cine and LGE MRI were compared by using the Pearson correlation, paired t test, and Bland-Altman analysis. Results Study participants included 212 patients with chronic MI (men, 171; age, 57.2 years +/- 12.5) and 87 healthy control patients (men, 42; age, 43.3 years +/- 15.5). Using the full cardiac cine MRI, the per-segment sensitivity and specificity for detecting chronic MI in the independent test set was 89.8% and 99.1%, respectively, with an AUC of 0.94. There were no differences between nonenhanced cardiac cine and LGE MRI analyses in number of MI segments (114 vs 127, respectively; P = .38), per-patient MI area (6.2 cm(2) +/- 2.8 vs 5.5 cm(2) +/- 2.3, respectively; P = .27; correlation coefficient, r = 0.88), and MI area percentage (21.5% +/- 17.3 vs 18.5% +/- 15.4; P = .17; correlation coefficient, r = 0.89). Conclusion The proposed deep learning framework on nonenhanced cardiac cine MRI enables the confirmation (presence), detection (position), and delineation (transmurality and size) of chronic myocardial infarction. However, future larger-scale multicenter studies are required for a full validation. Published under a CC BY 4.0 license. Online supplemental material is available for this article. See also the editorial by Leiner in this issue."
1,"Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network (vol 25, pg 65, 2019)",,
1,Extracting health-related causality from twitter messages using natural language processing,"BACKGROUND: Twitter messages (tweets) contain various types of topics in our daily life, which include health-related topics. Analysis of health-related tweets would help us understand health conditions and concerns encountered in our daily lives. In this paper we evaluate an approach to extracting causalities from tweets using natural language processing (NLP) techniques. METHODS: Lexico-syntactic patterns based on dependency parser outputs are used for causality extraction. We focused on three health-related topics: ""stress"", ""insomnia"", and ""headache."" A large dataset consisting of 24 million tweets are used. RESULTS: The results show the proposed approach achieved an average precision between 74.59 to 92.27% in comparisons with human annotations. CONCLUSIONS: Manual analysis on extracted causalities in tweets reveals interesting findings about expressions on health-related topic posted by Twitter users.","Extracting health-related causality from twitter messages using natural language processing. BACKGROUND: Twitter messages (tweets) contain various types of topics in our daily life, which include health-related topics. Analysis of health-related tweets would help us understand health conditions and concerns encountered in our daily lives. In this paper we evaluate an approach to extracting causalities from tweets using natural language processing (NLP) techniques. METHODS: Lexico-syntactic patterns based on dependency parser outputs are used for causality extraction. We focused on three health-related topics: ""stress"", ""insomnia"", and ""headache."" A large dataset consisting of 24 million tweets are used. RESULTS: The results show the proposed approach achieved an average precision between 74.59 to 92.27% in comparisons with human annotations. CONCLUSIONS: Manual analysis on extracted causalities in tweets reveals interesting findings about expressions on health-related topic posted by Twitter users."
1,"Automated quantitative tumour response assessment of MRI in neuro-oncology with artificial neural networks: a multicentre, retrospective study","BACKGROUND: The Response Assessment in Neuro-Oncology (RANO) criteria and requirements for a uniform protocol have been introduced to standardise assessment of MRI scans in both clinical trials and clinical practice. However, these criteria mainly rely on manual two-dimensional measurements of contrast-enhancing (CE) target lesions and thus restrict both reliability and accurate assessment of tumour burden and treatment response. We aimed to develop a framework relying on artificial neural networks (ANNs) for fully automated quantitative analysis of MRI in neuro-oncology to overcome the inherent limitations of manual assessment of tumour burden. METHODS: In this retrospective study, we compiled a single-institution dataset of MRI data from patients with brain tumours being treated at Heidelberg University Hospital (Heidelberg, Germany; Heidelberg training dataset) to develop and train an ANN for automated identification and volumetric segmentation of CE tumours and non-enhancing T2-signal abnormalities (NEs) on MRI. Independent testing and large-scale application of the ANN for tumour segmentation was done in a single-institution longitudinal testing dataset from the Heidelberg University Hospital and in a multi-institutional longitudinal testing dataset from the prospective randomised phase 2 and 3 European Organisation for Research and Treatment of Cancer (EORTC)-26101 trial (NCT01290939), acquired at 38 institutions across Europe. In both longitudinal datasets, spatial and temporal tumour volume dynamics were automatically quantified to calculate time to progression, which was compared with time to progression determined by RANO, both in terms of reliability and as a surrogate endpoint for predicting overall survival. We integrated this approach for fully automated quantitative analysis of MRI in neuro-oncology within an application-ready software infrastructure and applied it in a simulated clinical environment of patients with brain tumours from the Heidelberg University Hospital (Heidelberg simulation dataset). FINDINGS: For training of the ANN, MRI data were collected from 455 patients with brain tumours (one MRI per patient) being treated at Heidelberg hospital between July 29, 2009, and March 17, 2017 (Heidelberg training dataset). For independent testing of the ANN, an independent longitudinal dataset of 40 patients, with data from 239 MRI scans, was collected at Heidelberg University Hospital in parallel with the training dataset (Heidelberg test dataset), and 2034 MRI scans from 532 patients at 34 institutions collected between Oct 26, 2011, and Dec 3, 2015, in the EORTC-26101 study were of sufficient quality to be included in the EORTC-26101 test dataset. The ANN yielded excellent performance for accurate detection and segmentation of CE tumours and NE volumes in both longitudinal test datasets (median DICE coefficient for CE tumours 0.89 [95% CI 0.86-0.90], and for NEs 0.93 [0.92-0.94] in the Heidelberg test dataset; CE tumours 0.91 [0.90-0.92], NEs 0.93 [0.93-0.94] in the EORTC-26101 test dataset). Time to progression from quantitative ANN-based assessment of tumour response was a significantly better surrogate endpoint than central RANO assessment for predicting overall survival in the EORTC-26101 test dataset (hazard ratios ANN 2.59 [95% CI 1.86-3.60] vs central RANO 2.07 [1.46-2.92]; p<0.0001) and also yielded a 36% margin over RANO (p<0.0001) when comparing reliability values (ie, agreement in the quantitative volumetrically defined time to progression [based on radiologist ground truth vs automated assessment with ANN] of 87% [266 of 306 with sufficient data] compared with 51% [155 of 306] with local vs independent central RANO assessment). In the Heidelberg simulation dataset, which comprised 466 patients with brain tumours, with 595 MRI scans obtained between April 27, and Sept 17, 2018, automated on-demand processing of MRI scans and quantitative tumour response assessment within the simulated clinical environment required 10 min of computation time (average per scan). INTERPRETATION: Overall, we found that ANN enabled objective and automated assessment of tumour response in neuro-oncology at high throughput and could ultimately serve as a blueprint for the application of ANN in radiology to improve clinical decision making. Future research should focus on prospective validation within clinical trials and application for automated high-throughput imaging biomarker discovery and extension to other diseases. FUNDING: Medical Faculty Heidelberg Postdoc-Program, Else Kroner-Fresenius Foundation.","Automated quantitative tumour response assessment of MRI in neuro-oncology with artificial neural networks: a multicentre, retrospective study. BACKGROUND: The Response Assessment in Neuro-Oncology (RANO) criteria and requirements for a uniform protocol have been introduced to standardise assessment of MRI scans in both clinical trials and clinical practice. However, these criteria mainly rely on manual two-dimensional measurements of contrast-enhancing (CE) target lesions and thus restrict both reliability and accurate assessment of tumour burden and treatment response. We aimed to develop a framework relying on artificial neural networks (ANNs) for fully automated quantitative analysis of MRI in neuro-oncology to overcome the inherent limitations of manual assessment of tumour burden. METHODS: In this retrospective study, we compiled a single-institution dataset of MRI data from patients with brain tumours being treated at Heidelberg University Hospital (Heidelberg, Germany; Heidelberg training dataset) to develop and train an ANN for automated identification and volumetric segmentation of CE tumours and non-enhancing T2-signal abnormalities (NEs) on MRI. Independent testing and large-scale application of the ANN for tumour segmentation was done in a single-institution longitudinal testing dataset from the Heidelberg University Hospital and in a multi-institutional longitudinal testing dataset from the prospective randomised phase 2 and 3 European Organisation for Research and Treatment of Cancer (EORTC)-26101 trial (NCT01290939), acquired at 38 institutions across Europe. In both longitudinal datasets, spatial and temporal tumour volume dynamics were automatically quantified to calculate time to progression, which was compared with time to progression determined by RANO, both in terms of reliability and as a surrogate endpoint for predicting overall survival. We integrated this approach for fully automated quantitative analysis of MRI in neuro-oncology within an application-ready software infrastructure and applied it in a simulated clinical environment of patients with brain tumours from the Heidelberg University Hospital (Heidelberg simulation dataset). FINDINGS: For training of the ANN, MRI data were collected from 455 patients with brain tumours (one MRI per patient) being treated at Heidelberg hospital between July 29, 2009, and March 17, 2017 (Heidelberg training dataset). For independent testing of the ANN, an independent longitudinal dataset of 40 patients, with data from 239 MRI scans, was collected at Heidelberg University Hospital in parallel with the training dataset (Heidelberg test dataset), and 2034 MRI scans from 532 patients at 34 institutions collected between Oct 26, 2011, and Dec 3, 2015, in the EORTC-26101 study were of sufficient quality to be included in the EORTC-26101 test dataset. The ANN yielded excellent performance for accurate detection and segmentation of CE tumours and NE volumes in both longitudinal test datasets (median DICE coefficient for CE tumours 0.89 [95% CI 0.86-0.90], and for NEs 0.93 [0.92-0.94] in the Heidelberg test dataset; CE tumours 0.91 [0.90-0.92], NEs 0.93 [0.93-0.94] in the EORTC-26101 test dataset). Time to progression from quantitative ANN-based assessment of tumour response was a significantly better surrogate endpoint than central RANO assessment for predicting overall survival in the EORTC-26101 test dataset (hazard ratios ANN 2.59 [95% CI 1.86-3.60] vs central RANO 2.07 [1.46-2.92]; p<0.0001) and also yielded a 36% margin over RANO (p<0.0001) when comparing reliability values (ie, agreement in the quantitative volumetrically defined time to progression [based on radiologist ground truth vs automated assessment with ANN] of 87% [266 of 306 with sufficient data] compared with 51% [155 of 306] with local vs independent central RANO assessment). In the Heidelberg simulation dataset, which comprised 466 patients with brain tumours, with 595 MRI scans obtained between April 27, and Sept 17, 2018, automated on-demand processing of MRI scans and quantitative tumour response assessment within the simulated clinical environment required 10 min of computation time (average per scan). INTERPRETATION: Overall, we found that ANN enabled objective and automated assessment of tumour response in neuro-oncology at high throughput and could ultimately serve as a blueprint for the application of ANN in radiology to improve clinical decision making. Future research should focus on prospective validation within clinical trials and application for automated high-throughput imaging biomarker discovery and extension to other diseases. FUNDING: Medical Faculty Heidelberg Postdoc-Program, Else Kroner-Fresenius Foundation."
1,Deep learning for pollen allergy surveillance from twitter in Australia,"BACKGROUND: The paper introduces a deep learning-based approach for real-time detection and insights generation about one of the most prevalent chronic conditions in Australia - Pollen allergy. The popular social media platform is used for data collection as cost-effective and unobtrusive alternative for public health monitoring to complement the traditional survey-based approaches. METHODS: The data was extracted from Twitter based on pre-defined keywords (i.e. 'hayfever' OR 'hay fever') throughout the period of 6 months, covering the high pollen season in Australia. The following deep learning architectures were adopted in the experiments: CNN, RNN, LSTM and GRU. Both default (GloVe) and domain-specific (HF) word embeddings were used in training the classifiers. Standard evaluation metrics (i.e. Accuracy, Precision and Recall) were calculated for the results validation. Finally, visual correlation with weather variables was performed. RESULTS: The neural networks-based approach was able to correctly identify the implicit mentions of the symptoms and treatments, even unseen previously (accuracy up to 87.9% for GRU with GloVe embeddings of 300 dimensions). CONCLUSIONS: The system addresses the shortcomings of the conventional machine learning techniques with manual feature-engineering that prove limiting when exposed to a wide range of non-standard expressions relating to medical concepts. The case-study presented demonstrates an application of 'black-box' approach to the real-world problem, along with its internal workings demonstration towards more transparent, interpretable and reproducible decision-making in health informatics domain.","Deep learning for pollen allergy surveillance from twitter in Australia. BACKGROUND: The paper introduces a deep learning-based approach for real-time detection and insights generation about one of the most prevalent chronic conditions in Australia - Pollen allergy. The popular social media platform is used for data collection as cost-effective and unobtrusive alternative for public health monitoring to complement the traditional survey-based approaches. METHODS: The data was extracted from Twitter based on pre-defined keywords (i.e. 'hayfever' OR 'hay fever') throughout the period of 6 months, covering the high pollen season in Australia. The following deep learning architectures were adopted in the experiments: CNN, RNN, LSTM and GRU. Both default (GloVe) and domain-specific (HF) word embeddings were used in training the classifiers. Standard evaluation metrics (i.e. Accuracy, Precision and Recall) were calculated for the results validation. Finally, visual correlation with weather variables was performed. RESULTS: The neural networks-based approach was able to correctly identify the implicit mentions of the symptoms and treatments, even unseen previously (accuracy up to 87.9% for GRU with GloVe embeddings of 300 dimensions). CONCLUSIONS: The system addresses the shortcomings of the conventional machine learning techniques with manual feature-engineering that prove limiting when exposed to a wide range of non-standard expressions relating to medical concepts. The case-study presented demonstrates an application of 'black-box' approach to the real-world problem, along with its internal workings demonstration towards more transparent, interpretable and reproducible decision-making in health informatics domain."
1,Detection of Hemodynamically Significant Coronary Stenosis: CT Myocardial Perfusion versus Machine Learning CT Fractional Flow Reserve,"Background Direct intraindividual comparison of dynamic CT myocardial perfusion imaging (MPI) and machine learning (ML)-based CT fractional flow reserve (FFR) has not been explored for diagnosing hemodynamically significant coronary artery disease. Purpose To investigate the diagnostic performance of dynamic CT MPI and ML-based CT FFR for functional assessment of coronary stenosis. Materials and Methods Between January 2, 2017, and October 17, 2018, consecutive participants with stable angina were prospectively enrolled. All participants underwent dynamic CT MPI coronary CT angiography and invasive conventional coronary angiography (CCA) FFR within 2 weeks. Receiver operating characteristic (ROC) curve analysis was used to assess diagnostic performance. Results Eighty-six participants (mean age, 67 years +/- 12 [standard deviation]; 67 men) with 157 target vessels were included for final analysis. The mean radiation doses for dynamic CT MPI and coronary CT angiography were 3.6 mSv +/- 1.1 and 2.7 mSv +/- 0.8, respectively. Myocardial blood flow (MBF) was lower in ischemic segments compared with nonischemic segments and reference segments (defined as the territory of vessels without stenosis) (75 mL/100 mL/min +/- 20 vs 148 mL/100 mL/min +/- 22 and 169 mL/100 mL/min +/- 34, respectively, both P < .001). Similarly, CT FFR was also lower for hemodynamically significant lesions than for hemodynamically nonsignificant lesions (0.68 +/- 0.1 vs 0.83 +/- 0.1, respectively, P < .001). MBF had the largest area under the ROC curve (AUC) (using 99 mL/100 mL/min as a cutoff) among all parameters, outperforming ML-based CT FFR (AUC = 0.97 vs 0.85, P < .001). The vessel-based specificity and diagnostic accuracy of MBF were higher than those of ML-based CT FFR (93% vs 68%, P < .001 and 94% vs 78%, respectively, P = .04) whereas the sensitivity of both methods was similar (96% vs 88%, respectively, P = .11). Conclusion Dynamic CT myocardial perfusion imaging was able to help accurately evaluate the hemodynamic significance of coronary stenosis using a reduced amount of radiation. In addition, the myocardial blood flow derived from dynamic CT myocardial perfusion imaging outperformed machine learning-based CT fractional flow reserve for identifying lesions causing ischemia. (c) RSNA, 2019 Online supplemental material is available for this article.See also the editorial by Loewe in this issue.","Detection of Hemodynamically Significant Coronary Stenosis: CT Myocardial Perfusion versus Machine Learning CT Fractional Flow Reserve. Background Direct intraindividual comparison of dynamic CT myocardial perfusion imaging (MPI) and machine learning (ML)-based CT fractional flow reserve (FFR) has not been explored for diagnosing hemodynamically significant coronary artery disease. Purpose To investigate the diagnostic performance of dynamic CT MPI and ML-based CT FFR for functional assessment of coronary stenosis. Materials and Methods Between January 2, 2017, and October 17, 2018, consecutive participants with stable angina were prospectively enrolled. All participants underwent dynamic CT MPI coronary CT angiography and invasive conventional coronary angiography (CCA) FFR within 2 weeks. Receiver operating characteristic (ROC) curve analysis was used to assess diagnostic performance. Results Eighty-six participants (mean age, 67 years +/- 12 [standard deviation]; 67 men) with 157 target vessels were included for final analysis. The mean radiation doses for dynamic CT MPI and coronary CT angiography were 3.6 mSv +/- 1.1 and 2.7 mSv +/- 0.8, respectively. Myocardial blood flow (MBF) was lower in ischemic segments compared with nonischemic segments and reference segments (defined as the territory of vessels without stenosis) (75 mL/100 mL/min +/- 20 vs 148 mL/100 mL/min +/- 22 and 169 mL/100 mL/min +/- 34, respectively, both P < .001). Similarly, CT FFR was also lower for hemodynamically significant lesions than for hemodynamically nonsignificant lesions (0.68 +/- 0.1 vs 0.83 +/- 0.1, respectively, P < .001). MBF had the largest area under the ROC curve (AUC) (using 99 mL/100 mL/min as a cutoff) among all parameters, outperforming ML-based CT FFR (AUC = 0.97 vs 0.85, P < .001). The vessel-based specificity and diagnostic accuracy of MBF were higher than those of ML-based CT FFR (93% vs 68%, P < .001 and 94% vs 78%, respectively, P = .04) whereas the sensitivity of both methods was similar (96% vs 88%, respectively, P = .11). Conclusion Dynamic CT myocardial perfusion imaging was able to help accurately evaluate the hemodynamic significance of coronary stenosis using a reduced amount of radiation. In addition, the myocardial blood flow derived from dynamic CT myocardial perfusion imaging outperformed machine learning-based CT fractional flow reserve for identifying lesions causing ischemia. (c) RSNA, 2019 Online supplemental material is available for this article.See also the editorial by Loewe in this issue."
1,Building a tobacco user registry by extracting multiple smoking behaviors from clinical notes,"BACKGROUND: Usage of structured fields in Electronic Health Records (EHRs) to ascertain smoking history is important but fails in capturing the nuances of smoking behaviors. Knowledge of smoking behaviors, such as pack year history and most recent cessation date, allows care providers to select the best care plan for patients at risk of smoking attributable diseases. METHODS: We developed and evaluated a health informatics pipeline for identifying complete smoking history from clinical notes in EHRs. We utilized 758 patient-visit notes (from visits between 03/28/2016 and 04/04/2016) from our local EHR in addition to a public dataset of 502 clinical notes from the 2006 i2b2 Challenge to assess the performance of this pipeline. We used a machine-learning classifier to extract smoking status and a comprehensive set of text processing regular expressions to extract pack years and cessation date information from these clinical notes. RESULTS: We identified smoking status with an F1 score of 0.90 on both the i2b2 and local data sets. Regular expression identification of pack year history in the local test set was 91.7% sensitive and 95.2% specific, but due to variable context the pack year extraction was incomplete in 25% of cases, extracting packs per day or years smoked only. Regular expression identification of cessation date was 63.2% sensitive and 94.6% specific. CONCLUSIONS: Our work indicates that the development of an EHR-based Smokers' Registry containing information relating to smoking behaviors, not just status, from free-text clinical notes using an informatics pipeline is feasible. This pipeline is capable of functioning in external EHRs, reducing the amount of time and money needed at the institute-level to create a Smokers' Registry for improved identification of patient risk and eligibility for preventative and early detection services.","Building a tobacco user registry by extracting multiple smoking behaviors from clinical notes. BACKGROUND: Usage of structured fields in Electronic Health Records (EHRs) to ascertain smoking history is important but fails in capturing the nuances of smoking behaviors. Knowledge of smoking behaviors, such as pack year history and most recent cessation date, allows care providers to select the best care plan for patients at risk of smoking attributable diseases. METHODS: We developed and evaluated a health informatics pipeline for identifying complete smoking history from clinical notes in EHRs. We utilized 758 patient-visit notes (from visits between 03/28/2016 and 04/04/2016) from our local EHR in addition to a public dataset of 502 clinical notes from the 2006 i2b2 Challenge to assess the performance of this pipeline. We used a machine-learning classifier to extract smoking status and a comprehensive set of text processing regular expressions to extract pack years and cessation date information from these clinical notes. RESULTS: We identified smoking status with an F1 score of 0.90 on both the i2b2 and local data sets. Regular expression identification of pack year history in the local test set was 91.7% sensitive and 95.2% specific, but due to variable context the pack year extraction was incomplete in 25% of cases, extracting packs per day or years smoked only. Regular expression identification of cessation date was 63.2% sensitive and 94.6% specific. CONCLUSIONS: Our work indicates that the development of an EHR-based Smokers' Registry containing information relating to smoking behaviors, not just status, from free-text clinical notes using an informatics pipeline is feasible. This pipeline is capable of functioning in external EHRs, reducing the amount of time and money needed at the institute-level to create a Smokers' Registry for improved identification of patient risk and eligibility for preventative and early detection services."
1,Performance of a Deep-Learning Algorithm vs Manual Grading for Detecting Diabetic Retinopathy in India,"Importance: More than 60 million people in India have diabetes and are at risk for diabetic retinopathy (DR), a vision-threatening disease. Automated interpretation of retinal fundus photographs can help support and scale a robust screening program to detect DR. Objective: To prospectively validate the performance of an automated DR system across 2 sites in India. Design, Setting, and Participants: This prospective observational study was conducted at 2 eye care centers in India (Aravind Eye Hospital and Sankara Nethralaya) and included 3049 patients with diabetes. Data collection and patient enrollment took place between April 2016 and July 2016 at Aravind and May 2016 and April 2017 at Sankara Nethralaya. The model was trained and fixed in March 2016. Interventions: Automated DR grading system compared with manual grading by 1 trained grader and 1 retina specialist from each site. Adjudication by a panel of 3 retinal specialists served as the reference standard in the cases of disagreement. Main Outcomes and Measures: Sensitivity and specificity for moderate or worse DR or referable diabetic macula edema. Results: Of 3049 patients, 1091 (35.8%) were women and the mean (SD) age for patients at Aravind and Sankara Nethralaya was 56.6 (9.0) years and 56.0 (10.0) years, respectively. For moderate or worse DR, the sensitivity and specificity for manual grading by individual nonadjudicator graders ranged from 73.4% to 89.8% and from 83.5% to 98.7%, respectively. The automated DR system's performance was equal to or exceeded manual grading, with an 88.9% sensitivity (95% CI, 85.8-91.5), 92.2% specificity (95% CI, 90.3-93.8), and an area under the curve of 0.963 on the data set from Aravind Eye Hospital and 92.1% sensitivity (95% CI, 90.1-93.8), 95.2% specificity (95% CI, 94.2-96.1), and an area under the curve of 0.980 on the data set from Sankara Nethralaya. Conclusions and Relevance: This study shows that the automated DR system generalizes to this population of Indian patients in a prospective setting and demonstrates the feasibility of using an automated DR grading system to expand screening programs.","Performance of a Deep-Learning Algorithm vs Manual Grading for Detecting Diabetic Retinopathy in India. Importance: More than 60 million people in India have diabetes and are at risk for diabetic retinopathy (DR), a vision-threatening disease. Automated interpretation of retinal fundus photographs can help support and scale a robust screening program to detect DR. Objective: To prospectively validate the performance of an automated DR system across 2 sites in India. Design, Setting, and Participants: This prospective observational study was conducted at 2 eye care centers in India (Aravind Eye Hospital and Sankara Nethralaya) and included 3049 patients with diabetes. Data collection and patient enrollment took place between April 2016 and July 2016 at Aravind and May 2016 and April 2017 at Sankara Nethralaya. The model was trained and fixed in March 2016. Interventions: Automated DR grading system compared with manual grading by 1 trained grader and 1 retina specialist from each site. Adjudication by a panel of 3 retinal specialists served as the reference standard in the cases of disagreement. Main Outcomes and Measures: Sensitivity and specificity for moderate or worse DR or referable diabetic macula edema. Results: Of 3049 patients, 1091 (35.8%) were women and the mean (SD) age for patients at Aravind and Sankara Nethralaya was 56.6 (9.0) years and 56.0 (10.0) years, respectively. For moderate or worse DR, the sensitivity and specificity for manual grading by individual nonadjudicator graders ranged from 73.4% to 89.8% and from 83.5% to 98.7%, respectively. The automated DR system's performance was equal to or exceeded manual grading, with an 88.9% sensitivity (95% CI, 85.8-91.5), 92.2% specificity (95% CI, 90.3-93.8), and an area under the curve of 0.963 on the data set from Aravind Eye Hospital and 92.1% sensitivity (95% CI, 90.1-93.8), 95.2% specificity (95% CI, 94.2-96.1), and an area under the curve of 0.980 on the data set from Sankara Nethralaya. Conclusions and Relevance: This study shows that the automated DR system generalizes to this population of Indian patients in a prospective setting and demonstrates the feasibility of using an automated DR grading system to expand screening programs."
1,Attention gated networks: Learning to leverage salient regions in medical images,"We propose a novel attention gate (AG) model for medical image analysis that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules when using convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN models such as VGG or U-Net architectures with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed AG models are evaluated on a variety of tasks, including medical image classification and segmentation. For classification, we demonstrate the use case of AGs in scan plane detection for fetal ultrasound screening. We show that the proposed attention mechanism can provide efficient object localisation while improving the overall prediction performance by reducing false positives. For segmentation, the proposed architecture is evaluated on two large 3D CT abdominal datasets with manual annotations for multiple organs. Experimental results show that AG models consistently improve the prediction performance of the base architectures across different datasets and training sizes while preserving computational efficiency. Moreover, AGs guide the model activations to be focused around salient regions, which provides better insights into how model predictions are made. The source code for the proposed AG models is publicly available.","Attention gated networks: Learning to leverage salient regions in medical images. We propose a novel attention gate (AG) model for medical image analysis that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules when using convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN models such as VGG or U-Net architectures with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed AG models are evaluated on a variety of tasks, including medical image classification and segmentation. For classification, we demonstrate the use case of AGs in scan plane detection for fetal ultrasound screening. We show that the proposed attention mechanism can provide efficient object localisation while improving the overall prediction performance by reducing false positives. For segmentation, the proposed architecture is evaluated on two large 3D CT abdominal datasets with manual annotations for multiple organs. Experimental results show that AG models consistently improve the prediction performance of the base architectures across different datasets and training sizes while preserving computational efficiency. Moreover, AGs guide the model activations to be focused around salient regions, which provides better insights into how model predictions are made. The source code for the proposed AG models is publicly available."
1,Deep-learning based multiclass retinal fluid segmentation and detection in optical coherence tomography images using a fully convolutional neural network,"As a non-invasive imaging modality, optical coherence tomography (OCT) can provide micrometer-resolution 3D images of retinal structures. These images can help reveal disease-related alterations below the surface of the retina, such as the presence of edema, or accumulation of fluid which can distort vision, and are an indication of disruptions in the vasculature of the retina. In this paper, a new framework is proposed for multiclass fluid segmentation and detection in the retinal OCT images. Based on the intensity of OCT images and retinal layer segmentations provided by a graph-cut algorithm, a fully convolutional neural network was trained to recognize and label the fluid pixels. Random forest classification was performed on the segmented fluid regions to detect and reject the falsely labeled fluid regions. The proposed framework won the first place in the MICCAI RETOUCH challenge in 2017 on both the segmentation performance (mean Dice: 0.7667) and the detection performance (mean AUC: 1.00) tasks.","Deep-learning based multiclass retinal fluid segmentation and detection in optical coherence tomography images using a fully convolutional neural network. As a non-invasive imaging modality, optical coherence tomography (OCT) can provide micrometer-resolution 3D images of retinal structures. These images can help reveal disease-related alterations below the surface of the retina, such as the presence of edema, or accumulation of fluid which can distort vision, and are an indication of disruptions in the vasculature of the retina. In this paper, a new framework is proposed for multiclass fluid segmentation and detection in the retinal OCT images. Based on the intensity of OCT images and retinal layer segmentations provided by a graph-cut algorithm, a fully convolutional neural network was trained to recognize and label the fluid pixels. Random forest classification was performed on the segmented fluid regions to detect and reject the falsely labeled fluid regions. The proposed framework won the first place in the MICCAI RETOUCH challenge in 2017 on both the segmentation performance (mean Dice: 0.7667) and the detection performance (mean AUC: 1.00) tasks."
